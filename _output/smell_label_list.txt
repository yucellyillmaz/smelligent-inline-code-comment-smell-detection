"clean_comment";"label"
"usr bin python";"CODE"
"copyright 2014 google inc all rights reserved";"IRRE"
"copyright 2015 tim rae";"-"
"licensed under the apache license version 2 0 the license";"META"
"you may not use this file except in compliance with the license";"CODE"
"you may obtain a copy of the license at";"-"
"http www apache org licenses license 2 0";"CODE"
"unless required by applicable law or agreed to in writing software";"CODE"
"distributed under the license is distributed on an as is basis";"META"
"without warranties or conditions of any kind either express or implied";"-"
"see the license for the specific language governing permissions and";"CODE"
"limitations under the license";"-"
"usr bin env python";"CODE"
"copyright 2006 2007 google inc all rights reserved";"IRRE"
"author danderson google com david anderson";"META"
"script for uploading files to a google code project";"CODE"
"this is intended to be both a useful script for people who want to";"CODE"
"streamline project uploads and a reference implementation for";"CODE"
"uploading files to google code projects";"CODE"
"to upload a file to google code you need to provide a path to the";"CODE"
"file on your local machine a small summary of what the file is a";"-"
"project name and a valid account that is a member or owner of that";"-"
"project you can optionally provide a list of labels that apply to";"-"
"the file the file will be uploaded under the same name that it has";"CODE"
"in your local filesystem that is the basename or last path";"CODE"
"component run the script with help to get the exact syntax";"CODE"
"and available options";"-"
"note that the upload script requests that you enter your";"CODE"
"googlecode com password this is not your gmail account password";"CODE"
"this is the password you use on googlecode com for committing to";"CODE"
"subversion and uploading files you can find your password by going";"META"
"to http code google com hosting settings when logged in with your";"IRRE"
"gmail account if you have already committed to your project s";"CODE"
"subversion repository the script will automatically retrieve your";"IRRE"
"credentials from there unless disabled see the output of help";"IRRE"
"for details";"CODE"
"if you are looking at this script as a reference for implementing";"CODE"
"your own google code file uploader then you should take a look at";"CODE"
"the upload function which is the meat of the uploader you";"CODE"
"basically need to build a multipart form data post request with the";"CODE"
"right fields and send it to https project googlecode com files";"CODE"
"authenticate the request using http basic authentication as is";"CODE"
"shown below";"-"
"licensed under the terms of the apache software license 2 0";"-"
"http www apache org licenses license 2 0";"CODE"
"questions comments feature requests and patches are most welcome";"TASK"
"please direct all of these to the google code users group";"IRRE"
"http groups google com group google code hosting";"CODE"
"the login is the user part of user gmail com if the login provided";"-"
"is in the full user domain form strip it down";"CODE"
"add the metadata about the upload first";"TASK"
"now add the file itself";"TASK"
"the upload server determines the mime type no need to set it";"TASK"
"finalize the form body";"CODE"
"read username if not specified or loaded from svn config or on";"CODE"
"subsequent tries";"-"
"read password if not loaded from svn config or on subsequent tries";"CODE"
"returns 403 forbidden instead of 401 unauthorized for bad";"CODE"
"credentials as of 2007 07 17";"-"
"rest for another try";"CODE"
"we re done";"CODE"
"force loading of kivy modules";"CODE"
"check for silenced build";"CODE"
"force loading of all classes from factory";"CODE"
"directory of doc";"CODE"
"check touch file";"-"
"avoid to rewrite the file if the content didn t change";"CODE"
"activate kivy modules";"CODE"
"search all kivy module";"CODE"
"extract packages from modules";"CODE"
"create index";"IRRE"
"create index for all packages";"IRRE"
"note on displaying inherited members";"TASK"
"adding the directive inherited members to automodule achieves this";"CODE"
"but is not always desired please see";"META"
"https github com kivy kivy pull 3870";"CODE"
"template examples ref ref jump directly to examples";"-"
"don t take empty line";"CODE"
"ref mark";"-"
"search packages";"-"
"search modules";"CODE"
"create index for all module";"CODE"
"search examples";"-"
"try to found any example in framework directory";"CODE"
"extract filename without directory";"-"
"add a section";"TASK"
"put the file in";"-"
"generation finished";"TASK"
"coding utf 8";"-"
"kivy documentation build configuration file created by";"IRRE"
"sphinx quickstart on wed jan 21 22 37 12 2009";"-"
"this file is execfile d with the current directory set to its containing";"IRRE"
"dir";"-"
"the contents of this file are pickled so don t put values in the namespace";"CODE"
"that aren t pickleable module imports are okay they re removed";"CODE"
"automatically";"IRRE"
"all configuration values have a default value values that are commented out";"IRRE"
"serve to show the default value";"IRRE"
"if your extensions are in another directory add it here if the directory";"TASK"
"is relative to the documentation root use os path abspath to make it";"CODE"
"absolute like shown here";"-"
"general configuration";"-"
"add any sphinx extension module names here as strings they can be";"CODE"
"extensions coming with sphinx named sphinx ext or your custom ones";"-"
"in 4 0 and above has been added the support to substitute by s in the caption";"TASK"
"in 6 0 if the caption is a string it must contain s exactly once";"CODE"
"repo extlink caption s";"-"
"repo extlink caption";"-"
"todo configuration";"TASK"
"xxx hack mathieu monkey patch the autodoc module to give a better priority";"CODE"
"for classdocumenter or the cython class will be documented as attributeclass";"CODE"
"add any paths that contain templates here relative to this directory";"CODE"
"the suffix of source filenames";"CODE"
"the master toctree document";"CODE"
"general substitutions";"-"
"the default replacements for version and release also used in various";"CODE"
"other places throughout the built documents";"CODE"
"there are two options for replacing today either you set today to some";"CODE"
"non false value then it is used";"IRRE"
"today";"-"
"else today fmt is used as the format for a strftime call";"IRRE"
"suppress exclusion warnings";"-"
"the rest default role used for this markup text to use for all documents";"CODE"
"default role none";"CODE"
"if true will be appended to func etc cross reference text";"CODE"
"add function parentheses true";"TASK"
"if true the current module name will be prepended to all description";"CODE"
"unit titles such as function";"CODE"
"add module names true";"TASK"
"if true sectionauthor and moduleauthor directives will be shown in the";"META"
"output they are ignored by default";"IRRE"
"show authors false";"META"
"the name of the pygments syntax highlighting style to use";"CODE"
"options for html output";"IRRE"
"the style sheet to use for html and html help pages a file of that name";"CODE"
"must exist either in sphinx static path or in one of the custom paths";"CODE"
"given in html static path";"CODE"
"check for theme remove if when switched to rtd";"CODE"
"the name for this set of sphinx documents if none it defaults to";"CODE"
"project v release documentation";"CODE"
"html title none";"-"
"a shorter title for the navigation bar default is the same as html title";"CODE"
"html short title none";"-"
"the name of an image file within the static path to place at the top of";"CODE"
"the sidebar";"IRRE"
"the name of an image file within the static path to use as favicon of the";"CODE"
"docs this file should be a windows icon file ico being 16x16 or 32x32";"CODE"
"pixels large";"-"
"html favicon none";"-"
"add any paths that contain custom static files such as style sheets here";"TASK"
"relative to this directory they are copied after the builtin static files";"CODE"
"so a file named default css will overwrite the builtin default css";"CODE"
"if not a last updated on timestamp is inserted at every page bottom";"CODE"
"using the given strftime format";"CODE"
"if true smartypants will be used to convert quotes and dashes to";"OUTD"
"typographically correct entities";"IRRE"
"html use smartypants true";"-"
"custom sidebar templates maps document names to template names";"IRRE"
"html sidebars";"IRRE"
"additional templates that should be rendered to pages maps page names to";"TASK"
"template names";"-"
"html additional pages";"TASK"
"if false no module index is generated";"CODE"
"html use modindex true";"-"
"if false no index is generated";"-"
"html use index true";"-"
"if true the index is split into individual pages for each letter";"CODE"
"html split index false";"-"
"if true the rest sources are included in the html build as sources name";"CODE"
"html copy source true";"-"
"if true an opensearch description file will be output and all pages will";"CODE"
"contain a link tag referring to it the value of this option must be the";"TASK"
"base url from which the finished html is served";"TASK"
"html use opensearch";"CODE"
"if nonempty this is the file name suffix for html files e g xhtml";"CODE"
"html file suffix";"-"
"output file base name for html help builder";"IRRE"
"options for latex output";"IRRE"
"the paper size letter or a4";"CODE"
"latex paper size letter";"CODE"
"the font size 10pt 11pt or 12pt";"-"
"latex font size 10pt";"-"
"grouping the document tree into latex files list of tuples";"CODE"
"source start file target name title author document class manual";"CODE"
"the name of an image file relative to this directory to place at the top of";"CODE"
"the title page";"-"
"latex logo none";"-"
"for manual documents if this is true then toplevel headings are parts";"CODE"
"not chapters";"-"
"deprecated later use parts true";"OUTD"
"see sphinx builder latex validate config values";"IRRE"
"additional stuff for the latex preamble";"TASK"
"latex preamble";"-"
"documents to append as an appendix to all manuals";"CODE"
"latex appendices";"CODE"
"if false no module index is generated";"CODE"
"latex use modindex true";"-"
"if used in a code block the block has to be marked with";"-"
"parse literal otherwise it won t be replaced";"IRRE"
"doesn t work for code or code block";"CODE"
"coding utf 8";"-"
"add inheritance info if wanted";"TASK"
"kivy pygments style based on flask tango style";"CODE"
"the background color is set in kivystyle sty";"IRRE"
"no corresponding class for the following";"CODE"
"text class";"IRRE"
"whitespace underline ffffff class w";"IRRE"
"error ff0000 border ff0000 class err";"IRRE"
"other ff0000 class x";"IRRE"
"comment italic 666385 class c";"IRRE"
"comment preproc noitalic class cp";"IRRE"
"keyword bold 000000 class k";"IRRE"
"keyword constant bold 000000 class kc";"CODE"
"keyword declaration bold 000000 class kd";"IRRE"
"keyword namespace bold 000000 class kn";"IRRE"
"keyword pseudo bold 000000 class kp";"CODE"
"keyword reserved bold 000000 class kr";"IRRE"
"keyword type bold 000000 class kt";"IRRE"
"operator 582800 class o";"IRRE"
"operator word bold 000000 class ow like keywords";"IRRE"
"punctuation bold 000000 class p";"IRRE"
"because special names such as name class name function etc";"CODE"
"are not recognized as such later in the parsing we choose them";"CODE"
"to look the same as ordinary variables";"IRRE"
"name 000000 class n";"IRRE"
"name attribute c4a000 class na to be revised";"IRRE"
"name builtin 000000 class nb";"IRRE"
"name builtin pseudo aa1105 class bp";"CODE"
"name class db6500 class nc to be revised";"IRRE"
"name constant 000000 class no to be revised";"CODE"
"name decorator 888 class nd to be revised";"CODE"
"name entity ce5c00 class ni";"IRRE"
"name exception bold cc0000 class ne";"CODE"
"name function db6500 class nf";"CODE"
"name property 000000 class py";"IRRE"
"name label f57900 class nl";"IRRE"
"name namespace 000000 class nn to be revised";"IRRE"
"name other 000000 class nx";"IRRE"
"name tag bold 004461 class nt like a keyword";"IRRE"
"name variable 000000 class nv to be revised";"IRRE"
"name variable class 000000 class vc to be revised";"IRRE"
"name variable global 000000 class vg to be revised";"IRRE"
"name variable instance 000000 class vi to be revised";"IRRE"
"number 990000 class m";"IRRE"
"literal 000000 class l";"IRRE"
"literal date 000000 class ld";"IRRE"
"string 74171b class s";"CODE"
"string backtick 4e9a06 class sb";"CODE"
"string char 4e9a06 class sc";"CODE"
"string doc italic 640000 class sd like a comment";"CODE"
"string double 74171b class s2";"CODE"
"string escape 74171b class se";"CODE"
"string heredoc 74171b class sh";"CODE"
"string interpol 74171b class si";"CODE"
"string other 74171b class sx";"CODE"
"string regex 74171b class sr";"CODE"
"string single 74171b class s1";"CODE"
"string symbol 74171b class ss";"CODE"
"generic 000000 class g";"IRRE"
"generic deleted a40000 class gd";"CODE"
"generic emph italic 000000 class ge";"IRRE"
"generic error ef2929 class gr";"IRRE"
"generic heading bold 000080 class gh";"IRRE"
"generic inserted 00a000 class gi";"CODE"
"generic output 888 class go";"IRRE"
"generic prompt 745334 class gp";"IRRE"
"generic strong bold 000000 class gs";"IRRE"
"generic subheading bold 800080 class gu";"IRRE"
"generic traceback bold a40000 class gt";"IRRE"
"xxx i don t understand the impact of having a priority more than the";"CODE"
"attribute or instance method but the thing is if it s a cython module";"META"
"the attribute will be preferred over method";"META"
"try to check if the first line of the doc is a signature";"CODE"
"an identifier starts with a letter or underscore";"CODE"
"followed by optional numbers or letters or underscores";"CODE"
"test for cython cpdef";"CODE"
"match identifier identifier anything";"-"
"test for cython class";"IRRE"
"match identifier anything";"-"
"test for python method in cython class";"CODE"
"match identifier anything where";"-"
"remove empty lines";"-"
"if we still have lines remove the title";"TASK"
"if the title is followed by a separator remove it";"-"
"trick to realign the first line to the second one";"CODE"
"fixme fail if we finishing with";"TASK"
"calculate the minimum space available";"-"
"remove that kind of space now";"CODE"
"remove the first self argument because python autodoc don t";"CODE"
"add it for python method class so do the same for cython class";"CODE"
"get normal components";"-"
"get texture coordinate components";"-"
"get vertex components";"-"
"mesh calculate normals";"-"
"read the magnetic sensor from the hardware class";"CODE"
"calculate the angle";"-"
"fix animation transition around the unit circle";"-"
"add the number of revolutions to the result";"TASK"
"animate the needle";"-"
"when you are going on pause don t forget to stop the sensor";"CODE"
"reactivate the sensor when you are back to the app";"-"
"create an animation object this object could be stored";"IRRE"
"and reused each call or reused across different widgets";"IRRE"
"is a sequential step while is in parallel";"CODE"
"apply the animation on the button passed in the instance argument";"META"
"notice that default click animation changing the button";"CODE"
"color while the mouse is down is unchanged";"CODE"
"create a button and attach animate method as a on press handler";"IRRE"
"kivy require 1 8 0 1 8 is when kv directory became part of app";"CODE"
"note that importing floatlayout causes kivy to execute including";"CODE"
"starting up the logger and some other messages";"IRRE"
"the name idiom executes when run from command line but not from import";"CODE"
"else help";"-"
"return a button as a root widget";"IRRE"
"we don t actually need to set asyncio as the lib because it is";"TASK"
"the default but it doesn t hurt to be explicit";"CODE"
"get some sleep";"-"
"when canceled print that it finished";"CODE"
"we don t actually need to set asyncio as the lib because it is the";"TASK"
"default but it doesn t hurt to be explicit";"CODE"
"await async runtouchapp root async lib asyncio run kivy";"CODE"
"now cancel all the other tasks that may be running";"TASK"
"when canceled print that it finished";"CODE"
"root builder load string kv root widget";"CODE"
"trio needs to be set so that it ll be used for the event loop";"IRRE"
"get some sleep";"-"
"when canceled print that it finished";"CODE"
"trio needs to be set so that it ll be used for the event loop";"IRRE"
"await async runtouchapp root async lib trio run kivy";"CODE"
"now cancel all the other tasks that may be running";"TASK"
"when canceled print that it finished";"CODE"
"root builder load string kv root widget";"CODE"
"stop the sound if it s currently playing";"-"
"encoding utf8";"-"
"uncomment these lines to see all the messages";"-"
"from kivy logger import logger";"CODE"
"import logging";"CODE"
"logger setlevel logging trace";"IRRE"
"elf d 10 pixel tolerance when clicking on a point";"CODE"
"elf current point none index of point being dragged";"CODE"
"effect to reduce length while increase offset";"IRRE"
"effect to reduce length while increase offset";"IRRE"
"pacman";"-"
"wait that all the instructions are in the canvas to set texture";"CODE"
"trick to attach graphics instruction to fbo instead of canvas";"CODE"
"fun result with low segments";"IRRE"
"setting shader fs to new source code automatically compiles it";"IRRE"
"here we are binding a custom texture at index 1";"-"
"this will be used as texture1 in shader";"CODE"
"the filenames are misleading they do not correspond to the";"META"
"index here or in the shader";"CODE"
"create a rectangle with texture will be at index 0";"IRRE"
"set the texture1 to use texture index 1";"IRRE"
"call the constructor of parent";"CODE"
"if they are any graphics objects they will be added on our new";"CODE"
"canvas";"-"
"we ll update our glsl variables in a clock";"CODE"
"this is needed for the default vertex shader";"CODE"
"coding utf 8";"-"
"rectangle of default size 100x100";"CODE"
"roundedrectangles of default size 100x100";"CODE"
"textured";"-"
"colored";"-"
"textured colored";"-"
"color 3 3 3 1";"-"
"possible radius arguments";"-"
"1 same value for each corner";"CODE"
"with same radius 20x20";"-"
"with same radius dimensions 20x40";"-"
"2 different values for each corner";"IRRE"
"with different radiuses nxn";"-"
"with different radiuses";"-"
"default ellipses";"CODE"
"radius dimensions can t be bigger than half of the figure side";"-"
"segments parameter defines how many segments each corner has";"IRRE"
"more segments more roundness";"-"
"various sizes";"CODE"
"you can cut corners by setting segments to 1";"IRRE"
"you can set different segment count to corners";"IRRE"
"by using a list useful for lowering vertex count";"CODE"
"by using small amount on small corners while using";"CODE"
"bigger amount on bigger corners";"-"
"if radius dimension is 0 then the corner will be sharp";"-"
"90 degrees it is also possible to mix tuple values";"IRRE"
"with numeric";"-"
"the hollow square shape";"-"
"coding utf 8";"-"
"loading the content of root kv";"CODE"
"unload the content of the kv file";"CODE"
"reason it could have data from previous calls";"IRRE"
"clear the container";"-"
"load the content of the kv file";"CODE"
"add the content of the kv file to the container";"TASK"
"node puzzlenode texture subtexture";"-"
"size bs bs pos bx by";"-"
"config set graphics width 1024";"IRRE"
"config set graphics height 768";"IRRE"
"keycodes on osx";"-"
"this allows either ctrl or cmd but not both";"META"
"reset undo redo history";"CODE"
"local libraries";"IRRE"
"if g in self recognizer db not needed for testing";"CODE"
"local libraries";"IRRE"
"refuse heap permute for gestures with more strokes than 3";"CODE"
"you can increase it but 4 strokes 384 templates 5 3840";"META"
"recognize can block the ui with max gpf 100 show a message";"-"
"get a reference to the original gesturecontainer object";"CODE"
"reanalyze the candidate strokes using current database";"IRRE"
"tag the result with the gesture object it didn t change";"IRRE"
"tag the selected item with the updated progresstracker";"CODE"
"create a gesturevisualizer that draws the gesture on canvas";"IRRE"
"tag it with the result object so addgestureform load visualizer";"CODE"
"has the results to build labels in the scrollview";"IRRE"
"add the visualizer to the list of gestures in history screen";"TASK"
"make sure the top is visible";"-"
"fixme this seems inefficient is there a better way";"CODE"
"local libraries";"IRRE"
"don t bother creating label if it s not going to be drawn";"CODE"
"don t bother creating label if it s not going to be drawn";"CODE"
"setting notransition breaks the history screen possibly related";"IRRE"
"to some inexplicable rendering bugs on my particular system";"CODE"
"setup the gesturesurface and bindings to our recognizer";"IRRE"
"history is the list of gestures drawn on the surface";"-"
"database is the list of gesture templates in recognizer";"CODE"
"settings screen";"IRRE"
"wrap in a gridlayout so the main menu is always visible";"CODE"
"the root is created in pictures kv";"IRRE"
"get any files into images directory";"CODE"
"load the image";"CODE"
"add to the main field";"TASK"
"ifdef gl es";"CODE"
"endif";"CODE"
"ifdef gl es";"CODE"
"endif";"CODE"
"import random random random";"IRRE"
"this might mean we are on a device whose pressure value is";"IRRE"
"incorrectly reported by sdl3 like recent ios devices";"-"
"if pressure changed create a new point instruction";"CODE"
"install twisted rector must be called before importing the reactor";"CODE"
"a simple client that send messages to the echo server";"CODE"
"a simple kivy app with a textbox to enter messages and";"-"
"a large label to display all the messages received from";"CODE"
"the server";"-"
"install twisted rector must be called before importing and using the reactor";"CODE"
"reactor is already running so we just start the service collection";"CODE"
"add pre recorded gestures to database";"TASK"
"start collecting points in touch ud";"CODE"
"create a line to display the points";"IRRE"
"store points of the touch movement";"CODE"
"touch is over display information and check if it matches some";"IRRE"
"known gesture";"-"
"gestures to my gestures py";"-"
"print match scores between all known gestures";"CODE"
"use database to find the more alike gesture if any";"IRRE"
"erase the lines on the screen this is a bit quick dirty since we";"IRRE"
"can have another touch event on the way";"-"
"kivy require 1 0 6 replace with your current kivy version";"META"
"author zen code";"META"
"this example uses features introduced in kivy 1 8 0 namely being able";"CODE"
"to load custom json files from the app folder";"CODE"
"just a space taker to allow for the popup keyboard";"CODE"
"p3 b color ff0000 warning color b this is a system wide";"CODE"
"add the file in our app directory the json extension is required";"TASK"
"system keyboard keycode 122 z";"CODE"
"dock keyboard keycode z";"CODE"
"m none the root screen manager";"-"
"ifdef gl es";"CODE"
"endif";"CODE"
"change the default canvas to rendercontext we can change the shader";"CODE"
"add kinect depth provider and start the thread";"TASK"
"parent init";"IRRE"
"allocate texture for pushing depth";"CODE"
"create default canvas element";"IRRE"
"add a little clock to update our glsl";"TASK"
"update projection mat and uvsize";"CODE"
"print out the given parameter";"IRRE"
"check the status of the switch by referring on the id";"CODE"
"set the text of the label by referring on the id";"IRRE"
"set the text of the label by referring on the id";"IRRE"
"import clipboard kivy core clipboard clipboard";"CODE"
"save an image into bytesio";"CODE"
"joystick gamepad example";"-"
"stop fire from https wiki libsdl org sdl joyaxisevent";"CODE"
"fire trigger axis";"-"
"min value for user to actually trigger axis";"IRRE"
"current values event instance";"IRRE"
"get joystick events first";"-"
"show values in console";"IRRE"
"hat first returns max values";"IRRE"
"unschedule if at zero or at minimum fire";"-"
"schedule if over offset to prevent accidental event with low value";"IRRE"
"replace window instance with identifier";"CODE"
"get app instance to add function from widget";"CODE"
"add function to the list";"TASK"
"a function catching a dropped file";"CODE"
"if it s dropped in the widget s area";"CODE"
"on drop file s filename is bytes py3";"-"
"set an empty list that will be later populated";"IRRE"
"with functions from widgets themselves";"CODE"
"bind handling function to on drop file";"CODE"
"this will execute each function from list with arguments from";"CODE"
"window on drop file";"CODE"
"make sure window on drop file works on your system first";"CODE"
"otherwise the example won t work at all";"IRRE"
"coding utf 8";"-"
"import window kivy core window window";"CODE"
"this is a simple demo for advanced collisions and mesh creation from a set";"CODE"
"of points its purpose is only to give an idea on how to make complex stuff";"META"
"check garden collider for better performance";"CODE"
"cloud polygon 67 vertices custom origin 150 50";"-"
"keep references for offset";"CODE"
"shape properties";"-"
"position changed by touch";"-"
"position changed by call shape pos x";"IRRE"
"move polygon points by offset";"IRRE"
"stick label to bounding box widget";"-"
"move debug collider if available";"-"
"return if no mesh available";"IRRE"
"move mesh vertices by offset";"IRRE"
"grab single touch for shape";"CODE"
"get touches";"-"
"get offsets move trigger on pos event";"IRRE"
"ignore if no polygon area is set";"IRRE"
"compare point pairs via pip algo too long read";"CODE"
"https en wikipedia org wiki point in polygon";"CODE"
"get points within a circle with radius of r x r y";"CODE"
"add uv layout zeros for mesh see mesh docs";"CODE"
"draw mesh shape from generated poly points";"CODE"
"first point has to be the center of the convex shape s mass";"CODE"
"that s where the triangle fan starts from";"CODE"
"make the polygon smaller to fit 100x100 bounding box";"-"
"create list of vertices get edges of the polygon";"IRRE"
"add uv layout zeros for mesh";"TASK"
"get center of poly from edges";"CODE"
"get distance from the widget s center and push the points to";"CODE"
"the widget s origin so that min x and min y for the poly would";"CODE"
"result in 0 i e points moved as close as possible to 0 0";"CODE"
"no editor gives poly points moved to the origin directly";"OUTD"
"move polygon points to the bounding box touch";"CODE"
"move mesh points to the bounding box image";"CODE"
"has to contain the same points as polygon";"CODE"
"draw mesh shape from generated poly points";"CODE"
"debug polygon points with line to see the origin point";"CODE"
"and intersections with the other points";"CODE"
"line points poly";"CODE"
"register an event for collision";"CODE"
"get all combinations from all available shapes";"CODE"
"dispatch a custom event if the objects collide";"IRRE"
"draw collider only if debugging";"-"
"add circle collider only if the shape doesn t have one";"TASK"
"the environment for all 2d shapes";"CODE"
"list of 2d shapes starting with regular ones";"-"
"move shapes to some random position";"IRRE"
"check for simple collisions between the shapes";"CODE"
"enable shaped window";"CODE"
"set the window background color to transparent";"IRRE"
"define the kv layout";"CODE"
"list of image paths to cycle through";"CODE"
"move to the next image in the list";"CODE"
"import json json";"CODE"
"import c kivy utils get color from hex";"CODE"
"we first define our gui";"CODE"
"this json defines entries we want to appear in our app configuration screen";"CODE"
"the line below is optional you could leave it out or use one of the";"-"
"standard options such as settingswithsidebar settingswithspinner";"IRRE"
"etc";"-"
"we apply the saved configuration settings or the defaults";"CODE"
"we use the string defined above for our json but it could also be";"CODE"
"loaded from a file as follows";"CODE"
"settings add json panel my label self config settings json";"IRRE"
"plasma shader";"-"
"property to set the source code for fragment shader";"CODE"
"instead of using canvas we will use a rendercontext";"CODE"
"and change the default shader used";"CODE"
"call the constructor of parent";"CODE"
"if they are any graphics object they will be added on our new canvas";"CODE"
"we ll update our glsl variables in a clock";"CODE"
"set the fragment shader to our source code";"IRRE"
"this is needed for the default vertex shader";"CODE"
"imported early for side effects needed for shader";"CODE"
"property to set the source code for fragment shader";"CODE"
"instead of using canvas we will use a rendercontext";"CODE"
"and change the default fragment shader used";"CODE"
"call the constructor of parent";"CODE"
"if they are any graphics object they will be added on our new canvas";"CODE"
"we ll update our glsl variables in a clock";"CODE"
"set the fragment shader to our source code";"IRRE"
"from kivy core window import window side effects needed by shader";"CODE"
"pulse danguafer silexars 2010";"-"
"post processing by iq 2009";"-"
"property to set the source code for fragment shader";"CODE"
"texture of the framebuffer";"-"
"instead of using canvas we will use a rendercontext";"CODE"
"and change the default shader used";"CODE"
"call the constructor of parent";"CODE"
"if they are any graphics object they will be added on our new canvas";"CODE"
"we ll update our glsl variables in a clock";"CODE"
"set the fragment shader to our source code";"IRRE"
"now if we have new widget to add";"TASK"
"add their graphics canvas to our framebuffer not the usual canvas";"TASK"
"prepare shader list";"-"
"kivy 1 8 0";"-"
"from kivy uix image import image";"CODE"
"root add widget image source data logo kivy icon 512 png";"TASK"
"size 800 600";"-"
"coding utf 8";"-"
"import f kivy factory factory";"CODE"
"width sv width 25 don t draw below scrollbar";"IRRE"
"halign";"-"
"not supported by textinput";"CODE"
"halignbutton";"META"
"text justify";"-"
"base direction";"-"
"font context";"-"
"font size";"-"
"boxlayout";"-"
"orientation horizontal";"-"
"size hint y none";"CODE"
"height 30dp";"-"
"button";"META"
"color 0 1 0 1";"-"
"text add font user";"TASK"
"button";"META"
"color 0 1 0 1";"-"
"text add font system user";"TASK"
"togglebutton";"META"
"text enable markup";"-"
"state down";"CODE"
"on state";"-"
"for c in boxes children c markup self state down";"CODE"
"bounce ball off paddles";"TASK"
"bounce ball off bottom or top";"-"
"went off a side to score point";"CODE"
"bounce off top and bottom";"-"
"bounce off left and right";"-"
"bounce off paddles";"TASK"
"bounce ball off bottom or top";"-"
"went off to a side to score point";"CODE"
"kivy require 1 0 6 replace with your current kivy version";"META"
"kivy 1 0";"-"
"import os os";"CODE"
"each time a picture is created the image can delay the loading";"CODE"
"as soon as the image is loaded ensure that the center is changed";"CODE"
"to the center of the screen";"-"
"create initial image to be 400 pixels width";"IRRE"
"add shadow background";"TASK"
"the client area in which all editing is done";"CODE"
"we will be accessing this later as app main root widget";"CODE"
"start searching after the last selected node";"CODE"
"example for doing a triangle";"CODE"
"this will automatically recalculate px from pos size";"CODE"
"if you use a widget instead of scatter as base class you need that";"CODE"
"p1 self pos";"CODE"
"p2 self right self y";"CODE"
"p3 self center x self top";"CODE"
"draw something";"-"
"import vector kivy vector vector";"CODE"
"the effect string is glsl code defining an effect function";"CODE"
"needed to create fbo must be resolved in future kivy version";"TASK"
"wait that all the instructions are in the canvas to set texture";"CODE"
"trick to attach graphics instruction to fbo instead of canvas";"CODE"
"test with fbofloatlayout or floatlayout";"CODE"
"comment uncomment to test it";"IRRE"
"root floatlayout";"CODE"
"this part of creation can be slow try to optimize the loop a";"CODE"
"little bit";"-"
"make elements 29 9 un focusable the widgets are displayed in";"CODE"
"reverse order so 9 39 10";"-"
"similarly make 39 14 25 and 5 un focusable";"-"
"don t move focus passed this element";"CODE"
"exchange the links between the sides so that it ll skip to the other";"-"
"side in the middle remember that children are displayed reversed";"CODE"
"in layouts";"-"
"if it exists this widget is a vkeyboard object which you can use";"CODE"
"to change the keyboard layout";"-"
"keycode is composed of an integer a string";"CODE"
"if we hit escape release the keyboard";"IRRE"
"return true to accept the key otherwise it will be used by";"IRRE"
"the system";"CODE"
"copied from https en wikipedia org wiki a tale of two cities";"CODE"
"published in 1859 and public domain";"CODE"
"the newline after the title will help demonstrate halign";"CODE"
"note many of the widgets stacklayout togglebutton spinner have";"TASK"
"defaults set at the bottom of the kv where demolabel and headinglabel";"IRRE"
"are also defined";"CODE"
"all labels use these properties set to label defaults";"IRRE"
"initialize words generator";"IRRE"
"dynamic kv classes";"IRRE"
"import rgba kivy utils rgba";"CODE"
"magic value for the default height of the message";"CODE"
"bg color 223344";"-"
"create a message for the recycleview";"IRRE"
"when the label is updated we want to make sure the displayed size is";"CODE"
"proper";"-"
"one line dp 50 a bit of hack ymmv";"-"
"if the texture is too big limit its size";"-"
"if it was limited but is now too small to be limited raise the limit";"IRRE"
"just set the size";"IRRE"
"elf add message text right 223344";"TASK"
"elf add message do you really think so left 332211";"TASK"
"button widget for inner vertical recycleviews";"META"
"panel container holds a vertical recycleview";"CODE"
"header label";"CODE"
"inner vertical recycleview with 50 buttons";"META"
"main layout";"CODE"
"instructions";"CODE"
"outer horizontal recycleview";"-"
"do scroll x true important to set do scroll attributes";"CODE"
"populate outer recycleview with 20 panels";"-"
"using a thread to do a potentially long operation without blocking";"CODE"
"the ui";"-"
"sync one animated property to the value in the proxy";"IRRE"
"when we are assigned an animation proxy sync our properties to";"IRRE"
"the animated version";"META"
"if we lose our animation proxy we need to reset the animated";"TASK"
"property to their default values";"IRRE"
"the triggered decorator allows delaying the animation until after the";"CODE"
"blue effect on the button is removed to avoid a flash as widgets gets";"OUTD"
"reordered when that happens";"-"
"the animation we actually want to do on the item note that any";"TASK"
"property animated here needs to be synchronized from the proxy to the";"TASK"
"animated widget in on animation proxy and using methods for each";"CODE"
"animation";"-"
"animation is complete widget should be garbage collected";"CODE"
"my item number one";"-"
"my item number two with some more content";"-"
"my third item";"-"
"my four item";"-"
"import random random random";"IRRE"
"import slidetransition kivy uix screenmanager slidetransition";"CODE"
"import swaptransition kivy uix screenmanager swaptransition";"CODE"
"import wipetransition kivy uix screenmanager wipetransition";"CODE"
"import fadetransition kivy uix screenmanager fadetransition";"CODE"
"import riseintransition kivy uix screenmanager riseintransition";"CODE"
"import fallouttransition kivy uix screenmanager fallouttransition";"CODE"
"import notransition kivy uix screenmanager notransition";"CODE"
"create a default grid layout with custom width height";"IRRE"
"when we add children to the grid layout its size doesn t change at";"TASK"
"all we need to ensure that the height will be the minimum required";"TASK"
"to contain all the childs otherwise we ll child outside the";"CODE"
"bounding box of the childs";"-"
"add button into that grid";"TASK"
"create a scroll view with a size size of the grid";"IRRE"
"kv string defining the layout structure";"CODE"
"inner horizontal scrollview";"-"
"inner vertical scrollview";"-"
"main layout";"CODE"
"left side vertical outer with horizontal inner scrollviews";"-"
"right side horizontal outer with vertical inner scrollviews";"-"
"add row label";"TASK"
"add 15 buttons";"TASK"
"add column label";"TASK"
"add 20 buttons";"TASK"
"kv string defining the layout structure";"CODE"
"header row";"CODE"
"inner vertical scroll centered with spacers";"-"
"label left spacer";"-"
"label right spacer";"-"
"main layout";"CODE"
"info label at the top";"-"
"add 30 labels to make it scrollable";"TASK"
"color 0 2 0 6 1 1 blue color to distinguish inner items";"-"
"size label to fit text";"-"
"bind scrollview width to content width so it centers properly";"-"
"initialize variables";"IRRE"
"setup layouts";"IRRE"
"setup buttons in left frame";"IRRE"
"handle button press release";"META"
"position scatter";"-"
"bind function on on release";"CODE"
"add widgets to left frame";"TASK"
"set remove border for borderless widgets 16 16 16 16 by default";"CODE"
"add widgets to the main layout";"TASK"
"add main layout to root";"TASK"
"borderimage border by default is";"CODE"
"image to display depending on state";"TASK"
"reset animation if anim delay is changed";"IRRE"
"update self texture when image texture changes";"CODE"
"update image source when background image is changed";"CODE"
"replace the default tab with our custom tab class";"CODE"
"variable tab width";"CODE"
"replace the default tab with our custom tab";"CODE"
"allow variable tab width";"CODE"
"override tab switching method to animate on tab switch";"CODE"
"use dock virtual keyboard one instance";"CODE"
"use multi users virtual keyboard multiples instance";"-"
"use system keyboard one instance";"CODE"
"use automatic detection from current platform";"CODE"
"create a button to release everything";"IRRE"
"show current configuration";"-"
"coding utf 8";"-"
"import utils kivy";"CODE"
"import os os";"CODE"
"import factory kivy factory factory";"CODE"
"check what formats are supported for your targeted devices";"CODE"
"for example try h264 video and acc audo for android using an mp4";"CODE"
"container";"-"
"internals for post configuration";"CODE"
"user version";"META"
"current version";"META"
"not tag rev alpha 1 beta x allowed";"-"
"finally checking revision";"CODE"
"global settings options for kivy";"IRRE"
"read environment";"CODE"
"extract all needed path in kivy";"-"
"kivy directory";"-"
"kivy modules directory";"CODE"
"kivy data directory";"-"
"kivy binary deps directory";"-"
"kivy glsl shader directory";"-"
"kivy icons config path don t remove the last";"CODE"
"kivy user home storage directory";"-"
"kivy configuration filename";"-"
"kivy user modules directory";"CODE"
"kivy examples directory";"-"
"if there are deps import them so they can do their magic";"CODE"
"don t go further if we generate documentation";"CODE"
"configuration management";"-"
"detection if venv being used with the framework";"-"
"configuration";"-"
"set level of logger";"IRRE"
"can be overridden in command line";"CODE"
"save sys argv otherwise gstreamer use it and display help";"CODE"
"set argv to the non read args";"IRRE"
"needs to be first opt for support freeze to work";"TASK"
"when we are doing an executable on macosx with";"CODE"
"pyinstaller they are passing information with p so";"CODE"
"it will conflict with our current p option since the";"-"
"format is not the same just avoid it";"CODE"
"configure all activated modules";"CODE"
"android hooks force fullscreen and add android touch input provider";"CODE"
"this file is imported from init py and exec d from setup py";"CODE"
"if it s a rcx release it s not proceeded by a period if it is a";"-"
"devx release it must start with a period";"-"
"initialize";"IRRE"
"and later";"-"
"and later";"-"
"no more properties to animation kill the animation";"-"
"no more properties to animation kill the animation";"-"
"private";"CODE"
"get current values";"IRRE"
"install clock";"-"
"empty proxy widget is gone ref 2458";"-"
"calculate progression";"-"
"apply progression on widget";"-"
"time to stop";"-"
"user requested to animate only part of the dict";"CODE"
"copy the rest";"-"
"default handlers";"CODE"
"this property is impossible to implement";"CODE"
"repeat the sequence see repeating animation in the header";"CODE"
"documentation";"CODE"
"and more";"-"
"will search for the path to myatlas atlas and get the image id";"CODE"
"late import to prevent recursion";"CODE"
"late import to prevent recursive import";"CODE"
"must be a name finished by atlas";"TASK"
"load the image";"CODE"
"for all the uid load the image get the region and put";"CODE"
"it in our dict";"-"
"thanks to";"-"
"omnisaurusgames com 2011 06 texture atlas generation using python";"CODE"
"for its initial implementation";"TASK"
"open all of the images";"CODE"
"sort by image area";"-"
"free boxes are empty space in our output image set";"IRRE"
"the freebox tuple format is outidx x y w h";"CODE"
"full boxes are areas where we have placed images in the atlas";"CODE"
"the full box tuple format is image outidx x y w h filename";"CODE"
"do the actual atlasing by sticking the largest images we can";"CODE"
"have into the smallest valid free boxes";"CODE"
"find the smallest free box that will contain this image";"CODE"
"we found a valid spot remove the current";"-"
"freebox and split the leftover space into up to";"CODE"
"two new freeboxes";"CODE"
"keep this sorted";"CODE"
"oh crap there isn t room in any of our free";"-"
"boxes so we have to add a new output image";"TASK"
"now that we ve figured out where everything goes make the output";"IRRE"
"images and blit the source images to the appropriate locations";"-"
"save the output images";"IRRE"
"write out an json file that says where everything ended up";"CODE"
"fb 6 contain the filename";"CODE"
"use the path with separators replaced by";"IRRE"
"example data tiles green grass png becomes";"-"
"data tiles green grass";"-"
"remove leading dots and slashes";"CODE"
"replace remaining slashes with";"CODE"
"for example data tiles green grass png";"CODE"
"just get only green grass as the uniq id";"-"
"earlier import of kivy has already called getopt to remove kivy system";"CODE"
"arguments from this line that is all arguments up to the first";"CODE"
"pylint disable w0611";"CODE"
"private vars";"CODE"
"instance of a class exceptionmanagerbase implementation";"CODE"
"import kivy core window noqa";"CODE"
"xxx stop in reverse order that we started them like push";"-"
"pop very important because e g wm touch and wm pen both";"CODE"
"store old window proc and the restore if order is messed big";"CODE"
"problem happens crashing badly without error";"-"
"ensure any restart will not break anything later";"CODE"
"update available list";"CODE"
"dispatch to listeners";"-"
"dispatch grabbed touch";"-"
"non touch event must be handled by the event manager";"TASK"
"weak widget is a weak reference to widget";"CODE"
"object is gone stop";"IRRE"
"don t dispatch again touch in on touch down";"CODE"
"a down event are nearly uniq here";"CODE"
"wid dispatch on touch down touch";"CODE"
"remove the save event for the touch if exist";"CODE"
"first acquire input events";"CODE"
"execute post processing modules";"CODE"
"real dispatch input";"CODE"
"use exception manager first";"CODE"
"use exception manager first";"CODE"
"update dt";"CODE"
"read and dispatch input from providers";"CODE"
"flush all the canvas operation";"-"
"tick before draw";"CODE"
"flush all the canvas operation";"-"
"don t loop if we don t have listeners";"CODE"
"update dt";"CODE"
"read and dispatch input from providers";"CODE"
"flush all the canvas operation";"-"
"tick before draw";"CODE"
"flush all the canvas operation";"-"
"don t loop if we don t have listeners";"CODE"
"eventloop instance";"IRRE"
"ok we got one widget and we are not in embedded mode";"CODE"
"so user don t create the window let s create it for him";"CODE"
"instance all configured input";"CODE"
"split value";"IRRE"
"create provider";"IRRE"
"add postproc modules";"TASK"
"add main widget";"TASK"
"start event loop";"IRRE"
"remove presplash on the next frame";"-"
"in non embedded mode there are 2 issues";"-"
"1 if user created a window call the mainloop from window";"CODE"
"this is due to glut it need to be called with";"TASK"
"glutmainloop only freeglut got a glumainloopevent";"IRRE"
"so we are executing the dispatching function inside";"CODE"
"a redisplay event";"-"
"2 if no window is created we are dispatching event loop";"IRRE"
"ourself previous behavior";"CODE"
"we are in embedded mode don t do dispatching";"CODE"
"we are in embedded mode don t do dispatching";"CODE"
"register a new cache";"CODE"
"create an object id";"IRRE"
"retrieve the cached object";"CODE"
"check whether obj should not be cached first";"-"
"this check is added because of the case when key is none and";"CODE"
"one of purge methods gets called then loop in purge method will";"IRRE"
"call cache remove with key none which then clears entire";"IRRE"
"category from cache making next iteration of loop to raise a";"CODE"
"keyerror because next key will not exist";"TASK"
"see https github com kivy kivy pull 6950";"CODE"
"xxx got a lag that may be because the frame take lot of";"IRRE"
"time to draw and the timeout is not adapted to the current";"-"
"framerate so increase the timeout by two";"IRRE"
"ie if the timeout is 1 sec and framerate go to 0 7 newly";"CODE"
"object added will be automatically trashed";"IRRE"
"take the object timeout if available";"IRRE"
"no timeout cancel";"-"
"install the schedule clock for purging";"CODE"
"dt means delta time";"-"
"call my callback every 0 5 seconds";"IRRE"
"call my callback in 5 seconds";"IRRE"
"call my callback as soon as possible usually next frame";"IRRE"
"http docs python org library functools html functools partial python";"CODE"
"http docs python org 2 reference expressions html lambda expression";"CODE"
"a foo object is created and the method start is called";"IRRE"
"because no reference is kept to the instance returned from foo";"CODE"
"the object will be collected by the python garbage collector and";"IRRE"
"your callback will be never called";"IRRE"
"so you should do the following and keep a reference to the instance";"CODE"
"of foo until you don t need it anymore";"IRRE"
"clock schedule once my callback 0 call after the next frame";"IRRE"
"clock schedule once my callback 1 call before the next frame";"IRRE"
"will run the callback twice before the next frame";"CODE"
"will run the callback once before the next frame";"CODE"
"will also run the callback only once before the next frame";"CODE"
"event clock schedule once my callback now it s already scheduled";"IRRE"
"event won t be scheduled again";"-"
"call my callback every 0 5 seconds";"IRRE"
"call my callback in 5 seconds";"IRRE"
"unschedule using cancel";"-"
"unschedule using clock unschedule";"-"
"unschedule using clock unschedule with the callback";"IRRE"
"not recommended";"CODE"
"some reading http gameprogrammingpatterns com game loop html";"CODE"
"win32 sleep function is only 10 millisecond resolution so";"CODE"
"instead use a waitable timer object which has up to";"CODE"
"100 nanosecond resolution hardware and implementation";"TASK"
"dependent of course";"CODE"
"def get sleep obj noqa f811";"CODE"
"clockid 4 clock monotonic raw linux specific";"-"
"clockid constants from sys time h";"CODE"
"clockid 4 clock monotonic freebsd specific";"-"
"11 clock monotonic precise freebsd known ok for 10 2";"CODE"
"clockid 12";"-"
"12 clock monotonic fast freebsd specific";"-"
"clockid 3 clock monotonic";"-"
"clockid 1 clock monotonic";"-"
"default time libc clock gettime wrapper noqa f811";"CODE"
"importerror ctypes is not available on python for android";"CODE"
"attributeerror ctypes is now available on python for android but";"META"
"undefined symbol clock gettime cf 3797";"CODE"
"oserror if the libc cannot be read like with buildbot invalid elf";"OUTD"
"header";"CODE"
"tick the current time";"-"
"compute how long the event processing takes";"-"
"calculate fps things";"-"
"process event";"-"
"we don t know if this is called after things have already been";"CODE"
"scheduled so don t delay for a full frame before processing";"CODE"
"events";"-"
"1 fps remaining time";"CODE"
"self time self last tick elapsed time";"CODE"
"4 5 self get resolution resolution fudge factor";"CODE"
"anything scheduled from now on if scheduled for the upcoming frame";"CODE"
"will cause a timeout of the event on the next idle due to on schedule";"-"
"self last tick current must happen before clear otherwise the";"CODE"
"on schedule computation is wrong when exec between the clear and";"IRRE"
"the self last tick current bytecode";"CODE"
"anything scheduled from now on if scheduled for the upcoming frame";"CODE"
"will cause a timeout of the event on the next idle due to on schedule";"-"
"self last tick current must happen before clear otherwise the";"CODE"
"on schedule computation is wrong when exec between the clear and";"IRRE"
"the self last tick current bytecode";"CODE"
"if not event free only wake up for free events";"TASK"
"free events should use real time not frame time";"-"
"event clear this needs to stay after last tick";"TASK"
"event clear this needs to stay after last tick";"TASK"
"instance of class clockbasebehavior";"IRRE"
"example of input provider instance";"CODE"
"example for tuio provider";"CODE"
"version number of current configuration format";"META"
"if we try to open directly the configuration file in utf 8";"CODE"
"we correctly get the unicode value by default";"IRRE"
"but when we try to save it again all the values we didn t changed";"IRRE"
"are still unicode and then the pythonconfigparser internal do";"CODE"
"a str conversion fail";"META"
"instead we currently to the conversion to utf 8 when value are";"IRRE"
"get but we internally store them in ascii";"META"
"with codecs open filename r encoding utf 8 as f";"META"
"self readfp f";"CODE"
"when reading new file sections keys are only increased not removed";"CODE"
"if section not in old vals new section";"CODE"
"for k v in self items section just update new changed keys";"CODE"
"might be boolean int etc";"CODE"
"if config and widget associate this config with property";"CODE"
"keys are configparser names values are 2 tuple of ref configparser";"IRRE"
"widget ref where widget ref is same as in register named property";"-"
"if old name disconnect this parser from previously connected props";"CODE"
"if given new name connect it with property that used this name";"CODE"
"read analyse configuration file";"CODE"
"support upgrade of older config file versions";"TASK"
"create default configuration";"IRRE"
"read config file if exist";"CODE"
"add defaults section";"TASK"
"upgrade default configuration until we have the current version";"TASK"
"log level";"-"
"default graphics parameters";"IRRE"
"input configuration";"CODE"
"activate native input provider in configuration";"CODE"
"from 1 0 9 don t activate mactouch by default or app are";"CODE"
"unusable";"-"
"input postprocessing configuration";"CODE"
"default configuration for keyboard repetition";"CODE"
"was a version to automatically copy windows icon in the user";"CODE"
"directory but it s now not used anymore user can still change";"OUTD"
"the window icon by touching the config";"CODE"
"add token for scrollview";"TASK"
"remove old list token";"-"
"add keyboard token";"TASK"
"if the timeout is still the default value change it";"TASK"
"desktop bool indicating whether to use desktop specific features";"TASK"
"warning when adding a new version migration here";"TASK"
"don t forget to increment kivy config version";"CODE"
"for future";"TASK"
"pass to the next version";"META"
"indicate to the config that we ve upgrade to the latest version";"TASK"
"now activate log file";"-"
"if no configuration exist write the default one";"CODE"
"load configuration from env";"CODE"
"extract and check section";"-"
"extract and check the option name";"-"
"we don t avoid to set an unknown option because maybe";"CODE"
"an external modules or widgets in garden may want to";"CODE"
"save its own configuration here";"CODE"
"after popping context from stack update proxy s obj with";"CODE"
"instances in current context";"-"
"module activated in config";"CODE"
"import module";"CODE"
"module activated in config";"CODE"
"import module";"CODE"
"get the full expected path to the compiled pyd file";"CODE"
"filename is debug cp major minor platform pyd";"CODE"
"https github com python cpython blob master doc whatsnew 3 5 rst";"CODE"
"if hasattr sys gettotalrefcount debug";"-"
"does the compiled pyd exist at all";"CODE"
"tell user to provide dependency walker";"CODE"
"make file for the resultant log";"IRRE"
"little trick here don t activate gstreamer on window";"CODE"
"seem to have lot of crackle or something";"-"
"from kivy lib gstplayer import gstplayer noqa";"CODE"
"taken from https goo gl 015kvu";"CODE"
"ff opts vn true sn true only audio";"-"
"wait until loaded or failed shouldn t take long but just to make";"CODE"
"sure metadata is available";"-"
"we need to set the volume everytime it seems that stopping playing";"TASK"
"the sound reset the volume";"IRRE"
"load the appropriate providers";"CODE"
"self android camera setdisplayorientation";"IRRE"
"assert pf imageformat nv21 default format is nv21";"CODE"
"extension gl oes egl image external require";"CODE"
"ifdef gl es";"CODE"
"endif";"CODE"
"clear texture and it ll be reset in update pointing to new fbo";"CODE"
"add buffer back for reuse";"TASK"
"check if frame grabbing works";"IRRE"
"print self buffer len self frame data";"CODE"
"for k in range 2 double buffer";"CODE"
"buffer queue cleared as well to be recreated on next start";"IRRE"
"arr cvtcolor arr 93 nv21 bgr";"-"
"initialize the camera gi if the older version is used don t use camera gi";"IRRE"
"we don t care about the rest";"CODE"
"todo this doesn t work when camera resolution is resized at runtime";"CODE"
"there must be some other way to release the camera";"TASK"
"try to get the camera image size";"CODE"
"decode sample";"-"
"read the data from the buffer memory";"CODE"
"we cannot get the data out of mapinfo using gst 1 0 6 gi 3 8 0";"CODE"
"related bug report";"-"
"https bugzilla gnome org show bug cgi id 6t8663";"CODE"
"ie mapinfo data is normally a char but here we have an int";"CODE"
"so right now we use ctypes instead to read the mapinfo ourself";"CODE"
"now get the memory";"-"
"if we leave the python process with some video running we can hit a";"CODE"
"segfault this is forcing the stop unload of all remaining videos before";"CODE"
"exiting the python process";"CODE"
"todo make usage of thread or multiprocess";"CODE"
"opencv 1 case";"CODE"
"opencv 2 case and also opencv 3 because it still uses cv2 module name";"CODE"
"here missing this osx specific highgui thing";"CODE"
"i m not on osx so don t know if it is still valid in opencv 2";"CODE"
"we will need it because constants have";"CODE"
"different access paths between ver 2 and 3";"IRRE"
"consts have changed locations between versions 2 and 3";"META"
"create the device";"IRRE"
"set preferred resolution";"IRRE"
"and get frame to check if it s ok";"IRRE"
"just set the resolution to the frame we just got but don t use";"IRRE"
"self resolution for that as that would cause an infinite";"CODE"
"recursion with self init camera but slowly as we d have to";"IRRE"
"always get a frame";"-"
"get fps";"-"
"create the device";"IRRE"
"set preferred resolution";"IRRE"
"and get frame to check if it s ok";"IRRE"
"source";"-"
"http stackoverflow com questions 32468371 video capture propid parameters in opencv noqa";"CODE"
"get fps";"-"
"create the texture";"IRRE"
"frame is already of type ndarray";"CODE"
"which can be reshaped to 1 d";"-"
"todo make usage of thread or multiprocess";"CODE"
"see https picamera readthedocs io en release 1 13 recipes2 html capturing to a numpy array";"CODE"
"noqa";"-"
"create the texture";"IRRE"
"trim the buffer to fit the actual requested resolution";"CODE"
"todo is there a simpler way to do all this reshuffling";"CODE"
"import clipboard kivy core clipboard clipboard";"CODE"
"windows clipboard uses a utf 16 little endian encoding";"CODE"
"decode only if we don t have unicode";"CODE"
"we would still need to decode from utf 16 windows";"CODE"
"data is of type bytes in py3";"-"
"remove null strings mostly a windows issue";"CODE"
"load clipboard implementation";"TASK"
"versions previous to honeycomb";"META"
"put text data onto clipboard";"CODE"
"getclipboarddata returns a handle to the clipboard data";"CODE"
"which is a memory object containing the data";"IRRE"
"see https docs microsoft com en us windows win32 api winuser nf winuser getclipboarddata noqa e501";"CODE"
"if someone pastes a file the content is none for scf 13";"CODE"
"and the clipboard is locked if not closed properly";"CODE"
"the handle returned by getclipboarddata is a memory object";"CODE"
"and needs to be locked to get the actual pointer to the data";"CODE"
"the wsclen function returns the number of";"CODE"
"wide characters in a string not including the null character";"CODE"
"according to the docs regarding setclipboarddatam if the hmem";"CODE"
"parameter identifies a memory object the object must have";"IRRE"
"been allocated using the gmem moveable flag";"-"
"see https docs microsoft com en us windows win32 api winuser nf winuser setclipboarddata noqa e501";"CODE"
"the size of the memory object is the number of wide characters in";"CODE"
"the string plus one for the terminating null character";"CODE"
"since the memory object is allocated with gmem moveable should be";"IRRE"
"locked to get the actual pointer to the data";"CODE"
"finally set the clipboard data and then close the clipboard";"CODE"
"pylint disable w0611";"CODE"
"let the user know if his graphics hardware drivers are too old";"CODE"
"xxx in the android emulator latest version at 22 march 2013";"IRRE"
"this call was segfaulting the gl stack";"IRRE"
"to be able to use our gl provider we must have a window";"CODE"
"automatically import window auto to ensure the default window creation";"CODE"
"import kivy core window noqa";"CODE"
"late binding";"-"
"register image caching only for keep data true";"CODE"
"decoded image format one of a available texture format";"CODE"
"data for each mipmap";"CODE"
"image source if available";"-"
"indicate if the texture will need to be vertically flipped";"TASK"
"the original image which we might need to save if it is a memoryview";"TASK"
"first check if a texture with the same name already exist in the";"CODE"
"cache";"-"
"if not create it and append to the cache";"CODE"
"set as our current texture";"IRRE"
"release data if ask";"-"
"read zip in memory for faster access";"CODE"
"read all images inside the zip";"CODE"
"sort filename list";"-"
"read file and store it in mem with fileio struct around it";"CODE"
"loader failed continue trying";"CODE"
"append imagedata to local variable before its";"CODE"
"overwritten";"-"
"else if not image file skip to next";"-"
"replace image data with the array of all the images in the zip";"CODE"
"atlas";"-"
"remove the url";"-"
"last field is the id";"-"
"search if we already got the atlas loaded";"CODE"
"atlas already loaded so reupload the missing texture in cache";"CODE"
"because when it s not in use the texture can be removed from the";"IRRE"
"kv texture cache";"-"
"search with resource";"-"
"first time fill our texture cache";"-"
"extract extensions";"-"
"prevent url querystrings";"CODE"
"get actual image format instead of extension if possible";"CODE"
"special case when we are trying to load a zip file with image we";"CODE"
"will use the special zip loader in imageloader this might return a";"CODE"
"sequence of images contained in the zip";"CODE"
"this event should be fired on animation of sequenced img s";"CODE"
"indicator of images having been loded in cache";"-"
"do something";"CODE"
"this time image will be re loaded from disk";"CODE"
"start reset animation";"IRRE"
"or stop the animation";"-"
"set to 20 fps";"IRRE"
"stop animation";"-"
"construct uid as a key for cache";"CODE"
"in case of image have been asked with keep data";"CODE"
"check the kv image cache instead of texture";"CODE"
"we found an image yeah but reset the texture now";"IRRE"
"if image class is core image then it s a texture";"IRRE"
"from atlas or other sources and has no data so skip";"CODE"
"if we already got a texture it will be automatically reloaded";"CODE"
"if image not already in cache then load";"CODE"
"put the image into the cache if needed";"CODE"
"see if there is a available loader for it";"CODE"
"save an core image object";"CODE"
"save a texture";"CODE"
"bytesio like rawio rawiobase bytesio";"-"
"we might have a imagedata object to use";"IRRE"
"fast path use the raw data when keep data is used";"IRRE"
"the format is not rgba we need to convert it";"TASK"
"use texture for that";"IRRE"
"use the texture pixels";"IRRE"
"can t use this function without imagedata";"CODE"
"check bounds";"-"
"color reverse bgra";"-"
"color reverse rgba";"-"
"conversion for bgr rgb bgra rgba format";"CODE"
"load image loaders";"CODE"
"resolve binding";"-"
"from kivy graphics texture import texture textureregion noqa f811";"CODE"
"register";"-"
"see https www ffmpeg org general html image formats";"CODE"
"update internals";"CODE"
"register";"-"
"for python3";"CODE"
"pillow";"-"
"pil";"-"
"monkey patch frombytes and tobytes methods refs";"CODE"
"https github com kivy kivy issues 5460";"CODE"
"image loader work only with rgb rgba image";"CODE"
"read all images inside";"CODE"
"paste new frame over old so as to handle";"CODE"
"transparency properly";"-"
"update internals";"CODE"
"returns an array of type imagedata len 1 if not a sequence image";"IRRE"
"register";"-"
"update internals";"CODE"
"register";"-"
"register";"-"
"if no language was specified we just use the first one";"IRRE"
"that is available";"-"
"note we do not return enchant list dicts because that also returns";"CODE"
"the enchant dict objects and not only the language identifiers";"IRRE"
"don t show suggestions that are invalid";"OUTD"
"todo implement this";"TASK"
"nsspellchecker provides several functions that look like what we";"CODE"
"need but they re a slooow and b return a strange result";"IRRE"
"might be a snow leopard bug have to test further";"IRRE"
"see http paste pocoo org show 217968";"CODE"
"xxx both ways below work on osx 10 6 it has not been tested on any";"IRRE"
"other version but it should work";"META"
"this is deprecated as of osx 10 6 hence the try except";"CODE"
"from 10 6 onwards you re supposed to do it like this";"CODE"
"right this was much easier apple";"CODE"
"the label is usually not drawn until needed so force it to draw";"CODE"
"now access the texture of the label and use it wherever and";"IRRE"
"however you may please";"-"
"create a font context containing system fonts one custom ttf";"IRRE"
"these are now interchangeable ways to refer to the custom font";"CODE"
"you could also refer to a system font by family since this is a";"CODE"
"system font context";"CODE"
"include system fonts dir in resource paths";"CODE"
"this allows us to specify a font from those dirs";"CODE"
"elf internal size 0 0 the real computed text size inclds pad";"CODE"
"fonts append fonts 0 add regular font to list again";"CODE"
"is the font registered";"-"
"return the preferred font for the current bold italic combination";"CODE"
"xxx for compatibility check directly in the data dir";"CODE"
"register the font dirs";"-"
"if larger it won t fit so don t even try extents";"CODE"
"now find the first and last word";"-"
"if dir l center or right";"-"
"no split or the first word doesn t even fit";"CODE"
"at this point we do char by char so e1 must be zero";"CODE"
"both word fits and there s at least on split str";"-"
"if s2 e1 there s only on split str";"IRRE"
"both the first and last word fits and they start end at diff pos";"CODE"
"else left";"-"
"no split or the last word doesn t even fit";"CODE"
"if split str";"-"
"both word fits and there s at least on split str";"-"
"if s2 e1 there s only on split str";"IRRE"
"both the first and last word fits and they start end at diff pos";"CODE"
"if len line words get opts from first line first word";"CODE"
"fixme this should possibly use a config value and possibly we should";"IRRE"
"expose pango unichar direction pango bidi type for unichar";"CODE"
"uww uw padding left padding right real width of just text";"TASK"
"for layout line in lines for plain label each line has only one str";"CODE"
"right align rtl text";"-"
"right left justify";"-"
"divide left over space between spaces";"-"
"todo implement a better method of stretching glyphs";"TASK"
"number spaces needed to fill and remainder";"CODE"
"there s no trailing space when justify is selected";"CODE"
"words every even index is spaces just add ltr n spaces";"TASK"
"render the last word at the edge also add it to line";"TASK"
"last word lw uww ext 0 word was stretched";"-"
"last word lw uww word was stretched";"-"
"layout line w uww the line occupies full width";"CODE"
"if options is none there was no text to render";"TASK"
"ih self internal size 1 the real size of text not texture";"CODE"
"get data from provider";"CODE"
"if the text is 1px width usually the data is black";"-"
"don t blit that kind of data otherwise you have a little black bar";"CODE"
"center 1 pos of newline";"CODE"
"if a newline split text render from center down and up til uh";"CODE"
"layout from center down until half uh";"CODE"
"now layout from center upwards until uh is reached";"CODE"
"else if there s no new line layout everything";"CODE"
"else top or bottom";"IRRE"
"second pass render for real";"CODE"
"first pass calculating width height";"-"
"if no text are rendered return nothing";"CODE"
"create a delayed texture";"IRRE"
"load the appropriate provider";"CODE"
"fixme better way to do this";"CODE"
"for the first initialization register the default font";"CODE"
"color color color";"-"
"we need to do this trick when documentation is generated";"CODE"
"split markup words and lines";"-"
"result list of word with position and width height";"IRRE"
"during the first pass we don t care about h valign";"CODE"
"if shorten then don t split lines to fit uw because it will be";"CODE"
"flattened later when shortening and broken up lines if broken";"-"
"mid word will have space mid word when lines are joined";"-"
"if len lines remove any trailing spaces from the last line";"CODE"
"options ref none no refs for you";"CODE"
"when valign is not top for markup we layout everything text size 1";"CODE"
"is temporarily set to none and after layout cut to size if too tall";"IRRE"
"else middle";"-"
"top int h 2 uh 2 remove extra top portion";"CODE"
"i len lines 1 remove remaining bottom portion";"CODE"
"now justify the text";"-"
"xxx update refs to justified pos";"CODE"
"when justify each line should ve been stripped already";"CODE"
"if there s nothing to justify we re done";"CODE"
"parts none len words contains words split by space";"-"
"idxs none len words indices of the space in parts";"-"
"break each word into spaces and add spaces until it s full";"CODE"
"do first round of split in case we don t need to split all";"CODE"
"now we have the indices of the spaces in split list";"-"
"try to add single space at each space";"TASK"
"there s not a single space in the line";"CODE"
"now keep adding spaces to already split words until done";"CODE"
"try to add single space at each space";"TASK"
"if not completely full push last words to right edge";"CODE"
"find the last word that had a space";"-"
"split that word into left right and push right till uww";"CODE"
"now put words back together with right left inserted";"CODE"
"for layout line in lines for plain label each line has only one str";"CODE"
"the word height is not scaled by line height only lh was";"-"
"calculate sub super script pos";"CODE"
"should we record refs";"-"
"should we record anchors";"-"
"yield 1 1 total w this should never be reached really";"CODE"
"yield 1 1 total w this should never be reached really";"CODE"
"if i 1 and total w ww uw found and it fits";"-"
"if total w ww uw wasn t found and all fits";"-"
"now just find whatever amount of the word does fit";"CODE"
"if i 1 and total w ww uw found and it fits";"-"
"if total w ww uw wasn t found and all fits";"-"
"now just find whatever amount of the word does fit";"CODE"
"flatten lines into single line";"CODE"
"concatenate non empty inside lines with a space";"-"
"if that fits just return the flattened line";"IRRE"
"set new opts for ellipsis";"CODE"
"find the size of ellipsis that ll fit";"-"
"if elps s 0 uw even ellipsis didn t fit";"-"
"restore old opts";"CODE"
"now find the first left and right words that fit";"-"
"if dir l center or right";"-"
"if either was clipped or both don t fit just take first";"CODE"
"elif w1 e1 1 1 this shouldn t occur";"CODE"
"now we know that both the first and last word fit and that";"-"
"there s at least one instances of the split str in the line";"CODE"
"if w1 e1 w2 s2 more than one split str";"-"
"f n line c iterator";"-"
"assert next f 1 w1 e1 first word should match";"CODE"
"else center";"-"
"f n line c iterator";"-"
"f inv p line c iterator";"-"
"ww1 ee1 l1 next f hypothesize that next fit";"-"
"else left";"-"
"if either was clipped or both don t fit just take last";"CODE"
"elif w1 e1 1 1 this shouldn t occur";"CODE"
"now we know that both the first and last word fit and that";"-"
"there s at least one instances of the split str in the line";"CODE"
"if w1 e1 w2 s2 more than one split str";"-"
"f inv p line c iterator";"-"
"assert next f inv 1 w2 s2 last word should match";"CODE"
"now add back the left half";"TASK"
"now add back the right half";"TASK"
"from kivy core window import window opengl must be initialized";"CODE"
"used for fetching extends before creature image surface";"CODE"
"create a surface context font";"IRRE"
"adjust x and y position to avoid text cutoff";"CODE"
"load the appropriate provider";"CODE"
"from kivy lib gstplayer import gstplayer noqa";"CODE"
"surfacetexture surface";"-"
"fbo for kivy texture";"CODE"
"extension gl oes egl image external require";"CODE"
"ifdef gl es";"CODE"
"endif";"CODE"
"safely release mediaplayer";"-"
"property overrides";"CODE"
"controls";"-"
"self unload";"CODE"
"called automatically by videobase";"IRRE"
"first time we got a frame we know that video is read now";"CODE"
"this is only used when building ff opts to prevent starting";"CODE"
"player paused and can probably be removed as soon as the eof";"OUTD"
"receiving issue is solved";"-"
"see https github com matham ffpyplayer issues 142";"CODE"
"this causes a seek to zero";"CODE"
"once setup is done we make sure player state matches what user";"CODE"
"could have requested while player was being setup and it was in limbo";"CODE"
"also thread starts player in internal paused mode so this unpauses";"CODE"
"it if user didn t request pause meanwhile";"CODE"
"xxx fixme";"-"
"self texture add reload observer self reload buffer";"CODE"
"video starts in internal paused state";"CODE"
"fast path if the source video is yuv420p we ll use a glsl shader";"-"
"for buffer conversion to rgba";"META"
"wait until we get frame metadata";"-"
"ffpyplayer reports src pix fmt as bytes this may or may not";"IRRE"
"change in future so we check for both bytes and str";"TASK"
"now we ll be in internal paused state and loop will wait until";"IRRE"
"mainthread unpauses us when finishing setup";"CODE"
"get next frame if paused";"-"
"ffplayer set volume 0 0 try to do it silently";"CODE"
"we don t know concrete number of frames to skip";"CODE"
"this number worked fine on couple of tested videos";"IRRE"
"exit loop on invalid val";"IRRE"
"exit loop on seek queue updated";"IRRE"
"wait for next frame";"CODE"
"wait until we skipped enough frames";"-"
"assuming last frame is actual just get it";"-"
"todo this is not safe because user could have updated";"CODE"
"volume between us reading it and setting it";"IRRE"
"get next frame regular";"-"
"still save seek while thread is setting up";"CODE"
"if state hasn t been set empty there s no player if it s";"IRRE"
"paused nothing to do so just handle playing";"TASK"
"we could be in limbo while player is setting up so check player";"IRRE"
"will pause when finishing setting up";"TASK"
"even in limbo indicate to start in paused state";"-"
"state starts empty and is empty again after unloading";"CODE"
"player is already setup just handle unpausing";"IRRE"
"we re now either in limbo state waiting for thread to setup";"CODE"
"or no thread has been started";"CODE"
"in limbo just wait for thread to setup player";"CODE"
"in limbo still unpause for when player becomes ready";"CODE"
"load first unloads";"CODE"
"if no stream it starts internally paused but unpauses itself";"CODE"
"if stream and we start paused we sometimes receive eof after a";"-"
"few frames depending on the stream producer";"TASK"
"xxx this probably needs to be figured out in ffpyplayer using";"CODE"
"ffplay directly works";"-"
"disabled as an attempt to fix kivy issue 6210";"-"
"self ffplayer set volume self volume";"CODE"
"todo remove";"TASK"
"start in playing mode but ffplayer isn t set until ready we re";"IRRE"
"now in a limbo state";"-"
"no need to call self trigger cancel because ffplayer is set";"IRRE"
"to none below and it s not safe to call clock stuff from del";"IRRE"
"if thread is still alive set it to exit and wake it";"TASK"
"wait until it exits";"-"
"todo use callback don t block here";"CODE"
"reset for next load since thread is dead for sure";"CODE"
"if we still receive the video but no more player remove it";"TASK"
"texture is not allocated yet create it first";"TASK"
"pylint disable w0611";"CODE"
"coding utf 8";"-"
"late import";"CODE"
"keycodes mapping between str int these keycodes are";"CODE"
"currently taken from pygame key but when a new provider will be";"CODE"
"used it must do the translation to these keycodes too";"CODE"
"specials keys";"-"
"a z keys";"-"
"0 9 keys";"-"
"numpad";"-"
"f1 15";"-"
"other keys";"-"
"35 36";"-"
"window which the keyboard is attached too";"CODE"
"callback that will be called when the keyboard is released";"IRRE"
"target that have requested the keyboard";"CODE"
"vkeyboard widget if allowed by the configuration";"-"
"private properties";"CODE"
"the size is the same no need to resize anything";"TASK"
"red background color";"-"
"don t clear background at all";"CODE"
"make some property read only";"CODE"
"if use sdl3 placeholder until the sdl3 bootstrap supports this";"CODE"
"internal";"CODE"
"don t init window 2 times";"CODE"
"except if force is specified";"CODE"
"create a trigger for update create the window when one of window";"CODE"
"property changes";"-"
"create a trigger for updating the keyboard height";"IRRE"
"set the default window parameter according to the configuration";"IRRE"
"bind all the properties that need to recreate the window";"TASK"
"init privates";"IRRE"
"before creating the window";"CODE"
"import kivy core gl noqa";"CODE"
"configure the window";"CODE"
"manage keyboard s";"-"
"assign the default context of the widget creation";"IRRE"
"because window is created as soon as imported if we bound earlier";"CODE"
"metrics would be imported when dp is set during window creation";"CODE"
"instead don t process dpi changes until everything is set";"TASK"
"mark as initialized";"IRRE"
"attach modules listener event";"CODE"
"prevent any leftover that can crash the app later";"-"
"like if there is still some gl referenced values";"IRRE"
"they may be collected later but because it was already";"META"
"gone in the system it may collect invalid gl resources";"CODE"
"just clear everything to force reloading later on";"CODE"
"just to be sure if the trigger is set and if this method is";"IRRE"
"manually called unset the trigger";"IRRE"
"ensure the window creation will not be called twice";"IRRE"
"create the render context and canvas only the first time";"IRRE"
"if we get initialized more than once then reload opengl state";"CODE"
"after the second time";"-"
"xxx check how it s working on embed platform";"CODE"
"on linux it s safe for just sending a resize";"CODE"
"on other platform window are recreated we need to reload";"CODE"
"ensure the gl viewport is correct";"-"
"xxx fixme use late binding";"-"
"when using inner window an app have grab the touch";"CODE"
"but app is removed the touch can t access";"OUTD"
"to one of the parent i e self parent will be none";"CODE"
"and bam the bug happen";"-"
"todo use me push me pop methods because me is transformed";"CODE"
"clock execution of partial scrollview on touch up method and";"-"
"other similar cases should be changed so that me push me pop can";"CODE"
"be used restore previous values of event s attributes";"IRRE"
"me push";"-"
"me pop";"-"
"prepare the viewport";"-"
"do projection matrix";"CODE"
"do modelview matrix";"CODE"
"redraw canvas";"-"
"and update childs";"CODE"
"quit if user presses esc or the typical osx shortcuts cmd q or cmd w";"-"
"on android a back key gesture is mapped to 27 and initiates a pause";"IRRE"
"consume the event and tell android to pause";"-"
"todo if just cmd w is pressed only the window should be closed";"CODE"
"4999 https github com kivy kivy issues 4999 for";"CODE"
"configure how to provide keyboards virtual or not";"-"
"register system keyboard to listening keys from window";"CODE"
"use the device s real keyboard";"IRRE"
"use the device s real keyboard";"IRRE"
"one single vkeyboard shared between all widgets";"-"
"the single vkeyboard is always sitting at the same position";"-"
"now read the configuration";"CODE"
"adapt mode according to the configuration";"-"
"release any previous keyboard attached";"-"
"if we can use virtual vkeyboard activate it";"-"
"late import";"CODE"
"if the keyboard doesn t exist create it";"IRRE"
"configure vkeyboard";"-"
"add to the window";"TASK"
"only after add do dock mode";"CODE"
"sets vkeyboard position according to window softinput mode";"CODE"
"system keyboard just register the callback";"IRRE"
"use system hardware keyboard according to flag";"CODE"
"this way will prevent possible recursion";"CODE"
"instance of a class windowbase implementation";"TASK"
"default display ids";"CODE"
"found a way to include it more easily";"CODE"
"sdl keycode h https wiki libsdl org sdl keymod";"CODE"
"xxx ios keyboard suck when backspace is hit the delete";"CODE"
"keycode is sent fix it";"-"
"map android back button to escape";"META"
"on ios the did enter foreground is launched at the start";"CODE"
"of the application in our case we want it only when the app";"CODE"
"is resumed";"-"
"force kivy to render the frame now so that the canvas is drawn";"CODE"
"use custom behavior";"-"
"to handle aero snapping and rounded corners";"-"
"ensure we have an event filter";"-"
"setup window";"IRRE"
"we don t have a density or dpi yet set so let s ask for an update";"CODE"
"never stay with a none pos application using w center";"-"
"will be fired";"-"
"set mouse visibility";"IRRE"
"auto add input provider";"TASK"
"set window icon before calling set mode";"IRRE"
"transparent window background";"CODE"
"twb end";"CODE"
"for android ios we don t want to have any event nor executing our";"CODE"
"main loop while the pause is going on this loop wait any event not";"CODE"
"handled by the event filter and remove them from the queue";"IRRE"
"nothing happen during the pause on ios except gyroscope value sent";"CODE"
"over joystick so it s safe";"-"
"a drop is send while the app is still in pause loop";"CODE"
"we need to dispatch it";"TASK"
"app terminating event might be received while the app is paused";"CODE"
"in this case eventloop quit will be set at event filter";"CODE"
"for finger pass the raw event to sdl motion event provider";"CODE"
"xxx this is problematic on osx it generates touches with 0";"IRRE"
"0 coordinates at the same times as mouse but it works";"META"
"we have a conflict of using either the mouse or the finger";"-"
"right now we have no mechanism that we could use to know";"IRRE"
"which is the preferred one for the application";"CODE"
"don t dispatch motion if no button are pressed";"META"
"ignore if the cursor position is on the window title bar";"IRRE"
"or on its edges";"-"
"times x if y 0 else y";"-"
"times min abs times 100";"-"
"for k in range times";"CODE"
"video resize";"-"
"don t use trigger here we want to delay the resize event";"IRRE"
"the display has changed so the density and dpi";"-"
"may have changed too";"-"
"maybe in sdl3 this is not needed anymore";"OUTD"
"ignore the key it has been released";"-"
"if mod in self meta keys";"CODE"
"on android there is no encoding attribute";"META"
"on other platforms if stdout is redirected";"CODE"
"encoding may be none";"-"
"if shift in self modifiers and key";"CODE"
"not in self command keys keys";"CODE"
"return";"IRRE"
"don t dispatch more key if down event is accepted";"CODE"
"unhandled event";"-"
"x y are relative to window left top position";"CODE"
"should go to app pause mode desktop style";"-"
"xxx fixme wait for sdl resume";"CODE"
"why does this need to be rounded for now refactored it";"CODE"
"why abs needs a comment";"TASK"
"this is a jumping module required for python for android project";"CODE"
"because we are putting all the module into the same so there can be name";"CODE"
"conflict we have one conflict with pygame event and kivy event both are";"-"
"python extension and have the same initevent symbol so right now just";"IRRE"
"rename this one";"CODE"
"create additional resources bind callbacks to self window";"IRRE"
"handle touch event";"CODE"
"handle hover event";"-"
"release resources";"-"
"r comment preproc";"-"
"r using pythonlexer";"CODE"
"r punctuation pop";"-"
"if trying to access attributes like checking for bind";"CODE"
"then raise attributeerror";"META"
"no class to return import the module";"CODE"
"level 0 force absolute";"CODE"
"factory instance to use for getting new classes";"CODE"
"now import the file with all registers";"CODE"
"automatically generated by build factory";"IRRE"
"import kivy factory registers noqa";"CODE"
"auto generated file by setup py build factory";"IRRE"
"https kivy garden github io guideformigratingflowersfromlegacystructure";"CODE"
"to your pip conf https pip pypa io en stable user guide config file";"CODE"
"installing a garden package";"-"
"upgrade a garden package";"TASK"
"uninstall a garden package";"-"
"list all the garden packages installed";"-"
"search new packages";"CODE"
"search all the packages that contain graph";"-"
"show the help";"-"
"system path where garden modules can be installed";"CODE"
"application path where garden modules can be installed";"CODE"
"fixes issue 4030 in kivy where garden path is incorrect on ios";"CODE"
"insert the garden importer as ultimate importer";"CODE"
"determine a point p with the smallest y value";"IRRE"
"find a point q such that the angle of the line segment";"CODE"
"pq with the x axis is minimal";"-"
"return 1e10 max val if the same to skip";"IRRE"
"find r such that angle prq is minimal";"-"
"return 1e10 max val if the same to skip";"IRRE"
"check for case 1 angle prq is obtuse the circle is determined";"CODE"
"by two points p and q radius p q 2 center p q 2";"CODE"
"if angle rpq is obtuse make p r and try again";"IRRE"
"if angle pqr is obtuse make q r and try again";"IRRE"
"all angles were acute we just need the circle through the";"CODE"
"two points furthest apart";"CODE"
"find the circumcenter for triangle given by p q r";"CODE"
"create a gesture";"IRRE"
"add it to the database";"TASK"
"and for the next gesture try to find it";"CODE"
"these return the min and max coordinates of the stroke";"IRRE"
"if len point list 1 if there is only one point no length";"CODE"
"if there is only one point or the length is 0 don t normalize";"CODE"
"calculate how long each point should be in the stroke";"CODE"
"we loop on the points";"IRRE"
"the new point need to be inserted into the";"CODE"
"segment prev curr";"-"
"if this happens we are into troubles";"CODE"
"tolerance for evaluation using the operator";"CODE"
"map creates a list of min max coordinates of the strokes";"IRRE"
"in the gesture and min max pulls the lowest highest value";"IRRE"
"adds up all the points inside the stroke";"TASK"
"average to get the offset";"IRRE"
"apply the offset to the strokes";"IRRE"
"get orientation";"-"
"rotate the gesture to be in the same frame";"CODE"
"this is the normal orientation code";"CODE"
"if the gestures don t have the same number of strokes its";"CODE"
"definitely not the same gesture";"IRRE"
"add a red color";"TASK"
"add a rectangle";"TASK"
"very hacky way to avoid pyflakes warning";"CODE"
"pylint disable w0611";"CODE"
"me push save current type id and other values";"IRRE"
"dispatch mouse touch event to widgets which registered";"IRRE"
"to receive mouse touch";"IRRE"
"me pop restore";"-"
"will receive all motion events";"-"
"current position in 0 1 range";"-"
"first position set in 0 1 range";"IRRE"
"last position set in 0 1 range";"IRRE"
"delta from the last position and current one in 0 1 range";"CODE"
"current position in screen range";"-"
"first position set in screen range";"IRRE"
"last position set in 0 1 range";"IRRE"
"delta from the last position and current one in screen range";"CODE"
"true if the motionevent is a touch";"-"
"experimental string to identify event type";"CODE"
"versionadded 2 1 0";"TASK"
"experimental used by a event manager or a widget to assign";"IRRE"
"the dispatching mode defaults to";"CODE"
"const kivy eventmanager mode default dispatch see";"CODE"
"mod kivy eventmanager for available modes";"CODE"
"versionadded 2 1 0";"TASK"
"attributes to push by default when we use meth push x y z";"IRRE"
"dx dy dz ox oy oz px py pz";"-"
"uniq id of the event you can safely use this property it will be";"CODE"
"never the same across all existing events";"-"
"device used for creating this event";"CODE"
"for grab";"CODE"
"used to determine which widget the event is being dispatched to";"OUTD"
"check the meth grab function for more information";"CODE"
"currently pressed button";"META"
"profiles currently used in the event";"CODE"
"id of the event not unique this is generally the id set by the";"CODE"
"input provider like id in tuio if you have multiple tuio sources";"CODE"
"then same id can be used prefer to use attr uid attribute";"META"
"instead";"-"
"shape of the touch event subclass of";"IRRE"
"class kivy input shape shape";"CODE"
"by default the property is set to none";"IRRE"
"x position in 0 1 range";"-"
"y position in 0 1 range";"-"
"z position in 0 1 range";"-"
"origin x position in 0 1 range";"-"
"origin y position in 0 1 range";"-"
"origin z position in 0 1 range";"-"
"previous x position in 0 1 range";"-"
"previous y position in 0 1 range";"-"
"previous z position in 0 1 range";"-"
"delta between self sx and self psx in 0 1 range";"CODE"
"delta between self sy and self psy in 0 1 range";"CODE"
"delta between self sz and self psz in 0 1 range";"CODE"
"x position in window range";"CODE"
"y position in window range";"CODE"
"z position in window range";"CODE"
"origin x position in window range";"CODE"
"origin y position in window range";"CODE"
"origin z position in window range";"CODE"
"previous x position in window range";"CODE"
"previous y position in window range";"CODE"
"previous z position in window range";"CODE"
"delta between self x and self px in window range";"CODE"
"delta between self y and self py in window range";"CODE"
"delta between self z and self pz in window range";"CODE"
"position x y in window range";"CODE"
"initial time of the event creation";"IRRE"
"time of the last update";"CODE"
"time of the end event last event usage";"CODE"
"indicate if the touch event is a double tap or not";"CODE"
"indicate if the touch event is a triple tap or not";"CODE"
"versionadded 1 7 0";"TASK"
"if the touch is a attr is double tap this is the time";"CODE"
"between the previous tap and the current touch";"-"
"if the touch is a attr is triple tap this is the time";"CODE"
"between the first tap and the current touch";"-"
"versionadded 1 7 0";"TASK"
"user data dictionary use this dictionary to save your own data on";"CODE"
"the event";"-"
"if set to true default keeps first previous position";"IRRE"
"x y z in 0 1 range and ignore all other until";"CODE"
"meth motionevent dispatch done is called from the eventloop";"IRRE"
"this attribute is needed because event provider can make many calls";"IRRE"
"to meth motionevent move but for all those calls event is";"IRRE"
"dispatched to the listeners only once assigning false will keep";"IRRE"
"latest previous position see meth motionevent move";"IRRE"
"versionadded 2 1 0";"TASK"
"keep first previous position if attr sync with dispatch is";"-"
"true";"-"
"flag that first dispatch of this event is done";"CODE"
"sync origin previous current positions until the first";"-"
"dispatch etype begin is done";"CODE"
"update the delta";"CODE"
"i received my grabbed touch";"-"
"it s a normal touch";"-"
"i receive my grabbed touch i must ungrab it";"-"
"it s a normal touch";"-"
"adjust y for keyboard height";"CODE"
"update delta values";"IRRE"
"cache position";"-"
"facilities";"-"
"mapping of id to module";"CODE"
"don t go further if we generate documentation";"CODE"
"the right screen is just a display";"-"
"avoid doing any processing if there is no device to calibrate at all";"CODE"
"frame based logic below doesn t account for";"CODE"
"end events having been already processed";"CODE"
"some providers use the same event to update and end";"CODE"
"get the taxicab manhattan citiblock distance for efficiency reasons";"CODE"
"check whether the touch moved more than the jitter distance";"-"
"only if the touch has moved more than the jitter dist we take";"-"
"it into account and dispatch it otherwise suppress it";"CODE"
"first check if a touch down have a double tap";"CODE"
"add the touch internally";"TASK"
"second check if up touch is timeout for double tap";"CODE"
"format xmin ymin xmax ymax";"CODE"
"check if module is disabled";"IRRE"
"new touch found the nearest one";"CODE"
"eligible for continuation";"CODE"
"first check if a touch down have a triple tap";"CODE"
"add the touch internally";"TASK"
"second check if up touch is timeout for triple tap";"CODE"
"pylint disable w0611";"CODE"
"import kivy input providers leapfinger noqa";"CODE"
"pylint disable w0611";"CODE"
"import android noqa";"CODE"
"python for android do 1000";"CODE"
"new touch";"CODE"
"update touch";"CODE"
"avoid same touch position";"CODE"
"disappear";"-"
"coding utf 8";"-"
"devicename hidinput dev input eventxx";"CODE"
"example with stantum mtp4 3 screen";"-"
"late imports";"CODE"
"documentation hack";"CODE"
"this part is taken from linux source 2 6 32 include linux input h";"CODE"
"event types";"-"
"synchronization events";"-"
"misc events";"-"
"abs mt touch major 0x30 major axis of touching ellipse";"-"
"abs mt touch minor 0x31 minor axis omit if circular";"-"
"abs mt width major 0x32 major axis of approaching ellipse";"-"
"abs mt width minor 0x33 minor axis omit if circular";"-"
"abs mt orientation 0x34 ellipse orientation";"-"
"abs mt position x 0x35 center x ellipse position";"-"
"abs mt position y 0x36 center y ellipse position";"-"
"abs mt tool type 0x37 type of touching device";"-"
"abs mt blob id 0x38 group a set of packets as a blob";"IRRE"
"abs mt tracking id 0x39 unique id of initiated contact";"IRRE"
"abs mt pressure 0x3a pressure on contact area";"-"
"some ioctl base with 0 value";"IRRE"
"0x04 3";"-"
"todo combinations";"TASK"
"e0 37 prtscr";"-"
"e0 46 ctrl break";"CODE"
"e0 5b lwin usb lgui";"-"
"e0 5c rwin usb rgui";"-"
"e0 5d menu";"-"
"e0 5f sleep";"-"
"e0 5e power";"-"
"e0 63 wake";"-"
"e0 38 ralt";"-"
"e0 1d rctrl";"-"
"e0 52 insert";"CODE"
"e0 53 delete";"CODE"
"e0 47 home";"-"
"e0 4f end";"CODE"
"e0 49 pgup";"-"
"e0 51 pgdn";"-"
"e0 4b left";"-"
"e0 48 up";"-"
"e0 50 down";"CODE"
"e0 4d right";"-"
"e0 35 kp";"-"
"e0 1c kp enter";"-"
"e1 1d 45 77 pause";"-"
"sizeof struct input event";"CODE"
"split arguments";"-"
"read filename";"CODE"
"read parameters";"IRRE"
"ensure it s a key value";"IRRE"
"ensure the key exist";"-"
"ensure the value";"IRRE"
"all good";"-"
"prepare some vars to get limit of some component";"CODE"
"limit it to the screen area 0 1";"-"
"sync event";"-"
"compute multitouch track";"-"
"for scrolls we need to remove it as there is";"TASK"
"no up key";"-"
"elif ev code 8 wheel";"-"
"translates the wheel move to a button";"IRRE"
"or if it is not in this lut";"CODE"
"open the input";"CODE"
"get the controller name eviocgname";"-"
"get abs infos";"-"
"preserve this we may want other things than ev abs";"CODE"
"ev abs available for this device";"CODE"
"ask abs info keys to the devices";"-"
"init the point";"IRRE"
"read until the end";"CODE"
"extract each event";"-"
"extract timeval event infos";"-"
"dispatch all events from threads";"CODE"
"don t do the import at start or the error will be always displayed";"CODE"
"for user who don t have leap";"CODE"
"print hand id finger id finger tip";"CODE"
"registers";"-"
"documentation hack";"CODE"
"this part is taken from linux source 2 6 32 include linux input h";"CODE"
"event types";"-"
"synchronization events";"-"
"misc events";"-"
"abs misc 0x28 if 0 it s touch up";"-"
"abs mt touch major 0x30 major axis of touching ellipse";"-"
"abs mt touch minor 0x31 minor axis omit if circular";"-"
"abs mt width major 0x32 major axis of approaching ellipse";"-"
"abs mt width minor 0x33 minor axis omit if circular";"-"
"abs mt orientation 0x34 ellipse orientation";"-"
"abs mt position x 0x35 center x ellipse position";"-"
"abs mt position y 0x36 center y ellipse position";"-"
"abs mt tool type 0x37 type of touching device";"-"
"abs mt blob id 0x38 group a set of packets as a blob";"IRRE"
"abs mt tracking id 0x39 unique id of initiated contact";"IRRE"
"abs mt pressure 0x3a pressure on contact area";"-"
"some ioctl base with 0 value";"IRRE"
"sizeof struct input event";"CODE"
"split arguments";"-"
"read filename";"CODE"
"read parameters";"IRRE"
"ensure it s a key value";"IRRE"
"ensure the key exist";"-"
"ensure the value";"IRRE"
"all good";"-"
"prepare some vars to get limit of some component";"CODE"
"open the input";"CODE"
"get the controller name eviocgname";"-"
"get abs infos";"-"
"preserve this we may want other things than ev abs";"CODE"
"ev abs available for this device";"CODE"
"ask abs info keys to the devices";"-"
"read until the end";"CODE"
"extract each event";"-"
"extract timeval event infos";"-"
"dispatch all event from threads";"CODE"
"current state of unknown meaning";"-"
"normalized position and vector of the touch 0 to 1";"-"
"the area of the touch";"-"
"the following three define the ellipsoid of a finger";"CODE"
"global uid";"-"
"touches will be per devices";"-"
"lock needed to access on uid";"-"
"event queue to dispatch in main thread";"CODE"
"ok listing devices and attach";"-"
"create touch dict for this device";"CODE"
"start";"-"
"dispatch all event from threads";"CODE"
"i don t known how to stop it";"CODE"
"xxx create live touch we get one case that";"IRRE"
"the device announced by macosx don t match the device";"CODE"
"in mts callback";"IRRE"
"get pointer on data";"CODE"
"add this touch as an active touch";"TASK"
"extract identifier";"-"
"prepare argument position";"-"
"increment uid";"-"
"create a touch";"IRRE"
"create event";"IRRE"
"store touch";"-"
"check if he really moved";"IRRE"
"delete old touchs";"CODE"
"late binding";"-"
"don t overwrite previous profile";"CODE"
"create automatically touch on the surface";"IRRE"
"use same logic as windowbase on motion so we get correct";"CODE"
"coordinates when density 1";"CODE"
"split arguments";"-"
"trying to get if we currently have other touch than us";"CODE"
"discard touches generated from kinetic";"CODE"
"discard all kinetic touch";"-"
"not our instance stop mouse";"-"
"args append modifiers always adds modifiers";"CODE"
"only draw red circle if multitouch is not disabled and";"-"
"if the multitouch on demand feature is not enable";"TASK"
"because in that case we wait to see if multitouch sim";"CODE"
"is true or not before doing the multitouch";"CODE"
"divide by density because it s used by mouse pos";"-"
"alt just released";"-"
"special case if button is all";"META"
"then remove all the current touches";"-"
"registers";"-"
"devicename hidinput dev input eventxx";"CODE"
"example for inverting touch events";"CODE"
"documentation hack";"CODE"
"split arguments";"-"
"read filename";"CODE"
"read parameters";"IRRE"
"ensure it s a key value";"IRRE"
"ensure the key exist";"-"
"ensure the value";"IRRE"
"all good";"-"
"this can happen if we have a touch going on already at";"CODE"
"the start of the app";"-"
"except zerodivisionerror it s both in py2 and py3";"CODE"
"open mtdev device";"CODE"
"if e errno 13 permission denied";"-"
"prepare some vars to get limit of some component";"CODE"
"if device have disconnected lets try to connect";"CODE"
"input device is back online let s recreate device";"CODE"
"idle as much as we can";"-"
"got data read all without redoing idle";"CODE"
"set the working slot";"IRRE"
"fill the slot";"-"
"unrecognized command ignore";"CODE"
"push all changes";"-"
"dispatch all event from threads";"CODE"
"using mtdev";"-"
"using hidinput";"CODE"
"using mtdev with a match on name";"-"
"using hidinput with custom parameters to hidinput all on one line";"CODE"
"you can also match your wacom touchscreen";"-"
"and your wacom pen";"-"
"see linux input h";"CODE"
"hack to not return an instance of this provider";"CODE"
"ensure it s a key value";"IRRE"
"must ignore";"-"
"name tuio ip port";"-"
"you can also add a second tuio listener";"TASK"
"config set input source2 tuio 0 0 0 0 3334";"IRRE"
"then do the usual things";"CODE"
"create a class to handle the new tuio type path";"CODE"
"replace newpath with the pathname you want to handle";"CODE"
"in this method implement unpacking for the received";"CODE"
"arguments you basically translate from tuio args to kivy";"IRRE"
"motionevent variables if all you receive are x and y";"IRRE"
"values you can do it like this";"IRRE"
"register it with the tuio motionevent provider";"-"
"you obviously need to replace the path placeholders appropriately";"IRRE"
"read the queue with event";"CODE"
"queue is empty we re done for now";"CODE"
"verify commands";"CODE"
"move or create a new touch";"IRRE"
"new touch";"CODE"
"update a current touch";"CODE"
"alive event check for deleted touch";"CODE"
"touch up";"-"
"default argument for tuio touches";"CODE"
"fixme 3d shape are not supported";"-"
"registers";"-"
"check availability of registertouchwindow";"CODE"
"documentation hack";"CODE"
"it s a touch or a pen";"-"
"inject our own wndproc to handle messages";"-"
"before window manager does";"CODE"
"documentation hack";"CODE"
"get window handle and register to receive wm touch messages";"CODE"
"inject our own wndproc to handle messages";"-"
"before window manager does";"CODE"
"adjust x y to window coordinates 0 0 to 1 0";"IRRE"
"actually dispatch input";"META"
"we inject this wndproc into our main window to process";"CODE"
"wm touch and mouse messages before the window manager does";"CODE"
"this on pushes wm touch messages onto our event stack";"CODE"
"filter fake mouse events because touch and stylus";"IRRE"
"also make mouse events";"-"
"its a touch or a pen";"-"
"internals";"CODE"
"manually set the current window";"IRRE"
"generate a record filename";"-"
"elf record fd write recorder1 0 n";"TASK"
"needed for acting as an input provider";"CODE"
"if data 0 recorder1 0";"-"
"decompile data";"CODE"
"width of the rect";"-"
"height of the rect";"-"
"i press tab to list attributes of the app";"META"
"i root press tab to list attributes of the root widget";"META"
"app is boring attach a new widget";"CODE"
"the application is now blocked";"-"
"click on the screen several times";"CODE"
"the clicks will show up now";"CODE"
"erase artwork and start over";"-"
"mynewobject is unsafe";"CODE"
"mynewobject is now safe call at will";"IRRE"
"everything from this point on is just a series of thread safing proxy";"CODE"
"methods that make calls against ref and threadsafe whenever data will be";"IRRE"
"written to or if a method will be called safemembrane instances should";"IRRE"
"be unwrapped whenever passing them into the thread";"CODE"
"use type to determine if an object is a safemembrane while debugging";"CODE"
"proxy behavior starts after this is set before this point attaching";"CODE"
"widgets etc can only be done through the launcher s app attribute";"IRRE"
"act like the app instance even before ref is set";"IRRE"
"kivy 1 0";"-"
"content here";"-"
"syntax of a rule definition note that several rules can share the same";"IRRE"
"definition as in css note the braces they are part of the definition";"IRRE"
"definitions";"IRRE"
"definitions";"IRRE"
"syntax for creating a root widget";"CODE"
"definitions";"IRRE"
"syntax for creating a dynamic class";"CODE"
"definitions";"IRRE"
"syntax for creating a template";"CODE"
"definitions";"IRRE"
"with the braces it s a rule without them it s a root widget";"-"
"kivy 1 0";"-"
"or as an alternative syntax";"-"
"simple inheritance";"-"
"kv code here";"-"
"multiple inheritance";"-"
"kv code here";"-"
"simple inheritance";"-"
"multiple inheritance";"-"
"let s use the new classes in another rule";"CODE"
"a context to pass for the context will be ctx inside template";"CODE"
"a kv definition of the template";"IRRE"
"with only one base class";"IRRE"
"definitions";"IRRE"
"with more than one base class";"IRRE"
"definitions";"IRRE"
"create a template with hello world an image";"IRRE"
"the context values should be passed as kwargs to the builder template";"IRRE"
"function";"CODE"
"create a second template with other information";"IRRE"
"and use icon1 and icon2 as other widget";"-"
"this is the same as before";"CODE"
"now we are using the ctx for the variable part of the template";"CODE"
"you cannot use references other than root";"CODE"
"ctxkey mywidget value fail this references the id";"CODE"
"mywidget";"-"
"not all of the dynamic parts will be understood";"-"
"ctxkey value 1 if root prop1 else value2 even if";"IRRE"
"root prop1 is a property if it changes value ctxkey";"IRRE"
"will not be updated";"CODE"
"directivename options";"-"
"import alias package";"CODE"
"import os os";"CODE"
"import ut kivy utils";"CODE"
"import animation kivy animation animation";"CODE"
"set key expr";"IRRE"
"set my color 4 3 4";"IRRE"
"set my color hl 5 4 5";"IRRE"
"include force file";"CODE"
"test kv";"IRRE"
"include mycomponent kv";"CODE"
"include force mybutton kv";"CODE"
"mycomponent kv";"-"
"include mybutton kv";"CODE"
"mybutton kv";"META"
"late import";"CODE"
"delayed calls are canvas expression triggered during an loop it is one";"IRRE"
"directional linked list of args to call call fn with each element is a list";"IRRE"
"whose last element points to the next list of args to execute when";"CODE"
"builder sync is called";"IRRE"
"it s already on the list";"CODE"
"first remove all the old bound functions from s and down";"CODE"
"find the first attr from which we need to start rebinding";"TASK"
"bind all attrs except last to update intermediates";"CODE"
"if we need to dynamically rebind bindm otherwise just";"TASK"
"add the attr to the list";"TASK"
"fbind should not dispatch otherwise";"-"
"update intermediates might be called in the middle";"CODE"
"here messing things up";"-"
"for the last attr we bind directly to the setting function";"CODE"
"because that attr sets the value of the rule";"IRRE"
"when we rebind we have to update the";"CODE"
"rule with the most recent value otherwise the value might be wrong";"IRRE"
"and wouldn t be updated since we might not have tracked it before";"CODE"
"this only happens for a callback when rebind was true for the prop";"CODE"
"we need a hash for when delayed so we don t execute duplicate canvas";"CODE"
"callbacks from the same handler during a sync op";"IRRE"
"args element key value rule idmap none see delayed start";"IRRE"
"bind every key value";"IRRE"
"bind all attrs except last to update intermediates";"CODE"
"if we need to dynamically rebind bindm otherwise";"TASK"
"just add the attr to the list";"TASK"
"fbind should not dispatch otherwise";"-"
"update intermediates might be called in the middle";"CODE"
"here messing things up";"-"
"for the last attr we bind directly to the setting";"IRRE"
"function because that attr sets the value of the rule";"IRRE"
"uid f fbind keys 1 fn args f is not none";"IRRE"
"remove rules and templates";"-"
"unregister all the dynamic classes";"IRRE"
"put a warning if a file is loaded multiple times";"CODE"
"parse the string";"IRRE"
"merge rules with our rules";"-"
"add the template found by the parser into ours";"TASK"
"register all the dynamic classes";"IRRE"
"create root object is exist";"IRRE"
"save the loaded files only if there is a root without";"CODE"
"template dynamic classes";"IRRE"
"prevent naming clash with whatever the user might be putting into the";"CODE"
"ctx as key";"-"
"in previous versions ctx is passed as is as template ctx";"META"
"preventing widgets in it from be collected by the gc this was";"CODE"
"especially relevant to accordionitem s title template";"CODE"
"widget the current instantiated widget";"-"
"rule the current rule";"CODE"
"rootrule the current root rule for children of a rule";"CODE"
"will collect reference to all the id in children";"CODE"
"extract the context of the rootrule not rule";"-"
"if a template context is passed put it as ctx";"-"
"if we got an id put it in the root rule for a later global usage";"CODE"
"use only the first word as id discard the rest";"-"
"rule id rule id split 1 0 strip";"-"
"set id name as a attribute for root widget so one can in python";"CODE"
"code simply access root widget id name";"-"
"skip on self";"CODE"
"first ensure that the widget have all the properties used in";"-"
"the rule if not they will be created as objectproperty";"IRRE"
"build the widget canvas";"-"
"create children tree";"IRRE"
"depending if the child rule is a template or not we are not";"TASK"
"having the same approach";"-"
"we got a template so extract all the properties and";"-"
"handlers and push them in a ctx dictionary";"-"
"create the template with an explicit ctx";"IRRE"
"reference it on our root rule context";"CODE"
"we got a normal rule construct it manually";"CODE"
"we can t construct it without no builder true because the";"CODE"
"previous implementation was doing the add widget before";"TASK"
"apply and so we could use self parent";"CODE"
"append the properties and handlers to our final resolution task";"CODE"
"clear previously applied rules if asked";"-"
"if we are applying another rule that the root one then it s done for";"CODE"
"normally we can apply a list of properties with a proper context";"-"
"if there s a rule";"-"
"build handlers";"-"
"hack for on parent";"CODE"
"rule finished forget it";"TASK"
"is this try except still needed yes in case widget died in this";"CODE"
"frame after the call was scheduled";"IRRE"
"if fn is none it s not a kivy prop";"-"
"proxy widget is already gone that s cool";"CODE"
"if fn is none it s not a kivy prop";"-"
"proxy widget is already gone that s cool";"CODE"
"main instance of a class builderbase";"CODE"
"import kivy lang builder imported as absolute to avoid circular import";"CODE"
"register cache for creating new classtype template";"CODE"
"all previously included files";"CODE"
"precompile regexp expression";"CODE"
"all the widget handlers used to correctly unbind all the callbacks then the";"IRRE"
"widget is deleted";"CODE"
"proxy app object";"IRRE"
"taken from http code activestate com recipes 496741 object proxying";"CODE"
"clear cached application instance when it stops";"-"
"associated parser";"IRRE"
"line of the rule";"-"
"name of the property";"-"
"value of the property";"IRRE"
"compiled value";"IRRE"
"compilation mode";"-"
"watched keys";"-"
"stats";"-"
"whether previous rules targeting name should be cleared";"-"
"first remove all the string from the value";"CODE"
"detecting how to handle the value according to the key name";"IRRE"
"if we don t detect any string key in it we can eval and give the";"CODE"
"result";"IRRE"
"ok we can compile";"CODE"
"for exec mode we don t need to watch any keys";"CODE"
"now detect obj prop";"-"
"find all the fstrings in the value";"CODE"
"first remove all the string from the value";"CODE"
"idx tmp find";"-"
"detect key value inside value and split them";"IRRE"
"build full dotted attribute path e g root object property";"CODE"
"level of the rule in the kv";"CODE"
"associated parser";"IRRE"
"line of the rule";"-"
"name of the rule";"-"
"list of children to create";"IRRE"
"id given to the rule";"-"
"properties associated to the rule";"-"
"canvas normal";"-"
"canvas before";"CODE"
"canvas after";"-"
"handlers associated to the rule";"-"
"properties cache list mark which class have already been checked";"CODE"
"indicate if any previous rules should be avoided";"CODE"
"check first if the widget class already been processed by this rule";"CODE"
"if the very first name start with a avoid previous rules";"CODE"
"new class creation";"CODE"
"ensure the name is correctly written";"-"
"save the name in the dynamic classes dict";"CODE"
"classical selectors";"CODE"
"if include force path with quotes around";"CODE"
"resolve the whole thing";"CODE"
"read and parse the lines of the file";"IRRE"
"strip all comments";"-"
"execute directives";"CODE"
"get object from the first level";"CODE"
"precompile rules tree";"CODE"
"after parsing there should be no remaining lines";"CODE"
"or there s an error we did not catch earlier";"CODE"
"i e a comment line s first non whitespace character must be a";"TASK"
"extract directives";"-"
"if stripped 2";"-"
"if stripped 1";"-"
"get the number of space";"-"
"replace any tab with 4 spaces";"-"
"first indent designates the indentation";"IRRE"
"level finished";"TASK"
"current level create an object";"IRRE"
"not x 1 lstrip startswith";"-"
"if it s not a root rule then we got some restriction";"CODE"
"aka a valid name without point or everything else";"CODE"
"next level is it a property or an object";"IRRE"
"it s a class add to the current object as a children";"IRRE"
"it s a property";"-"
"if ignore prev it wasn t consumed";"-"
"two more levels";"-"
"too much indentation invalid";"OUTD"
"check the next line";"-"
"ruff noqa";"-"
"ddsurfacedesc2 dwflags";"-"
"ddpixelformat dwflags";"CODE"
"ddscaps2 dwcaps1";"-"
"ddscaps2 dwcaps2";"-"
"common fourcc codes";"-"
"read header";"CODE"
"depack";"-"
"check header validity";"CODE"
"first image set defaults";"IRRE"
"ruff noqa";"-"
"load library";"CODE"
"from linux input h";"CODE"
"mtdev code slot 0x2f mt slot being modified";"-"
"mtdev code touch major 0x30 major axis of touching ellipse";"-"
"mtdev code touch minor 0x31 minor axis omit if circular";"-"
"mtdev code width major 0x32 major axis of approaching ellipse";"-"
"mtdev code width minor 0x33 minor axis omit if circular";"-"
"mtdev code orientation 0x34 ellipse orientation";"-"
"mtdev code position x 0x35 center x ellipse position";"-"
"mtdev code position y 0x36 center y ellipse position";"-"
"mtdev code tool type 0x37 type of touching device";"-"
"mtdev code blob id 0x38 group a set of packets as a blob";"IRRE"
"mtdev code tracking id 0x39 unique id of initiated contact";"IRRE"
"mtdev code pressure 0x3a pressure on contact area";"-"
"mtdev code btn tool quadtap 0x14f four fingers on trackpad";"-"
"binding";"-"
"linux kernel creates input devices then hands permission changes";"IRRE"
"off to udev this results in a period of time when the device is";"CODE"
"readable only by root device reconnects can be processed by";"CODE"
"mtdmotioneventprovider faster than udev can get a chance to run";"CODE"
"so we spin for a period of time to allow udev to fix permissions";"CODE"
"we limit the loop time in case the system is misconfigured and";"IRRE"
"the user really does not and will not have permission to access";"CODE"
"the device";"-"
"note udev takes about 0 6 s on a raspberry pi 4";"TASK"
"register a cache for loader";"CODE"
"you can either add ext at the end of the url or use this array";"CODE"
"if blank filename then return";"IRRE"
"with recent changes to coreimage we must keep data otherwise";"-"
"we might be unable to recreate the texture afterwise";"IRRE"
"note it s important to load smbhandler every time";"CODE"
"otherwise the data is occasionally not loaded";"IRRE"
"read from samba shares";"CODE"
"read from internet";"CODE"
"a custom context is only needed on android and ios";"CODE"
"as we need to use the certs provided via certifi";"TASK"
"if in filename";"-"
"allow extension override from url fragment";"CODE"
"uffix filename split 1";"-"
"strip query string and split on path";"CODE"
"strip out blanks from";"CODE"
"we don t want com net etc as the extension";"CODE"
"write to local filename";"TASK"
"load data";"TASK"
"fixme create a clean api for that";"CODE"
"close file when remote file not found or download error";"CODE"
"update client";"CODE"
"got one client to update";"CODE"
"want to start it";"-"
"in pause mode don t unqueue anything";"CODE"
"create the image";"IRRE"
"image data proxyimage data";"-"
"update client";"CODE"
"got one client to update";"CODE"
"found image if data is not here need to reload";"TASK"
"if data is none this is really the first time";"CODE"
"already queued for loading";"CODE"
"loader implementation";"TASK"
"get path to log directory";"-"
"if maxfiles 0 no log file limit set";"IRRE"
"get all files from log directory and corresponding creation timestamps";"CODE"
"sort files by ascending timestamp";"CODE"
"more log files than allowed maximum";"-"
"delete files starting with oldest creation timestamp";"CODE"
"or edit timestamp on linux";"-"
"if n 10000 prevent maybe flooding";"-"
"during the startup store the message in the history";"CODE"
"startup done if the logfile is not activated avoid history";"CODE"
"deactivate filehandler";"-"
"this message was scraped from stderr";"CODE"
"emit it without formatting";"CODE"
"don t pass it to the formatted emitter";"CODE"
"included for backward compatibility only";"CODE"
"could be used to override colors";"OUTD"
"kivy default logger instance";"CODE"
"versionchanged 2 2 0";"META"
"issue 7891 describes an undocumented feature that was since removed";"TASK"
"detect if a client was depending on it";"CODE"
"versionchanged 2 2 0";"META"
"add default kivy logger";"TASK"
"use the custom handler instead of streaming one";"IRRE"
"don t output to stderr if it is set to none";"IRRE"
"stderr is set to none by pythonw and pyinstaller 5 7";"CODE"
"no additional control characters will be inserted inside the";"CODE"
"levelname field 7 chars will fit warning";"CODE"
"levelname field width need to take into account the length of";"TASK"
"the color control codes 7 4 chars for bold color and reset";"IRRE"
"add the kivy handlers to the root logger so they will be used";"TASK"
"for all propagated log messages";"CODE"
"root logger defaults to warning let logger be the limiting factor";"CODE"
"install stderr handlers";"CODE"
"caution if any logging handlers output to sys stderr they should be";"IRRE"
"configured before this reconfiguration is done to avoid loops";"CODE"
"sends all messages written to stderr to the logger after prefixing it";"CODE"
"with stderr";"CODE"
"add the kivy handlers to the kivy logger so they will be used";"TASK"
"for all messages sent through logger only";"CODE"
"don t spread kivy related log messages to the root logger";"CODE"
"don t set stderr redirection it is too likely to cause loops with other";"IRRE"
"handlers client can manually add it if desired";"TASK"
"else kivy log mode python";"CODE"
"don t add handlers or redirect stderr client can manually add if desired";"CODE"
"kivy 1 5 0";"-"
"for all other platforms";"CODE"
"because dp prop binds to dpi etc its getter will be executed";"CODE"
"before dispatch pixel scale bound to dpi was called so we need to";"TASK"
"call this to make sure it s updated";"CODE"
"we bind to all dpi density fontscale even though not all may be";"-"
"used for a specific suffix because we don t want to rely on the";"CODE"
"internal details of dpi2px but it will be one of the three but it s";"META"
"an issue since it won t trigger the prop if the value doesn t change";"IRRE"
"uncomment to activate";"-"
"monitor";"-"
"keybinding";"-"
"activate the touchring module";"CODE"
"accept only python extensions";"CODE"
"protect against missing module dependency crash";"CODE"
"basic check on module";"CODE"
"ensure the module has been configured";"CODE"
"convert configuration like";"-"
"m mjpegserver port 8080 fps 8";"-"
"and pass it in context config token";"-"
"call configure if module have one";"IRRE"
"ignore modules without docstring";"CODE"
"make sure we don t get indexerror along the way";"CODE"
"then pretty format the header";"CODE"
"n 12s s 12 spaces";"-"
"coding utf 8";"-"
"ruff noqa";"-"
"data files";"-"
"color eee";"-"
"background url f background jpg repeat x 718693";"-"
"background color bccad5";"-"
"color 516673";"-"
"error html connection lost show";"CODE"
"error hide";"-"
"div class panel panel rid div id r rid div div appendto metrics";"CODE"
"h ids key html key data key i 1";"-"
"function a b function cy a return f iswindow a a a nodetype 9 a defaultview a parentwindow 1 function cv a if ck a var b c body d f a appendto b e d css display d remove if e none e cl cl c createelement iframe cl frameborder cl width cl height 0 b appendchild cl if cm cl createelement cm cl contentwindow cl contentdocument document cm write c compatmode css1compat doctype html html body cm close d cm createelement a cm body appendchild d e f css d display b removechild cl ck a e return ck a function cu a b var c f each cq concat apply cq slice 0 b function c this a return c function ct cr b function cs settimeout ct 0 return cr f now function cj try return new a activexobject microsoft xmlhttp catch b function ci try return new a xmlhttprequest catch b function cc a c a datafilter c a datafilter c a datatype var d a datatypes e g h i d length j k d 0 l m n o p for g 1 g i g if g 1 for h in a converters typeof h string e h tolowercase a converters h l k k d g if k k l else if l l k m l k n e m e k if n p b for o in e j o split if j 0 l j 0 p e j 1 k if p o e o o 0 n p p 0 n o break n p f error no conversion from m replace to n 0 c n n c p o c return c function cb a c d var e a contents f a datatypes g a responsefields h i j k for i in g i in d c g i d i while f 0 f shift h b h a mimetype c getresponseheader content type if h for i in e if e i e i test h f unshift i break if f 0 in d j f 0 else for i in d if f 0 a converters i f 0 j i break k k i j j k if j j f 0 f unshift j return d j function ca a b c d if f isarray b f each b function b e c be test a d a e ca a typeof e object f isarray e b e c d else if c b null typeof b object for var e in b ca a e b e c d else d a b function b a c var d e g f ajaxsettings flatoptions for d in c c d b g d a e e d c d e f extend 0 a e function b a c d e f g f f c datatypes 0 g g g f 0 var h a f i 0 j h h length 0 k a bt l for i j k l i l h i c d e typeof l string k g l l b c datatypes unshift l l b a c d e l g k l g l b a c d e g return l function bz a return function b c typeof b string c b b if f isfunction c var d b tolowercase split bp e 0 g d length h i j for e g e h d e j test h j h h substr 1 i a h a h i j unshift push c function bc a b c var d b width a offsetwidth a offsetheight e b width bx by g 0 h e length if d 0 if c border for g h g c d parsefloat f css a padding e g 0 c margin d parsefloat f css a c e g 0 d parsefloat f css a border e g width 0 return d px d bz a b b if d 0 d null d a style b 0 d parsefloat d 0 if c for g h g d parsefloat f css a padding e g 0 c padding d parsefloat f css a border e g width 0 c margin d parsefloat f css a c e g 0 return d px function bp a b b src f ajax url b src async 1 datatype script f globaleval b text b textcontent b innerhtml replace bf 0 b parentnode b parentnode removechild b function bo a var b c createelement div bh appendchild b b innerhtml a outerhtml return b firstchild function bn a var b a nodename tolowercase b input bm a b script typeof a getelementsbytagname undefined f grep a getelementsbytagname input bm function bm a if a type checkbox a type radio a defaultchecked a checked function bl a return typeof a getelementsbytagname undefined a getelementsbytagname typeof a queryselectorall undefined a queryselectorall function bk a b var c if b nodetype 1 b clearattributes b clearattributes b mergeattributes b mergeattributes a c b nodename tolowercase if c object b outerhtml a outerhtml else if c input a type checkbox a type radio if c option b selected a defaultselected else if c input c textarea b defaultvalue a defaultvalue else a checked b defaultchecked b checked a checked b value a value b value a value b removeattribute f expando function bj a b if b nodetype 1 f hasdata a var c d e g f data a h f data b g i g events if i delete h handle h events for c in i for d 0 e i c length d e d f event add b c i c d namespace i c d namespace i c d i c d data h data h data f extend h data function bi a b return f nodename a table a getelementsbytagname tbody 0 a appendchild a ownerdocument createelement tbody a function u a var b v split c a createdocumentfragment if c createelement while b length c createelement b pop return c function t a b c b b 0 if f isfunction b return f grep a function a d var e b call a d a return e c if b nodetype return f grep a function a d return a b c if typeof b string var d f grep a function a return a nodetype 1 if o test b return f filter b d c b f filter b d return f grep a function a d return f inarray a b 0 c function s a return a a parentnode a parentnode nodetype 11 function k return 0 function j return 1 function n a b c var d b defer e b queue g b mark h f data a d h c queue f data a e c mark f data a g settimeout function f data a e f data a g f removedata a d 0 h fire 0 function m a for var b in a if b data f isemptyobject a b continue if b tojson return 1 return 0 function l a c d if d b a nodetype 1 var e data c replace k 1 tolowercase d a getattribute e if typeof d string try d d true 0 d false 1 d null null f isnumeric d parsefloat d j test d f parsejson d d catch g f data a c d else d b return d function h a var b g a c d a a split s for c 0 d a length c d c b a c 0 return b var c a document d a navigator e a location f function function j if e isready try c documentelement doscroll left catch a settimeout j 1 return e ready var e function a b return new e fn init a b h f a jquery g a h i w w w j s k s l s m w s 1 n s o bfnrt u 0 9a fa f 4 g p n r true false null d d ee d g q s g r webkit w s opera version w t msie w u mozilla rv w v a z 0 9 ig w ms x function a b return b touppercase y d useragent z a b c object prototype tostring d object prototype hasownproperty e array prototype push f array prototype slice g string prototype trim h array prototype indexof i e fn e prototype constructor e init function a d f var g h j k if a return this if a nodetype this context this 0 a this length 1 return this if a body d c body this context c this 0 c body this selector a this length 1 return this if typeof a string a charat 0 a charat a length 1 a length 3 g i exec a g null a null if g g 1 d if g 1 d d instanceof e d 0 d k d d ownerdocument d c j m exec a j e isplainobject d a c createelement j 1 e fn attr call a d 0 a k createelement j 1 j e buildfragment g 1 k a j cacheable e clone j fragment j fragment childnodes return e merge this a h c getelementbyid g 2 if h h parentnode if h id g 2 return f find a this length 1 this 0 h this context c this selector a return this return d d jquery d f find a this constructor d find a if e isfunction a return f ready a a selector b this selector a selector this context a context return e makearray a this selector jquery 1 7 1 length 0 size function return this length toarray function return f call this 0 get function a return a null this toarray a 0 this this length a this a pushstack function a b c var d this constructor e isarray a e apply d a e merge d a d prevobject this d context this context b find d selector this selector this selector c b d selector this selector b c return d each function a b return e each this a b ready function a e bindready a add a return this eq function a a a return a 1 this slice a this slice a a 1 first function return this eq 0 last function return this eq 1 slice function return this pushstack f apply this arguments slice f call arguments join map function a return this pushstack e map this function b c return a call b c b end function return this prevobject this constructor null push e sort sort splice splice e fn init prototype e fn e extend e fn extend function var a c d f g h i arguments 0 j 1 k arguments length l 1 typeof i boolean l i i arguments 1 j 2 typeof i object e isfunction i i k j i this j for j k j if a arguments j null for c in a d i c f a c if i f continue l f e isplainobject f g e isarray f g g 1 h d e isarray d d h d e isplainobject d d i c e extend l h f f b i c f return i e extend noconflict function b a e a g b a jquery e a jquery f return e isready 1 readywait 1 holdready function a a e readywait e ready 0 ready function a if a 0 e readywait a 0 e isready if c body return settimeout e ready 1 e isready 0 if a 0 e readywait 0 return a firewith c e e fn trigger e c trigger ready off ready bindready function if a a e callbacks once memory if c readystate complete return settimeout e ready 1 if c addeventlistener c addeventlistener domcontentloaded b 1 a addeventlistener load e ready 1 else if c attachevent c attachevent onreadystatechange b a attachevent onload e ready var b 1 try b a frameelement null catch d c documentelement doscroll b j isfunction function a return e type a function isarray array isarray function a return e type a array iswindow function a return a typeof a object setinterval in a isnumeric function a return isnan parsefloat a isfinite a type function a return a null string a i c call a object isplainobject function a if a e type a object a nodetype e iswindow a return 1 try if a constructor d call a constructor d call a constructor prototype isprototypeof return 1 catch c return 1 var d for d in a return d b d call a d isemptyobject function a for var b in a return 1 return 0 error function a throw new error a parsejson function b if typeof b string b return null b e trim b if a json a json parse return a json parse b if n test b replace o replace p replace q return new function return b e error invalid json b parsexml function c var d f try a domparser f new domparser d f parsefromstring c text xml d new activexobject microsoft xmldom d async false d loadxml c catch g d b d d documentelement d getelementsbytagname parsererror length e error invalid xml c return d noop function globaleval function b b j test b a execscript function b a eval call a b b camelcase function a return a replace w ms replace v x nodename function a b return a nodename a nodename touppercase b touppercase each function a c d var f g 0 h a length i h b e isfunction a if d if i for f in a if c apply a f d 1 break else for g h if c apply a g d 1 break else if i for f in a if c call a f f a f 1 break else for g h if c call a g g a g 1 break return a trim g function a return a null g call a function a return a null a replace k replace l makearray function a b var c b if a null var d e type a a length null d string d function d regexp e iswindow a e call c a e merge c a return c inarray function a b c var d if b if h return h call b a c d b length c c c 0 math max 0 d c c 0 for c d c if c in b b c a return c return 1 merge function a c var d a length e 0 if typeof c length number for var f c length e f e a d c e else while c e b a d c e a length d return a grep function a b c var d e c c for var f 0 g a length f g f e b a f f c e d push a f return d map function a c d var f g h i 0 j a length k a instanceof e j b typeof j number j 0 a 0 a j 1 j 0 e isarray a if k for i j i f c a i i d f null h h length f else for g in a f c a g g d f null h h length f return h concat apply h guid 1 proxy function a c if typeof c string var d a c c a a d if e isfunction a return b var f f call arguments 2 g function return a apply c f concat f call arguments g guid a guid a guid g guid e guid return g access function a c d f g h var i a length if typeof c object for var j in c e access a j c j f g d return a if d b f h f e isfunction d for var k 0 k i k g a k c f d call a k k g a k c d h return a return i g a 0 c b now function return new date gettime uamatch function a a a tolowercase var b r exec a s exec a t exec a a indexof compatible 0 u exec a return browser b 1 version b 2 0 sub function function a b c return new a fn init b c e extend 0 a this a superclass this a fn a prototype this a fn constructor a a sub this sub a fn init function d f f f instanceof e f instanceof a f a f return e fn init call this d f b a fn init prototype a fn var b a c return a browser e each boolean number string function array date regexp object split function a b i object b b tolowercase z e uamatch y z browser e browser z browser 0 e browser version z version e browser webkit e browser safari 0 j test k s xa0 l s xa0 h e c c addeventlistener b function c removeeventlistener domcontentloaded b 1 e ready c attachevent b function c readystate complete c detachevent onreadystatechange b e ready return e g f callbacks function a a a g a h a var c d e i j k l m function b var d e g h i for d 0 e b length d e d g b d h f type g h array m g h function a unique o has g c push g n function b f f f e a memory b f i 0 l j 0 j 0 k c length for c l k l if c l apply b f 1 a stoponfalse e 0 break i 1 c a once e 0 o disable c d d length e d shift o firewith e 0 e 1 o add function if c var a c length m arguments i k c length e e 0 j a n e 0 e 1 return this remove function if c var b arguments d 0 e b length for d e d for var f 0 f c length f if b d c f i f k k f l l c splice f 1 if a unique break return this has function a if c var b 0 d c length for b d b if a c b return 0 return 1 empty function c return this disable function c d e b return this disabled function return c lock function d b e e 0 o disable return this locked function return d firewith function b c d i a once d push b c a once e n b c return this fire function o firewith this arguments return this fired function return e return o var i slice f extend deferred function a var b f callbacks once memory c f callbacks once memory d f callbacks memory e pending g resolve b reject c notify d h done b add fail c add progress d add state function return e isresolved b fired isrejected c fired then function a b c i done a fail b progress c return this always function i done apply i arguments fail apply i arguments return this pipe function a b c return f deferred function d f each done a resolve fail b reject progress c notify function a b var c b 0 e b 1 g f isfunction c i a function g c apply this arguments g f isfunction g promise g promise then d resolve d reject d notify d e with this i d this g i a d e promise promise function a if a null a h else for var b in h a b h b return a i h promise j for j in g i j g j fire i j with g j firewith i done function e resolved c disable d lock fail function e rejected b disable d lock a a call i i return i when function a function m a return function b e a arguments length 1 i call arguments 0 b j notifywith k e function l a return function c b a arguments length 1 i call arguments 0 c g j resolvewith j b var b i call arguments 0 c 0 d b length e array d g d h d j d 1 a f isfunction a promise a f deferred k j promise if d 1 for c d c b c b c promise f isfunction b c promise b c promise then l c j reject m c g g j resolvewith j b else j a j resolvewith j d a return k f support function var b d e g h i j k l m n o p q c createelement div r c documentelement q setattribute classname t q innerhtml link table table a href a style top 1px float left opacity 55 a a input type checkbox d q getelementsbytagname e q getelementsbytagname a 0 if d d length e return g c createelement select h g appendchild c createelement option i q getelementsbytagname input 0 b leadingwhitespace q firstchild nodetype 3 tbody q getelementsbytagname tbody length htmlserialize q getelementsbytagname link length style top test e getattribute style hrefnormalized e getattribute href a opacity 0 55 test e style opacity cssfloat e style cssfloat checkon i value on optselected h selected getsetattribute q classname t enctype c createelement form enctype html5clone c createelement nav clonenode 0 outerhtml nav nav submitbubbles 0 changebubbles 0 focusinbubbles 1 deleteexpando 0 nocloneevent 0 inlineblockneedslayout 1 shrinkwrapblocks 1 reliablemarginright 0 i checked 0 b noclonechecked i clonenode 0 checked g disabled 0 b optdisabled h disabled try delete q test catch s b deleteexpando 1 q addeventlistener q attachevent q fireevent q attachevent onclick function b nocloneevent 1 q clonenode 0 fireevent onclick i c createelement input i value t i setattribute type radio b radiovalue i value t i setattribute checked checked q appendchild i k c createdocumentfragment k appendchild q lastchild b checkclone k clonenode 0 clonenode 0 lastchild checked b appendchecked i checked k removechild i k appendchild q q innerhtml a getcomputedstyle j c createelement div j style width 0 j style marginright 0 q style width 2px q appendchild j b reliablemarginright parseint a getcomputedstyle j null marginright 0 marginright 10 0 0 if q attachevent for o in submit 1 change 1 focusin 1 n on o p n in q p q setattribute n return p typeof q n function b o bubbles p k removechild q k g h j q i null f function var a d e g h i j k m n o r c getelementsbytagname body 0 r j 1 k position absolute top 0 left 0 width 1px height 1px margin 0 m visibility hidden border 0 n style k border 5px solid 000 padding 0 o div n div div div table n cellpadding 0 cellspacing 0 tr td td tr table a c createelement div a style csstext m width 0 height 0 position static top 0 margin top j px r insertbefore a r firstchild q c createelement div a appendchild q q innerhtml table tr td style padding 0 border 0 display none td td t td tr table l q getelementsbytagname td p l 0 offsetheight 0 l 0 style display l 1 style display none b reliablehiddenoffsets p l 0 offsetheight 0 q innerhtml q style width q style paddingleft 1px f boxmodel b boxmodel q offsetwidth 2 typeof q style zoom undefined q style display inline q style zoom 1 b inlineblockneedslayout q offsetwidth 2 q style display q innerhtml div style width 4px div b shrinkwrapblocks q offsetwidth 2 q style csstext k m q innerhtml o d q firstchild e d firstchild h d nextsibling firstchild firstchild i doesnotaddborder e offsettop 5 doesaddborderfortableandcells h offsettop 5 e style position fixed e style top 20px i fixedposition e offsettop 20 e offsettop 15 e style position e style top d style overflow hidden d style position relative i subtractsborderforoverflownotvisible e offsettop 5 i doesnotincludemargininbodyoffset r offsettop j r removechild a q a null f extend b i return b var j k a z g f extend cache uuid 0 expando jquery f fn jquery math random replace d g nodata embed 0 object clsid d27cdb6e ae6d 11cf 96b8 444553540000 applet 0 hasdata function a a a nodetype f cache a f expando a f expando return a m a data function a c d e if f acceptdata a var g h i j f expando k typeof c string l a nodetype m l f cache a n l a j a j j o c events if n m n o e m n data k d b return n l a j n f uuid n j m n m n l m n tojson f noop if typeof c object typeof c function e m n f extend m n c m n data f extend m n data c g h m n e h data h data h h data d b h f camelcase c d if o h c return g events k i h c i null i h f camelcase c i h return i removedata function a b c if f acceptdata a var d e g h f expando i a nodetype j i f cache a k i a h h if j k return if b d c j k j k data if d f isarray b b in d b b b f camelcase b b in d b b b b split for e 0 g b length e g e delete d b e if c m f isemptyobject d return if c delete j k data if m j k return f support deleteexpando j setinterval delete j k j k null i f support deleteexpando delete a h a removeattribute a removeattribute h a h null data function a b c return f data a b c 0 acceptdata function a if a nodename var b f nodata a nodename tolowercase if b return b 0 a getattribute classid b return 0 f fn extend data function a c var d e g h null if typeof a undefined if this length h f data this 0 if this 0 nodetype 1 f data this 0 parsedattrs e this 0 attributes for var i 0 j e length i j i g e i name g indexof data 0 g f camelcase g substring 5 l this 0 g h g f data this 0 parsedattrs 0 return h if typeof a object return this each function f data this a d a split d 1 d 1 d 1 if c b h this triggerhandler getdata d 1 d 0 h b this length h f data this 0 a h l this 0 a h return h b d 1 this data d 0 h return this each function var b f this e d 0 c b triggerhandler setdata d 1 e f data this a c b triggerhandler changedata d 1 e removedata function a return this each function f removedata this a f extend mark function a b a b b fx mark f data a b f data a b 0 1 unmark function a b c a 0 c b b a a 1 if b c c fx var d c mark e a 0 f data b d 1 1 e f data b d e f removedata b d 0 n b c mark queue function a b c var d if a b b fx queue d f data a b c d f isarray c d f data a b f makearray c d push c return d dequeue function a b b b fx var c f queue a b d c shift e d inprogress d c shift d b fx c unshift inprogress f data a b run e d call a function f dequeue a b e c length f removedata a b queue b run 0 n a b queue f fn extend queue function a c typeof a string c a a fx if c b return f queue this 0 a return this each function var b f queue this a c a fx b 0 inprogress f dequeue this a dequeue function a return this each function f dequeue this a delay function a b a f fx f fx speeds a a a b b fx return this queue b function b c var d settimeout b a c stop function cleartimeout d clearqueue function a return this queue a fx promise function a c function m h d resolvewith e e typeof a string c a a b a a fx var d f deferred e this g e length h 1 i a defer j a queue k a mark l while g if l f data e g i b 0 f data e g j b 0 f data e g k b 0 f data e g i f callbacks once memory 0 h l add m m return d promise var o n t r g p s q r g r button input i s button input object select textarea i t a rea i u autofocus autoplay async checked controls defer disabled hidden loop multiple open readonly required scoped selected i v f support getsetattribute w x y f fn extend attr function a b return f access this a b 0 f attr removeattr function a return this each function f removeattr this a prop function a b return f access this a b 0 f prop removeprop function a a f propfix a a return this each function try this a b delete this a catch c addclass function a var b c d e g h i if f isfunction a return this each function b f this addclass a call this b this classname if a typeof a string b a split p for c 0 d this length c d c e this c if e nodetype 1 if e classname b length 1 e classname a else g e classname for h 0 i b length h i h g indexof b h g b h e classname f trim g return this removeclass function a var c d e g h i j if f isfunction a return this each function b f this removeclass a call this b this classname if a typeof a string a b c a split p for d 0 e this length d e d g this d if g nodetype 1 g classname if a h g classname replace o for i 0 j c length i j i h h replace c i g classname f trim h else g classname return this toggleclass function a b var c typeof a d typeof b boolean if f isfunction a return this each function c f this toggleclass a call this c this classname b b return this each function if c string var e g 0 h f this i b j a split p while e j g i d i h hasclass e h i addclass removeclass e else if c undefined c boolean this classname f data this classname this classname this classname this classname a 1 f data this classname hasclass function a var b a c 0 d this length for c d c if this c nodetype 1 this c classname replace o indexof b 1 return 0 return 1 val function a var c d e g this 0 if arguments length e f isfunction a return this each function d var g f this h if this nodetype 1 e h a call this d g val h a h null h typeof h number h f isarray h h f map h function a return a null a c f valhooks this nodename tolowercase f valhooks this type if c set in c c set this h value b this value h if g c f valhooks g nodename tolowercase f valhooks g type if c get in c d c get g value b return d d g value return typeof d string d replace q d null d f extend valhooks option get function a var b a attributes value return b b specified a value a text select get function a var b c d e g a selectedindex h i a options j a type select one if g 0 return null c j g 0 d j g 1 i length for c d c e i c if e selected f support optdisabled e disabled e getattribute disabled null e parentnode disabled f nodename e parentnode optgroup b f e val if j return b h push b if j h length i length return f i g val return h set function a b var c f makearray b f a find option each function this selected f inarray f this val c 0 c length a selectedindex 1 return c attrfn val 0 css 0 html 0 text 0 data 0 width 0 height 0 offset 0 attr function a c d e var g h i j a nodetype if a j 3 j 8 j 2 if e c in f attrfn return f a c d if typeof a getattribute undefined return f prop a c d i j 1 f isxmldoc a i c c tolowercase h f attrhooks c u test c x w if d b if d null f removeattr a c return if h set in h i g h set a d c b return g a setattribute c d return d if h get in h i g h get a c null return g g a getattribute c return g null b g removeattr function a b var c d e g h 0 if b a nodetype 1 d b tolowercase split p g d length for h g h e d h e c f propfix e e f attr a e a removeattribute v e c u test e c in a a c 1 attrhooks type set function a b if r test a nodename a parentnode f error type property can t be changed else if f support radiovalue b radio f nodename a input var c a value a setattribute type b c a value c return b value get function a b if w f nodename a button return w get a b return b in a a value null set function a b c if w f nodename a button return w set a b c a value b propfix tabindex tabindex readonly readonly for htmlfor class classname maxlength maxlength cellspacing cellspacing cellpadding cellpadding rowspan rowspan colspan colspan usemap usemap frameborder frameborder contenteditable contenteditable prop function a c d var e g h i a nodetype if a i 3 i 8 i 2 h i 1 f isxmldoc a h c f propfix c c g f prophooks c return d b g set in g e g set a d c b e a c d g get in g e g get a c null e a c prophooks tabindex get function a var c a getattributenode tabindex return c c specified parseint c value 10 s test a nodename t test a nodename a href 0 b f attrhooks tabindex f prophooks tabindex x get function a c var d e f prop a c return e 0 typeof e boolean d a getattributenode c d nodevalue 1 c tolowercase b set function a b c var d b 1 f removeattr a c d f propfix c c d in a a d 0 a setattribute c c tolowercase return c v y name 0 id 0 w f valhooks button get function a c var d d a getattributenode c return d y c d nodevalue d specified d nodevalue b set function a b d var e a getattributenode d e e c createattribute d a setattributenode e return e nodevalue b f attrhooks tabindex set w set f each width height function a b f attrhooks b f extend f attrhooks b set function a c if c a setattribute b auto return c f attrhooks contenteditable get w get set function a b c b b false w set a b c f support hrefnormalized f each href src width height function a c f attrhooks c f extend f attrhooks c get function a var d a getattribute c 2 return d null b d f support style f attrhooks style get function a return a style csstext tolowercase b set function a b return a style csstext b f support optselected f prophooks selected f extend f prophooks selected get function a var b a parentnode b b selectedindex b parentnode b parentnode selectedindex return null f support enctype f propfix enctype encoding f support checkon f each radio checkbox function f valhooks this get function a return a getattribute value null on a value f each radio checkbox function f valhooks this f extend f valhooks this set function a b if f isarray b return a checked f inarray f a val b 0 var z textarea input select i a b bhover s b c key d mouse contextmenu click e focusinfocus focusoutblur f w w w g function a var b f exec a b b 1 b 1 tolowercase b 3 b 3 new regexp s b 3 s return b h function a b var c a attributes return b 1 a nodename tolowercase b 1 b 2 c id value b 2 b 3 b 3 test c class value i function a return f event special hover a a replace b mouseenter 1 mouseleave 1";"CODE"
"f event add function a c d e g var h i j k l m n o p q r s if a nodetype 3 a nodetype 8 c d h f data a d handler p d d p handler d guid d guid f guid j h events j h events j i h handle i h handle i function a return typeof f undefined a f event triggered a type f event dispatch apply i elem arguments b i elem a c f trim i c split for k 0 k c length k l a exec c k m l 1 n l 2 split sort s f event special m m g s delegatetype s bindtype m s f event special m o f extend type m origtype l 1 data e handler d guid d guid selector g quick g g namespace n join p r j m if r r j m r delegatecount 0 if s setup s setup call a e n i 1 a addeventlistener a addeventlistener m i 1 a attachevent a attachevent on m i s add s add call a o o handler guid o handler guid d guid g r splice r delegatecount 0 o r push o f event global m 0 a null global remove function a b c d e var g f hasdata a f data a h i j k l m n o p q r s if g o g events b f trim i b split for h 0 h b length h i a exec b h j k i 1 l i 2 if j for j in o f event remove a j b h c d 0 continue p f event special j j d p delegatetype p bindtype j r o j m r length l l new regexp l split sort join null for n 0 n r length n s r n e k s origtype c c guid s guid l l test s namespace d d s selector d s selector r splice n 1 s selector r delegatecount p remove p remove call a s r length 0 m r length p teardown p teardown call a l 1 f removeevent a j g handle delete o j f isemptyobject o q g handle q q elem null f removedata a events handle 0 customevent getdata 0 setdata 0 changedata 0 trigger function c d e g if e e nodetype 3 e nodetype 8 var h c type c i j k l m n o p q r s if e test h f event triggered return h indexof 0 h h slice 0 1 k 0 h indexof 0 i h split h i shift i sort if e f event customevent h f event global h return c typeof c object c f expando c new f event h c new f event h c type h c istrigger 0 c exclusive k c namespace i join c namespace re c namespace new regexp i join null o h indexof 0 on h if e j f cache for l in j j l events j l events h f event trigger c d j l handle elem 0 return c result b c target c target e d d null f makearray d d unshift c p f event special h if p trigger p trigger apply e d 1 return r e p bindtype h if g p nobubble f iswindow e s p delegatetype h m e test s h e e parentnode n null for m m m parentnode r push m s n m n n e ownerdocument r push n defaultview n parentwindow a s for l 0 l r length c ispropagationstopped l m r l 0 c type r l 1 q f data m events c type f data m handle q q apply m d q o m o q f acceptdata m q apply m d 1 c preventdefault c type h g c isdefaultprevented p default p default apply e ownerdocument d 1 h click f nodename e a f acceptdata e o e h h focus h blur c target offsetwidth 0 f iswindow e n e o n e o null f event triggered h e h f event triggered b n e o n return c result dispatch function c c f event fix c a event var d f data this events c type e d delegatecount g slice call arguments 0 h c exclusive c namespace i j k l m n o p q r s t g 0 c c delegatetarget this if e c target disabled c button c type click m f this m context this ownerdocument this for l c target l this l l parentnode this o q m 0 l for j 0 j e j r d j s r selector o s b o s r quick h l r quick m is s o s q push r q length i push elem l matches q d length e i push elem this matches d slice e for j 0 j i length c ispropagationstopped j p i j c currenttarget p elem for k 0 k p matches length c isimmediatepropagationstopped k r p matches k if h c namespace r namespace c namespace re c namespace re test r namespace c data r data c handleobj r n f event special r origtype handle r handler apply p elem g n b c result n n 1 c preventdefault c stoppropagation return c result props attrchange attrname relatednode srcelement altkey bubbles cancelable ctrlkey currenttarget eventphase metakey relatedtarget shiftkey target timestamp view which split fixhooks keyhooks props char charcode key keycode split filter function a b a which null a which b charcode null b charcode b keycode return a mousehooks props button buttons clientx clienty fromelement offsetx offsety pagex pagey screenx screeny toelement split filter function a d var e f g h d button i d fromelement a pagex null d clientx null e a target ownerdocument c f e documentelement g e body a pagex d clientx f f scrollleft g g scrollleft 0 f f clientleft g g clientleft 0 a pagey d clienty f f scrolltop g g scrolltop 0 f f clienttop g g clienttop 0 a relatedtarget i a relatedtarget i a target d toelement i a which h b a which h 1 1 h 2 3 h 4 2 0 return a fix function a if a f expando return a var d e g a h f event fixhooks a type i h props this props concat h props this props a f event g for d i length d e i d a e g e a target a target g srcelement c a target nodetype 3 a target a target parentnode a metakey b a metakey a ctrlkey return h filter h filter a g a special ready setup f bindready load nobubble 0 focus delegatetype focusin blur delegatetype focusout beforeunload setup function a b c f iswindow this this onbeforeunload c teardown function a b this onbeforeunload b this onbeforeunload null simulate function a b c d var e f extend new f event c type a issimulated 0 originalevent d f event trigger e null b f event dispatch call b e e isdefaultprevented c preventdefault f event handle f event dispatch f removeevent c removeeventlistener function a b c a removeeventlistener a removeeventlistener b c 1 function a b c a detachevent a detachevent on b c f event function a b if this instanceof f event return new f event a b a a type this originalevent a this type a type this isdefaultprevented a defaultprevented a returnvalue 1 a getpreventdefault a getpreventdefault k j this type a b f extend this b this timestamp a a timestamp f now this f expando 0 f event prototype preventdefault function this isdefaultprevented k var a this originalevent a a preventdefault a preventdefault a returnvalue 1 stoppropagation function this ispropagationstopped k var a this originalevent a a stoppropagation a stoppropagation a cancelbubble 0 stopimmediatepropagation function this isimmediatepropagationstopped k this stoppropagation isdefaultprevented j ispropagationstopped j isimmediatepropagationstopped j f each mouseenter mouseover mouseleave mouseout function a b f event special a delegatetype b bindtype b handle function a var c this d a relatedtarget e a handleobj g e selector h if d d c f contains c d a type e origtype h e handler apply this arguments a type b return h f support submitbubbles f event special submit setup function if f nodename this form return 1 f event add this click submit keypress submit function a var c a target d f nodename c input f nodename c button c form b d d submit attached f event add d submit submit function a this parentnode a istrigger f event simulate submit this parentnode a 0 d submit attached 0 teardown function if f nodename this form return 1 f event remove this submit f support changebubbles f event special change setup function if z test this nodename if this type checkbox this type radio f event add this propertychange change function a a originalevent propertyname checked this just changed 0 f event add this click change function a this just changed a istrigger this just changed 1 f event simulate change this a 0 return 1 f event add this beforeactivate change function a var b a target z test b nodename b change attached f event add b change change function a this parentnode a issimulated a istrigger f event simulate change this parentnode a 0 b change attached 0 handle function a var b a target if this b a issimulated a istrigger b type radio b type checkbox return a handleobj handler apply this arguments teardown function f event remove this change return z test this nodename f support focusinbubbles f each focus focusin blur focusout function a b var d 0 e function a f event simulate b a target f event fix a 0 f event special b setup function d 0 c addeventlistener a e 0 teardown function d 0 c removeeventlistener a e 0 f fn extend on function a c d e g var h i if typeof a object typeof c string d c c b for i in a this on i c d a i g return this d null e null e c d c b e null typeof c string e d d b e d d c c b if e 1 e j else if e return this g 1 h e e function a f off a return h apply this arguments e guid h guid h guid f guid return this each function f event add this a e d c one function a b c d return this on call this a b c d 1 off function a c d if a a preventdefault a handleobj var e a handleobj f a delegatetarget off e namespace e type e namespace e type e selector e handler return this if typeof a object for var g in a this off g c a g return this if c 1 typeof c function d c c b d 1 d j return this each function f event remove this a d c bind function a b c return this on a null b c unbind function a b return this off a null b live function a b c f this context on a this selector b c return this die function a b f this context off a this selector b return this delegate function a b c d return this on b a c d undelegate function a b c return arguments length 1 this off a this off b a c trigger function a b return this each function f event trigger a b this triggerhandler function a b if this 0 return f event trigger a b this 0 0 toggle function a var b arguments c a guid f guid d 0 e function c var e f data this lasttoggle a guid 0 d f data this lasttoggle a guid e 1 c preventdefault return b e apply this arguments 1 e guid c while d b length b d guid c return this click e hover function a b return this mouseenter a mouseleave b a f each blur focus focusin focusout load resize scroll unload click dblclick mousedown mouseup mousemove mouseover mouseout mouseenter mouseleave change select submit keydown keypress keyup error contextmenu split function a b f fn b function a c c null c a a null return arguments length 0 this on b null a c this trigger b f attrfn f attrfn b 0 c test b f event fixhooks b f event keyhooks d test b f event fixhooks b f event mousehooks function function x a b c e f g for var h 0 i e length h i h var j e h if j var k 1 j j a while j if j d c k e j sizset break if j nodetype 1 g j d c j sizset h if typeof b string if j b k 0 break else if m filter b j length 0 k j break j j a e h k function w a b c e f g for var h 0 i e length h i h var j e h if j var k 1 j j a while j if j d c k e j sizset break j nodetype 1 g j d c j sizset h if j nodename tolowercase b k j break j j a e h k var a s s r n g d sizcache math random replace e 0 g object prototype tostring h 1 i 0 j g k r n g l w 0 0 sort function i 1 return 0 var m function b d e f e e d d c var h d if d nodetype 1 d nodetype 9 return if b typeof b string return e var i j k l n q r t u 0 v m isxml d w x b do a exec i a exec x if i x i 3 w push i 1 if i 2 l i 3 break while i if w length 1 p exec b if w length 2 o relative w 0 j y w 0 w 1 d f else j o relative w 0 d m w shift d while w length b w shift o relative b b w shift j y b j f else f w length 1 d nodetype 9 v o match id test w 0 o match id test w w length 1 n m find w shift d v d n expr m filter n expr n set 0 n set 0 if d n f expr w pop set s f m find w pop w length 1 w 0 w 0 d parentnode d parentnode d v j n expr m filter n expr n set n set w length 0 k s j u 1 while w length q w pop r q o relative q r w pop q r null r d o relative q k r v else k w k k j k m error q b if g call k object array if u e push apply e k else if d d nodetype 1 for t 0 k t null t k t k t 0 k t nodetype 1 m contains d k t e push j t else for t 0 k t null t k t k t nodetype 1 e push j t else s k e l m l h e f m uniquesort e return e m uniquesort function a if u h i a sort u if h for var b 1 b a length b a b a b 1 a splice b 1 return a m matches function a b return m a null null b m matchesselector function a b return m b null null a length 0 m find function a b c var d e f g h i if a return for e 0 f o order length e f e h o order e if g o leftmatch h exec a i g 1 g splice 1 1 if i substr i length 1 g 1 g 1 replace j d o find h g b c if d null a a replace o match h break d d typeof b getelementsbytagname undefined b getelementsbytagname return set d expr a m filter function a c d e var f g h i j k l n p q a r s c t c c 0 m isxml c 0 while a c length for h in o filter if f o leftmatch h exec a null f 2 k o filter h l f 1 g 1 f splice 1 1 if l substr l length 1 continue s r r if o prefilter h f o prefilter h f s d r e t if f g i 0 else if f 0 continue if f for n 0 j s n null n j i k j f n s p e i d i null p g 0 s n 1 p r push j g 0 if i b d s r a a replace o match h if g return break if a q if g null m error a else break q a return s m error function a throw new error syntax error unrecognized expression a var n m gettext function a var b c d a nodetype e if d if d 1 d 9 if typeof a textcontent string return a textcontent if typeof a innertext string return a innertext replace k for a a firstchild a a a nextsibling e n a else if d 3 d 4 return a nodevalue else for b 0 c a b b c nodetype 8 e n c return e o m selectors order id name tag match id w u00c0 uffff class w u00c0 uffff name name w u00c0 uffff attr s w u00c0 uffff s s s 3 w u00c0 uffff s tag w u00c0 uffff child only nth last first child s even odd d d n s s d s pos nth eq gt lt first last even odd d pseudo w u00c0 uffff 2 leftmatch attrmap class classname for htmlfor attrhandle href function a return a getattribute href type function a return a getattribute type relative function a b var c typeof b string d c l test b e c d d b b tolowercase for var f 0 g a length h f g f if h a f while h h previoussibling h nodetype 1 a f e h h nodename tolowercase b h 1 h b e m filter b a 0 function a b var c d typeof b string e 0 f a length if d l test b b b tolowercase for e f e c a e if c var g c parentnode a e g nodename tolowercase b g 1 else for e f e c a e c a e d c parentnode c parentnode b d m filter b a 0 function a b c var d f e g x typeof b string l test b b b tolowercase d b g w g parentnode b f a d c function a b c var d f e g x typeof b string l test b b b tolowercase d b g w g previoussibling b f a d c find id function a b c if typeof b getelementbyid undefined c var d b getelementbyid a 1 return d d parentnode d name function a b if typeof b getelementsbyname undefined var c d b getelementsbyname a 1 for var e 0 f d length e f e d e getattribute name a 1 c push d e return c length 0 null c tag function a b if typeof b getelementsbytagname undefined return b getelementsbytagname a 1 prefilter class function a b c d e f a a 1 replace j if f return a for var g 0 h h b g null g h e h classname h classname replace t n r g indexof a 0 c d push h c b g 1 return 1 id function a return a 1 replace j tag function a b return a 1 replace j tolowercase child function a if a 1 nth a 2 m error a 0 a 2 a 2 replace s g var b d n d exec a 2 even 2n a 2 odd 2n 1 d test a 2 0n a 2 a 2 a 2 b 1 b 2 1 0 a 3 b 3 0 else a 2 m error a 0 a 0 e return a attr function a b c d e f var g a 1 a 1 replace j f o attrmap g a 1 o attrmap g a 4 a 4 a 5 replace j a 2 a 4 a 4 return a pseudo function b c d e f if b 1 not if a exec b 3 length 1 w test b 3 b 3 m b 3 null null c else var g m filter b 3 c d 0 f d e push apply e g return 1 else if o match pos test b 0 o match child test b 0 return 0 return b pos function a a unshift 0 return a filters enabled function a return a disabled 1 a type hidden disabled function a return a disabled 0 checked function a return a checked 0 selected function a a parentnode a parentnode selectedindex return a selected 0 parent function a return a firstchild empty function a return a firstchild has function a b c return m c 3 a length header function a return h d i test a nodename text function a var b a getattribute type c a type return a nodename tolowercase input text c b c b null radio function a return a nodename tolowercase input radio a type checkbox function a return a nodename tolowercase input checkbox a type file function a return a nodename tolowercase input file a type password function a return a nodename tolowercase input password a type submit function a var b a nodename tolowercase return b input b button submit a type image function a return a nodename tolowercase input image a type reset function a var b a nodename tolowercase return b input b button reset a type button function a var b a nodename tolowercase return b input button a type b button input function a return input select textarea button i test a nodename focus function a return a a ownerdocument activeelement setfilters first function a b return b 0 last function a b c d return b d length 1 even function a b return b 2 0 odd function a b return b 2 1 lt function a b c return b c 3 0 gt function a b c return b c 3 0 nth function a b c return c 3 0 b eq function a b c return c 3 0 b filter pseudo function a b c d var e b 1 f o filters e if f return f a c b d if e contains return a textcontent a innertext n a indexof b 3 0 if e not var g b 3 for var h 0 i g length h i h if g h a return 1 return 0 m error e child function a b var c e f g h i j k b 1 l a switch k case only case first while l l previoussibling if l nodetype 1 return 1 if k first return 0 l a case last while l l nextsibling if l nodetype 1 return 1 return 0 case nth c b 2 e b 3 if c 1 e 0 return 0 f b 0 g a parentnode if g g d f a nodeindex i 0 for l g firstchild l l l nextsibling l nodetype 1 l nodeindex i g d f j a nodeindex e return c 0 j 0 j c 0 j c 0 id function a b return a nodetype 1 a getattribute id b tag function a b return b a nodetype 1 a nodename a nodename tolowercase b class function a b return a classname a getattribute class indexof b 1 attr function a b var c b 1 d m attr m attr a c o attrhandle c o attrhandle c a a c null a c a getattribute c e d f b 2 g b 4 return d null f f m attr d null f e g f e indexof g 0 f e indexof g 0 g f e g f e indexof g 0 f e substr e length g length g f e g e substr 0 g length 1 g 1 e d 1 pos function a b c d var e b 2 f o setfilters e if f return f a c b d p o match pos q function a b return b 0 1 for var r in o match o match r new regexp o match r source source o leftmatch r new regexp r n source o match r source replace d g q var s function a b a array prototype slice call a 0 if b b push apply b a return b return a try array prototype slice call c documentelement childnodes 0 0 nodetype catch t s function a b var c 0 d b if g call a object array array prototype push apply d a else if typeof a length number for var e a length c e c d push a c else for a c c d push a c return d var u v c documentelement comparedocumentposition u function a b if a b h 0 return 0 if a comparedocumentposition b comparedocumentposition return a comparedocumentposition 1 1 return a comparedocumentposition b 4 1 1 u function a b if a b h 0 return 0 if a sourceindex b sourceindex return a sourceindex b sourceindex var c d e f g a parentnode i b parentnode j g if g i return v a b if g return 1 if i return 1 while j e unshift j j j parentnode j i while j f unshift j j j parentnode c e length d f length for var k 0 k c k d k if e k f k return v e k f k return k c v a f k 1 v e k b 1 v function a b c if a b return c var d a nextsibling while d if d b return 1 d d nextsibling return 1 function var a c createelement div d script new date gettime e c documentelement a innerhtml a name d e insertbefore a e firstchild c getelementbyid d o find id function a c d if typeof c getelementbyid undefined d var e c getelementbyid a 1 return e e id a 1 typeof e getattributenode undefined e getattributenode id nodevalue a 1 e b o filter id function a b var c typeof a getattributenode undefined a getattributenode id return a nodetype 1 c c nodevalue b e removechild a e a null function var a c createelement div a appendchild c createcomment a getelementsbytagname length 0 o find tag function a b var c b getelementsbytagname a 1 if a 1 var d for var e 0 c e e c e nodetype 1 d push c e c d return c a innerhtml a href a a firstchild typeof a firstchild getattribute undefined a firstchild getattribute href o attrhandle href function a return a getattribute href 2 a null c queryselectorall function var a m b c createelement div d sizzle b innerhtml p class test p if b queryselectorall b queryselectorall test length 0 m function b e f g e e c if g m isxml e var h w w w exec b if h e nodetype 1 e nodetype 9 if h 1 return s e getelementsbytagname b f if h 2 o find class e getelementsbyclassname return s e getelementsbyclassname h 2 f if e nodetype 9 if b body e body return s e body f if h h 3 var i e getelementbyid h 3 if i i parentnode return s f if i id h 3 return s i f try return s e queryselectorall b f catch j else if e nodetype 1 e nodename tolowercase object var k e l e getattribute id n l d p e parentnode q s test b l n n replace g e setattribute id n q p e e parentnode try if q p return s e queryselectorall id n b f catch r finally l k removeattribute id return a b e f g for var e in a m e a e b null function var a c documentelement b a matchesselector a mozmatchesselector a webkitmatchesselector a msmatchesselector if b var d b call c createelement div div e 1 try b call c documentelement test sizzle catch f e 0 m matchesselector function a c c c replace s s g 1 if m isxml a try if e o match pseudo test c test c var f b call a c if f d a document a document nodetype 11 return f catch g return m c null null a length 0 function var a c createelement div a innerhtml div class test e div div class test div if a getelementsbyclassname a getelementsbyclassname e length 0 a lastchild classname e if a getelementsbyclassname e length 1 return o order splice 1 0 class o find class function a b c if typeof b getelementsbyclassname undefined c return b getelementsbyclassname a 1 a null c documentelement contains m contains function a b return a b a contains a contains b 0 c documentelement comparedocumentposition m contains function a b return a comparedocumentposition b 16 m contains function return 1 m isxml function a var b a a ownerdocument a 0 documentelement return b b nodename html 1 var y function a b c var d e f g b nodetype b b while d o match pseudo exec a f d 0 a a replace o match pseudo a o relative a a a for var h 0 i g length h i h m a g h e c return m filter f e m attr f attr m selectors attrmap f find m f expr m selectors f expr f expr filters f unique m uniquesort f text m gettext f isxmldoc m isxml f contains m contains var l until m parents prevuntil prevall n o p array prototype slice q f expr match pos r children 0 contents 0 next 0 prev 0 f fn extend find function a var b this c d if typeof a string return f a filter function for c 0 d b length c d c if f contains b c this return 0 var e this pushstack find a g h i for c 0 d this length c d c g e length f find a this c e if c 0 for h g h e length h for i 0 i g i if e i e h e splice h 1 break return e has function a var b f a return this filter function for var a 0 c b length a c a if f contains this b a return 0 not function a return this pushstack t this a 1 not a filter function a return this pushstack t this a 0 filter a is function a return a typeof a string q test a f a this context index this 0 0 f filter a this length 0 this filter a length 0 closest function a b var c d e g this 0 if f isarray a var h 1 while g g ownerdocument g b for d 0 d a length d f g is a d c push selector a d elem g level h g g parentnode h return c var i q test a typeof a string f a b this context 0 for d 0 e this length d e d g this d while g if i i index g 1 f find matchesselector g a c push g break g g parentnode if g g ownerdocument g b g nodetype 11 break c c length 1 f unique c c return this pushstack c closest a index function a if a return this 0 this 0 parentnode this prevall length 1 if typeof a string return f inarray this 0 f a return f inarray a jquery a 0 a this add function a b var c typeof a string f a b f makearray a a nodetype a a d f merge this get c return this pushstack s c 0 s d 0 d f unique d andself function return this add this prevobject f each parent function a var b a parentnode return b b nodetype 11 b null parents function a return f dir a parentnode parentsuntil function a b c return f dir a parentnode c next function a return f nth a 2 nextsibling prev function a return f nth a 2 previoussibling nextall function a return f dir a nextsibling prevall function a return f dir a previoussibling nextuntil function a b c return f dir a nextsibling c prevuntil function a b c return f dir a previoussibling c siblings function a return f sibling a parentnode firstchild a children function a return f sibling a firstchild contents function a return f nodename a iframe a contentdocument a contentwindow document f makearray a childnodes function a b f fn a function c d var e f map this b c l test a d c d typeof d string e f filter d e e this length 1 r a f unique e e this length 1 n test d m test a e e reverse return this pushstack e a p call arguments join f extend filter function a b c c a not a return b length 1 f find matchesselector b 0 a b 0 f find matches a b dir function a c d var e g a c while g g nodetype 9 d b g nodetype 1 f g is d g nodetype 1 e push g g g c return e nth function a b c d b b 1 var e 0 for a a a c if a nodetype 1 e b break return a sibling function a b var c for a a a nextsibling a nodetype 1 a b c push a return c var v abbr article aside audio canvas datalist details figcaption figure footer header hgroup mark meter nav output progress section summary time video w jquery d d null g x s y area br col embed hr img input link meta param w ig z w tbody i w ba script style i bb script object embed option style i bc new regexp v i bd checked s s checked i be java ecma script i bf s cdata bg option 1 select multiple multiple select legend 1 fieldset fieldset thead 1 table table tr 2 table tbody tbody table td 3 table tbody tr tr tbody table col 2 table tbody tbody colgroup colgroup table area 1 map map default 0 bh u c bg optgroup bg option bg tbody bg tfoot bg colgroup bg caption bg thead bg th bg td f support htmlserialize bg default 1 div div div f fn extend text function a if f isfunction a return this each function b var c f this c text a call this b c text if typeof a object a b return this empty append this 0 this 0 ownerdocument c createtextnode a return f text this wrapall function a if f isfunction a return this each function b f this wrapall a call this b if this 0 var b f a this 0 ownerdocument eq 0 clone 0 this 0 parentnode b insertbefore this 0 b map function var a this while a firstchild a firstchild nodetype 1 a a firstchild return a append this return this wrapinner function a if f isfunction a return this each function b f this wrapinner a call this b return this each function var b f this c b contents c length c wrapall a b append a wrap function a var b f isfunction a return this each function c f this wrapall b a call this c a unwrap function return this parent each function f nodename this body f this replacewith this childnodes end append function return this dommanip arguments 0 function a this nodetype 1 this appendchild a prepend function return this dommanip arguments 0 function a this nodetype 1 this insertbefore a this firstchild before function if this 0 this 0 parentnode return this dommanip arguments 1 function a this parentnode insertbefore a this if arguments length var a f clean arguments a push apply a this toarray return this pushstack a before arguments after function if this 0 this 0 parentnode return this dommanip arguments 1 function a this parentnode insertbefore a this nextsibling if arguments length var a this pushstack this after arguments a push apply a f clean arguments return a remove function a b for var c 0 d d this c null c if a f filter a d length b d nodetype 1 f cleandata d getelementsbytagname f cleandata d d parentnode d parentnode removechild d return this empty function";"CODE"
"for var a 0 b b this a null a b nodetype 1 f cleandata b getelementsbytagname while b firstchild b removechild b firstchild return this clone function a b a a null 1 a b b null a b return this map function return f clone this a b html function a if a b return this 0 this 0 nodetype 1 this 0 innerhtml replace w null if typeof a string ba test a f support leadingwhitespace x test a bg z exec a 1 tolowercase a a replace y 1 2 try for var c 0 d this length c d c this c nodetype 1 f cleandata this c getelementsbytagname this c innerhtml a catch e this empty append a else f isfunction a this each function b var c f this c html a call this b c html this empty append a return this replacewith function a if this 0 this 0 parentnode if f isfunction a return this each function b var c f this d c html c replacewith a call this b d typeof a string a f a detach return this each function var b this nextsibling c this parentnode f this remove b f b before a f c append a return this length this pushstack f f isfunction a a a replacewith a this detach function a return this remove a 0 dommanip function a c d var e g h i j a 0 k if f support checkclone arguments length 3 typeof j string bd test j return this each function f this dommanip a c d 0 if f isfunction j return this each function e var g f this a 0 j call this e c g html b g dommanip a c d if this 0 i j j parentnode f support parentnode i i nodetype 11 i childnodes length this length e fragment i e f buildfragment a this k h e fragment h childnodes length 1 g h h firstchild g h firstchild if g c c f nodename g tr for var l 0 m this length n m 1 l m l d call c bi this l g this l e cacheable m 1 l n f clone h 0 0 h k length f each k bp return this f buildfragment function a b d var e g h i j a 0 b b 0 i b 0 ownerdocument b 0 i createdocumentfragment i c a length 1 typeof j string j length 512 i c j charat 0 bb test j f support checkclone bd test j f support html5clone bc test j g 0 h f fragments j h h 1 e h e e i createdocumentfragment f clean a i e d g f fragments j h e 1 return fragment e cacheable g f fragments f each appendto append prependto prepend insertbefore before insertafter after replaceall replacewith function a b f fn a function c var d e f c g this length 1 this 0 parentnode if g g nodetype 11 g childnodes length 1 e length 1 e b this 0 return this for var h 0 i e length h i h var j h 0 this clone 0 this get f e h b j d d concat j return this pushstack d a e selector f extend clone function a b c var d e g h f support html5clone bc test a nodename a clonenode 0 bo a if f support nocloneevent f support noclonechecked a nodetype 1 a nodetype 11 f isxmldoc a bk a h d bl a e bl h for g 0 d g g e g bk d g e g if b bj a h if c d bl a e bl h for g 0 d g g bj d g e g d e null return h clean function a b d e var g b b c typeof b createelement undefined b b ownerdocument b 0 b 0 ownerdocument c var h i for var j 0 k k a j null j typeof k number k if k continue if typeof k string if test k k b createtextnode k else k k replace y 1 2 var l z exec k 1 tolowercase m bg l bg default n m 0 o b createelement div b c bh appendchild o u b appendchild o o innerhtml m 1 k m 2 while n o o lastchild if f support tbody var p test k q l table p o firstchild o firstchild childnodes m 1 table p o childnodes for i q length 1 i 0 i f nodename q i tbody q i childnodes length q i parentnode removechild q i f support leadingwhitespace x test k o insertbefore b createtextnode x exec k 0 o firstchild k o childnodes var r if f support appendchecked if k 0 typeof r k length number for i 0 i r i bn k i else bn k k nodetype h push k h f merge h k if d g function a return a type be test a type for j 0 h j j if e f nodename h j script h j type h j type tolowercase text javascript e push h j parentnode h j parentnode removechild h j h j else if h j nodetype 1 var s f grep h j getelementsbytagname script g h splice apply h j 1 0 concat s d appendchild h j return h cleandata function a var b c d f cache e f event special g f support deleteexpando for var h 0 i i a h null h if i nodename f nodata i nodename tolowercase continue c i f expando if c b d c if b b events for var j in b events e j f event remove i j f removeevent i j b handle b handle b handle elem null g delete i f expando i removeattribute i removeattribute f expando delete d c var bq alpha i br opacity bs a z ms g bt d px i bu d bv de bw position absolute visibility hidden display block bx left right by top bottom bz ba bb f fn css function a c if arguments length 2 c b return this return f access this a c 0 function a c d return d b f style a c d f css a c f extend csshooks opacity get function a b if b var c bz a opacity opacity return c 1 c return a style opacity cssnumber fillopacity 0 fontweight 0 lineheight 0 opacity 0 orphans 0 widows 0 zindex 0 zoom 0 cssprops float f support cssfloat cssfloat stylefloat style function a c d e if a a nodetype 3 a nodetype 8 a style var g h i f camelcase c j a style k f csshooks i c f cssprops i i if d b if k get in k g k get a 1 e b return g return j c h typeof d h string g bv exec d d g 1 1 g 2 parsefloat f css a c h number if d null h number isnan d return h number f cssnumber i d px if k set in k d k set a d b try j c d catch l css function a c d var e g c f camelcase c g f csshooks c c f cssprops c c c cssfloat c float if g get in g e g get a 0 d b return e if bz return bz a c swap function a b c var d for var e in b d e a style e a style e b e c call a for e in b a style e d e f curcss f css f each height width function a b f csshooks b get function a c d var e if c if a offsetwidth 0 return bc a b d f swap a bw function e bc a b d return e set function a b if bt test b return b b parsefloat b if b 0 return b px f support opacity f csshooks opacity get function a b return br test b a currentstyle a currentstyle filter a style filter parsefloat regexp 1 100 b 1 set function a b var c a style d a currentstyle e f isnumeric b alpha opacity b 100 g d d filter c filter c zoom 1 if b 1 f trim g replace bq c removeattribute filter if d d filter return c filter bq test g g replace bq e g e f function f support reliablemarginright f csshooks marginright get function a b var c f swap a display inline block function b c bz a margin right marginright c a style marginright return c c defaultview c defaultview getcomputedstyle ba function a b var c d e b b replace bs 1 tolowercase d a ownerdocument defaultview e d getcomputedstyle a null c e getpropertyvalue b c f contains a ownerdocument documentelement a c f style a b return c c documentelement currentstyle bb function a b var c d e f a currentstyle a currentstyle b g a style f null g e g b f e bt test f bu test f c g left d a runtimestyle a runtimestyle left d a runtimestyle left a currentstyle left g left b fontsize 1em f 0 f g pixelleft px g left c d a runtimestyle left d return f auto f bz ba bb f expr f expr filters f expr filters hidden function a var b a offsetwidth c a offsetheight return b 0 c 0 f support reliablehiddenoffsets a style a style display f css a display none f expr filters visible function a return f expr filters hidden a var bd 20 g be bf r n g bg bh t r n r mg bi color date datetime datetime local email hidden month number password range search tel text time url week i bj about app app storage extension file res widget bk get head bl bm bn script b script script gi bo select textarea i bp s bq br w d bs f fn load bt bu bv bw bx try bv e href catch by bv c createelement a bv href bv bv href bw br exec bv tolowercase f fn extend load function a c d if typeof a string bs return bs apply this arguments if this length return this var e a indexof if e 0 var g a slice e a length a a slice 0 e var h get c f isfunction c d c c b typeof c object c f param c f ajaxsettings traditional h post var i this f ajax url a type h datatype html data c complete function a b c c a responsetext a isresolved a done function a c a i html g f div append c replace bn find g c d i each d c b a return this serialize function return f param this serializearray serializearray function return this map function return this elements f makearray this elements this filter function return this name this disabled this checked bo test this nodename bi test this type map function a b var c f this val return c null null f isarray c f map c function a c return name b name value a replace bf r n name b name value c replace bf r n get f each ajaxstart ajaxstop ajaxcomplete ajaxerror ajaxsuccess ajaxsend split function a b f fn b function a return this on b a f each get post function a c f c function a d e g f isfunction d g g e e d d b return f ajax type c url a data d success e datatype g f extend getscript function a c return f get a b c script getjson function a b c return f get a b c json ajaxsetup function a b b b a f ajaxsettings b a a f ajaxsettings b a b return a ajaxsettings url bv islocal bj test bw 1 global 0 type get contenttype application x www form urlencoded processdata 0 async 0 accepts xml application xml text xml html text html text text plain json application json text javascript bx contents xml xml html html json json responsefields xml responsexml text responsetext converters text a string text html 0 text json f parsejson text xml f parsexml flatoptions context 0 url 0 ajaxprefilter bz bt ajaxtransport bz bu ajax function a c function w a c l m if s 2 s 2 q cleartimeout q p b n m v readystate a 0 4 0 var o r u w c x l cb d v l b y z if a 200 a 300 a 304 if d ifmodified if y v getresponseheader last modified f lastmodified k y if z v getresponseheader etag f etag k z if a 304 w notmodified o 0 else try r cc d x w success o 0 catch a w parsererror u a else u w if w a w error a 0 a 0 v status a v statustext c w o h resolvewith e r w v h rejectwith e v w u v statuscode j j b t g trigger ajax o success error v d o r u i firewith e v w t g trigger ajaxcomplete v d f active f event trigger ajaxstop typeof a object c a a b c c var d f ajaxsetup c e d context d g e d e nodetype e instanceof f f e f event h f deferred i f callbacks once memory j d statuscode k l m n o p q r s 0 t u v readystate 0 setrequestheader function a b if s var c a tolowercase a m c m c a l a b return this getallresponseheaders function return s 2 n null getresponseheader function a var c if s 2 if o o while c bh exec n o c 1 tolowercase c 2 c o a tolowercase return c b null c overridemimetype function a s d mimetype a return this abort function a a a abort p p abort a w 0 a return this h promise v v success v done v error v fail v complete i add v statuscode function a if a var b if s 2 for b in a j b j b a b else b a v status v then b b return this d url a d url replace bg replace bl bw 1 d datatypes f trim d datatype tolowercase split bp d crossdomain null r br exec d url tolowercase d crossdomain r r 1 bw 1 r 2 bw 2 r 3 r 1 http 80 443 bw 3 bw 1 http 80 443 d data d processdata typeof d data string d data f param d data d traditional b bt d c v if s 2 return 1 t d global d type d type touppercase d hascontent bk test d type t f active 0 f event trigger ajaxstart if d hascontent d data d url bm test d url d data delete d data k d url if d cache 1 var x f now y d url replace bq 1 x d url y y d url bm test d url x d data d hascontent d contenttype 1 c contenttype v setrequestheader content type d contenttype d ifmodified k k d url f lastmodified k v setrequestheader if modified since f lastmodified k f etag k v setrequestheader if none match f etag k v setrequestheader accept d datatypes 0 d accepts d datatypes 0 d accepts d datatypes 0 d datatypes 0 bx q 0 01 d accepts for u in d headers v setrequestheader u d headers u if d beforesend d beforesend call e v d 1 s 2 v abort return 1 for u in success 1 error 1 complete 1 v u d u p b bu d c v if p w 1 no transport else v readystate 1 t g trigger ajaxsend v d d async d timeout 0 q settimeout function v abort timeout d timeout try s 1 p send l w catch z if s 2 w 1 z else throw z return v param function a c var d e function a b b f isfunction b b b d d length encodeuricomponent a encodeuricomponent b c b c f ajaxsettings traditional if f isarray a a jquery f isplainobject a f each a function e this name this value else for var g in a ca g a g c e return d join replace bd f extend active 0 lastmodified etag var cd f now ce i f ajaxsetup jsonp callback jsonpcallback function return f expando cd f ajaxprefilter json jsonp function b c d var e b contenttype application x www form urlencoded typeof b data string if b datatypes 0 jsonp b jsonp 1 ce test b url e ce test b data var g h b jsonpcallback f isfunction b jsonpcallback b jsonpcallback b jsonpcallback i a h j b url k b data l 1 h 2 b jsonp 1 j j replace ce l b url j e k k replace ce l b data k j test j b jsonp h b url j b data k a h function a g a d always function a h i g f isfunction i a h g 0 b converters script json function g f error h was not called return g 0 b datatypes 0 json return script f ajaxsetup accepts script text javascript application javascript application ecmascript application x ecmascript contents script javascript ecmascript converters text script function a f globaleval a return a f ajaxprefilter script function a a cache b a cache 1 a crossdomain a type get a global 1 f ajaxtransport script function a if a crossdomain var d e c head c getelementsbytagname head 0 c documentelement return send function f g d c createelement script d async async a scriptcharset d charset a scriptcharset d src a url d onload d onreadystatechange function a c if c d readystate loaded complete test d readystate d onload d onreadystatechange null e d parentnode e removechild d d b c g 200 success e insertbefore d e firstchild abort function d d onload 0 1 var cf a activexobject function for var a in ch ch a 0 1 1 cg 0 ch f ajaxsettings xhr a activexobject function return this islocal ci cj ci function a f extend f support ajax a cors a withcredentials in a f ajaxsettings xhr f support ajax f ajaxtransport function c if c crossdomain f support cors var d return send function e g var h c xhr i j c username h open c type c url c async c username c password h open c type c url c async if c xhrfields for j in c xhrfields h j c xhrfields j c mimetype h overridemimetype h overridemimetype c mimetype c crossdomain e x requested with e x requested with xmlhttprequest try for j in e h setrequestheader j e j catch k h send c hascontent c data null d function a e var j k l m n try if d e h readystate 4 d b i h onreadystatechange f noop cf delete ch i if e h readystate 4 h abort else j h status l h getallresponseheaders m n h responsexml n n documentelement m xml n m text h responsetext try k h statustext catch o k j c islocal c crossdomain j m text 200 404 j 1223 j 204 catch p e g 1 p m g j k m l c async h readystate 4 d i cg cf ch ch f a unload cf ch i d h onreadystatechange d abort function d d 0 1 var ck cl cm cn toggle show hide co d a z i cp cq height margintop marginbottom paddingtop paddingbottom width marginleft marginright paddingleft paddingright opacity cr f fn extend show function a b c var d e if a a 0 return this animate cu show 3 a b c for var g 0 h this length g h g d this g d style e d style display f data d olddisplay e none e d style display e f css d display none f data d olddisplay cv d nodename for g 0 g h g d this g if d style e d style display if e e none d style display f data d olddisplay return this hide function a b c if a a 0 return this animate cu hide 3 a b c var d e g 0 h this length for g h g d this g d style e f css d display e none f data d olddisplay f data d olddisplay e for g 0 g h g this g style this g style display none return this toggle f fn toggle toggle function a b c var d typeof a boolean f isfunction a f isfunction b this toggle apply this arguments a null d this each function var b d a f this is hidden f this b show hide this animate cu toggle 3 a b c return this fadeto function a b c d return this filter hidden css opacity 0 show end animate opacity b a c d animate function a b c d function g e queue 1 f mark this var b f extend e c this nodetype 1 d c f this is hidden g h i j k l m n o b animatedproperties for i in a g f camelcase i i g a g a i delete a i h a g f isarray h b animatedproperties g h 1 h a g h 0 b animatedproperties g b specialeasing b specialeasing g b easing swing if h hide d h show d return b complete call this c g height g width b overflow this style overflow this style overflowx this style overflowy f css this display inline f css this float none f support inlineblockneedslayout cv this nodename inline this style display inline block this style zoom 1 b overflow null this style overflow hidden for i in a j new f fx this b i h a i cn test h o f data this toggle i h toggle d show hide 0 o f data this toggle i o show hide show j o j h k co exec h l j cur k m parsefloat k 2 n k 3 f cssnumber i px n px f style this i m 1 n l m 1 j cur l f style this i l n k 1 m k 1 1 1 m l j custom l m n j custom l h return 0 var e f speed b c d if f isemptyobject a return this each e complete 1 a f extend a return e queue 1 this each g this queue e queue g stop function a c d typeof a string d c c a a b c a 1 this queue a fx return this each function function h a b c var e b c f removedata a c 0 e stop d var b c 1 e f timers g f data this d f unmark 0 this if a null for b in g g b g b stop b indexof run b length 4 h this g b else g b a run g b stop h this g b for b e length b e b elem this a null e b queue a d e b 0 e b savestate c 0 e splice b 1 d c f dequeue this a f each slidedown cu show 1 slideup cu hide 1 slidetoggle cu toggle 1 fadein opacity show fadeout opacity hide fadetoggle opacity toggle function a b f fn a function a c d return this animate b a c d f extend speed function a b c var d a typeof a object f extend a complete c c b f isfunction a a duration a easing c b b f isfunction b b d duration f fx off 0 typeof d duration number d duration d duration in f fx speeds f fx speeds d duration f fx speeds default if d queue null d queue 0 d queue fx d old d complete d complete function a f isfunction d old d old call this d queue f dequeue this d queue a 1 f unmark this return d easing linear function a b c d return c d a swing function a b c d return math cos a math pi 2 5 d c timers fx function a b c this options b this elem a this prop c b orig b orig f fx prototype update function this options step this options step call this elem this now this f fx step this prop f fx step default this cur function if this elem this prop null this elem style this elem style this prop null return this elem this prop var a b f css this elem this prop return isnan a parsefloat b b b auto 0 b a custom function a c d function h a return e step a var e this g f fx this starttime cr cs this end c this now this start a this pos this state 0 this unit d this unit f cssnumber this prop px h queue this options queue h elem this elem h savestate function e options hide f data e elem fxshow e prop b f data e elem fxshow e prop e start h f timers push h cp cp setinterval g tick g interval show function var a f data this elem fxshow this prop this options orig this prop a f style this elem this prop this options show 0 a b this custom this cur a this custom this prop width this prop height 1 0 this cur f this elem show hide function this options orig this prop f data this elem fxshow this prop f style this elem this prop this options hide 0 this custom this cur 0 step function a var b c d e cr cs g 0 h this elem i this options if a e i duration this starttime this now this end this pos this state 1 this update i animatedproperties this prop 0 for b in i animatedproperties i animatedproperties b 0 g 1 if g i overflow null f support shrinkwrapblocks f each x y function a b h style overflow b i overflow a i hide f h hide if i hide i show for b in i animatedproperties f style h b i orig b f removedata h fxshow b 0 f removedata h toggle b 0 d i complete d i complete 1 d call h return 1 i duration infinity this now e c e this starttime this state c i duration this pos f easing i animatedproperties this prop this state c 0 1 i duration this now this start this end this start this pos this update return 0 f extend f fx tick function var a b f timers c 0 for c b length c a b c a b c a b splice c 1 b length f fx stop interval 13 stop function clearinterval cp cp null speeds slow 600 fast 200 default 400 step opacity function a f style a elem opacity a now default function a a elem style a elem style a prop null a elem style a prop a now a unit a elem a prop a now f each width height function a b f fx step b function a f style a elem b math max 0 a now a unit f expr f expr filters f expr filters animated function a return f grep f timers function b return a b elem length var cw t able d h i cx body html i getboundingclientrect in c documentelement f fn offset function a var b this 0 c if a return this each function b f offset setoffset this a b if b b ownerdocument return null if b b ownerdocument body return f offset bodyoffset b try c b getboundingclientrect catch d var e b ownerdocument g e documentelement if c f contains g b return c top c top left c left top 0 left 0 var h e body i cy e j g clienttop h clienttop 0 k g clientleft h clientleft 0 l i pageyoffset f support boxmodel g scrolltop h scrolltop m i pagexoffset f support boxmodel g scrollleft h scrollleft n c top l j o c left m k return top n left o f fn offset function a var b this 0 if a return this each function b f offset setoffset this a b if b b ownerdocument return null if b b ownerdocument body return f offset bodyoffset b var c d b offsetparent e b g b ownerdocument h g documentelement i g body j g defaultview k j j getcomputedstyle b null b currentstyle l b offsettop m b offsetleft while b b parentnode b i b h if f support fixedposition k position fixed break c j j getcomputedstyle b null b currentstyle l b scrolltop m b scrollleft b d l b offsettop m b offsetleft f support doesnotaddborder f support doesaddborderfortableandcells cw test b nodename l parsefloat c bordertopwidth 0 m parsefloat c borderleftwidth 0 e d d b offsetparent f support subtractsborderforoverflownotvisible c overflow visible l parsefloat c bordertopwidth 0 m parsefloat c borderleftwidth 0 k c if k position relative k position static l i offsettop m i offsetleft f support fixedposition k position fixed l math max h scrolltop i scrolltop m math max h scrollleft i scrollleft return top l left m f offset bodyoffset function a var b a offsettop c a offsetleft f support doesnotincludemargininbodyoffset b parsefloat f css a margintop 0 c parsefloat f css a marginleft 0 return top b left c setoffset function a b c var d f css a position d static a style position relative var e f a g e offset h f css a top i f css a left j d absolute d fixed f inarray auto h i 1 k l m n j l e position m l top n l left m parsefloat h 0 n parsefloat i 0 f isfunction b b b call a c g b top null k top b top g top m b left null k left b left g left n using in b b using call a k e css k f fn extend position function if this 0 return null var a this 0 b this offsetparent c this offset d cx test b 0 nodename top 0 left 0 b offset c top parsefloat f css a margintop 0 c left parsefloat f css a marginleft 0 d top parsefloat f css b 0 bordertopwidth 0 d left parsefloat f css b 0 borderleftwidth 0 return top c top d top left c left d left offsetparent function return this map function var a this offsetparent c body while a cx test a nodename f css a position static a a offsetparent return a f each left top function a c var d scroll c f fn d function c var e g if c b e this 0 if e return null g cy e return g pagexoffset in g g a pageyoffset pagexoffset f support boxmodel g document documentelement d g document body d e d return this each function g cy this g g scrollto a f g scrollleft c a c f g scrolltop this d c f each height width function a c var d c tolowercase f fn inner c function var a this 0 return a a style parsefloat f css a d padding this d null f fn outer c function a var b this 0 return b b style parsefloat f css b d a margin border this d null f fn d function a var e this 0 if e return a null null this if f isfunction a return this each function b var c f this c d a call this b c d if f iswindow e var g e document documentelement client c h e document body return e document compatmode css1compat g h h client c g if e nodetype 9 return math max e documentelement client c e body scroll c e documentelement scroll c e body offset c e documentelement offset c if a b var i f css e d j parsefloat i return f isnumeric j j i return this css d typeof a string a a px a jquery a f typeof define function define amd define amd jquery define jquery function return f window";"CODE"
"raphael el popup function d k h g var c this paper this 0 paper f j b e a if c return switch this type case text case circle case ellipse b true break default b false d d null up d k k 5 f this getbbox h typeof h number h b f x f width 2 f x g typeof g number g b f y f height 2 f y e math max f width 2 k 0 a math max f height 2 k 0 this translate h f x b f width 2 0 g f y b f height 2 0 f this getbbox var i up m h g l k k e 0 a k k 0 0 1 k k l 0 f height a k k 0 0 1 k k l k 2 e 2 0 a k k 0 0 1 k k l 0 f height a k k 0 0 1 k k l e 0 z join down m h g l k k e 0 a k k 0 0 1 k k l 0 f height a k k 0 0 1 k k l k 2 e 2 0 a k k 0 0 1 k k l 0 f height a k k 0 0 1 k k l e 0 z join left m h g l k k 0 a a k k 0 0 1 k k l f width 0 a k k 0 0 1 k k l 0 k 2 a 2 a k k 0 0 1 k k l f width 0 a k k 0 0 1 k k l 0 a z join right m h g l k k 0 a a k k 0 0 1 k k l f width 0 a k k 0 0 1 k k l 0 k 2 a 2 a k k 0 0 1 k k l f width 0 a k k 0 0 1 k k l 0 a z join j up x b f width 2 y k 2 b f height 2 f height down x b f width 2 y k 2 b f height 2 f height left x k 2 b f width 2 f width y b f height 2 right x k 2 b f width 2 f width y b f height 2 d this translate j x j y return c path i d attr fill 000 stroke none insertbefore this node this this 0 raphael el tag function f b l k var i 3 e this paper this 0 paper if e return var c e path attr fill 000 stroke 000 j this getbbox m h a g switch this type case text case circle case ellipse a true break default a false f f 0 l typeof l number l a j x j width 2 j x k typeof k number k a j y j height 2 j y b b null 5 b h 0 5522 b if j height b 2 c attr path m l k b a b b 0 1 1 0 b 2 b b 0 1 1 0 b 2 m 0 b 2 i a b i b i 0 1 0 0 b i 2 l l b i k j height 2 i l j width 2 i 0 0 j height 2 i j width 2 i 0 l l k b i join else m math sqrt math pow b i 2 math pow j height 2 i 2 c attr path m l k b c h 0 b h b b b 0 h b h b b b h 0 b b h b b 0 h h b b b b m l m k j height 2 i a b i b i 0 1 0 0 j height 2 i l b i m j width 2 i 0 0 j height 2 i l l m k j height 2 i join f 360 f c rotate f l k if this attrs this attr this attrs x x cx l b i a this type text j width 0 j width 2 attr y a k k j height 2 this rotate f l k f 90 f 270 this attr this attrs x x cx l b i a j width j width 2 rotate 180 l k else if f 90 f 270 this translate l j x j width b i k j y j height 2 this rotate f 180 j x j width b i j y j height 2 else this translate l j x b i k j y j height 2 this rotate f j x b i j y j height 2 return c insertbefore this node this this 0 raphael el drop function d g f var e this getbbox c this paper this 0 paper a j b i h if c return switch this type case text case circle case ellipse a true break default a false d d 0 g typeof g number g a e x e width 2 e x f typeof f number f a e y e height 2 e y j math max e width e height math min e width e height b c path m g f l j 0 a j 0 4 j 0 4 0 1 0 g j 0 7 f j 0 7 z attr fill 000 stroke none rotate 22 5 d g f d d 90 math pi 180 i g j math sin d a 0 e width 2 h f j math cos d a 0 e height 2 this attrs this attr this attrs x x cx i attr this attrs y y cy h this translate i e x h e y return b insertbefore this node this this 0 raphael el flag function e k j var g 3 c this paper this 0 paper if c return var b c path attr fill 000 stroke 000 i this getbbox f i height 2 a switch this type case text case circle case ellipse a true break default a false e e 0 k typeof k number k a i x i width 2 i x j typeof j number j a i y i height 2 i y b attr path m k j l f g f g i width 2 g 0 0 i height 2 g i width 2 g 0 z join e 360 e b rotate e k j if this attrs this attr this attrs x x cx k f g a this type text i width 0 i width 2 attr y a j j i height 2 this rotate e k j e 90 e 270 this attr this attrs x x cx k f g a i width i width 2 rotate 180 k j else if e 90 e 270 this translate k i x i width f g j i y i height 2 this rotate e 180 i x i width f g i y i height 2 else this translate k i x f g j i y i height 2 this rotate e i x f g i y i height 2 return b insertbefore this node this this 0 raphael el label function var c this getbbox b this paper this 0 paper a math min 20 c width 10 c height 10 2 if b return return b rect c x a 2 c y a 2 c width a c height a a attr stroke none fill 000 insertbefore this node this this 0 raphael el blob function z j i var g this getbbox b math pi 180 n this paper this 0 paper r a q if n return switch this type case text case circle case ellipse a true break default a false r n path attr fill 000 stroke none z z 1 z 45 90 q math min g height g width j typeof j number j a g x g width 2 g x i typeof i number i a g y g height 2 g y var m math max g width q q 25 12 t math max g height q q 25 12 u j q math sin z 22 5 b b i q math cos z 22 5 b v j q math sin z 22 5 b d i q math cos z 22 5 b o v u 2 l d b 2 f m 2 e t 2 s math sqrt math abs f f e e f f l l e e o o f f l l e e o o c s f l e v u 2 a s e o f d b 2 r attr x c y a path m j i l v d a f e 0 1 1 u b z join this translate c g x g width 2 a g y g height 2 return r insertbefore this node this this 0 raphael fn label function a d b var c this set b this text a d b attr raphael g txtattr return c push b label b raphael fn popup function a f d b c var e this set d this text a f d attr raphael g txtattr return e push d popup b c d raphael fn tag function a f d c b var e this set d this text a f d attr raphael g txtattr return e push d tag c b d raphael fn flag function a e c b var d this set c this text a e c attr raphael g txtattr return d push c flag b c raphael fn drop function a e c b var d this set c this text a e c attr raphael g txtattr return d push c drop b c raphael fn blob function a e c b var d this set c this text a e c attr raphael g txtattr return d push c blob b c raphael el lighter function b b b 2 var a this attrs fill this attrs stroke this fs this fs a 0 a 1 a 0 raphael rgb2hsb raphael getrgb a 0 hex a 1 raphael rgb2hsb raphael getrgb a 1 hex a 0 b math min a 0 b b 1 a 0 s a 0 s b a 1 b math min a 1 b b 1 a 1 s a 1 s b this attr fill hsb a 0 h a 0 s a 0 b stroke hsb a 1 h a 1 s a 1 b return this raphael el darker function b b b 2 var a this attrs fill this attrs stroke this fs this fs a 0 a 1 a 0 raphael rgb2hsb raphael getrgb a 0 hex a 1 raphael rgb2hsb raphael getrgb a 1 hex a 0 s math min a 0 s b 1 a 0 b a 0 b b a 1 s math min a 1 s b 1 a 1 b a 1 b b this attr fill hsb a 0 h a 0 s a 0 b stroke hsb a 1 h a 1 s a 1 b return this raphael el resetbrightness function if this fs this attr fill this fs 0 stroke this fs 1 delete this fs return this function var c lighter darker resetbrightness a popup tag flag label drop blob for var b in a function d raphael st d function return raphael el d apply this arguments a b for var b in c function d raphael st d function for var e 0 e this length e this e d apply this e arguments return this c b raphael g shim stroke none fill 000 fill opacity 0 txtattr font 12px arial sans serif fill fff colors function var c 0 6 0 2 0 05 0 1333 0 75 0 a for var b 0 b 10 b if b c length a push hsb c b 75 75 else a push hsb c b c length 1 5 return a snapends function j k h var e j l k if e l return from e to l power 0 function m d return math abs d 0 5 0 25 d 0 5 math round d var g l e h a g c a b 0 if a while c b c g math pow 10 b math pow 10 b b else while a b b 1 a g math pow 10 b math pow 10 b b b b l m k math pow 10 b math pow 10 b if l k l m k 0 5 math pow 10 b math pow 10 b e m j b 0 0 0 5 math pow 10 b math pow 10 b return from e to l power b axis function p o k d e g g j h a q a a null 2 a h h t g g 10 q arguments arguments length 1 var c h h m p 0 5 o l 0 0 001 g 1 g 3 m p 0 5 o l 0 k m p o 0 5 l k 0 s this snapends d e g h s from z s to f s power e 0 w font 11px fontin sans fontin sans sans serif v q set i i z h g var n h m f 0 f 0 r k g if g 1 g 3 var b o u g 1 1 1 a 3 g 1 while b o k h h c c concat m p h h a g 1 a 2 b 0 5 l a 2 1 0 v push q text p u b j j e math round n n n n tofixed m attr w attr text anchor g 1 start end n i b r if math round b r o k h h c c concat m p h h a g 1 a 2 o k 0 5 l a 2 1 0 v push q text p u o k j j e math round n n n n tofixed m attr w attr text anchor g 1 start end else n h m f 0 f u g 1 1 a 9 g var c p r k g a 0 b 0 while c p k h h c c concat m c 0 5 o h a g a 2 l 0 a 2 1 v push a q text c o u j j e math round n n n n tofixed m attr w var l a getbbox if b l x 5 v pop v length 1 remove else b l x l width n i c r if math round c r p k h h c c concat m p k 0 5 o h a g a 2 l 0 a 2 1 v push q text p k o u j j e math round n n n n tofixed m attr w var k q path c k text v k all q set k v k remove function this text remove this constructor prototype remove call this return k labelise function a c b if a return a replace g function d f e if f return c tofixed f replace g length if e return c 100 b tofixed e replace g length else return c tofixed 0";"CODE"
"function function a g n var f g length n h 0 e f m 0 i while h g length e if e 0 m g h 1 e i push m f m g h e e f else m g h return i function d f e p n k j var h p f 2 g k p 2 q math atan p f math abs n e o math atan k p math abs n j q e n math pi q q o j n math pi o o var i math pi 2 q o math pi 2 2 s h math sin i q m h math cos i q r g math sin i o l g math cos i o return x1 p s y1 n m x2 p r y2 n l function b f p o e h a z j var s this j j if f raphael is a 0 array a a if f raphael is z 0 array z z var q j gutter 10 b math max a 0 length z 0 length t j symbol s j colors s colors v null p null ad f set t for var ac 0 l z length ac l ac b math max b z ac length var ae f set for ac 0 l z length ac l ac if j shade ae push f path attr stroke none fill s ac opacity j nostroke 1 0 3 if z ac length e 2 q z ac a z ac e 2 q b e 2 q if a ac a ac length e 2 q a ac a a ac e 2 q var w array prototype concat apply a u array prototype concat apply z u s snapends math min apply math w math max apply math w a 0 length 1 e u from o u to n s snapends math min apply math u math max apply math u z 0 length 1 c n from n n to z e q 2 o e 1 v h q 2 n c 1 var g f set if j axis var m j axis split s m 0 g push s axis p q o q e 2 q e o j axisxstep math floor e 2 q 20 2 f m 1 g push s axis p e q o h q h 2 q c n j axisystep math floor h 2 q 20 3 f m 2 g push s axis p q o h q e 2 q e o j axisxstep math floor e 2 q 20 0 f m 3 g push s axis p q o h q h 2 q c n j axisystep math floor h 2 q 20 1 f var m f set aa f set r for ac 0 l z length ac l ac if j nostroke m push r f path attr stroke s ac stroke width j width 2 stroke linejoin round stroke linecap round stroke dasharray j dash var g raphael is t array t ac t h f set t for var ab 0 w z ac length ab w ab var l p q a ac a 0 ab e z k o h q z ac ab c v raphael is g array g ab g h push f raphael is g array g ab g l k j width 2 3 attr fill s ac stroke none if j smooth if ab ab w 1 var r p q a ac a 0 ab 1 e z f o h q z ac ab 1 c v q p q a ac a 0 ab 1 e z d o h q z ac ab 1 c v af d r f l k q d t t concat af x1 af y1 l k af x2 af y2 if ab t m l k c l k else t t concat ab l m l k if j smooth t t concat l k l k aa push h if j shade ae ac attr path t concat l l o h q l p q a ac a 0 0 e z o h q z join j nostroke r attr path t join function k an var ak for var al 0 ap a length al ap al ak ak concat a al ak sort var aq ah for al 0 ap ak length al ap al ak al ak al 1 aq push ak al ah push p q ak al e z ak aq ap ak length var ag an f set for al 0 al ap al var y ah al ah al ah al 1 p 2 ao ah al 1 p e ah al 2 ah al ah al 1 p 2 x an x ag push x f rect y 1 o math max ao 1 1 h attr stroke none fill 000 opacity 0 x values x symbols f set x y x x ah al x axis ak al for var aj 0 am z length aj am aj aq a aj a 0 for var ai 0 y aq length ai y ai if aq ai ak al x values push z aj ai x y push o h q z aj ai c v x symbols push ad symbols aj ai an an call x an v ag function i al var ah al f set x for var aj 0 an z length aj an aj for var ai 0 ak z aj length ai ak ai var ag p q a aj a 0 ai e z am p q a aj a 0 ai ai 1 1 e z y o h q z aj ai c v al x ah push x f circle ag y math abs am ag 2 attr stroke none fill 000 opacity 0 x x ag x y y x value z aj ai x line ad lines aj x shade ad shades aj x symbol ad symbols aj ai x symbols ad symbols aj x axis a aj a 0 ai al al call x al p ah ad push m ae aa g v p ad lines m ad shades ae ad symbols aa ad axis g ad hovercolumn function j i v k v mouseover j mouseout i return this ad clickcolumn function i v k v click i return this ad hrefcolumn function y var ag f raphael is arguments 0 array arguments 0 arguments if arguments length 1 typeof y object for var j in y for var y 0 x v length y x y if v y axis j v y attr href y j v k for y 0 x ag length y x y v y v y attr href ag y return this ad hover function j i p i p mouseover j mouseout i return this ad click function i p i p click i return this ad each function i i i return this ad eachcolumn function i k i return this return ad var c function c prototype raphael g b prototype new c raphael fn linechart function f k g e j i h return new b this f k g e j i h";"CODE"
"function a var b 0 3 4 c hasownproperty d e f function g function a b return a b h i j n k function a b var c j d i e array prototype slice call arguments 2 f k listeners a l 0 m 1 n o p q r h s h a i 0 for var t 0 u f length t u t zindex in f t o push f t zindex f t zindex 0 p f t zindex f t o sort g while o l 0 n p o l q push n apply b e if i i d return q for t 0 t u t n f t if zindex in n if n zindex o l q push n apply b e if i break do l n p o l n q push n apply b e if i break while n else p n zindex n else q push n apply b e if i break i d h r return q length q null k listeners function a var b a split d c j f g h i k l m n o c p for i 0 k b length i k i n for l 0 m o length l m l c o l n g c b i c e h 2 while h f g h f n push f p p concat f f o n return p k on function a b var c a split d e j for var g 0 h c length g h g e e n e c g e c g n e e c g e f e f for g 0 h e f length g h g if e f g b return f e f push b return function a a a b zindex a k stop function i 1 k nt function a if a return new regexp a test h return h k off k unbind function a b var f a split d g h i k l m n o j for k 0 l f length k l k for m 0 m o length m i length 2 i m 1 g o m n if f k e g f k i push g f k else for h in g g c h i push g h o splice apply o i for k 0 l o length k l k g o k while g n if b if g f for m 0 n g f length m n m if g f m b g f splice m 1 break g f length delete g f for h in g n if g n c h g n h f var p g n h f for m 0 n p length m n m if p m b p splice m 1 break p length delete g n h f else delete g f for h in g n g n c h g n h f delete g n h f g g n k once function a b var c function var d b apply this arguments k unbind a c return d return k on a c k version b k tostring function return you are running eve b typeof module undefined module exports module exports k typeof define undefined define eve function return k a eve k this function function cf a for var b 0 b cy length b cy b el paper a cy splice b 1 function ce b d e f h i e q e var j k l m o p q t b ms u v w if f for y 0 z cy length y z y var x cy y if x el id d id x anim b x percent e cy splice y 1 l 1 k x d attr x totalorigin break else f v for var y 0 z b percents length y z y if b percents y e b percents y f b top e b percents y p b percents y 1 0 t t b top e p o b percents y 1 j b anim e break f d attr b anim b percents y if j if k for var a in j if j g a if u g a d paper customattributes g a u a d attr a u a null u a t a v a j a switch u a case c w a v a u a t break case colour u a a getrgb u a var b a getrgb v a w a r b r u a r t g b g u a g t b b b u a b t break case path var d br u a v a e d 1 u a d 0 w a for y 0 z u a length y z y w a y 0 for var f 1 g u a y length f g f w a y f e y f u a y f t break case transform var h d i ca h a v a if i u a i from v a i to w a w a real 0 for y 0 z u a length y z y w a y u a y 0 for f 1 g u a y length f g f w a y f v a y f u a y f t else var j d matrix new cb k transform h transform getbbox function return d getbbox 1 u a j a j b j c j d j e j f b k v a v a k transform w a k matrix a j a t k matrix b j b t k matrix c j c t k matrix d j d t k matrix e j e t k matrix f j f t break case csv var l r j a s c m r u a s c if a clip rect u a m w a y m length while y w a y l y u a y t v a l break default l n j a m n u a w a y d paper customattributes a length while y w a y l y 0 m y 0 t var o j easing p a easing formulas o if p p r o match n if p p length 5 var r p p function a return cc a r 1 r 2 r 3 r 4 t else p bf q j start b start new date x anim b percent e timestamp q start q b del 0 status 0 initstatus f 0 stop 1 ms t easing p from u diff w to v el d callback j callback prev p next o repeat i b times origin d attr totalorigin h cy push x if f k l x stop 0 x start new date t f if cy length 1 return ca l x start new date x ms f cy length 1 cz ca else k initstatus f k start new date k ms f eve raphael anim start d id d b function cd a b var c d this ms b this times 1 if a for var e in a a g e d q e a e c push q e c sort bd this anim d this top c c length 1 this percents c function cc a b c d e f function o a b var c d e f j k for e a k 0 k 8 k f m e a if z f b return e j 3 i e 2 h e g if z j 1e 6 break e e f j c 0 d 1 e a if e c return c if e d return d while c d f m e if z f a b return e a f c e d e e d c 2 c return e function n a b var c o a b return l c k c j c function m a return i a h a g a var g 3 b h 3 d b g i 1 g h j 3 c k 3 e c j l 1 j k return n a 1 200 f function cq return this x q this y q this width this height function cp return this x q this y function cb a b c d e f a null this a a this b b this c c this d d this e e this f f this a 1 this b 0 this c 0 this d 1 this e 0 this f 0 function bh b c d b a path2curve b c a path2curve c var e f g h i j k l m n o d 0 for var p 0 q b length p q p var r b p if r 0 m e i r 1 f j r 2 else r 0 c m e f concat r slice 1 e m 6 f m 7 m e f e f i j i j e i f j for var s 0 t c length s t s var u c s if u 0 m g k u 1 h l u 2 else u 0 c n g h concat u slice 1 g n 6 h n 7 n g h g h k l k l g k h l var v bg m n d if d o v else for var w 0 x v length w x w v w segment1 p v w segment2 s v w bez1 m v w bez2 n o o concat v return o function bg b c d var e a bezierbbox b f a bezierbbox c if a isbboxintersect e f return d 0 var g bb apply 0 b h bb apply 0 c i g 5 j h 5 k l m n d 0 for var o 0 o i 1 o var p a finddotsatsegment apply a b concat o i k push x p x y p y t o i for o 0 o j 1 o p a finddotsatsegment apply a c concat o j l push x p x y p y t o j for o 0 o i o for var q 0 q j q var r k o s k o 1 t l q u l q 1 v z s x r x 001 y x w z u x t x 001 y x x bd r x r y s x s y t x t y u x u y if x if m x x tofixed 4 x y tofixed 4 continue m x x tofixed 4 x y tofixed 4 var y r t z x v r v s v r v s t r t a t t z x w t w u w t w u t t t y 0 y 1 a 0 a 1 d n n push x x x y x y t1 y t2 a return n function bf a b return bg a b 1 function be a b return bg a b function bd a b c d e f g h if x a c y e g y a c x e g x b d y f h y b d x f h var i a d b c e g a c e h f g j a d b c f h b d e h f g k a c f h b d e g if k return var l i k m j k n l tofixed 2 o m tofixed 2 if n y a c tofixed 2 n x a c tofixed 2 n y e g tofixed 2 n x e g tofixed 2 o y b d tofixed 2 o x b d tofixed 2 o y f h tofixed 2 o x f h tofixed 2 return return x l y m function bc a b c d e f g h i if i 0 bb a b c d e f g h i var j 1 k j 2 l j k m n 01 m bb a b c d e f g h l while z m i n k 2 l m i 1 1 k m bb a b c d e f g h l return l function bb a b c d e f g h i i null i 1 i i 1 1 i 0 0 i var j i 2 k 12 l 0 1252 1252 0 3678 3678 0 5873 5873 0 7699 7699 0 9041 9041 0 9816 9816 m 2491 2491 2335 2335 2032 2032 1601 1601 1069 1069 0472 0472 n 0 for var o 0 o k o var p j l o j q ba p a c e g r ba p b d f h s q q r r n m o w sqrt s return j n function ba a b c d e var f 3 b 9 c 9 d 3 e g a f 6 b 12 c 6 d return a g 3 b 3 c function by a b var c for var d 0 e a length e 2 b d d 2 var f x a d 2 y a d 1 x a d y a d 1 x a d 2 y a d 3 x a d 4 y a d 5 b d e 4 d f 3 x a 0 y a 1 e 2 d f 2 x a 0 y a 1 f 3 x a 2 y a 3 f 0 x a e 2 y a e 1 e 4 d f 3 f 2 d f 0 x a d y a d 1 c push c f 0 x 6 f 1 x f 2 x 6 f 0 y 6 f 1 y f 2 y 6 f 1 x 6 f 2 x f 3 x 6 f 1 y 6 f 2 y f 3 y 6 f 2 x f 2 y return c function bx return this hex function bv a b c function d var e array prototype slice call arguments 0 f e join h d cache d cache i d count d count if h g f bu i f return c c h f h f i length 1e3 delete h i shift i push f h f a m b e return c c h f h f return d function bu a b for var c 0 d a length c d c if a c b return a push a splice c 1 0 function bm a if object a a return a var b new a constructor for var c in a a g c b c bm a c return b function a c if a is c function return b c eve on raphael domload c if a is c e return a engine create m a c splice 0 3 a is c 0 c add c var d array prototype slice call arguments 0 if a is d d length 1 function var e d pop return b e call a engine create m a d eve on raphael domload function e call a engine create m a d return a engine create m a arguments a version 2 1 0 a eve eve var b c d circle 1 rect 1 path 1 ellipse 1 text 1 image 1 e d g f prototype g hasownproperty h doc document win window i was object prototype g call h win raphael is h win raphael j function this ca this customattributes k l appendchild m apply n concat o createtouch in h doc p q r string s split t click dblclick mousedown mousemove mouseout mouseover mouseup touchstart touchmove touchend touchcancel s q u mousedown touchstart mousemove touchmove mouseup touchend v r prototype tolowercase w math x w max y w min z w abs a w pow b w pi c number d string e array f tostring g fill h object prototype tostring i j push k a isurl url i l s a f d 6 a f d 3 rgba s d s s d s s d s s d s hsba s d deg xb0 s s d s s d s s d s hsla s d deg xb0 s s d s s d s s d s s i m nan 1 infinity 1 infinity 1 n cubic bezier o w round p setattribute q parsefloat r parseint s r prototype touppercase t a availableattrs arrow end none arrow start none blur 0 clip rect 0 0 1e9 1e9 cursor default cx 0 cy 0 fill fff fill opacity 1 font 10px arial font family arial font size 10 font style normal font weight 400 gradient 0 height 0 href http raphaeljs com letter spacing 0 opacity 1 path m0 0 r 0 rx 0 ry 0 src stroke 000 stroke dasharray stroke linecap butt stroke linejoin butt stroke miterlimit 0 stroke opacity 1 stroke width 1 target blank text anchor middle title raphael transform width 0 x 0 y 0 u a availableanimattrs blur c clip rect csv cx c cy c fill colour fill opacity c font size c height c opacity c path path r c rx c ry c stroke colour stroke opacity c stroke width c transform transform width c x c y c v x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 g w x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 x hs 1 rg 1 y achlmqrstvxz gi z achlmrqstvz x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 d d e d x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 ig rstm x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 d d e d x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 ig d d e d x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 ig ba a radial gradient r x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 x09 x0a x0b x0c x0d x20 xa0 u1680 u180e u2000 u2001 u2002 u2003 u2004 u2005 u2006 u2007 u2008 u2009 u200a u202f u205f u3000 u2028 u2029 bb bc function a b return a key b key bd function a b return q a q b be function bf function a return a bg a rectpath function a b c d e if e return m a e b l c e 2 0 a e e 0 0 1 e e l 0 d e 2 a e e 0 0 1 e e l e 2 c 0 a e e 0 0 1 e e l 0 e 2 d a e e 0 0 1 e e z return m a b l c 0 l 0 d l c 0 z bh function a b c d d null d c return m a b m 0 d a c d 0 1 1 0 2 d a c d 0 1 1 0 2 d z bi a getpath path function a return a attr path circle function a var b a attrs return bh b cx b cy b r ellipse function a var b a attrs return bh b cx b cy b rx b ry rect function a var b a attrs return bg b x b y b width b height b r image function a var b a attrs return bg b x b y b width b height text function a var b a getbbox return bg b x b y b width b height bj a mappath function a b if b return a var c d e f g h i a br a for e 0 g a length e g e i a e for f 1 h i length f h f 2 c b x i f i f 1 d b y i f i f 1 i f c i f 1 d return a a g h a type h win svgangle h doc implementation hasfeature http www w3 org tr svg11 feature basicstructure 1 1 svg vml if a type vml var bk h doc createelement div bl bk innerhtml v shape adj 1 bl bk firstchild bl style behavior url default vml if bl typeof bl adj object return a type p bk null a svg a vml a type vml a paper j a fn k j prototype a prototype a id 0 a oid 0 a is function a b b v call b if b finite return m g a if b array return a instanceof array return b null a null b typeof a a null b object a object a b array array isarray array isarray a h call a slice 8 1 tolowercase b a angle function b c d e f g if f null var h b d i c e if h i return 0 return 180 w atan2 i h 180 b 360 360 return a angle b c f g a angle d e f g a rad function a return a 360 b 180 a deg function a return a 180 b 360 a snapto function b c d d a is d finite d 10 if a is b e var e b length while e if z b e c d return b e else b b var f c b if f d return c f if f b d return c f b return c var bn a createuuid function a b return function return xxxxxxxx xxxx 4xxx yxxx xxxxxxxxxxxx replace a b touppercase xy g function a var b w random 16 0 c a x b b 3 8 return c tostring 16 a setwindow function b eve raphael setwindow a h win b h win b h doc h win document a engine initwin a engine initwin h win var bo function b if a vml var c s s g d try var e new activexobject htmlfile e write body e close d e body catch f d createpopup document body var g d createtextrange bo bv function a try d style color r a replace c p var b g querycommandvalue forecolor b b 255 16 b 65280 b 16711680 16 return 000000 b tostring 16 slice 6 catch e return none else var i h doc createelement i i title rapha l colour picker i style display none h doc body appendchild i bo bv function a i style color a return h doc defaultview getcomputedstyle i p getpropertyvalue color return bo b bp function return hsb this h this s this b bq function return hsl this h this s this l br function return this hex bs function b c d c null a is b object r in b g in b b in b d b b c b g b b r if c null a is b d var e a getrgb b b e r c e g d e b if b 1 c 1 d 1 b 255 c 255 d 255 return b c d bt function b c d e b 255 c 255 d 255 var f r b g c b d hex a rgb b c d tostring br a is e finite f opacity e return f a color function b var c a is b object h in b s in b b in b c a hsb2rgb b b r c r b g c g b b c b b hex c hex a is b object h in b s in b l in b c a hsl2rgb b b r c r b g c g b b c b b hex c hex a is b string b a getrgb b a is b object r in b g in b b in b c a rgb2hsl b b h c h b s c s b l c l c a rgb2hsb b b v c b b hex none b r b g b b b h b s b v b l 1 b tostring br return b a hsb2rgb function a b c d this is a object h in a s in a b in a c a b b a s a a h d a o a 360 var e f g h i a a 360 60 i c b h i 1 z a 2 1 e f g c i a a e i h 0 0 h i a f h i i h 0 0 a g 0 0 h i i h a return bt e f g d a hsl2rgb function a b c d this is a object h in a s in a l in a c a l b a s a a h if a 1 b 1 c 1 a 360 b 100 c 100 a 360 var e f g h i a a 360 60 i 2 b c 5 c 1 c h i 1 z a 2 1 e f g c i 2 a a e i h 0 0 h i a f h i i h 0 0 a g 0 0 h i i h a return bt e f g d a rgb2hsb function a b c c bs a b c a c 0 b c 1 c c 2 var d e f g f x a b c g f y a b c d g 0 null f a b c g f b c a g 2 a b g 4 d d 360 6 60 360 e g 0 0 g f return h d s e b f tostring bp a rgb2hsl function a b c c bs a b c a c 0 b c 1 c c 2 var d e f g h i g x a b c h y a b c i g h d i 0 null g a b c i g b c a i 2 a b i 4 d d 360 6 60 360 f g h 2 e i 0 0 f 5 i 2 f i 2 2 f return h d s e l f tostring bq a path2string function return this join replace y 1 var bw a preload function a b var c h doc createelement img c style csstext position absolute left 9999em top 9999em c onload function b call this this onload null h doc body removechild this c onerror function h doc body removechild this h doc body appendchild c c src a a getrgb bv function b if b b r b indexof 1 return r 1 g 1 b 1 hex none error 1 tostring bx if b none return r 1 g 1 b 1 hex none tostring bx x g b tolowercase substring 0 2 b charat b bo b var c d e f h i j k b match l if k k 2 f r k 2 substring 5 16 e r k 2 substring 3 5 16 d r k 2 substring 1 3 16 k 3 f r i k 3 charat 3 i 16 e r i k 3 charat 2 i 16 d r i k 3 charat 1 i 16 k 4 j k 4 s w d q j 0 j 0 slice 1 d 2 55 e q j 1 j 1 slice 1 e 2 55 f q j 2 j 2 slice 1 f 2 55 k 1 tolowercase slice 0 4 rgba h q j 3 j 3 j 3 slice 1 h 100 if k 5 j k 5 s w d q j 0 j 0 slice 1 d 2 55 e q j 1 j 1 slice 1 e 2 55 f q j 2 j 2 slice 1 f 2 55 j 0 slice 3 deg j 0 slice 1 d 360 k 1 tolowercase slice 0 4 hsba h q j 3 j 3 j 3 slice 1 h 100 return a hsb2rgb d e f h if k 6 j k 6 s w d q j 0 j 0 slice 1 d 2 55 e q j 1 j 1 slice 1 e 2 55 f q j 2 j 2 slice 1 f 2 55 j 0 slice 3 deg j 0 slice 1 d 360 k 1 tolowercase slice 0 4 hsla h q j 3 j 3 j 3 slice 1 h 100 return a hsl2rgb d e f h k r d g e b f tostring bx k hex 16777216 f e 8 d 16 tostring 16 slice 1 a is h finite k opacity h return k return r 1 g 1 b 1 hex none error 1 tostring bx a a hsb bv function b c d return a hsb2rgb b c d hex a hsl bv function b c d return a hsl2rgb b c d hex a rgb bv function a b c return 16777216 c b 8 a 16 tostring 16 slice 1 a getcolor function a var b this getcolor start this getcolor start h 0 s 1 b a 75 c this hsb2rgb b h b s b b b h 075 b h 1 b h 0 b s 2 b s 0 this getcolor start h 0 s 1 b b b return c hex a getcolor reset function delete this start a parsepathstring function b if b return null var c bz b if c arr return bj c arr var d a 7 c 6 h 1 l 2 m 2 r 4 q 4 s 4 t 2 v 1 z 0 e a is b e a is b 0 e e bj b e length r b replace z function a b c var f g b tolowercase c replace function a b b f push b g m f length 2 e push b n f splice 0 2 g l b b m l l if g r e push b n f else while f length d g e push b n f splice 0 d g if d g break e tostring a path2string c arr bj e return e a parsetransformstring bv function b if b return null var c r 3 s 4 t 2 m 6 d a is b e a is b 0 e d bj b d length r b replace function a b c var e f v call b c replace function a b b e push b d push b n e d tostring a path2string return d var bz function a var b bz ps bz ps b a b a sleep 100 b a sleep 100 settimeout function for var c in b b g c c a b c sleep b c sleep delete b c return b a a finddotsatsegment function a b c d e f g h i var j 1 i k a j 3 l a j 2 m i i n m i o k a l 3 i c j 3 i i e n g p k b l 3 i d j 3 i i f n h q a 2 i c a m e 2 c a r b 2 i d b m f 2 d b s c 2 i e c m g 2 e c t d 2 i f d m h 2 f d u j a i c v j b i d x j e i g y j f i h z 90 w atan2 q s r t 180 b q s r t z 180 return x o y p m x q y r n x s y t start x u y v end x x y y alpha z a bezierbbox function b c d e f g h i a is b array b b c d e f g h i var j bq apply null b return x j min x y j min y x2 j max x y2 j max y width j max x j min x height j max y j min y a ispointinsidebbox function a b c return b a x b a x2 c a y c a y2 a isbboxintersect function b c var d a ispointinsidebbox return d c b x b y d c b x2 b y d c b x b y2 d c b x2 b y2 d b c x c y d b c x2 c y d b c x c y2 d b c x2 c y2 b x c x2 b x c x c x b x2 c x b x b y c y2 b y c y c y b y2 c y b y a pathintersection function a b return bh a b a pathintersectionnumber function a b return bh a b 1 a ispointinsidepath function b c d var e a pathbbox b return a ispointinsidebbox e c d bh b m c d h e x2 10 1 2 1 a removedfactory function a return function eve raphael log null rapha l you are calling to method a of removed object a var bi a pathbbox function a var b bz a if b bbox return b bbox if a return x 0 y 0 width 0 height 0 x2 0 y2 0 a br a var c 0 d 0 e f g for var h 0 i a length h i h g a h if g 0 m c g 1 d g 2 e push c f push d else var j bq c d g 1 g 2 g 3 g 4 g 5 g 6 e e n j min x j max x f f n j min y j max y c g 5 d g 6 var k y m 0 e l y m 0 f o x m 0 e p x m 0 f q x k y l x2 o y2 p width o k height p l b bbox bm q return q bj function b var c bm b c tostring a path2string return c bk a pathtorelative function b var c bz b if c rel return bj c rel if a is b e a is b b 0 e b a parsepathstring b var d e 0 f 0 g 0 h 0 i 0 b 0 0 m e b 0 1 f b 0 2 g e h f i d push m e f for var j i k b length j k j var l d j m b j if m 0 v call m 0 l 0 v call m 0 switch l 0 case a l 1 m 1 l 2 m 2 l 3 m 3 l 4 m 4 l 5 m 5 l 6 m 6 e tofixed 3 l 7 m 7 f tofixed 3 break case v l 1 m 1 f tofixed 3 break case m g m 1 h m 2 default for var n 1 o m length n o n l n m n n 2 e f tofixed 3 else l d j m 0 m g m 1 e h m 2 f for var p 0 q m length p q p d j p m p var r d j length switch d j 0 case z e g f h break case h e d j r 1 break case v f d j r 1 break default e d j r 2 f d j r 1 d tostring a path2string c rel bj d return d bl a pathtoabsolute function b var c bz b if c abs return bj c abs if a is b e a is b b 0 e b a parsepathstring b if b b length return m 0 0 var d e 0 f 0 g 0 h 0 i 0 b 0 0 m e b 0 1 f b 0 2 g e h f i d 0 m e f var j b length 3 b 0 0 m b 1 0 touppercase r b 2 0 touppercase z for var k l m i o b length m o m d push k l b m if l 0 s call l 0 k 0 s call l 0 switch k 0 case a k 1 l 1 k 2 l 2 k 3 l 3 k 4 l 4 k 5 l 5 k 6 l 6 e k 7 l 7 f break case v k 1 l 1 f break case h k 1 l 1 e break case r var p e f n l slice 1 for var q 2 r p length q r q p q p q e p q p q f d pop d d n by p j break case m g l 1 e h l 2 f default for q 1 r l length q r q k q l q q 2 e f else if l 0 r p e f n l slice 1 d pop d d n by p j k r n l slice 2 else for var s 0 t l length s t s k s l s switch k 0 case z e g f h break case h e k 1 break case v f k 1 break case m g k k length 2 h k k length 1 default e k k length 2 f k k length 1 d tostring a path2string c abs bj d return d bm function a b c d return a b c d c d bn function a b c d e f var g 1 3 h 2 3 return g a h c g b h d g e h c g f h d e f bo function a b c d e f g h i j var k b 120 180 l b 180 e 0 m o p bv function a b c var d a w cos c b w sin c e a w sin c b w cos c return x d y e if j o p a b l a o x b o y o p h i l h o x i o y var q w cos b 180 e r w sin b 180 e t a h 2 u b i 2 v t t c c u u d d v 1 v w sqrt v c v c d v d var x c c y d d a f g 1 1 w sqrt z x y x u u y t t x u u y t t c a c u d a h 2 d a d t c b i 2 e w asin b d d tofixed 9 f w asin i d d tofixed 9 e a c b e e f h c b f f e 0 e b 2 e f 0 f b 2 f g e f e e b 2 g f e f f b 2 else e j 0 f j 1 c j 2 d j 3 var g f e if z g k var h f i h j i f e k g f e 1 1 h c c w cos f i d d w sin f m bo h i c d e 0 g i j f h c d g f e var k w cos e l w sin e m w cos f n w sin f o w tan g 4 p 4 3 c o q 4 3 d o r a b s a p l b q k t h p n i q m u h i s 0 2 r 0 s 0 s 1 2 r 1 s 1 if j return s t u n m m s t u n m join s var v for var w 0 x m length w x w v w w 2 p m w 1 m w l y p m w m w 1 l x return v bp function a b c d e f g h i var j 1 i return x a j 3 a a j 2 3 i c j 3 i i e a i 3 g y a j 3 b a j 2 3 i d j 3 i i f a i 3 h bq bv function a b c d e f g h var i e 2 c a g 2 e c j 2 c a 2 e c k a c l j w sqrt j j 4 i k 2 i n j w sqrt j j 4 i k 2 i o b h p a g q z l 1e12 l 5 z n 1e12 n 5 l 0 l 1 q bp a b c d e f g h l p push q x o push q y n 0 n 1 q bp a b c d e f g h n p push q x o push q y i f 2 d b h 2 f d j 2 d b 2 f d k b d l j w sqrt j j 4 i k 2 i n j w sqrt j j 4 i k 2 i z l 1e12 l 5 z n 1e12 n 5 l 0 l 1 q bp a b c d e f g h l p push q x o push q y n 0 n 1 q bp a b c d e f g h n p push q x o push q y return min x y m 0 p y y m 0 o max x x m 0 p y x m 0 o br a path2curve bv function a b var c b bz a if b c curve return bj c curve var d bl a e b bl b f x 0 y 0 bx 0 by 0 x 0 y 0 qx null qy null g x 0 y 0 bx 0 by 0 x 0 y 0 qx null qy null h function a b var c d if a return c b x b y b x b y b x b y a 0 in t 1 q 1 b qx b qy null switch a 0 case m b x a 1 b y a 2 break case a a c n bo m 0 b x b y n a slice 1 break case s c b x b x b bx b x d b y b y b by b y a c c d n a slice 1 break case t b qx b x b x b qx b x b qy b y b y b qy b y a c n bn b x b y b qx b qy a 1 a 2 break case q b qx a 1 b qy a 2 a c n bn b x b y a 1 a 2 a 3 a 4 break case l a c n bm b x b y a 1 a 2 break case h a c n bm b x b y a 1 b y break case v a c n bm b x b y b x a 1 break case z a c n bm b x b y b x b y return a i function a b if a b length 7 a b shift var c a b while c length a splice b 0 c n c splice 0 6 a splice b 1 l x d length e e length 0 j function a b c f g a b a g 0 m b g 0 m b splice g 0 m f x f y c bx 0 c by 0 c x a g 1 c y a g 2 l x d length e e length 0 for var k 0 l x d length e e length 0 k l k d k h d k f i d k e e k h e k g e i e k j d e f g k j e d g f k var o d k p e e k q o length r e p length f x o q 2 f y o q 1 f bx q o q 4 f x f by q o q 3 f y g bx e q p r 4 g x g by e q p r 3 g y g x e p r 2 g y e p r 1 e c curve bj d return e d e d null bj bs a parsedots bv function b var c for var d 0 e b length d e d var f g b d match d f color a getrgb g 1 if f color error return null f color f color hex g 2 f offset g 2 c push f for d 1 e c length 1 d e d if c d offset var h q c d 1 offset 0 i 0 for var j d 1 j e j if c j offset i c j offset break i i 100 j e i q i var k i h j d 1 for d j d h k c d offset h return c bt a tear function a b a b top b top a prev a b bottom b bottom a next a next a next prev a prev a prev a prev next a next bu a tofront function a b b top a bt a b a next null a prev b top b top next a b top a bv a toback function a b b bottom a bt a b a next b bottom a prev null b bottom prev a b bottom a bw a insertafter function a b c bt a c b c top c top a b next b next prev a a next b next a prev b b next a bx a insertbefore function a b c bt a c b c bottom c bottom a b prev b prev next a a prev b prev b prev a a next b by a tomatrix function a b var c bi a d transform p getbbox function return c b d b return d matrix bz a transformpath function a b return bj a by a b b a extracttransform function b c if c null return b transform c r c replace 3 u2026 g b transform p var d a parsetransformstring c e 0 f 0 g 0 h 1 i 1 j b k new cb j transform d if d for var l 0 m d length l m l var n d l o n length q r n 0 tolowercase s n 0 q t s k invert 0 u v w x y q t o 3 s u t x 0 0 v t y 0 0 w t x n 1 n 2 x t y n 1 n 2 k translate w u x v k translate n 1 n 2 q r o 2 y y b getbbox 1 k rotate n 1 y x y width 2 y y y height 2 e n 1 o 4 s w t x n 2 n 3 x t y n 2 n 3 k rotate n 1 w x k rotate n 1 n 2 n 3 e n 1 q s o 2 o 3 y y b getbbox 1 k scale n 1 n o 1 y x y width 2 y y y height 2 h n 1 i n o 1 o 5 s w t x n 3 n 4 x t y n 3 n 4 k scale n 1 n 2 w x k scale n 1 n 2 n 3 n 4 h n 1 i n 2 q m o 7 k add n 1 n 2 n 3 n 4 n 5 n 6 j dirtyt 1 b matrix k b matrix k j sx h j sy i j deg e j dx f k e j dy g k f h 1 i 1 e j bbox j bbox x f j bbox y g j dirtyt 1 b function a var b a 0 switch b tolowercase case t return b 0 0 case m return b 1 0 0 1 0 0 case r return a length 4 b 0 a 2 a 3 b 0 case s return a length 5 b 1 1 a 3 a 4 a length 3 b 1 1 b 1 ca a equalisetransform function b c c r c replace 3 u2026 g b b a parsetransformstring b c a parsetransformstring c var d x b length c length e f g 0 h i j k for g d g j b g b c g k c g b j if j 0 k 0 j 0 tolowercase r j 2 k 2 j 3 k 3 j 0 tolowercase s j 3 k 3 j 4 k 4 return e g f g for h 0 i x j length k length h i h h in j e g h j h h in k f g h k h return from e to f a getcontainer function b c d e var f f e null a is b object h doc getelementbyid b b if f null if f tagname return c null container f width f style pixelwidth f offsetwidth height f style pixelheight f offsetheight container f width c height d return container 1 x b y c width d height e a pathtorelative bk a engine a path2curve br a matrix function a b c d e f return new cb a b c d e f function b function d a var b w sqrt c a a 0 a 0 b a 1 a 1 b function c a return a 0 a 0 a 1 a 1 b add function a b c d e f var g h this a this c this e this b this d this f 0 0 1 i a c e b d f 0 0 1 j k l m a a instanceof cb i a a a c a e a b a d a f 0 0 1 for j 0 j 3 j for k 0 k 3 k m 0 for l 0 l 3 l m h j l i l k g j k m this a g 0 0 this b g 1 0 this c g 0 1 this d g 1 1 this e g 0 2 this f g 1 2 b invert function var a this b a a a d a b a c return new cb a d b a b b a c b a a b a c a f a d a e b a b a e a a a f b b clone function return new cb this a this b this c this d this e this f b translate function a b this add 1 0 0 1 a b b scale function a b c d b null b a c d this add 1 0 0 1 c d this add a 0 0 b 0 0 c d this add 1 0 0 1 c d b rotate function b c d b a rad b c c 0 d d 0 var e w cos b tofixed 9 f w sin b tofixed 9 this add e f f e c d this add 1 0 0 1 c d b x function a b return a this a b this c this e b y function a b return a this b b this d this f b get function a return this r fromcharcode 97 a tofixed 4 b tostring function return a svg matrix this get 0 this get 1 this get 2 this get 3 this get 4 this get 5 join this get 0 this get 2 this get 1 this get 3 0 0 join b tofilter function return progid dximagetransform microsoft matrix m11 this get 0 m12 this get 2 m21 this get 1 m22 this get 3 dx this get 4 dy this get 5 sizingmethod auto expand b offset function return this e tofixed 4 this f tofixed 4 b split function var b b dx this e b dy this f var e this a this c this b this d b scalex w sqrt c e 0 d e 0 b shear e 0 0 e 1 0 e 0 1 e 1 1 e 1 e 1 0 e 0 0 b shear e 1 1 e 0 1 b shear b scaley w sqrt c e 1 d e 1 b shear b scaley var f e 0 1 g e 1 1 g 0 b rotate a deg w acos g f 0 b rotate 360 b rotate b rotate a deg w asin f b issimple b shear tofixed 9 b scalex tofixed 9 b scaley tofixed 9 b rotate b issupersimple b shear tofixed 9 b scalex tofixed 9 b scaley tofixed 9 b rotate b norotation b shear tofixed 9 b rotate return b b totransformstring function a var b a this s if b issimple b scalex b scalex tofixed 4 b scaley b scaley tofixed 4 b rotate b rotate tofixed 4 return b dx b dy t b dx b dy p b scalex 1 b scaley 1 s b scalex b scaley 0 0 p b rotate r b rotate 0 0 p return m this get 0 this get 1 this get 2 this get 3 this get 4 this get 5 cb prototype var cc navigator useragent match version s navigator useragent match chrome d navigator vendor apple computer inc cc cc 1 4 navigator platform slice 0 2 ip navigator vendor google inc cc cc 1 8 k safari function var a this rect 99 99 this width 99 this height 99 attr stroke none settimeout function a remove k safari be var cd function this returnvalue 1 ce function return this originalevent preventdefault cf function this cancelbubble 0 cg function return this originalevent stoppropagation ch function if h doc addeventlistener return function a b c d var e o u b u b b f function e var f h doc documentelement scrolltop h doc body scrolltop i h doc documentelement scrollleft h doc body scrollleft j e clientx i k e clienty f if o u g b for var l 0 m e targettouches e targettouches length l m l if e targettouches l target a var n e e e targettouches l e originalevent n e preventdefault ce e stoppropagation cg break return c call d e j k a addeventlistener e f 1 return function a removeeventlistener e f 1 return 0 if h doc attachevent return function a b c d var e function a a a h win event var b h doc documentelement scrolltop h doc body scrolltop e h doc documentelement scrollleft h doc body scrollleft f a clientx e g a clienty b a preventdefault a preventdefault cd a stoppropagation a stoppropagation cf return c call d a f g a attachevent on b e var f function a detachevent on b e return 0 return f ci cj function a var b a clientx c a clienty d h doc documentelement scrolltop h doc body scrolltop e h doc documentelement scrollleft h doc body scrollleft f g ci length while g f ci g if o var i a touches length j while i j a touches i if j identifier f el drag id b j clientx c j clienty a originalevent a originalevent a preventdefault break else a preventdefault var k f el node l m k nextsibling n k parentnode p k style display h win opera n removechild k k style display none l f el paper getelementbypoint b c k style display p h win opera m n insertbefore k m n appendchild k l eve raphael drag over f el id f el l b e c d eve raphael drag move f el id f move scope f el b f el drag x c f el drag y b c a ck function b a unmousemove cj unmouseup ck var c ci length d while c d ci c d el drag eve raphael drag end d el id d end scope d start scope d move scope d el b ci cl a el for var cm t length cm function b a b cl b function c d a is c function this events this events this events push name b f c unbind ch this shape this node h doc b c d this return this a un b cl un b function a var c this events d c length while d if c d name b c d f a c d unbind c splice d 1 c length delete this events return this return this t cm cl data function b c var d bb this id bb this id if arguments length 1 if a is b object for var e in b b g e this data e b e return this eve raphael data get this id this d b b return d b d b c eve raphael data set this id this c b return this cl removedata function a a null bb this id bb this id delete bb this id a return this cl hover function a b c d return this mouseover a c mouseout b d c cl unhover function a b return this unmouseover a unmouseout b var cn cl drag function b c d e f g function i i i originalevent i preventdefault var j h doc documentelement scrolltop h doc body scrolltop k h doc documentelement scrollleft h doc body scrollleft this drag x i clientx k this drag y i clienty j this drag id i identifier ci length a mousemove cj mouseup ck ci push el this move scope e start scope f end scope g c eve on raphael drag start this id c b eve on raphael drag move this id b d eve on raphael drag end this id d eve raphael drag start this id f e this i clientx k i clienty j i this drag cn push el this start i this mousedown i return this cl ondragover function a a eve on raphael drag over this id a eve unbind raphael drag over this id cl undrag function var b cn length while b cn b el this this unmousedown cn b start cn splice b 1 eve unbind raphael drag this id cn length a unmousemove cj unmouseup ck k circle function b c d var e a engine circle this b 0 c 0 d 0 this set this set push e return e k rect function b c d e f var g a engine rect this b 0 c 0 d 0 e 0 f 0 this set this set push g return g k ellipse function b c d e var f a engine ellipse this b 0 c 0 d 0 e 0 this set this set push f return f k path function b b a is b d a is b 0 e b p var c a engine path a format m a arguments this this set this set push c return c k image function b c d e f var g a engine image this b about blank c 0 d 0 e 0 f 0 this set this set push g return g k text function b c d var e a engine text this b 0 c 0 r d this set this set push e return e k set function b a is b array b array prototype splice call arguments 0 arguments length var c new cg b this set this set push c return c k setstart function a this set a this set k setfinish function a var b this set delete this set return b k setsize function b c return a engine setsize call this b c k setviewbox function b c d e f return a engine setviewbox call this b c d e f k top k bottom null k raphael a var co function a var b a getboundingclientrect c a ownerdocument d c body e c documentelement f e clienttop d clienttop 0 g e clientleft d clientleft 0 i b top h win pageyoffset e scrolltop d scrolltop f j b left h win pagexoffset e scrollleft d scrollleft g return y i x j k getelementbypoint function a b var c this d c canvas e h doc elementfrompoint a b if h win opera e tagname svg var f co d g d createsvgrect g x a f x g y b f y g width g height 1 var i d getintersectionlist g null i length e i i length 1 if e return null while e parentnode e d parentnode e raphael e e parentnode e c canvas parentnode e d e e e raphael c getbyid e raphaelid null return e k getbyid function a var b this bottom while b if b id a return b b b next return null k foreach function a b var c this bottom while c if a call b c 1 return this c c next return this k getelementsbypoint function a b var c this set this foreach function d d ispointinside a b c push d return c cl ispointinside function b c var d this realpath this realpath bi this type this return a ispointinsidepath d b c cl getbbox function a if this removed return var b this if a if b dirty b bboxwt this realpath bi this type this b bboxwt bi this realpath b bboxwt tostring cq b dirty 0 return b bboxwt if b dirty b dirtyt b bbox if b dirty this realpath b bboxwt 0 this realpath bi this type this b bbox bi bj this realpath this matrix b bbox tostring cq b dirty b dirtyt 0 return b bbox cl clone function if this removed return null var a this paper this type attr this attr this set this set push a return a cl glow function a if this type text return null a a var b width a width 10 this attr stroke width 1 fill a fill 1 opacity a opacity 5 offsetx a offsetx 0 offsety a offsety 0 color a color 000 c b width 2 d this paper e d set f this realpath bi this type this f this matrix bj f this matrix f for var g 1 g c 1 g e push d path f attr stroke b color fill b fill b color none stroke linejoin round stroke linecap round stroke width b width c g tofixed 3 opacity b opacity c tofixed 3 return e insertbefore this translate b offsetx b offsety var cr cs function b c d e f g h i j return j null bb b c d e f g h i a finddotsatsegment b c d e f g h i bc b c d e f g h i j ct function b c return function d e f d br d var g h i j k l m n 0 for var o 0 p d length o p o i d o if i 0 m g i 1 h i 2 else j cs g h i 1 i 2 i 3 i 4 i 5 i 6 if n j e if c l start m cs g h i 1 i 2 i 3 i 4 i 5 i 6 e n k c m start x m start y m m x m m y m x m y if f return k l start k k m m x m y c m n x m n y m end x m end y i 5 i 6 join n j g i 5 h i 6 continue if b c m cs g h i 1 i 2 i 3 i 4 i 5 i 6 e n return x m x y m y alpha m alpha n j g i 5 h i 6 k i shift i l end k m b n c l a finddotsatsegment g h i 0 i 1 i 2 i 3 i 4 i 5 1 m alpha m x m x y m y alpha m alpha return m cu ct 1 cv ct cw ct 0 1 a gettotallength cu a getpointatlength cv a getsubpath function a b c if this gettotallength a c 1e 6 return cw a b end var d cw a c 1 return b cw d b end d cl gettotallength function if this type path if this node gettotallength return this node gettotallength return cu this attrs path cl getpointatlength function a if this type path return cv this attrs path a cl getsubpath function b c if this type path return a getsubpath this attrs path b c var cx a easing formulas linear function a return a function a return a a 1 7 function a return a a 48 function a var b 48 a 1 04 c w sqrt 1734 b b d c b e a z d 1 3 d 0 1 1 f c b g a z f 1 3 f 0 1 1 h e g 5 return 1 h 3 h h h h h backin function a var b 1 70158 return a a b 1 a b backout function a a a 1 var b 1 70158 return a a b 1 a b 1 elastic function a if a a return a return a 2 10 a w sin a 075 2 b 3 1 bounce function a var b 7 5625 c 2 75 d a 1 c d b a a a 2 c a 1 5 c d b a a 75 a 2 5 c a 2 25 c d b a a 9375 a 2 625 c d b a a 984375 return d cx easein cx ease in cx cx easeout cx ease out cx cx easeinout cx ease in out cx cx back in cx backin cx back out cx backout var cy cz window requestanimationframe window webkitrequestanimationframe window mozrequestanimationframe window orequestanimationframe window msrequestanimationframe function a settimeout a 16 ca function var b new date c 0 for c cy length c var d cy c if d el removed d paused continue var e b d start f d ms h d easing i d from j d diff k d to l d t m d el o p r s d initstatus e d initstatus d anim top d prev d percent d prev f d status d initstatus delete d initstatus d stop cy splice c 1 d status d prev d percent d prev e f d anim top if e 0 continue if e f var t h e f for var u in i if i g u switch u u case c p i u t f j u break case colour p rgb cb o i u r t f j u r cb o i u g t f j u g cb o i u b t f j u b join break case path p for var v 0 w i u length v w v p v i u v 0 for var x 1 y i u v length x y x p v x i u v x t f j u v x p v p v join q p p join q break case transform if j u real p for v 0 w i u length v w v p v i u v 0 for x 1 y i u v length x y x p v x i u v x t f j u v x else var z function a return i u a t f j u a p m z 0 z 1 z 2 z 3 z 4 z 5 break case csv if u clip rect p v 4 while v p v i u v t f j u v break default var a n i u p v m paper customattributes u length while v p v a v t f j u v o u p m attr o function a b c settimeout function eve raphael anim frame a b c m id m d anim else function b c d settimeout function eve raphael anim frame c id c d eve raphael anim finish c id c d a is b function b call c d callback m d anim m attr k cy splice c 1 if d repeat 1 d next for s in k k g s r s d totalorigin s d el attr r ce d anim d el d anim percents 0 null d totalorigin d repeat 1 d next d stop ce d anim d el d next null d totalorigin d repeat a svg m m paper m paper safari cy length cz ca cb function a return a 255 255 a 0 0 a cl animatewith function b c d e f g var h this if h removed g g call h return h var i d instanceof cd d a animation d e f g j k ce i h i percents 0 null h attr for var l 0 m cy length l m l if cy l anim c cy l el b cy m 1 start cy l start break return h cl onanimation function a a eve on raphael anim frame this id a eve unbind raphael anim frame this id return this cd prototype delay function a var b new cd this anim this ms b times this times b del a 0 return b cd prototype repeat function a var b new cd this anim this ms b del this del b times w floor x a 0 1 return b a animation function b c d e if b instanceof cd return b if a is d function d e e d null d null b object b c c 0 var f h i for i in b b g i q i i q i i h 0 f i b i if h return new cd b c d f easing d e f callback e return new cd 100 f c cl animate function b c d e var f this if f removed e e call f return f var g b instanceof cd b a animation b c d e ce g f g percents 0 null f attr return f cl settime function a b a b null this status a y b a ms a ms return this cl status function a b var c d 0 e f if b null ce a this 1 y b 1 return this e cy length for d e d f cy d if f el id this id a f anim a if a return f status c push anim f anim status f status if a return 0 return c cl pause function a for var b 0 b cy length b cy b el id this id a cy b anim a eve raphael anim pause this id this cy b anim 1 cy b paused 0 return this cl resume function a for var b 0 b cy length b if cy b el id this id a cy b anim a var c cy b eve raphael anim resume this id this c anim 1 delete c paused this status c anim c status return this cl stop function a for var b 0 b cy length b cy b el id this id a cy b anim a eve raphael anim stop this id this cy b anim 1 cy splice b 1 return this eve on raphael remove cf eve on raphael clear cf cl tostring function return rapha l s object var cg function a this items this length 0 this type set if a for var b 0 c a length b c b a b a b constructor cl constructor a b constructor cg this this items length this items this items length a b this length ch cg prototype ch push function var a b for var c 0 d arguments length c d c a arguments c a a constructor cl constructor a constructor cg b this items length this b this items b a this length return this ch pop function this length delete this this length return this items pop ch foreach function a b for var c 0 d this items length c d c if a call b this items c c 1 return this return this for var ci in cl cl g ci ch ci function a return function var b arguments return this foreach function c c a m c b ci ch attr function b c if b a is b e a is b 0 object for var d 0 e b length d e d this items d attr b d else for var f 0 g this items length f g f this items f attr b c return this ch clear function while this length this pop ch splice function a b c a a 0 x this length a 0 a b x 0 y this length a b var d e f g for g 2 g arguments length g f push arguments g for g 0 g b g e push this a g for g this length a g d push this a g var h f length for g 0 g h d length g this items a g this a g g h f g d g h g this items length this length b h while this g delete this g return new cg e ch exclude function a for var b 0 c this length b c b if this b a this splice b 1 return 0 ch animate function b c d e a is d function d e d null var f this items length g f h i this j if f return this e j function f e call i d a is d d d j var k a animation b c d j h this items g animate k while g this items g this items g removed this items g animatewith h k k return this ch insertafter function a var b this items length while b this items b insertafter a return this ch getbbox function var a b c d for var e this items length e if this items e removed var f this items e getbbox a push f x b push f y c push f x f width d push f y f height a y m 0 a b y m 0 b c x m 0 c d x m 0 d return x a y b x2 c y2 d width c a height d b ch clone function a a new cg for var b 0 c this items length b c b a push this items b clone return a ch tostring function return rapha l s set a registerfont function a if a face return a this fonts this fonts var b w a w face glyphs c a face font family for var d in a face a face g d b face d a face d this fonts c this fonts c push b this fonts c b if a svg b face units per em r a face units per em 10 for var e in a glyphs if a glyphs g e var f a glyphs e b glyphs e w f w k d f d m f d replace mlcxtrv g function a return l l c c x z t m r l v c a m z if f k for var h in f k f g h b glyphs e k h f k h return a k getfont function b c d e e e normal d d normal c c normal 400 bold 700 lighter 300 bolder 800 c 400 if a fonts var f a fonts b if f var h new regexp s b replace w d s g p s i for var i in a fonts if a fonts g i h test i f a fonts i break var j if f for var k 0 l f length k l k j f k if j face font weight c j face font style d j face font style j face font stretch e break return j k print function b d e f g h i h h middle i x y i 0 1 1 var j r e s p k 0 l 0 m p n a is f e f this getfont f if f n g 16 f face units per em var o f face bbox s c q o 0 t o 3 o 1 u 0 v o 1 h baseline t f face descent t 2 for var w 0 z j length w z w if j w n k 0 b 0 l 0 u t else var a l f glyphs j w 1 b f glyphs j w k l a w f w a k a k j w 0 f w i 0 l 1 b b d m a transformpath b d t k n u n s n n q v t b q n d v n return this path m attr fill 000 stroke none k add function b if a is b array var c this set e 0 f b length h for e f e h b e d g h type c push this h type attr h return c a format function b c var d a is c e 0 n c arguments b a is b d d length 1 b b replace e function a b return d b null p d b return b p a fullfill function var a g b 2 g c function a c d var e d c replace b function a b c d f b b d e b in e e e b typeof e function f e e e e null e d a e return e return function b d return string b replace a function a b return c a b d a ninja function i was h win raphael i is delete raphael return a a st ch function b c d function e in test b readystate settimeout e 9 a eve raphael domload b readystate null b addeventlistener b addeventlistener c d function b removeeventlistener c d 1 b readystate complete 1 b readystate loading e document domcontentloaded i was h win raphael a raphael a eve on raphael domload function b 0 window raphael svg function a var b hasownproperty c string d parsefloat e parseint f math g f max h f abs i f pow j k a eve l m n http www w3 org 1999 xlink o block m5 0 0 2 5 5 5z classic m5 0 0 2 5 5 5 3 5 3 3 5 2z diamond m2 5 0 5 2 5 2 5 5 0 2 5z open m6 1 1 3 5 6 6 oval m2 5 0a2 5 2 5 0 0 1 2 5 5 2 5 2 5 0 0 1 2 5 0z p a tostring function return your browser supports svg nyou are running rapha l this version var q function d e if e typeof d string d q d for var f in e e b f f substring 0 6 xlink d setattributens n f substring 6 c e f d setattribute f c e f else d a g doc createelementns http www w3 org 2000 svg d d style d style webkittaphighlightcolor rgba 0 0 0 0 return d r function b e var j linear k b id e m 5 n 5 o b node p b paper r o style s a g doc getelementbyid k if s e c e replace a radial gradient function a b c j radial if b c m d b n d c var e n 5 2 1 i m 5 2 i n 5 2 25 n f sqrt 25 i m 5 2 e 5 n 5 n n tofixed 5 1e 5 e return l e e split s s if j linear var t e shift t d t if isnan t return null var u 0 0 f cos a rad t f sin a rad t v 1 g h u 2 h u 3 1 u 2 v u 3 v u 2 0 u 0 u 2 u 2 0 u 3 0 u 1 u 3 u 3 0 var w a parsedots e if w return null k k replace s xb0 g b gradient k b gradient id p defs removechild b gradient delete b gradient if b gradient s q j gradient id k b gradient s q s j radial fx m fy n x1 u 0 y1 u 1 x2 u 2 y2 u 3 gradienttransform b matrix invert p defs appendchild s for var x 0 y w length x y x s appendchild q stop offset w x offset w x offset x 100 0 stop color w x color fff q o fill url k opacity 1 fill opacity 1 r fill l r opacity 1 r fillopacity 1 return 1 s function a var b a getbbox 1 q a pattern patterntransform a matrix invert translate b x b y t function d e f if d type path var g c e tolowercase split h d paper i f end start j d node k d attrs m k stroke width n g length r classic s t u v w x 3 y 3 z 5 while n switch g n case block case classic case oval case diamond case open case none r g n break case wide y 5 break case narrow y 2 break case long x 5 break case short x 2 r open x 2 y 2 z 2 u 1 v f 4 1 w fill none stroke k stroke v u x 2 w fill k stroke stroke none d arrows f d arrows endpath p d arrows endpath d arrows endmarker p d arrows endmarker d arrows startpath p d arrows startpath d arrows startmarker p d arrows startmarker d arrows if r none var a raphael marker r b raphael marker i r x y a g doc getelementbyid a p a h defs appendchild q q path stroke linecap round d o r id a p a 1 var c a g doc getelementbyid b d c p b d c getelementsbytagname use 0 c q q marker id b markerheight y markerwidth x orient auto refx v refy y 2 d q q use xlink href a transform f rotate 180 x 2 y 2 l scale x z y z stroke width 1 x z y z 2 tofixed 4 c appendchild d h defs appendchild c p b 1 q d w var f u r diamond r oval f s d arrows startdx m 0 t a gettotallength k path f m s f m t a gettotallength k path d arrows enddx m 0 w w marker i url b if t s w d raphael getsubpath k path s t q j w d arrows i path a d arrows i marker b d arrows i dx f d arrows i type r d arrows i string e else f s d arrows startdx m 0 t a gettotallength k path s s 0 t a gettotallength k path d arrows enddx m 0 d arrows i path q j d raphael getsubpath k path s t delete d arrows i path delete d arrows i marker delete d arrows i dx delete d arrows i type delete d arrows i string for w in p if p b w p w var g a g doc getelementbyid w g g parentnode removechild g u 0 none 0 3 1 1 1 3 1 1 1 3 1 1 1 1 1 1 3 4 3 8 3 4 3 1 3 8 3 1 3 8 3 1 3 1 3 v function a b d b u c b tolowercase if b var e a attrs stroke width 1 f round e square e butt 0 a attrs stroke linecap d stroke linecap 0 g h b length while h g h b h e h 2 1 1 f q a node stroke dasharray g join w function d f var i d node k d attrs m i style visibility i style visibility hidden for var o in f if f b o if a availableattrs b o continue var p f o k o p switch o case blur d blur p break case href case title case target var u i parentnode if u tagname tolowercase a var w q a u insertbefore w i w appendchild i u w o target u setattributens n show p blank new p u setattributens n o p break case cursor i style cursor p break case transform d transform p break case arrow start t d p break case arrow end t d p 1 break case clip rect var x c p split j if x length 4 d clip d clip parentnode parentnode removechild d clip parentnode var z q clippath a q rect z id a createuuid q a x x 0 y x 1 width x 2 height x 3 z appendchild a d paper defs appendchild z q i clip path url z id d clip a if p var b i getattribute clip path if b var c a g doc getelementbyid b replace url g l c c parentnode removechild c q i clip path l delete d clip break case path d type path q i d p k path a pathtoabsolute p m0 0 d dirty 1 d arrows startstring in d arrows t d d arrows startstring endstring in d arrows t d d arrows endstring 1 break case width i setattribute o p d dirty 1 if k fx o x p k x else break case x k fx p k x k width 0 case rx if o rx d type rect break case cx i setattribute o p d pattern s d d dirty 1 break case height i setattribute o p d dirty 1 if k fy o y p k y else break case y k fy p k y k height 0 case ry if o ry d type rect break case cy i setattribute o p d pattern s d d dirty 1 break case r d type rect q i rx p ry p i setattribute o p d dirty 1 break case src d type image i setattributens n href p break case stroke width if d sx 1 d sy 1 p g h d sx h d sy 1 d paper vbsize p d paper vbsize i setattribute o p k stroke dasharray v d k stroke dasharray f d arrows startstring in d arrows t d d arrows startstring endstring in d arrows t d d arrows endstring 1 break case stroke dasharray v d p f break case fill var d c p match a isurl if d z q pattern var f q image z id a createuuid q z x 0 y 0 patternunits userspaceonuse height 1 width 1 q f x 0 y 0 xlink href d 1 z appendchild f function b a preload d 1 function var a this offsetwidth c this offsetheight q b width a height c q f width a height c d paper safari z d paper defs appendchild z q i fill url z id d pattern z d pattern s d break var g a getrgb p if g error delete f gradient delete k gradient a is k opacity undefined a is f opacity undefined q i opacity k opacity a is k fill opacity undefined a is f fill opacity undefined q i fill opacity k fill opacity else if d type circle d type ellipse c p charat r r d p if opacity in k fill opacity in k var h a g doc getelementbyid i getattribute fill replace url g l if h var i h getelementsbytagname stop q i i length 1 stop opacity opacity in k k opacity 1 fill opacity in k k fill opacity 1 k gradient p k fill none break g b opacity q i fill opacity g opacity 1 g opacity 100 g opacity case stroke g a getrgb p i setattribute o g hex o stroke g b opacity q i stroke opacity g opacity 1 g opacity 100 g opacity o stroke d arrows startstring in d arrows t d d arrows startstring endstring in d arrows t d d arrows endstring 1 break case gradient d type circle d type ellipse c p charat r r d p break case opacity k gradient k b stroke opacity q i stroke opacity p 1 p 100 p case fill opacity if k gradient h a g doc getelementbyid i getattribute fill replace url g l h i h getelementsbytagname stop q i i length 1 stop opacity p break default o font size p e p 10 px var j o replace g function a return a substring 1 touppercase i style j p d dirty 1 i setattribute o p y d f i style visibility m x 1 2 y function d f if d type text f b text f b font f b font size f b x f b y var g d attrs h d node i h firstchild e a g doc defaultview getcomputedstyle h firstchild l getpropertyvalue font size 10 10 if f b text g text f text while h firstchild h removechild h firstchild var j c f text split n k m for var n 0 o j length n o n m q tspan n q m dy i x x g x m appendchild a g doc createtextnode j n h appendchild m k n m else k h getelementsbytagname tspan for n 0 o k length n o n n q k n dy i x x g x q k 0 dy 0 q h x g x y g y d dirty 1 var p d getbbox r g y p y p height 2 r a is r finite q k 0 dy r z function b c var d 0 e 0 this 0 this node b b raphael 0 this id a oid b raphaelid this id this matrix a matrix this realpath null this paper c this attrs this attrs this transform sx 1 sy 1 deg 0 dx 0 dy 0 dirty 1 c bottom c bottom this this prev c top c top c top next this c top this this next null a a el z prototype a a constructor z a engine path function a b var c q path b canvas b canvas appendchild c var d new z c b d type path w d fill none stroke 000 path a return d a rotate function a b e if this removed return this a c a split j a length 1 b d a 1 e d a 2 a d a 0 e null b e if b null e null var f this getbbox 1 b f x f width 2 e f y f height 2 this transform this transform concat r a b e return this a scale function a b e f if this removed return this a c a split j a length 1 b d a 1 e d a 2 f d a 3 a d a 0 b null b a f null e f if e null f null var g this getbbox 1 e e null g x g width 2 e f f null g y g height 2 f this transform this transform concat s a b e f return this a translate function a b if this removed return this a c a split j a length 1 b d a 1 a d a 0 0 b b 0 this transform this transform concat t a b return this a transform function c var d this if c null return d transform a extracttransform this c this clip q this clip transform this matrix invert this pattern s this this node q this node transform this matrix if d sx 1 d sy 1 var e this attrs b stroke width this attrs stroke width 1 this attr stroke width e return this a hide function this removed this paper safari this node style display none return this a show function";"CODE"
"this removed this paper safari this node style display return this a remove function if this removed this node parentnode var b this paper b set b set exclude this k unbind raphael this id this gradient b defs removechild this gradient a tear this b this node parentnode tagname tolowercase a this node parentnode parentnode removechild this node parentnode this node parentnode removechild this node for var c in this this c typeof this c function a removedfactory c null this removed 0 a getbbox function if this node style display none this show var a 0 var b try b this node getbbox catch c finally b b a this hide return b a attr function c d if this removed return this if c null var e for var f in this attrs this attrs b f e f this attrs f e gradient e fill none e fill e gradient delete e gradient e transform this transform return e if d null a is c string if c fill this attrs fill none this attrs gradient return this attrs gradient if c transform return this transform var g c split j h for var i 0 l g length i l i c g i c in this attrs h c this attrs c a is this paper customattributes c function h c this paper customattributes c def h c a availableattrs c return l 1 h h g 0 if d null a is c array h for i 0 l c length i l i h c i this attr c i return h if d null var m m c d else c null a is c object m c for var n in m k raphael attr n this id this m n for n in this paper customattributes if this paper customattributes b n m b n a is this paper customattributes n function var o this paper customattributes n apply this concat m n this attrs n m n for var p in o o b p m p o p w this m return this a tofront function if this removed return this this node parentnode tagname tolowercase a this node parentnode parentnode appendchild this node parentnode this node parentnode appendchild this node var b this paper b top this a tofront this b return this a toback function if this removed return this var b this node parentnode b tagname tolowercase a b parentnode insertbefore this node parentnode this node parentnode parentnode firstchild b firstchild this node b insertbefore this node this node parentnode firstchild a toback this this paper var c this paper return this a insertafter function b if this removed return this var c b node b b length 1 node c nextsibling c parentnode insertbefore this node c nextsibling c parentnode appendchild this node a insertafter this b this paper return this a insertbefore function b if this removed return this var c b node b 0 node c parentnode insertbefore this node c a insertbefore this b this paper return this a blur function b var c this if b 0 var d q filter e q fegaussianblur c attrs blur b d id a createuuid q e stddeviation b 1 5 d appendchild e c paper defs appendchild d c blur d q c node filter url d id else c blur c blur parentnode removechild c blur delete c blur delete c attrs blur c node removeattribute filter a engine circle function a b c d var e q circle a canvas a canvas appendchild e var f new z e a f attrs cx b cy c r d fill none stroke 000 f type circle q e f attrs return f a engine rect function a b c d e f var g q rect a canvas a canvas appendchild g var h new z g a h attrs x b y c width d height e r f 0 rx f 0 ry f 0 fill none stroke 000 h type rect q g h attrs return h a engine ellipse function a b c d e var f q ellipse a canvas a canvas appendchild f var g new z f a g attrs cx b cy c rx d ry e fill none stroke 000 g type ellipse q f g attrs return g a engine image function a b c d e f var g q image q g x c y d width e height f preserveaspectratio none g setattributens n href b a canvas a canvas appendchild g var h new z g a h attrs x c y d width e height f src b h type image return h a engine text function b c d e var f q text b canvas b canvas appendchild f var g new z f b g attrs x c y d text anchor middle text e font a availableattrs font stroke none fill 000 g type text w g g attrs return g a engine setsize function a b this width a this width this height b this height this canvas setattribute width this width this canvas setattribute height this height this viewbox this setviewbox apply this this viewbox return this a engine create function var b a getcontainer apply 0 arguments c b b container d b x e b y f b width g b height if c throw new error svg container not found var h q svg i overflow hidden j d d 0 e e 0 f f 512 g g 342 q h height g version 1 1 width f xmlns http www w3 org 2000 svg c 1 h style csstext i position absolute left d px top e px a g doc body appendchild h j 1 h style csstext i position relative c firstchild c insertbefore h c firstchild c appendchild h c new a paper c width f c height g c canvas h c clear c left c top 0 j c renderfix function c renderfix return c a engine setviewbox function a b c d e k raphael setviewbox this this viewbox a b c d e var f g c this width d this height h this top i e meet xminymin j l a null this vbsize f 1 delete this vbsize j 0 0 this width m this height this vbsize f j a m b m c m d q this canvas viewbox j preserveaspectratio i while f h l stroke width in h attrs h attrs stroke width 1 h attr stroke width l h dirty 1 h dirtyt 1 h h prev this viewbox a b c d e return this a prototype renderfix function var a this canvas b a style c try c a getscreenctm a createsvgmatrix catch d c a createsvgmatrix var e c e 1 f c f 1 if e f e this left this left e 1 b left this left px f this top this top f 1 b top this top px a prototype clear function a eve raphael clear this var b this canvas while b firstchild b removechild b firstchild this bottom this top null this desc q desc appendchild a g doc createtextnode created with rapha l a version b appendchild this desc b appendchild this defs q defs a prototype remove function k raphael remove this this canvas parentnode this canvas parentnode removechild this canvas for var b in this this b typeof this b function a removedfactory b null var b a st for var c in a a b c b b c b c function a return function var b arguments return this foreach function c c a apply c b c window raphael window raphael vml function a var b hasownproperty c string d parsefloat e math f e round g e max h e min i e abs j fill k l a eve m progid dximagetransform microsoft n o p m m l l c c z x m t l r c v z x q clmz clmz gi r progid s blur g s s g t position absolute left 0 top 0 width 1px height 1px u 21600 v path 1 rect 1 image 1 w circle 1 ellipse 1 x function b var d ahqstv ig e a pathtoabsolute c b match d e a path2curve d clmz g if e a pathtoabsolute c b match d var g c b replace q function a b c var d e b tolowercase m g p b c replace s function a e d length 2 g d p b m l l d d push f a u return g d return g var h e b i j g for var k 0 l h length k l k i h k j h k 0 tolowercase j z j x for var m 1 r i length m r m j f i m u m r 1 o g push j return g join n y function b c d var e a matrix e rotate b 5 5 return dx e x c d dy e y c d z function a b c d e f var g a h a matrix k g fillpos l a node m l style o 1 p q r u b s u c m visibility hidden if b c l coordsize i r n i s m rotation f b c 0 1 1 if f var t y f d e d t dx e t dy b 0 p x c 0 p y o 1 m flip p l coordorigin d r n e s if k g fillsize var v l getelementsbytagname j v v v 0 l removechild v k t y f h x k 0 k 1 h y k 0 k 1 v position t dx o n t dy o g fillsize v size g fillsize 0 i b n g fillsize 1 i c l appendchild v m visibility visible a tostring function return your browser doesn t support svg falling down to vml nyou are running rapha l this version var a function a b d var e c b tolowercase split f d end start g e length h classic i medium j medium while g switch e g case block case classic case oval case diamond case open case none h e g break case wide case narrow j e g break case long case short i e g var k a node getelementsbytagname stroke 0 k f arrow h k f arrowlength i k f arrowwidth j b function e i e attrs e attrs var l e node m e attrs p l style q r v e type i x m x i y m y i width m width i height m height i cx m cx i cy m cy i rx m rx i ry m ry i r m r s w e type m cx i cx m cy i cy m r i r m rx i rx m ry i ry t e for var y in i i b y m y i y r m path a getpath e type e e dirty 1 i href l href i href i title l title i title i target l target i target i cursor p cursor i cursor blur in i e blur i blur if i path e type path r l path x c m path tolowercase indexof r a pathtoabsolute m path m path e type image e fillpos m x m y e fillsize m width m height z e 1 1 0 0 0 transform in i e transform i transform if s var b m cx d m cy e m rx m r 0 g m ry m r 0 l path a format ar 0 1 2 3 4 1 4 1 x f b e u f d g u f b e u f d g u f b u if clip rect in i var h c i clip rect split k if h length 4 h 2 h 2 h 0 h 3 h 3 h 1 var i l cliprect a g doc createelement div j i style j clip a format rect 1 px 2 px 3 px 0 px h l cliprect j position absolute j top 0 j left 0 j width e paper width px j height e paper height px l parentnode insertbefore i l i appendchild l l cliprect i i clip rect l cliprect l cliprect style clip auto if e textpath var k e textpath style i font k font i font i font family k fontfamily i font family split 0 replace g o i font size k fontsize i font size i font weight k fontweight i font weight i font style k fontstyle i font style arrow start in i a t i arrow start arrow end in i a t i arrow end 1 if i opacity null i stroke width null i fill null i src null i stroke null i stroke width null i stroke opacity null i fill opacity null i stroke dasharray null i stroke miterlimit null i stroke linejoin null i stroke linecap null var l l getelementsbytagname j m 1 l l l 0 l m l f j e type image i src l src i src i fill l on 0 if l on null i fill none i fill null l on 1 if l on i fill var n c i fill match a isurl if n l parentnode l l removechild l l rotate 0 l src n 1 l type tile var o e getbbox 1 l position o x n o y e fillpos o x o y a preload n 1 function e fillsize this offsetwidth this offsetheight else l color a getrgb i fill hex l src o l type solid a getrgb i fill error t type in circle 1 ellipse 1 c i fill charat r c t i fill l m fill none m gradient i fill l rotate 1 if fill opacity in i opacity in i var p m fill opacity 1 2 1 m opacity 1 2 1 a getrgb i fill o 1 2 1 p h g p 0 1 l opacity p l src l color none l appendchild l var q l getelementsbytagname stroke l getelementsbytagname stroke 0 t 1 q t q f stroke if i stroke i stroke none i stroke width i stroke opacity null i stroke dasharray i stroke miterlimit i stroke linejoin i stroke linecap q on 0 i stroke none i stroke null q on null i stroke 0 i stroke width 0 q on 1 var u a getrgb i stroke q on i stroke q color u hex p m stroke opacity 1 2 1 m opacity 1 2 1 u o 1 2 1 var v d i stroke width 1 75 p h g p 0 1 i stroke width null v m stroke width i stroke width q weight v v v 1 p v q weight 1 q opacity p i stroke linejoin q joinstyle i stroke linejoin miter q miterlimit i stroke miterlimit 8 i stroke linecap q endcap i stroke linecap butt flat i stroke linecap square square round if i stroke dasharray var w shortdash shortdot shortdashdot shortdashdotdot dot dash longdash dashdot longdashdot longdashdotdot q dashstyle w b i stroke dasharray w i stroke dasharray o t l appendchild q if t type text t paper canvas style display o var x t paper span y 100 z m font m font match d d px p x style m font p font m font m font family p fontfamily m font family m font weight p fontweight m font weight m font style p fontstyle m font style z d m font size z z 0 10 p fontsize z y px t textpath string x innerhtml c t textpath string replace g 60 replace g 38 replace n g br var x getboundingclientrect t w m w right left y t h m h bottom top y t x m x t y m y t h 2 x in i y in i t path v a format m 0 1 l 2 1 f m x u f m y u f m x u 1 var x y text font font family font weight font style font size for var ba 0 bb length ba bb ba if ba in i t dirty 1 break switch m text anchor case start t textpath style v text align left t bbx t w 2 break case end t textpath style v text align right t bbx t w 2 break default t textpath style v text align center t bbx 0 t textpath style v text kern 0 c function b f g b attrs b attrs var h b attrs i math pow j k l linear m 5 5 b attrs gradient f f c f replace a radial gradient function a b c l radial b c b d b c d c i b 5 2 i c 5 2 25 c e sqrt 25 i b 5 2 c 5 2 1 5 m b n c return o f f split s s if l linear var p f shift p d p if isnan p return null var q a parsedots f if q return null b b shape b node if q length b removechild g g on 0 g method none g color q 0 color g color2 q q length 1 color var r for var s 0 t q length s t s q s offset r push q s offset n q s color g colors r length r join 0 g color l radial g type gradienttitle g focus 100 g focussize 0 0 g focusposition m g angle 0 g type gradient g angle 270 p 360 b appendchild g return 1 d function b c this 0 this node b b raphael 0 this id a oid b raphaelid this id this x 0 this y 0 this attrs this paper c this matrix a matrix this transform sx 1 sy 1 dx 0 dy 0 deg 0 dirty 1 dirtyt 1 c bottom c bottom this this prev c top c top c top next this c top this this next null e a el d prototype e e constructor d e transform function b if b null return this transform var d this paper viewboxshift e d s d scale d scale 1 1t d dx d dy o f d f b c b replace 3 u2026 g this transform o a extracttransform this e b var g this matrix clone h this skew i this node j k c this attrs fill indexof l c this attrs fill indexof url g translate 0 5 0 5 if l k this type image h matrix 1 0 0 1 h offset 0 0 j g split if k j norotation j issimple i style filter g tofilter var m this getbbox p this getbbox 1 q m x p x r m y p y i coordorigin q u n r u z this 1 1 q r 0 else i style filter o z this j scalex j scaley j dx j dy j rotate else i style filter o h matrix c g h offset g offset f this transform f return this e rotate function a b e if this removed return this if a null a c a split k a length 1 b d a 1 e d a 2 a d a 0 e null b e if b null e null var f this getbbox 1 b f x f width 2 e f y f height 2 this dirtyt 1 this transform this transform concat r a b e return this e translate function a b if this removed return this a c a split k a length 1 b d a 1 a d a 0 0 b b 0 this bbox this bbox x a this bbox y b this transform this transform concat t a b return this e scale function a b e f if this removed return this a c a split k a length 1 b d a 1 e d a 2 f d a 3 isnan e e null isnan f f null a d a 0 b null b a f null e f if e null f null var g this getbbox 1 e e null g x g width 2 e f f null g y g height 2 f this transform this transform concat s a b e f this dirtyt 1 return this e hide function this removed this node style display none return this e show function this removed this node style display o return this e getbbox function if this removed return return x this x this bbx 0 this w 2 y this y this h width this w height this h e remove function if this removed this node parentnode this paper set this paper set exclude this a eve unbind raphael this id a tear this this paper this node parentnode removechild this node this shape this shape parentnode removechild this shape for var b in this this b typeof this b function a removedfactory b null this removed 0 e attr function c d if this removed return this if c null var e for var f in this attrs this attrs b f e f this attrs f e gradient e fill none e fill e gradient delete e gradient e transform this transform return e if d null a is c string if c j this attrs fill none this attrs gradient return this attrs gradient var g c split k h for var i 0 m g length i m i c g i c in this attrs h c this attrs c a is this paper customattributes c function h c this paper customattributes c def h c a availableattrs c return m 1 h h g 0 if this attrs d null a is c array h for i 0 m c length i m i h c i this attr c i return h var n d null n n c d d null a is c object n c for var o in n l raphael attr o this id this n o if n for o in this paper customattributes if this paper customattributes b o n b o a is this paper customattributes o function var p this paper customattributes o apply this concat n o this attrs o n o for var q in p p b q n q p q n text this type text this textpath string n text b this n return this e tofront function this removed this node parentnode appendchild this node this paper this paper top this a tofront this this paper return this e toback function if this removed return this this node parentnode firstchild this node this node parentnode insertbefore this node this node parentnode firstchild a toback this this paper return this e insertafter function b if this removed return this b constructor a st constructor b b b length 1 b node nextsibling b node parentnode insertbefore this node b node nextsibling b node parentnode appendchild this node a insertafter this b this paper return this e insertbefore function b if this removed return this b constructor a st constructor b b 0 b node parentnode insertbefore this node b node a insertbefore this b this paper return this e blur function b var c this node runtimestyle d c filter d d replace r o b 0 this attrs blur b c filter d n m blur pixelradius b 1 5 c margin a format 0 px 0 0 0 px f b 1 5 c filter d c margin 0 delete this attrs blur a engine path function a b var c f shape c style csstext t c coordsize u n u c coordorigin b coordorigin var d new d c b e fill none stroke 000 a e path a d type path d path d path o b d e b canvas appendchild c var f f skew f on 0 c appendchild f d skew f d transform o return d a engine rect function b c d e f g var h a rectpath c d e f g i b path h j i attrs i x j x c i y j y d i w j width e i h j height f j r g j path h i type rect return i a engine ellipse function a b c d e var f a path g f attrs f x b d f y c e f w d 2 f h e 2 f type ellipse b f cx b cy c rx d ry e return f a engine circle function a b c d var e a path f e attrs e x b d e y c d e w e h d 2 e type circle b e cx b cy c r d return e a engine image function b c d e f g var h a rectpath d e f g i b path h attr stroke none k i attrs l i node m l getelementsbytagname j 0 k src c i x k x d i y k y e i w k width f i h k height g k path h i type image m parentnode l l removechild m m rotate 0 m src c m type tile i fillpos d e i fillsize f g l appendchild m z i 1 1 0 0 0 return i a engine text function b d e g var h f shape i f path j f textpath d d 0 e e 0 g g i v a format m 0 1 l 2 1 f d u f e u f d u 1 i textpathok 0 j string c g j on 0 h style csstext t h coordsize u n u h coordorigin 0 0 var k new d h b l fill 000 stroke none font a availableattrs font text g k shape h k path i k textpath j k type text k attrs text c g k attrs x d k attrs y e k attrs w 1 k attrs h 1 b k l h appendchild j h appendchild i b canvas appendchild h var m f skew m on 0 h appendchild m k skew m k transform o return k a engine setsize function b c var d this canvas style this width b this height c b b b px c c c px d width b d height c d clip rect 0 b c 0 this viewbox a engine setviewbox apply this this viewbox return this a engine setviewbox function b c d e f a eve raphael setviewbox this this viewbox b c d e f var h this width i this height j 1 g d h e i k l f k i e l h d d k h b h d k 2 k e l i c i e l 2 l this viewbox b c d e f this viewboxshift dx b dy c scale j this foreach function a a transform return this var f a engine initwin function a var b a document b createstylesheet addrule rvml behavior url default vml try b namespaces rvml b namespaces add rvml urn schemas microsoft com vml f function a return b createelement rvml a class rvml catch c f function a return b createelement a xmlns urn schemas microsoft com vml class rvml a engine initwin a g win a engine create function var b a getcontainer apply 0 arguments c b container d b height e f b width g b x h b y if c throw new error vml container not found var i new a paper j i canvas a g doc createelement div k j style g g 0 h h 0 f f 512 d d 342 i width f i height d f f f px d d d px i coordsize u 1e3 n u 1e3 i coordorigin 0 0 i span a g doc createelement span i span style csstext position absolute left 9999em top 9999em padding 0 margin 0 line height 1 j appendchild i span k csstext a format top 0 left 0 width 0 height 1 display inline block position relative clip rect 0 0 1 0 overflow hidden f d c 1 a g doc body appendchild j k left g px k top h px k position absolute c firstchild c insertbefore j c firstchild c appendchild j i renderfix function return i a prototype clear function a eve raphael clear this this canvas innerhtml o this span a g doc createelement span this span style csstext position absolute left 9999em top 9999em padding 0 margin 0 line height 1 display inline this canvas appendchild this span this bottom this top null a prototype remove function a eve raphael remove this this canvas parentnode removechild this canvas for var b in this this b typeof this b function a removedfactory b null return 0 var g a st for var h in e e b h g b h g h function a return function var b arguments return this foreach function c c a apply c b h window raphael";"CODE"
"background jpg xff xd8 xff xe0 x00 x10jfif x00 x01 x02 x00 x00d x00d x00 x00 xff xec x00 x11ducky x00 x01 x00 x04 x00 x00 x00d x00 x00 xff xee x00 x0eadobe x00d xc0 x00 x00 x00 x01 xff xdb x00 x84 x00 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x01 x02 x02 x02 x02 x02 x02 x02 x02 x02 x02 x02 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x01 x01 x01 x01 x01 x01 x01 x02 x01 x01 x02 x02 x02 x01 x02 x02 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 x03 xff xc0 x00 x11 x08 x03 x00 x00 n x03 x01 x11 x00 x02 x11 x01 x03 x11 x01 xff xc4 x00k x00 x01 x01 x01 x01 x01 x01 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x03 x02 x01 x04 t x01 x01 x01 x01 x01 x01 x01 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x01 x02 x03 x06 x07 x10 x01 x00 x02 x01 x04 x01 x05 x01 x01 x00 x00 x00 x00 x00 x00 x00 x00 x01 x12q xf0 x11a xa1q x91 xd1 xe1 x02 x13 x81b x11 x01 x01 x01 x01 x00 x03 x01 x01 x00 x00 x00 x00 x00 x00 x00 x00 x00 x11 x01 x12 qa x02 xff xda x00 x0c x03 x01 x00 x02 x11 x03 x11 x00 x00 xf9 xc6 xf7 x0f x9e x80 x00 x0e xb5 xc8 x1c x8a x8e x00 x00 x005 xb15 xa9 x86 xc4 xd2c xad x00 x00 xe8 xb3 x9a xady xe9nz x8a xf3 xd1s xd1 xd3h xc8 x00 x00 xad x86 xae x14 xf0x 6 x8c x80 x00 n xd25 x0b xe1 xab x9e x8af xa0 xf0 xf4 xda2 x03 xa2 xcd x02j xd5 x8c xd2 xe9x xc1t xba xea x0e xed8 x9ff xa7 xd2 xe1 xb4 xe2 t xf4 xb8 xa8 xe6 x00 x006 xd3 xa0 x00 x00 xa5 7 xcf xd5 xf0ru xb1 xcf xd3 xc2 xaa xc8 x0e xed 5f x9bi4 x9a xb6 xd1 x88 x0b xa6 xd1 x88 x0b xae xa2 x00 xb5c x95 xabt xacrr xebh x80 o x1d xaf x85 xf0s xc7g x83 xc2 x88 x805yjb xc2 xe2 x83 x00 y xd6 xe4xvu xb9 x08 xb8 x80 xb4 xaf xa5 x9am xa2k xd0 xda x00m 4 xdabj xfbf x9bf xbam x9a x xc1un x95 x8c x17k xad x00 x0bv x16 xe9j xc1t xad x02 xb4 x82 x85 xa3 x02 xbf x9f x9dg xa7 xcf xcf x9e xben xb3 xd1 xda xac xb9 x80 xa55 xa8 x0b xe0 xa6 xb5 x07 x83 xc2 x88 x80 xf9 xf8u x9f x9fa xf9 xf8 xe7 xd0 xaa x005ik xc0ro x02 xcc x80 y xc4 xb53 xd8vq xcfb xec x80 x0b5 x02h xbe xd1 x88 x16 xe9 xb4b x02 xeb xa4 xd4 t xa3 xd1o xae x9ft xeb xba xfd x14 xfa xe3 xb9 xf7 xd3 xaf xd3h xc8 n xd22 xb7 xe1 xe0 xa4d xbf x0f r xa0 x03t x9cw x0e x9c xfe x85 x1d xc1 xcf xe7 xe8 xb0 x00 xd4 t xa2 xfbf xd0 xba x1bf xd0 xba x8a x02 x94 x9c xc3w x02 x93 x98 n2 x005 xce xa0s xa2 xfbf x95 xbam x18 x80 xba xe8 x00 xd5 x1d xc7 xbb xca x14 x9cw x1e xe7 xf2 xca x80 x00 x0b xfe xeb xaf x95 xef xe34 xfc xbf xd7 x7f n xb3 x9b x0b7 xd6 x817 xd6 x8a xd3 x9e xbe zb x9c xf5 xf2t xca x80 x00 rw xed x8e xe1 xbb x88w xed x8e xe0 xb8 xc2 x80 x00 x03 xa0r h xe7 xc3 xad ng xfe x1dh xdb x00 x00 rs xa8 x1c xe8 xdd x85 xeb x02 x93 xc1 xd6 n xb0 xa0 xff xd9";"CODE"
"coding utf 8";"-"
"console instance";"CODE"
"determine if we can just highlight the current one";"-"
"or if we need to rebuild the breadcrumb";"TASK"
"ok so just toggle this one instead";"CODE"
"we need to rebuild the breadcrumb";"TASK"
"normal call tree node focus";"IRRE"
"nested call widget prop value prop key index in dict list";"IRRE"
"normal call";"IRRE"
"nested call we might edit subvalue";"IRRE"
"trying to resolve type dynamically";"IRRE"
"widget no longer exists just remove it";"OUTD"
"array of addons that will be created at console creation";"CODE"
"addons consoleaddonmode";"CODE"
"display mode of the console either docked at the bottom or as a";"CODE"
"floating window";"CODE"
"current widget being selected";"CODE"
"indicate if the inspector inspection is enabled if yes the next";"-"
"touch down will select a the widget under the touch";"CODE"
"true if the console is activated showed";"CODE"
"instantiate all addons";"TASK"
"select the first panel";"CODE"
"reverse the loop look at children on top first and";"IRRE"
"modalviews before others";"CODE"
"no widget to highlight reduce rectangle to 0 0";"CODE"
"try to filter widgets that are not visible invalid inspect target";"OUTD"
"reverse the loop look at children on top first";"IRRE"
"self win remove widget self";"CODE"
"if scancode 273 top";"-"
"elif scancode 274 down";"CODE"
"elif scancode 276 left";"-"
"elif scancode 275 right";"-"
"top bar";"IRRE"
"bottom bar";"IRRE"
"reverse the loop look at children on top first and";"IRRE"
"modalviews before others";"CODE"
"no widget to highlight reduce rectangle to 0 0";"CODE"
"try to filter widgets that are not visible invalid inspect target";"OUTD"
"reverse the loop look at children on top first";"IRRE"
"normal call tree node focus";"IRRE"
"nested call widget prop value prop key index in dict list";"IRRE"
"normal call";"IRRE"
"nested call we might edit subvalue";"IRRE"
"trying to resolve type dynamically";"IRRE"
"widget no longer exists just remove it";"OUTD"
"dunno why but if we are creating inspector within the start no lang";"META"
"rules are applied";"-"
"draw cursor";"-"
"pull joycursor to the front when added";"TASK"
"as a child directly to the window";"CODE"
"bind unbind when joycursor s state is changed";"-"
"create cursor points";"IRRE"
"check axes and set offset if a movement is registered";"IRRE"
"invert y axis to behave like mouse";"-"
"set intensity of joycursor with joystick buttons";"IRRE"
"window event correction necessary";"CODE"
"move joycursor as a mouse";"-"
"pin the cursor to the mouse pos";"CODE"
"always listen for joystick input to open the module";"CODE"
"like a keyboard listener";"-"
"if key 293 and modifiers f12";"-"
"elif key 292 and modifiers f11";"-"
"elif key 292 and modifiers shift shift f11";"-"
"late import to avoid breaking module loading";"CODE"
"if key 289 f8";"-"
"elif key 288 f7";"-"
"elif key 287 f6";"-"
"attributes";"META"
"profile mask";"-"
"filename";"-"
"taken from http en wikipedia org wiki list of displays by pixel density";"CODE"
"device name width height dpi density";"-"
"taken from design google com devices";"CODE"
"please consider using another data instead of";"CODE"
"a dict for autocompletion to work";"CODE"
"these are all in landscape";"-"
"simulate with the android bar";"IRRE"
"fixme should be configurable";"-"
"xxx use ctx";"-"
"coding utf 8";"-"
"this will call max on the result dictionary so it s best to store";"IRRE"
"it instead of calling it 3 times consecutively";"IRRE"
"bind your callbacks to track all matching operations";"IRRE"
"the format below is referred to as strokes a list of stroke paths";"CODE"
"note that each path shown here consists of two points ie a straight";"TASK"
"line if you plot them it looks like a t hence the name";"-"
"now you can search for the t gesture using similar data user input";"CODE"
"this will trigger both of the callbacks bound above";"IRRE"
"same as above but keep track of progress using returned value";"IRRE"
"print progress progress 0";"CODE"
"assuming a kivy clock clock tick here";"-"
"print result progress 1";"IRRE"
"default number of gesture matches per frame";"CODE"
"fixme relevant number";"-"
"algorithm data";"-"
"recognizer";"-"
"will match all names that start with a capital n";"CODE"
"ie next new n nebraska etc but not n or next";"META"
"exactly n";"-"
"nebraska teletubbies france fraggle n n etc";"CODE"
"max priority 50";"-"
"max priority 50 same result as above";"IRRE"
"min priority 50 max 100";"-"
"prepare a correctly sorted tasklist";"TASK"
"now test each gesture in the database against filter criteria";"CODE"
"fixme use a try block maybe shelve or something";"CODE"
"fixme match them all with protractor and don t load exacts or";"CODE"
"just compare the data or something seems better to do this on import";"CODE"
"than on every subsequent call to recognize and fix it in general";"IRRE"
"too";"-"
"obtain a list of multistrokegesture objects matching filter arguments";"IRRE"
"initialize the candidate and result objects";"IRRE"
"this is done to inform caller if they bind to on complete and there";"CODE"
"is nothing to do perhaps should just return none";"IRRE"
"this callback is scheduled once per frame until completed";"CODE"
"get the best distance and number of matching operations done";"CODE"
"the loop has ended prepare to dispatch complete";"CODE"
"dispatch or reschedule another run";"CODE"
"end recognize tick";"CODE"
"recognize helper function do not use directly set up a";"CODE"
"candidate object from arguments either use a specified object";"CODE"
"or make a new one from strokes and apply safe skip settings to";"CODE"
"use less resources";"-"
"default event handlers";"CODE"
"progresstracker";"-"
"fired by recognize";"-"
"fired locally";"IRRE"
"results self results to avoid too many self lookups";"CODE"
"add a result used internally by the recognize function";"CODE"
"multistrokegesture";"-"
"vector x1 y1 vector x2 y2 stroke 1";"-"
"vector vector vector vector stroke 2";"-"
"stroke 3 stroke 4";"-"
"optimal cosine distance inlined here for performance";"CODE"
"if you put the below directly into math acos you will get a domain";"CODE"
"error when a 1 0 and angle 0 0 ie math cos angle 1 0 it seems to";"-"
"be because float representation of 1 0 1 0 is 1 0 ie 1 00000 001";"CODE"
"and this is problematic for math acos";"CODE"
"if you try math acos 1 0 1 0 in interpreter it does not happen";"CODE"
"only with exact match at runtime";"CODE"
"fixme i m sure there is a better way to do it but";"TASK"
"elif result 1 has not happened to me but i leave it here";"IRRE"
"handle a theoretical case where a multistrokegesture is composed";"CODE"
"manually and the orientation sensitive flag is true and contains";"-"
"a unistroketemplate that has orientation sensitive false or vice";"-"
"versa this would cause keyerror requesting nonexistent vector";"CODE"
"count as a match operation now since the call to get";"IRRE"
"angle similarity below will force vector calculation";"CODE"
"even if it doesn t make it to get distance";"CODE"
"note with this implementation we always resample the candidate";"TASK"
"to any encountered unistroketemplate numpoints here the filter";"CODE"
"is only applied to multistrokegesture see theoretical case";"CODE"
"above should not matter normally";"-"
"skip if candidate gesture angles are too far off";"-"
"get the distance between cand tpl paths";"-"
"seed with index of each stroke";"-"
"prepare orders";"-"
"generate unistroke permutations";"-"
"heap permute algorithm";"-"
"create unistroke permutations from self strokes";"CODE"
"while b pow 2 len r use b s bits for directions";"CODE"
"if b i 1 1 is b s bit at index i 1";"-"
"unistroketemplate";"-"
"all previously computed data is now void";"CODE"
"used to lazily prepare the template";"OUTD"
"how many points are we resampling to";"CODE"
"p rotate by p radians restore";"-"
"now store it using the number of points in the resampled path as the";"CODE"
"dict key on the next call to get it will be returned instead of";"IRRE"
"recomputed implicitly you must reset self db or call prepare for";"IRRE"
"all the keys once you manipulate self points";"CODE"
"compute startangleindex as n 8";"IRRE"
"candidate";"-"
"used to lazily prepare the candidate";"OUTD"
"angle between unit vectors inlined here for performance";"CODE"
"fixme domain error on float representation of 1 0 exact match";"CODE"
"see comments in multistrokegesture get distance";"-"
"inlined combine strokes for performance";"CODE"
"compute startangleindex as n 8";"IRRE"
"full rotation invariance";"CODE"
"rotation bounded invariance";"CODE"
"bound points rotate by points radians restore";"CODE"
"helper functions from this point on this is all directly related to the";"CODE"
"recognition algorithm and is almost 100 transcription from the javascript";"CODE"
"resample a path to n points";"CODE"
"work insert i q q is the next i";"CODE"
"rounding error insert the last point";"CODE"
"rotate points around centroid";"CODE"
"1d or 2d gesture test";"IRRE"
"translate points around centroid";"CODE"
"helper function for the protractor algorithm";"CODE"
"depending the platform if openssl support wasn t compiled before python";"CODE"
"this class is not available";"CODE"
"list to save urlrequest and prevent gc on un referenced objects";"CODE"
"url of the request";"CODE"
"request body passed in init";"IRRE"
"request headers passed in init";"CODE"
"save our request to prevent gc";"CODE"
"using trigger can result in a missed on success event";"IRRE"
"clean ourself when the queue is empty";"TASK"
"ok authorize the gc to clean us";"TASK"
"parse and fetch the current url";"IRRE"
"read content";"CODE"
"before starting the download send a fake progress to permit the";"CODE"
"user to initialize his ui";"IRRE"
"ensure that results are dispatched for the last chunk";"IRRE"
"avoid trigger";"CODE"
"if it s an image decoding would not work";"-"
"return everything";"IRRE"
"entry to decode url from the content type";"CODE"
"for example if the content type is a json it will be automatically";"IRRE"
"decoded";"-"
"read the result pushed on the queue and dispatch to the client";"CODE"
"small workaround in order to prevent the situation mentioned";"-"
"in the comment below";"CODE"
"xxx usage of dict can be dangerous if multiple headers";"CODE"
"are set even if it s invalid but it look like it s ok";"IRRE"
"http stackoverflow com questions 2454494";"CODE"
"urllib2 multiple set cookie headers in response";"CODE"
"append user pass to hostname if specified";"CODE"
"parse url";"IRRE"
"translate scheme to connection class";"CODE"
"reconstruct path to pass on the request";"CODE"
"path parse fragment";"IRRE"
"create connection instance";"IRRE"
"send request";"CODE"
"read header";"CODE"
"get method";"-"
"send request";"CODE"
"show warning and return a sane value";"IRRE"
"rgb";"-"
"rgba";"-"
"rrggbb";"-"
"rrggbbaa";"-"
"default r g b values to 1 if greater than 255 else x 255";"IRRE"
"in case of invalid input like rgb rgb r rgb r g";"CODE"
"if text 0";"-"
"raise colorexception invalid color format for r text";"CODE"
"ambiguous case";"META"
"put some values";"IRRE"
"using the same index key erases all previously added key value pairs";"TASK"
"get a value using a index key and key";"IRRE"
"or guess the key entry for a part of the key";"CODE"
"original store get tito";"-"
"original store put tito name mathieu";"-"
"original store delete tito";"CODE"
"original store count";"-"
"original store exists tito";"-"
"original for key in store keys";"CODE"
"get all the key entry availables";"CODE"
"get only the entry from key entry";"CODE"
"operators";"-"
"used for implementation";"TASK"
"privates";"CODE"
"xxx not entirely sure about the best value 0 or 1";"TASK"
"backward compatibility first argument was a dict";"-"
"don t import redis during the documentation generation";"CODE"
"already installed don t do it twice";"CODE"
"get gobject mainloop context";"IRRE"
"schedule the iteration each frame";"CODE"
"xxx we need to loop over context here otherwise we might have a lag";"TASK"
"android support";"-"
"after wakeup we need to redraw more than once otherwise we get a";"TASK"
"black screen";"-"
"init the library";"IRRE"
"check if android should be paused or not";"IRRE"
"if pause is requested just leave the app";"CODE"
"do nothing until android asks for it";"CODE"
"try to get the current running application";"CODE"
"no running application stop our loop";"IRRE"
"try to go to pause mode";"CODE"
"app goes in pause mode wait";"-"
"is it a stop or resume";"-"
"app must stop";"-"
"app resuming now";"-"
"g android redraw count 25 5 frames seconds for 5 seconds";"CODE"
"app doesn t support pause mode just stop it";"CODE"
"prevent installing more than once";"-"
"don t let twisted handle signals unless specifically requested";"CODE"
"install threaded select reactor to use with own event loop";"CODE"
"now we can import twisted reactor as usual";"CODE"
"will hold callbacks to twisted callbacks";"IRRE"
"twisted will call the wake function when it needs to do work";"TASK"
"called every frame to process the reactors work in main thread";"CODE"
"start the reactor by telling twisted how to wake and process";"-"
"make sure twisted reactor is shutdown if eventloop exists";"IRRE"
"twisted 24 3 0";"-"
"start and stop the reactor along with kivy eventloop";"IRRE"
"prevent uninstalling more than once";"-"
"async app tests would be skipped due to async run forcing it to skip so";"CODE"
"it s ok to be none as it won t be used anyway";"-"
"window is its own parent oo";"CODE"
"don t check the child we checked before moving up";"CODE"
"window is its own parent oo";"CODE"
"check what the gl backend might be we can t know for sure";"CODE"
"what it ll be until actually initialized by the window";"IRRE"
"this prevent in some case to be stuck if the screen doesn t refresh";"CODE"
"and we wait for a number of self framecount that never goes down";"CODE"
"reset for the next test but nobody will know if it will be used";"IRRE"
"use default kivy configuration don t load user file";"CODE"
"force window size remove all inputs";"CODE"
"bind ourself for the later screenshot";"CODE"
"ensure our window is correctly created";"IRRE"
"don t save screenshot until we have enough frames";"CODE"
"log debug framecount d self framecount";"CODE"
"check if there is framecount otherwise just";"IRRE"
"assume zero e g if handling runtouchapp manually";"CODE"
"don t create screenshots if not requested manually";"CODE"
"just get a temporary name";"-"
"get a filename for the current unit test";"IRRE"
"capture the screen";"-"
"search the file to compare to";"IRRE"
"get sourcecode";"-"
"s at render d images are different";"CODE"
"generate html";"-"
"color ffdddd if not match else ffffff";"-"
"fd write h2 s d h2 self id self test counter";"CODE"
"device tuio id args";"IRRE"
"set profile to accept x y and pos properties";"IRRE"
"set sx sy properties to ratio e g x win width";"IRRE"
"run depack after we set the values";"IRRE"
"https gist github com tito f111b6916aa6a4ed0851";"CODE"
"subclass for touch event in unit test";"IRRE"
"async app tests would be skipped due to async run forcing it to skip so";"CODE"
"it s ok to fail here as it won t be used anyway";"-"
"from https docs pytest org en latest example simple html";"CODE"
"from https docs pytest org en latest example simple html";"CODE"
"force window size remove all inputs";"CODE"
"ensure our window is correctly created";"IRRE"
"need to do it to reset the global value";"TASK"
"keep track of all the kivy app fixtures so that we can check that it";"-"
"properly dies";"-"
"force window size remove all inputs";"CODE"
"have to make sure all global kv files are loaded before this because";"CODE"
"globally read kv files e g on module import will not be loaded again";"CODE"
"in the new builder except if manually loaded which we don t do";"CODE"
"release all the resources";"-"
"check that the project works normally before packaging";"CODE"
"create pyinstaller package";"IRRE"
"test package";"IRRE"
"right after the animation starts";"-"
"during the first half of the animation";"-"
"during the second half of the animation";"-"
"after the animation completed";"TASK"
"right after the animation starts";"-"
"during the first half of the first round of the animation";"CODE"
"during the second half of the first round of the animation";"CODE"
"during the first half of the second round of the animation";"CODE"
"during the second half of the second round of the animation";"CODE"
"after the animation stopped";"-"
"right after the animation started";"-"
"during the first half of the animation";"-"
"ec assert 1 false 0 n progress is still 0";"TASK"
"during the second half of the animation";"-"
"ec assert 1 false 0 n progress is still 0";"TASK"
"after the animation compeleted";"CODE"
"ec assert 1 false 1 n progress is still 0";"TASK"
"activate widget";"-"
"just tests whether the app is gc d after the test is complete";"IRRE"
"create property in kv and set app to it";"IRRE"
"just tests whether the app is gc d after the test is complete";"IRRE"
"simulate what backend does after first frame render";"CODE"
"ensure that the gstreamer play stop doesn t mess up the volume";"CODE"
"create one just so we don t incur loading cost";"CODE"
"create one just so we don t incur loading cost";"CODE"
"create one just so we don t incur loading cost";"CODE"
"create one just so we don t incur loading cost";"CODE"
"create one just so we don t incur loading cost";"CODE"
"create one just so we don t incur loading cost";"CODE"
"create one just so we don t incur loading cost";"CODE"
"move touch outside";"-"
"move touch outside button bounds";"META"
"first touch";"-"
"second touch";"-"
"on press should be called only once";"IRRE"
"initial state";"IRRE"
"first touch";"-"
"second touch";"-"
"third touch";"-"
"release first touch still pressed";"TASK"
"release second touch still pressed";"TASK"
"release last touch now not pressed";"-"
"three touches down";"CODE"
"cancel first touch move outside";"-"
"on cancel assert not called still have active touches";"TASK"
"release second touch inside bounds";"-"
"on release assert not called still have active touches";"TASK"
"cancel last touch";"-"
"on cancel assert called once now cancel fires";"CODE"
"on release assert not called should not fire release";"IRRE"
"monkey patch to ignore exception";"CODE"
"we should be able to create the event";"IRRE"
"with pytest raises typeerror";"IRRE"
"kivy clock start clock";"-"
"for now it doesn t yet raise a error";"CODE"
"is the time reset was called no point of testing it";"IRRE"
"assert list e history 20";"CODE"
"coding utf 8";"-"
"out of bounds of fbo";"-"
"in fbo black";"-"
"color 0 0 56789 0 5";"-"
"color 0 56789 0 0 5";"-"
"overlap above 2 w alpha";"-"
"color 0 0 56789 0 1";"-"
"color 0 56789 0 0 1";"-"
"overlap above 2 w o alpha";"-"
"returned class";"IRRE"
"returned types in container";"IRRE"
"returned values";"IRRE"
"coding utf 8";"-"
"xxx please be careful to only save this file with an utf 8 editor";"CODE"
"existing files";"-"
"on macos and ios files ending in uffff etc are changed";"CODE"
"we cannot predict the filenames that will be created";"IRRE"
"if it works on window and linux we can safely assume it also works";"CODE"
"on macos and ios";"-"
"coding utf 8";"-"
"add a fake garden module to the temporary directory";"CODE"
"with initial arguments";"IRRE"
"changing properties later";"-"
"the size of the rectangle containing the fbo texture shadow needs";"TASK"
"to be adjusted according to the size of the shadow otherwise there";"IRRE"
"will be an unwanted cropping behavior";"-"
"now we will turn on the inset mode it is expected that";"IRRE"
"there will be no size adjustments";"-"
"now turning off and reverting back to the default mode";"CODE"
"testing with initial arguments";"IRRE"
"now turning off and reverting back to the default mode";"CODE"
"now we will turn on the inset mode it is expected that";"IRRE"
"there will be no size adjustments";"-"
"if the size of the rectangle containing the fbo texture shadow";"CODE"
"changes its position will need to be adjusted";"TASK"
"now we will turn on the inset mode it is expected that";"IRRE"
"there will be no position adjustments";"-"
"now turning off and reverting back to the default mode";"CODE"
"testing with initial arguments";"IRRE"
"now turning off and reverting back to the default mode";"CODE"
"now we will turn on the inset mode it is expected that";"IRRE"
"there will be no position adjustments";"-"
"there is a bug in roundedrectangle that distorts the texture if the";"CODE"
"radius value is less than 1 otherwise it could be 0";"IRRE"
"testing with initial arguments";"IRRE"
"basic circle";"-"
"reduced circle";"-"
"moving circle";"-"
"ellipse";"-"
"1 point";"CODE"
"25 points";"CODE"
"basic rounded rectangle";"-"
"the largest angle allowed is equal to the smallest dimension width";"CODE"
"or height minus the largest angle value between the anterior angle";"IRRE"
"and the posterior angle";"-"
"same approach as above";"-"
"a circle should be generated if width and height are equal and all";"-"
"angles passed are greater than or equal to the smallest dimension";"CODE"
"currently the minimum radius should be 1 to avoid rendering issues";"CODE"
"angles adjustment avoid issue if radius is less than 1";"CODE"
"if width and or height 2px the figure should not be rendered";"CODE"
"this avoids some known smoothline rendering issues";"CODE"
"normal line with width 1";"CODE"
"normal line with width 3";"CODE"
"enlarged line that should look width 3";"-"
"translate 30 10 1 so the enlargement goes around 0 0 0";"-"
"scale 3 1 1 x scaled by 3 so the line width should become 3";"CODE"
"code defined in the points setter of antialiasingline class";"CODE"
"the same function present in the antialiasingline code";"CODE"
"externalized for testing";"IRRE"
"at least 3 points are required otherwise we will return an empty";"CODE"
"list which means there are no valid points";"OUTD"
"we expect a type error to be thrown if stencil mask is not an object";"CODE"
"of type instruction";"CODE"
"no typeerror here";"-"
"test the gradient responsible for the smooth line effect";"IRRE"
"check the width defined through tests with the custom texture";"IRRE"
"this set of points must remain unchanged";"CODE"
"this set of points should be reduced from 16 to 8";"CODE"
"this set of points should be reduced from 72 to 50";"CODE"
"this set of points should be reduced to";"CODE"
"line closed default";"CODE"
"without closing the line";"-"
"test if antialiasing is disabled if the graphic has one of its";"IRRE"
"dimensions decreased to less than 4px";"-"
"re enable antialiasing line rendering";"CODE"
"disabling antialiasing";"-"
"re enable antialiasing";"-"
"test if antialiasing is disabled if the graphic is initialized with";"IRRE"
"at least one of its dimensions less than 4px";"-"
"test if antialiasing is disabled if the graphic has one of its";"IRRE"
"dimensions decreased to less than 4px";"-"
"re enable antialiasing line rendering";"CODE"
"disabling antialiasing";"-"
"re enable antialiasing";"-"
"test if antialiasing is disabled if the graphic is initialized with";"IRRE"
"at least one of its dimensions less than 4px";"-"
"test if antialiasing is disabled if the graphic has one of its";"IRRE"
"dimensions decreased to less than 4px";"-"
"re enable antialiasing line rendering";"CODE"
"disabling antialiasing";"-"
"re enable antialiasing";"-"
"test if antialiasing is disabled if the graphic is initialized with";"IRRE"
"at least one of its dimensions less than 4px";"-"
"500 500 400 400 irrelevant just for testing purposes";"IRRE"
"500 500 400 400 irrelevant just for testing purposes";"IRRE"
"test disabling antialiasing if the points are very close";"CODE"
"re enable antialiasing line rendering";"CODE"
"disabling antialiasing";"-"
"re enable antialiasing";"-"
"test disabling antialiasing if the points are very close";"CODE"
"re enable antialiasing line rendering";"CODE"
"disabling antialiasing";"-"
"re enable antialiasing";"-"
"normal args";"IRRE"
"key word args";"IRRE"
"create a root widget";"IRRE"
"put some graphics instruction on it";"CODE"
"render and capture it directly";"IRRE"
"create a root widget";"IRRE"
"put some graphics instruction on it";"CODE"
"render and capture it directly";"IRRE"
"create a root widget";"IRRE"
"put some graphics instruction on it";"CODE"
"render and capture it directly";"IRRE"
"xxx on osx ci builder img sdl3 is not used";"OUTD"
"therefore the test below wont work yet with imageio only";"TASK"
"load kivy logo";"CODE"
"try to save without any format";"CODE"
"save it in png";"CODE"
"if false then there is no provider";"IRRE"
"try to save in a filename";"CODE"
"xxx test wrote but temporary commented";"IRRE"
"xxx because of the issue 6123 on osx";"-"
"xxx https github com kivy kivy issues 6123";"CODE"
"with open filename rb as fd2";"CODE"
"pngdatafile fd2 read";"CODE"
"check the png file data is the same as bytesio";"-"
"self asserttrue pngdata pngdatafile";"CODE"
"save it in jpeg";"CODE"
"if false then there is no provider";"IRRE"
"you need an image testsuite to run this for information see";"CODE"
"kivy tools image testsuite readme md";"IRRE"
"kivy image test protocol v0 pixel values";"IRRE"
"v0 pixels note t is not included here see match prediction";"CODE"
"kivy image test protocol v0 file name";"IRRE"
"width x height pattern alpha fmtinfo testname encoder ext";"IRRE"
"converts predicted rgba pixels to the format claimed by image loader";"CODE"
"rgb bgr default to 4 byte alignment";"CODE"
"assume pitch 0 unaligned";"-"
"print pixnum ptr bpp format pixnum ptr bpp pix";"CODE"
"required for ffpy memview";"CODE"
"fixme bytearray for py2 compat i can t be bothered to research";"CODE"
"assume pitch 0 unaligned";"-"
"gif format not listed as supported in sdl3 loader";"CODE"
"test activating first checkbox";"IRRE"
"test activating second checkbox deactivates first";"IRRE"
"test activating first checkbox again";"IRRE"
"test activating second checkbox again";"IRRE"
"test deactivating when allow no selection false";"CODE"
"this should not work at least one must stay active";"CODE"
"since allow no selection false b should remain active";"CODE"
"test with allow no selection true";"IRRE"
"now deactivating should work";"-"
"activate again";"-"
"bug fixed";"-"
"put utf 8 in string and validate no more crash due to str encoding";"CODE"
"put utf 8 in string validate close open the app and edit the value no";"CODE"
"more weird space due to ascii utf8 encoding";"-"
"create an unicode directory and select it with path no more crash at";"IRRE"
"validation";"-"
"create an unicode directory and select it with path and restart the";"IRRE"
"path is still correct";"TASK"
"the old error before utf 8 was standard";"OUTD"
"import knspace kivy uix behaviors knspace knspace";"CODE"
"this could actually be none rather than raising depending";"CODE"
"on when the class was instantiated so if this fails change the";"CODE"
"test to assert is none";"IRRE"
"base class needed for builder";"CODE"
"invalid indent";"OUTD"
"builder load string dedent kivy 1 0";"CODE"
"call the on press and check the result";"IRRE"
"this test cover a large part of the lang";"IRRE"
"and was used for testing the validity of the new rewrite lang";"CODE"
"however it s not self explained enough";"CODE"
"check that all the events match";"-"
"check that the ids are properly saved during on kv post dispatch";"CODE"
"check that the root widget is as expected";"-"
"check that the base widget is as expected";"-"
"check that the properties have expected values";"IRRE"
"make a copy of the ids at the current moment";"-"
"except attributeerror python 2";"CODE"
"used to determine which tests must be skipped";"TASK"
"restores handler to original state";"-"
"create the default file first so it gets deleted so names match";"CODE"
"wait a little so the timestamps are different for different files";"CODE"
"files that should have remained after purge";"CODE"
"one of the remaining files is the current open log remove it";"CODE"
"the open log may or may not have been counted in the remaining";"CODE"
"files remove one from expected to match removed open file";"CODE"
"pytest mark xfail issue 7986 not yet fixed";"TASK"
"assert pytest approx 7 factor dpi2px 7 unit 2 inch in";"CODE"
"assert pytest approx 10 new factor dpi2px 10 unit";"CODE"
"import factory kivy factory factory";"CODE"
"kill kv lang logging too long test";"IRRE"
"add the logging back";"TASK"
"build the widget tree add window as the main el";"CODE"
"activate inspector with root as ctx";"-"
"pull the inspector drawer from bottom";"CODE"
"by default is inspector appended as the first child";"CODE"
"to the window and positioned at the bottom";"CODE"
"close inspector";"CODE"
"stop inspector completely";"CODE"
"build the widget tree add window as the main el";"CODE"
"activate inspector with root as ctx";"-"
"pull the inspector drawer from top";"CODE"
"by default is inspector appended as the first child";"CODE"
"to the window we move it to the top";"CODE"
"elf advance frames 20 drawer is moving like with activate";"-"
"close inspector";"CODE"
"stop inspector completely";"CODE"
"build the widget tree add window as the main el";"CODE"
"checked widget";"-"
"activate inspector with root as ctx";"-"
"pull the inspector drawer from bottom";"CODE"
"touch button center";"META"
"open inspector properties";"CODE"
"check if the button is selected";"IRRE"
"stored instance";"-"
"data in properties";"-"
"slice because the string is displayed with quotes";"IRRE"
"close inspector";"CODE"
"stop inspector completely";"CODE"
"build the widget tree add window as the main el";"CODE"
"checked widget";"-"
"activate inspector with root as ctx";"-"
"pull the inspector drawer from bottom";"CODE"
"but don t inspect yet";"TASK"
"touch button center to open the popup";"META"
"start inspecting";"-"
"inspect firstmodal s button";"META"
"open inspector properties";"CODE"
"check if the popup is selected";"IRRE"
"stored instance";"-"
"check with new popup instance if the properties match";"CODE"
"data in properties";"-"
"slice because the string is displayed with quotes";"IRRE"
"close popup";"CODE"
"close inspector";"CODE"
"stop inspector completely";"CODE"
"build the widget tree add window as the main el";"CODE"
"checked widget";"-"
"activate inspector with root as ctx";"-"
"pull the inspector drawer from bottom";"CODE"
"but don t inspect yet";"TASK"
"touch button center to open the popup";"META"
"touch window center to open";"CODE"
"the second and the third popup";"-"
"fixed order first opened last closed";"CODE"
"start inspecting";"-"
"inspect button";"META"
"check if the popup is selected";"IRRE"
"stored instance";"-"
"close popup";"CODE"
"close inspector";"CODE"
"stop inspector completely";"CODE"
"patch win on close method to prevent eventloop from removing";"CODE"
"window from event listeners list";"CODE"
"restore method on close to window";"CODE"
"not rendering widgets in tests so don t do screenshots";"CODE"
"cleanup";"TASK"
"cleanup";"TASK"
"cleanup";"TASK"
"cleanup";"TASK"
"cleanup";"TASK"
"cleanup";"TASK"
"test begin event";"IRRE"
"test update event";"IRRE"
"test end event";"IRRE"
"test begin event";"IRRE"
"test update event";"IRRE"
"test end event";"IRRE"
"cleanup";"TASK"
"cleanup";"TASK"
"helper methods";"-"
"flip because the mouse provider uses system s";"IRRE"
"raw one and it s changed to bottom left origin";"-"
"with window s system size 1 for mouse pos";"CODE"
"prepare mousemotioneventprovider";"-"
"and widget it interacts with";"CODE"
"defaults from me it s missing because we use";"CODE"
"the provider directly instead of me";"CODE"
"touch dot appears touch again dot disappears";"CODE"
"register mouse provider";"-"
"no mouse touch anywhere";"IRRE"
"right button down red dot should appear";"CODE"
"doesn t do anything on a pure button";"CODE"
"cleanup";"TASK"
"remove mouse provider";"-"
"multitouch sim is changed in on touch down";"CODE"
"method of the widget that s able to handle";"CODE"
"multiple touches therefore for scatter we";"CODE"
"need to dispatch the method and because we";"TASK"
"triggered only on mouse down directly i e";"CODE"
"without me dispatch on touch down was not";"CODE"
"called multitouch sim is false";"IRRE"
"elf advance frames 1 initialize stuff";"IRRE"
"the red dot isn t present";"CODE"
"the red dot is present";"CODE"
"xxx right button up";"META"
"first release the touch then check so that we";"IRRE"
"have the red dot drawn in on demand and in the";"CODE"
"default multitouch everywhere because in the";"CODE"
"multitouch on demand is the circle drawn after";"-"
"the touch is released in on mouse release";"-"
"because the red dot is removed by the left button";"IRRE"
"the red dot is present";"CODE"
"button is down on the previous dot s position";"CODE"
"if the multitouch is disabled the touch event";"-"
"increments the counter";"-"
"the right click is ignored test ends here";"CODE"
"cleanup";"TASK"
"remove mouse provider";"-"
"the red dot is present";"CODE"
"ellipse proxy 3 1318 instruction proxy ref";"CODE"
"the dot is removed after the touch is released";"OUTD"
"when right touch is preserved dot remains";"CODE"
"when left touch is destroyed dot removed";"OUTD"
"no matter where";"-"
"the touch which holds the only ref to the dot";"CODE"
"instance ellipse is collected therefore the";"CODE"
"proxy can confirm the dot is removed";"OUTD"
"indirect ref at least it would be nasty for";"CODE"
"checking if the ellipse remained on visible on";"CODE"
"the canvas after being gc ed if not impossible";"-"
"without the instruction object trick";"CODE"
"the red dot is present";"CODE"
"cleanup";"TASK"
"remove mouse provider";"-"
"touch dot appears move touch dot moves";"CODE"
"release touch touch dot disappear";"IRRE"
"register mouse provider";"-"
"no mouse touch anywhere";"IRRE"
"right button down red dot should appear";"CODE"
"if the multitouch on demand is disabled";"-"
"doesn t do anything on a pure button";"CODE"
"cleanup";"TASK"
"remove mouse provider";"-"
"xxx right button up";"META"
"first release the touch then check so that we";"IRRE"
"have the red dot drawn in on demand and in the";"CODE"
"default multitouch everywhere because in the";"CODE"
"multitouch on demand is the circle drawn after";"-"
"the touch is released in on mouse release";"-"
"on demand works after the touch is up";"-"
"multitouch sim is changed in on touch down";"CODE"
"method of the widget that s able to handle";"CODE"
"multiple touches therefore for scatter we";"CODE"
"need to dispatch the method and because we";"TASK"
"triggered only on mouse down directly i e";"CODE"
"without me dispatch on touch down was not";"CODE"
"called multitouch sim is false";"IRRE"
"elf advance frames 1 initialize stuff";"IRRE"
"the red dot isn t present";"CODE"
"the red dot is present";"CODE"
"do not make any hard refs to drawelement";"CODE"
"the right click doesn t draw the red dot";"CODE"
"the instructions aren t present test ends";"CODE"
"the red dot isn t present";"CODE"
"cleanup";"TASK"
"remove mouse provider";"-"
"the red dot moves when the touch is moving";"CODE"
"bounding box from rectangle r 10 20 width";"CODE"
"right button up";"META"
"because the red dot is removed by the left button";"IRRE"
"the red dot is present";"CODE"
"the dot is at 11 11 but the touch is in";"META"
"its bounding box therefore it can move it";"CODE"
"manipulating already existing touch";"CODE"
"no new one was created";"CODE"
"the red dot is present";"CODE"
"the red dot moves when the touch is moving";"CODE"
"bounding box from rectangle r 10 20 width";"CODE"
"the dot is removed after the touch is released";"OUTD"
"when right touch is preserved dot remains";"CODE"
"when left touch is destroyed dot removed";"OUTD"
"no matter where";"-"
"the red dot is present";"CODE"
"cleanup";"TASK"
"remove mouse provider";"-"
"tests";"IRRE"
"register mouse provider";"-"
"no mouse touch anywhere";"IRRE"
"left button down";"META"
"the red dot isn t present";"CODE"
"left button up";"META"
"after the releasing the touch disappears";"-"
"but the counter remains";"META"
"cleanup";"TASK"
"remove mouse provider";"-"
"register mouse provider";"-"
"no mouse touch anywhere";"IRRE"
"right button down red dot should appear";"CODE"
"the red dot is present";"CODE"
"do not make any hard refs to drawelement";"CODE"
"check ellipse s position";"-"
"bounding box from rectangle r 10 20 width";"CODE"
"almost equal because the correct y uses the same";"IRRE"
"float float which returns decimal garbage";"CODE"
"the red dot moves when the touch is moving";"CODE"
"bounding box from rectangle r 10 20 width";"CODE"
"bounding box from rectangle r 10 20 width";"CODE"
"because the red dot is removed by the left button";"IRRE"
"the red dot is present";"CODE"
"cleanup";"TASK"
"remove mouse provider";"-"
"these are taken from the examples in javascript code but made unistrokes";"CODE"
"dataset that matches n pretty well";"IRRE"
"recognizer scheduling";"-"
"recognize tick is scheduled here compares to tinvar";"IRRE"
"now complete the search operation";"CODE"
"clock tick first run scheduled here 9 left";"CODE"
"clock tick 8 left";"-"
"clock tick 7 left";"-"
"run some immediate searches should not interfere";"CODE"
"clock tick matches tbound in this tick";"CODE"
"clock tick should match ninv but times out got t";"META"
"clock tick matches tbound in this tick";"CODE"
"clock tick matches ninvar in this tick";"CODE"
"clock tick should match tinvar but times out";"META"
"recognizer filter tests";"IRRE"
"misc tests";"IRRE"
"test protractor";"IRRE"
"a set wid 88 number shouldn t be accepted";"IRRE"
"try";"CODE"
"a set wid string shouldn t be accepted";"IRRE"
"self fail number accept string fail";"CODE"
"except valueerror";"IRRE"
"pass";"-"
"test observer";"IRRE"
"test observer";"IRRE"
"button togglebutton active false explicit initial state";"META"
"should not update rebind false";"CODE"
"elf assertequal obj false text inactive still old value";"IRRE"
"color set wid 00ff00";"IRRE"
"color set wid 7f7fff7f";"IRRE"
"initial checks";"IRRE"
"get value should call getter once";"IRRE"
"setter should raise an attributeerror";"IRRE"
"initial checks";"IRRE"
"set property should call setter to set the value";"IRRE"
"getter and callback should not be called because set prop doesn t";"IRRE"
"returns true";"IRRE"
"set property to same value as before should only call setter";"IRRE"
"get value of the property should call getter once";"IRRE"
"initial checks";"IRRE"
"get value of the property should call getter once";"IRRE"
"get value of the property should return cached value";"IRRE"
"getter should not be called";"IRRE"
"set value of property should call getter and setter";"IRRE"
"assert values when setting x width or right properties";"IRRE"
"callback should be called only when property changes";"IRRE"
"initial checks";"IRRE"
"set property should call setter to set the value and getter to";"IRRE"
"to get the value for dispatch call";"IRRE"
"set property to same value as before setter and getter and callback";"IRRE"
"are called";"IRRE"
"initial checks";"IRRE"
"change the base value should trigger an update for the cache";"IRRE"
"now read the value again should use the cache";"IRRE"
"change the prop itself should trigger an update for the cache";"CODE"
"initial checks";"IRRE"
"set alias property some value should call setter and then getter to";"IRRE"
"pass the value to callback";"IRRE"
"same as the step above should call setter getter and callback";"IRRE"
"get the value of property should use cached value";"IRRE"
"widget2 name pasta does not raise a valueerror";"CODE"
"widget3 name pasta does not raise a valueerror";"CODE"
"def test object init error the above 3 test rely on this";"IRRE"
"pass touch file";"-"
"coding utf 8";"-"
"xxx mathieu i tried to fix the window context to prevent segfault here";"CODE"
"but nothing actually works works alone but not after a window restart";"META"
"on linux";"-"
"1 0x00007ffff12807e9 in at usr lib libnvidia glcore so 418 43";"-"
"2 0x00007ffff1288554 in at usr lib libnvidia glcore so 418 43";"-"
"3 0x00007ffff0e2e3db in at usr lib libnvidia glcore so 418 43";"-"
"4 0x00007ffff5d5ae15 in pyx f 4kivy 8graphics 3vbo 11vertexbatch draw noqa";"-"
"pyx v self 0x7fffed641390 at kivy graphics vbo c 6529";"CODE"
"on osx";"-"
"thread 1 queue com apple main thread";"CODE"
"stop reason exc bad access code 1 address 0x0";"TASK"
"frame 0 0x00007fff555d9d42 glengine glerunvertexsubmitimmediate 1234 noqa";"CODE"
"frame 1 0x00007fff554c1544 glengine gldrawelements exec 563";"-"
"frame 2 0x000000010429d273 vbo cpython 36m darwin so";"CODE"
"pyx f 4kivy 8graphics 3vbo 11vertexbatch draw";"-"
"pyx v self 0x000000010cf344f8 at vbo c 6575 opt";"CODE"
"rstdocument scatter gridlayout rstparagraph";"CODE"
"anchor and ref might change in the future";"TASK"
"test queries";"IRRE"
"basic functionality tests";"IRRE"
"toggle timing tests toggle on property";"IRRE"
"call as class method";"IRRE"
"remove btn1";"-"
"use class method to get remaining widgets";"CODE"
"try to modify returned list";"CODE"
"verify internal group is unchanged";"CODE"
"memory management tests";"IRRE"
"get group reference";"CODE"
"create weak references to track garbage collection";"IRRE"
"delete buttons and the group list";"CODE"
"verify widgets were garbage collected";"-"
"verify only btn1 remains in group after gc";"CODE"
"validation and edge cases";"CODE"
"event callback tests";"IRRE"
"copied from actionbar example edited for the test";"IRRE"
"press release";"-"
"kill kv lang logging too long test";"IRRE"
"add the logging back";"TASK"
"mustn t allow more than one dropdown opened";"CODE"
"passed";"-"
"click on group 2 to open its dropdown";"CODE"
"dropdown shows up";"CODE"
"then click away";"CODE"
"group 2 dropdown disappears";"CODE"
"click on group 1 to open its dropdown";"CODE"
"dropdown shows up";"CODE"
"then click away";"CODE"
"group 1 dropdown disappears";"CODE"
"no dropdown present yet";"TASK"
"click on active group";"CODE"
"active group dropdown shows up";"CODE"
"active group dropdown value in weakproxy";"IRRE"
"click away";"CODE"
"wait for closed group dropdown to disappear";"CODE"
"go to the next frame after the dropdown disappeared";"CODE"
"no dropdown is open";"CODE"
"click on group 2 to open its dropdown";"CODE"
"dropdown shows up";"CODE"
"then click on group 1 to open its dropdown";"CODE"
"group 2 dropdown disappears group 1 dropdown shows up";"CODE"
"click away";"CODE"
"no dropdown is opened";"CODE"
"no dropdown present yet";"TASK"
"click on group 2";"CODE"
"group 2 dropdown shows up";"CODE"
"group 2 dropdown value in weakproxy";"IRRE"
"click away from actionbar and wait for it to disappear";"CODE"
"click on group 1";"CODE"
"wait for closed group 2 dropdown to disappear";"CODE"
"and for group 1 dropdown to appear there are 2 dds now";"CODE"
"go to the next frame after the dropdown disappeared";"CODE"
"group 1 dropdown value in weakproxy group 2 dd";"IRRE"
"click away from actionbar";"CODE"
"wait for closed group dropdown to disappear";"CODE"
"go to the next frame after the dropdown disappeared";"CODE"
"no dropdown present in window";"CODE"
"click on group 2 to open its dropdown";"CODE"
"dropdown shows up";"CODE"
"then click on group 2 dropdown button";"CODE"
"dropdown disappears";"CODE"
"click on group 1 to open its dropdown";"CODE"
"dropdown shows up";"CODE"
"then click on group 1 dropdown button";"CODE"
"dropdown disappears";"CODE"
"no dropdown present yet";"TASK"
"click on active group";"CODE"
"active group dropdown shows up";"CODE"
"active group dropdown value in weakproxy";"IRRE"
"click on active group dropdown button needed to window";"CODE"
"wait for closed group dropdown to disappear";"CODE"
"go to the next frame after the dropdown disappeared";"CODE"
"no dropdown is open";"CODE"
"click on group to open its dropdown";"CODE"
"dropdown shows up";"CODE"
"then click on group dropdown button";"CODE"
"dropdown disappears";"CODE"
"repeat";"-"
"no dropdown present yet";"TASK"
"click on group";"CODE"
"group dropdown shows up";"CODE"
"group dropdown value in weakproxy";"IRRE"
"click on group dropdown button";"CODE"
"wait for closed group dropdown to disappear";"CODE"
"go to the next frame after the dropdown disappeared";"CODE"
"no dropdown is open";"CODE"
"load zip with images named 000 png 001 png";"CODE"
"bind to on load because there are various";"CODE"
"steps where the image is re loaded but";"META"
"the event is triggered only at the end";"CODE"
"cube zip has 63 pngs used for animation";"CODE"
"ref loader load urllib";"CODE"
"pure delay fps isn t enough and";"-"
"just 1 isn t either index collisions";"-"
"cube zip has 63 pngs used for animation";"CODE"
"pure delay fps isn t enough and";"-"
"just 1 isn t either index collisions";"-"
"check whether it really changes the images";"-"
"in the anim delay interval";"CODE"
"different frames sequence is changing";"-"
"bubble width button height arrow pos";"META"
"158 9 34 3 bottom left noqa e201 e241";"-"
"651 4 26 1 bottom mid noqa e201 e241";"-"
"6 5 44 7 bottom right noqa e201 e241";"-"
"754 6 50 6 top left noqa e201 e241";"-"
"957 8 74 1 top mid noqa e201 e241";"-"
"852 1 33 1 top right noqa e201 e241";"-"
"472 9 45 1 left top noqa e201 e241";"-"
"578 3 52 7 left mid noqa e201 e241";"-"
"687 8 17 7 left bottom noqa e201 e241";"-"
"313 7 8 6 right top noqa e201 e241";"-"
"194 3 46 4 right mid noqa e201 e241";"-"
"59 3 29 7 right bottom noqa e201 e241";"-"
"101 3 346 0 0 0 73 6 l noqa e201 e241";"-"
"489 0 535 1 0 0 442 1 l noqa e201 e241";"-"
"390 9 728 1 0 0 114 3 l noqa e201 e241";"-"
"224 5 675 5 0 0 560 6 l noqa e201 e241";"-"
"264 9 677 3 0 0 8 3 l noqa e201 e241";"-"
"544 6 126 0 0 0 120 9 l noqa e201 e241";"-"
"290 9 962 6 0 0 275 5 l noqa e201 e241";"-"
"358 4 514 4 0 0 427 1 l noqa e201 e241";"-"
"604 3 648 2 0 0 226 1 l noqa e201 e241";"-"
"287 4 875 6 0 0 446 5 l noqa e201 e241";"-"
"755 7 103 5 444 6 0 0 b noqa e201 e241";"-"
"307 9 471 7 80 9 0 0 b noqa e201 e241";"-"
"849 9 194 8 652 7 0 0 b noqa e201 e241";"-"
"975 7 691 0 120 9 0 0 b noqa e201 e241";"-"
"539 1 903 3 530 6 0 0 b noqa e201 e241";"-"
"37 5 727 5 37 0 0 0 b noqa e201 e241";"-"
"856 5 779 0 565 5 0 0 b noqa e201 e241";"-"
"536 7 228 3 48 4 0 0 b noqa e201 e241";"-"
"170 9 870 4 127 6 0 0 b noqa e201 e241";"-"
"955 7 530 6 526 0 0 0 b noqa e201 e241";"-"
"878 1 690 4 878 1 18 8 r noqa e201 e241";"-"
"771 6 365 2 771 6 31 1 r noqa e201 e241";"-"
"679 7 305 4 679 7 259 6 r noqa e201 e241";"-"
"700 2 614 6 700 2 105 4 r noqa e201 e241";"-"
"444 1 864 5 444 1 152 3 r noqa e201 e241";"-"
"189 0 790 4 189 0 602 9 r noqa e201 e241";"-"
"376 0 993 9 376 0 486 4 r noqa e201 e241";"-"
"518 5 338 5 518 5 194 6 r noqa e201 e241";"-"
"982 1 666 1 982 1 282 5 r noqa e201 e241";"-"
"926 4 565 1 926 4 187 3 r noqa e201 e241";"-"
"375 2 746 6 36 2 746 6 t noqa e201 e241";"-"
"448 9 228 5 297 4 228 5 t noqa e201 e241";"-"
"792 3 593 5 746 2 593 5 t noqa e201 e241";"-"
"856 1 89 7 23 1 89 7 t noqa e201 e241";"-"
"721 3 319 0 356 5 319 0 t noqa e201 e241";"-"
"127 7 355 7 69 3 355 7 t noqa e201 e241";"-"
"412 3 493 8 163 2 493 8 t noqa e201 e241";"-"
"40 8 115 8 15 8 115 8 t noqa e201 e241";"-"
"233 9 148 5 189 4 148 5 t noqa e201 e241";"-"
"982 4 661 5 105 9 661 5 t noqa e201 e241";"-"
"assert content size";"CODE"
"assert arrow layout size";"CODE"
"assert content position";"CODE"
"assert arrow layout position";"CODE"
"assert arrow position within arrow layout";"CODE"
"hal arrow length 2 hal half arrow length";"-"
"assert arrow rotation";"CODE"
"assert content size";"CODE"
"assert content position";"CODE"
"haw bubble arrow width 2 half arrow width";"-"
"test issue 6370";"IRRE"
"remove a slide smaller index than the current slide s";"-"
"remove a slide bigger index than the current slide s";"-"
"remove the current slide the last one left";"-"
"colorpicker has a stated default colour opaque white";"CODE"
"colorwheel has a different default color transparent black";"CODE"
"click on corner of widget";"CODE"
"too far from the center no effect";"CODE"
"click in middle half the radius up";"CODE"
"colorpicker has a stated default colour opaque white";"CODE"
"colorwheel has a different default color transparent black";"CODE"
"set without alpha";"IRRE"
"set with alpha";"IRRE"
"just press button";"META"
"open dropdown";"CODE"
"press within dropdown area should stay open";"CODE"
"start in dropdown but release outside should stay open";"CODE"
"start outside but release in dropdown should close";"CODE"
"open dropdown again";"CODE"
"press outside dropdown area to close it should close";"CODE"
"ref github issue 5278 init rows cols sizes fix";"IRRE"
"this combination could trigger an error";"CODE"
"set pos to some random value to make this test more reliable";"IRRE"
"0 1 2";"-"
"2 1 0";"-"
"0 1";"-"
"2 3";"-"
"2 3";"-"
"0 1";"-"
"1 0";"-"
"3 2";"-"
"3 2";"-"
"1 0";"-"
"0 2";"-"
"1 3";"-"
"2 0";"-"
"3 1";"-"
"1 3";"-"
"0 2";"-"
"3 1";"-"
"2 0";"-"
"noinspection pyprotectedmember";"CODE"
"just press button";"META"
"open modal";"CODE"
"press within modal area should stay open";"CODE"
"start in modal but release outside should stay open";"META"
"start outside but release in modal should close";"META"
"open modal again";"CODE"
"press outside modal area should close";"CODE"
"use kv because recycleview cannot be constructed from python";"CODE"
"0 1 2";"-"
"2 1 0";"-"
"0 1";"-"
"0 1";"-"
"1 0";"-"
"1 0";"-"
"0 2";"-"
"2 0";"-"
"0 2";"-"
"2 0";"-"
"use kv because recycleview cannot be constructed from python";"CODE"
"if scrollable width avoids zerodivisionerror";"CODE"
"if scrollable height avoids zerodivisionerror";"CODE"
"1 2";"-"
"2 1";"-"
"4 5";"-"
"4 5";"-"
"5 4";"-"
"5 4";"-"
"4 7";"-"
"7 4";"-"
"4 7";"-"
"7 4";"-"
"eventloop window add widget rl do layout called";"IRRE"
"we start with the default top left corner";"CODE"
"check the collision with the margin empty area";"-"
"check the scroll position";"-"
"reset scroll to original state";"IRRE"
"get widgets ready";"CODE"
"get widgets ready";"CODE"
"get widgets ready";"CODE"
"get widgets ready";"CODE"
"touch in the half of the bar";"IRRE"
"get widgets ready";"CODE"
"touch in the half of the bar";"IRRE"
"get widgets ready";"CODE"
"touch in the half of the bar";"IRRE"
"xxx this shouldn t be needed but previous tests apparently";"IRRE"
"don t cleanup";"TASK"
"get widgets ready";"CODE"
"eventloop post dispatch input update touch";"CODE"
"wait for velocity to die off";"CODE"
"eventloop post dispatch input update touch";"CODE"
"kill kv lang logging too long test";"IRRE"
"add the logging back";"TASK"
"get widgets ready";"CODE"
"default pos new pos slider id";"CODE"
"custom touch";"-"
"touch down";"CODE"
"touch on handle";"-"
"touch in widget area ignored previous value";"IRRE"
"touch on handle";"-"
"touch in widget area";"-"
"move from default to new pos";"CODE"
"move from handle to center";"CODE"
"move to center ignored previous value";"IRRE"
"touch on handle";"-"
"touch in widget area";"-"
"touch up";"-"
"default orientation is lr tb";"CODE"
"default orientation is lr tb";"CODE"
"default orientation is lr tb";"CODE"
"floating point error requires almost equal";"CODE"
"happens when padding is too big";"TASK"
"check if text is modified while recreating from lines and lines flags";"CODE"
"check if wordbreaking is correctly done";"CODE"
"if so secondvery should start from the 7th line";"CODE"
"none displayed str";"-"
"none internal str";"CODE"
"enter internal action";"CODE"
"1 scale";"-"
"assert cursor is here";"CODE"
"multiline";"-"
"text";"-"
"move and check position";"-"
"mult iline";"-"
"text";"-"
"ti key down push selection";"CODE"
"none displayed str";"-"
"none internal str";"CODE"
"shift internal action";"CODE"
"1 scale";"-"
"pop selection";"CODE"
"overwrite selection with n";"CODE"
"assert cursor is here";"CODE"
"singleline";"-"
"move and check position";"-"
"single line";"-"
"push selection";"CODE"
"pop selection";"CODE"
"try to overwrite selection with n";"CODE"
"shouldn t work because single line";"-"
"assert cursor is here";"CODE"
"cursor at the place of";"-"
"some random te xt";"IRRE"
"push selection";"CODE"
"pop selection";"CODE"
"cursor at the place of selection between chars";"CODE"
"some rando m te xt";"IRRE"
"cursor now at some rando xt";"IRRE"
"assert cursor is here";"CODE"
"cursor at the place of";"-"
"some random te xt";"IRRE"
"push selection";"CODE"
"pop selection";"CODE"
"cursor at the place of selection between chars";"CODE"
"some rando m te xt";"IRRE"
"assert cursor is here";"CODE"
"overwrite blinking event because too long delay";"TASK"
"from kwargs cursor blink true";"IRRE"
"set whether to blink check if resets";"IRRE"
"no blinking cursor visible";"-"
"cursor position col and row should not be";"-"
"changed by ctrl cursor down and ctrl cursor up";"CODE"
"textinput on touch down was checking the possibility to scroll up";"CODE"
"using the positions of the rendered lines rects these positions";"CODE"
"don t change when the lines are skipped e g during fast scroll";"CODE"
"or ctrl cursor home which lead to scroll freeze";"-"
"move viewport to the first line";"-"
"slowly scroll to the last line to render all lines at least once";"CODE"
"little overscroll is important for detection";"CODE"
"lines scrolled at once will follow the lines to scroll property";"-"
"jump to the first line again";"-"
"temp fix only change of cursor position triggers update as for now";"CODE"
"scrolling up should work now";"-"
"select all";"CODE"
"win dispatch event name key scancode kstr modifiers";"-"
"copy";"-"
"home";"-"
"paste";"-"
"create textinput instance with dummy contents";"IRRE"
"use container to have flexible textinput size";"CODE"
"change textinput s size to contain the needed amount of lines";"CODE"
"in case the root widget is scrollview this cushion is needed";"CODE"
"because scrollview s direct child is always at pos 0 0";"-"
"test label attribute inside button";"META"
"we should have 2 progress minimum and one success";"CODE"
"ref 2983 5568";"-"
"300 0 sometimes is 299 9 or 300 1 however";"-"
"we just want to know that it s really close";"CODE"
"fix issue https github com kivy kivy issues 2275";"CODE"
"attributeerror nonetype object has no attribute texture";"META"
"none of them should work";"-"
"currently rejected with a shader didn t link but work alone";"META"
"rotated rotate tree i shift list to start at i";"-"
"walk starting with i";"-"
"comparison should be performed with unrotated size";"CODE"
"as it also takes into account the window density";"CODE"
"this is automatically set by the window provider but we can";"IRRE"
"force it manually for this specific test";"CODE"
"test that setting the size property to 100 100";"IRRE"
"sets the system size to 100 100 when the window is not rotated";"CODE"
"and the density is 1 0";"-"
"test that setting the size property to 100 50 sets the system size";"IRRE"
"to 50 100 when the window is rotated 90 degrees and the density is";"CODE"
"1 0";"-"
"test that setting the size property to 200 100 sets the system size";"IRRE"
"to 100 50 when the window is unrotated and the density is 2 0";"CODE"
"test that setting the size property to 100 200 sets the system size";"IRRE"
"to 100 50 when the window is rotated 90 degrees and the density is";"CODE"
"2 0";"-"
"the native handle is implemented on all the known";"TASK"
"supported platforms if it is not implemented we likely do";"CODE"
"not have a windowinfo implementation and returns none";"TASK"
"even if we have a windowinfo implementation and therefore";"TASK"
"is not none if returns 0 it means something is wrong";"IRRE"
"skip the test if the system returns unknown";"IRRE"
"we expect a valid theme either light or dark";"-"
"import tlp visual test label layout perf";"CODE"
"import tlrp visual test label layout real perf";"CODE"
"if in words i or n in words i skip spaces";"-"
"tick for texture creation";"CODE"
"tick for texture creation";"CODE"
"clean cache to prevent weird case";"TASK"
"force gc before next test";"CODE"
"print f found no component label for n";"CODE"
"print f found more than one component label for n";"CODE"
"don t actually execute anything";"CODE"
"from os path import join as slash just like that name better";"CODE"
"from here to the kivy top";"CODE"
"image dir images examples relative to generation dir";"-"
"info is a dict built up from";"CODE"
"straight filename information more from reading the docstring";"CODE"
"and more from parsing the description text errors are often";"CODE"
"shown by setting the key error with the value being the error message";"IRRE"
"it doesn t quite meet the requirements for a class but is a vocabulary";"CODE"
"word in this module";"CODE"
"continue don t want to show ugly entries";"CODE"
"make text a set of long lines one per paragraph";"IRRE"
"add links where the files are referenced";"TASK"
"now break up text into array of paragraphs each an array of lines";"CODE"
"ignore wrapping if note or similar block";"TASK"
"include images";"CODE"
"double separator if building on windows sphinx skips backslash";"CODE"
"else code";"-"
"prevent highlight errors with none";"-"
"make sure all the directories has been created before";"IRRE"
"trying to write to the file";"CODE"
"ios 7";"-"
"ios 6 1 and earlier";"-"
"itunes artwork ad hoc";"-"
"ensure the destination directory will be set";"IRRE"
"read the source image and do some quality checks";"CODE"
"a define directive to redirect to that symbol from the symbol name without";"CODE"
"if line startswith define";"CODE"
"there is no double type in gles functions that were using";"CODE"
"a double were renamed with the suffix f";"CODE"
"print define s s symbol1 symbol2";"CODE"
"see explanation about doubles on gles above";"CODE"
"print define s s symbol1 symbol2";"CODE"
"generate";"-"
"pipe to kivy kivy graphics common subset h";"IRRE"
"print pragma once";"CODE"
"print include gl2platform h";"CODE"
"print ifdef cplusplus";"CODE"
"print endif";"CODE"
"don t add the same symbol more than once";"TASK"
"define gl shader binary formats 0x8df8";"CODE"
"define gl rgb565 0x8d62";"CODE"
"print ifdef cplusplus";"CODE"
"print endif";"CODE"
"usr bin env python";"CODE"
"ruff noqa";"-"
"test suite configuration key is test name values are";"IRRE"
"alpha global alpha used for all pixels except t";"CODE"
"patterns allowed v0 pattern characters force include and exclude";"CODE"
"image gimp layer types to export for this test w target extensions";"CODE"
"kivy image test protocol v0 data key is pattern char value is pixel w o a";"IRRE"
"kivy image test protocol v0 return pixel data for given pattern char";"CODE"
"kivy image test protocol v0 filename";"IRRE"
"saves an image to one or more files we can t specify these details when";"CODE"
"saving by extension this declaration is pep8 compliant s all good";"CODE"
"fixme last argument to file tga save is undocumented not sure what";"CODE"
"interlaced bkgd block gama block";"CODE"
"draw pattern on layer helper for make images below";"CODE"
"create an image from the given pattern with the specified layertype in";"IRRE"
"draw the pattern with given alpha and save to the given extensions gimp";"CODE"
"adjust the encoder accordingly";"-"
"cheat for indexed formats draw in rgb a and let gimp make palette";"CODE"
"indexed layer types are drawn in rgb rgba and converted later";"-"
"we need to supply pixels of the format of the layer";"TASK"
"pick the correct layer type for indexed formats";"CODE"
"draw pattern nx1 and 1xn variations";"CODE"
"create the gimp image and the layer we will draw on";"IRRE"
"add alpha layer if we are planning on encoding alpha information";"TASK"
"draw it";"-"
"convert to indexed before saving if needed";"CODE"
"save each individual extension";"CODE"
"fixme this fails";"CODE"
"pdb gimp image delete img";"CODE"
"fixme pattern generation needs thought this sucks";"TASK"
"usr bin env python";"CODE"
"ref https github com cython cython issues 1968";"CODE"
"ensure we don t have any thing like doc running";"CODE"
"check ignore list first";"-"
"print ignored ignore list";"CODE"
"special case core providers";"CODE"
"print ignored not a init py";"IRRE"
"fd write auto generated file by setup py build factory n";"TASK"
"https pythonhosted org pyinstaller hook global variables e g";"CODE"
"pyinstaller 6";"-"
"pyinstaller 6";"-"
"for mod name in core mods process remaining default modules";"CODE"
"if hasattr m mod name capitalize e g video video";"CODE"
"hiddenimports get deps all hiddenimports";"CODE"
"usr bin env python";"CODE"
"pycodestyle py check python source code formatting according to pep 8";"CODE"
"copyright c 2006 2009 johann c rocholl johann rocholl net";"-"
"copyright c 2009 2014 florent xicluna florent xicluna gmail com";"-"
"copyright c 2014 2016 ian lee ianlee1521 gmail com";"-"
"permission is hereby granted free of charge to any person";"CODE"
"obtaining a copy of this software and associated documentation files";"CODE"
"the software to deal in the software without restriction";"CODE"
"including without limitation the rights to use copy modify merge";"-"
"publish distribute sublicense and or sell copies of the software";"META"
"and to permit persons to whom the software is furnished to do so";"TASK"
"subject to the following conditions";"-"
"the above copyright notice and this permission notice shall be";"CODE"
"included in all copies or substantial portions of the software";"CODE"
"the software is provided as is without warranty of any kind";"-"
"express or implied including but not limited to the warranties of";"META"
"merchantability fitness for a particular purpose and";"CODE"
"noninfringement in no event shall the authors or copyright holders";"OUTD"
"be liable for any claim damages or other liability whether in an";"CODE"
"action of contract tort or otherwise arising from out of or in";"CODE"
"connection with the software or the use or other dealings in the";"CODE"
"software";"-"
"errortoken is triggered by backticks in python 3";"CODE"
"work around python 2 6 behavior which does not generate nl after";"CODE"
"a comment which is on a line by itself";"CODE"
"comment with nl tokenize generate tokens n pop send none 1 n";"CODE"
"plugins check functions for physical lines";"CODE"
"physical line physical line rstrip n chr 10 newline";"IRRE"
"physical line physical line rstrip r chr 13 carriage return";"IRRE"
"physical line physical line rstrip x0c chr 12 form feed l";"IRRE"
"special case for long urls in multi line docstrings or comments";"CODE"
"but still report the error when the 72 first chars are whitespaces";"TASK"
"len chunks 2 and chunks 0 and";"-"
"if hasattr line decode python 2";"CODE"
"the line could contain multi byte characters";"CODE"
"plugins check functions for logical lines";"CODE"
"return don t expect blank lines before the first line";"CODE"
"search backwards for a def ancestor or tree root top level";"CODE"
"assert char in";"CODE"
"code e202 if char in else e203 if char in";"CODE"
"continue slice syntax no space required";"CODE"
"continue allow tuple with only one element 3";"CODE"
"indent next tells us whether the next block is indented assuming";"-"
"that it is indented by 4 spaces then we should not allow 4 space";"-"
"indents on the final continuation line in turn some other";"CODE"
"indents are allowed to have an extra 4 spaces";"-"
"remember how many brackets were opened on each line";"CODE"
"relative indents of physical lines";"IRRE"
"for each depth collect a list of opening rows";"CODE"
"for each depth memorize the hanging indentation";"CODE"
"visual indents";"-"
"for each depth memorize the visual indent column";"CODE"
"this is the beginning of a continuation line";"CODE"
"record the initial indent";"IRRE"
"identify closing bracket";"-"
"is the indent relative to an opening bracket line";"CODE"
"is there any chance of visual indent";"-"
"closing bracket for visual indent";"CODE"
"closing bracket matches indentation of opening bracket s line";"CODE"
"visual indent is broken";"-"
"hanging indent is verified";"-"
"visual indent is verified";"-"
"ignore token lined up with matching one from a previous line";"CODE"
"indent is broken";"-"
"look for visual indenting";"CODE"
"deal with implicit string concatenation";"CODE"
"special case for the if statement because len if 4";"CODE"
"keep track of bracket depth";"-"
"parent indents should not be more than this one";"CODE"
"allow lining up tokens";"-"
"syntax class a b is allowed but avoid it";"CODE"
"allow return a foo for a in range 5";"IRRE"
"found a probably needed space";"-"
"tolerate the operator even if running python 3";"CODE"
"deal with python 3 s annotated return value";"CODE"
"a needed trailing space was not found";"-"
"allow keyword args or defaults foo bar none";"IRRE"
"check if the operator is being used as a binary operator";"IRRE"
"allow unary operators 123 x 1";"-"
"allow argument unpacking foo args kwargs";"IRRE"
"surrounding space is optional but ensure that";"META"
"trailing space matches opening space";"CODE"
"a needed opening space was not found";"CODE"
"bad prefix symbol not in and symbol lstrip 1 or";"-"
"yield start e262 inline comment should start with";"-"
"if bad prefix";"-"
"yield start e265 block comment should start with";"-"
"yield start e266 too many leading for block comment";"CODE"
"if indent level allow imports in conditional statements or functions";"CODE"
"if not logical line allow empty lines or comments";"IRRE"
"allow try except else finally keywords intermixed with imports in";"CODE"
"order to support conditional importing";"CODE"
"the first literal is a docstring allow it otherwise report error";"CODE"
"if counts counts and a 1 dict";"-"
"counts counts and 1 2 slice";"-"
"counts counts annotation";"-"
"the character is strictly speaking a binary operator but the";"META"
"common usage seems to be to put it next to the format parameters";"IRRE"
"after a line break";"CODE"
"previous non newline token types and text";"CODE"
"return allow comparison for types which are not obvious";"CODE"
"identifiers on the lhs of an assignment operator";"IRRE"
"identifiers bound to a value with as global or nonlocal";"IRRE"
"helper functions";"CODE"
"python 2 implicit encoding";"CODE"
"fall back if file encoding is improperly declared";"-"
"the physical line contains only this token";"IRRE"
"the comment also ends a physical line";"IRRE"
"don t care about expected errors or warnings";"CODE"
"pycodestyle section pep8 deprecated";"OUTD"
"first read the default values";"IRRE"
"second parse the configuration";"IRRE"
"third overwrite with the command line options";"CODE"
"don t read the command line if the module is used as a library";"CODE"
"if parse argv is true and arglist is none arguments are";"IRRE"
"parsed from the command line sys argv";"CODE"
"e125 continuation line does not";"CODE"
"distinguish itself from next logical line";"CODE"
"e126 continuation line over indented for hanging indent";"CODE"
"e127 continuation line over indented for visual indent";"CODE"
"e128 continuation line under indented for visual indent";"CODE"
"e402 module level import not at top of file";"CODE"
"e741 ambiguous variable name";"IRRE"
"e731 do not assign a lambda expression use a def";"CODE"
"w503 allow putting binary operators after line split";"-"
"file couldn t be opened so was deleted apparently";"CODE"
"don t check deleted files";"CODE"
"got a single file to check";"CODE"
"report dict one key value pair for each title";"CODE"
"this method sends report to gist different file in a single gist and";"CODE"
"returns the url";"IRRE"
"start output debugging";"IRRE"
"prints the entire output";"IRRE"
"on windows system the console leave directly after the end";"CODE"
"of the dump that s not cool if we want get report url";"-"
"ruff noqa";"-"
"cdef void glgetshaderprecisionformat cgl glenum shadertype cgl glenum precisiontype cgl glint range cgl glint precision";"CODE"
"cdef void glreleaseshadercompiler";"CODE"
"cdef void glshaderbinary cgl glsizei n cgl gluint shaders cgl glenum binaryformat cgl glvoid binary cgl glsizei length";"CODE"
"this file was automatically generated with kivy tools stub gl debug py";"IRRE"
"if x startswith";"-"
"there are some functions that either do not exist or break on osx";"CODE"
"just skip those";"-"
"print skipping generation of s x";"CODE"
"credits sean anderson";"-"
"1 open the source image and get the dimensions";"CODE"
"2 search the nearest 2";"-"
"3 invoke etc1tool";"-"
"5 write texture info";"TASK"
"1 open the source image and get the dimensions";"CODE"
"2 search the nearest 2";"-"
"3 for pvr the image must be a square use the bigger size then";"TASK"
"4 invoke texture tool";"-"
"5 write texture info";"TASK"
"one container for the title bar";"IRRE"
"one container for the content";"CODE"
"real is open independent on public event";"CODE"
"create dropdown for the group and save its state to is open";"CODE"
"put open close responsibility to the event";"CODE"
"trigger dropdown opening when clicked";"CODE"
"trigger dropdown closing when an item";"CODE"
"in the dropdown is clicked";"CODE"
"opening only if the dropdown is closed";"CODE"
"closing is open manually dismiss manually";"CODE"
"if container was set incorrectly and or is missing";"IRRE"
"set dropdown width manually or if not set then widen";"IRRE"
"the actiongroup dropdown until the widest child fits";"CODE"
"set the dropdown children s height";"IRRE"
"dismiss dropdown manually";"CODE"
"auto dismiss applies to touching outside of the dropdown";"META"
"if adding actionseparator normal mode";"TASK"
"everything visible add it to the parent";"TASK"
"normal mode items can fit to the view";"CODE"
"display overflow and its items if widget s directly added to it";"TASK"
"all the items can fit to the view so expand everything";"CODE"
"layout all the items in order to pack them per group";"-"
"layout the items in order to pack all of them grouped and display";"-"
"only the action items having important";"CODE"
"if space is left then display actionitem inside their";"CODE"
"actiongroup";"-"
"if space is left then display other actionitems";"CODE"
"for all the remaining actionitems and actionitems with in";"CODE"
"actiongroups display them inside overflow group";"-"
"determine the layout to use";"-"
"can we display all of them";"-"
"can we display them per group";"-"
"ok we can display all the items grouped";"-"
"none of the solutions worked display them in pack mode";"-"
"xxx clean the first registration done from main here";"CODE"
"otherwise kivy uix actionbar actionprevious main actionprevious";"IRRE"
"track touches active vs cancelled";"-"
"note internal hooks for subclassing";"CODE"
"these methods are called internally before dispatching the corresponding";"CODE"
"public events they allow subclasses to implement internal state changes";"CODE"
"e g togglebuttonbehavior updating its state separately from";"META"
"user facing event handlers maintaining a clean separation between";"CODE"
"internal logic and external event dispatch";"CODE"
"do not call these directly or bind to them use";"IRRE"
"on press on release on cancel events instead";"-"
"let parent handle first";"CODE"
"ignore scroll events";"-"
"only handle touches within bounds";"CODE"
"prevent double handling";"CODE"
"grab touch to track its lifecycle";"-"
"check if this is the first touch before adding";"CODE"
"only dispatch press event on first touch";"-"
"we own this touch";"CODE"
"cancel if moved outside and cancellation is enabled";"CODE"
"move from active to cancelled";"CODE"
"dispatch cancel event if this was the last active touch";"CODE"
"let parent handle";"CODE"
"we touched this widget before";"CODE"
"not our touch";"-"
"sanity check";"-"
"release ownership";"-"
"remove from active tracking";"CODE"
"check if this touch was cancelled";"IRRE"
"dispatch release if not cancelled and conditions met";"-"
"only dispatch release after all touches are released";"-"
"cleanup cancelled touch tracking";"TASK"
"anchor none the last anchor node selected e g shift relative node";"CODE"
"the idx may be out of sync";"-"
"anchor idx 0 cache indexes in case list hasn t changed";"CODE"
"last selected node none the absolute last node selected";"CODE"
"ctrl down false if it s pressed for e g shift selection";"CODE"
"holds str used to find node e g if word is typed passed to goto node";"OUTD"
"last key time 0 time since last press for finding whole strs in node";"CODE"
"key list keys that are already pressed to not press continuously";"CODE"
"offset counts cache of counts for faster access";"IRRE"
"if node in self selected nodes and not range select selected";"CODE"
"keep anchor only if not multiselect ctrl type selection";"CODE"
"else it s not selected at this point";"CODE"
"if s not in keys don t keep adding while holding down";"CODE"
"doesn t invert indices here";"CODE"
"for offset selection we have a anchor and we select everything";"CODE"
"between anchor and added offset relative to last node";"IRRE"
"list changed cannot do select across them";"CODE"
"try just in case";"CODE"
"elf anchor node in case idx was reversed reset";"IRRE"
"keep the anchor and last selected node";"IRRE"
"empty beforehand so lookup in deselect will be fast";"CODE"
"try just in case";"CODE"
"bind covering";"-"
"return a decimal approximation of an aspect ratio";"IRRE"
"return scaled size based on sizer where sizer n none scales x";"CODE"
"to n and none n scales y to n";"-"
"return if no reference size yet";"CODE"
"same aspect ratio";"-"
"scale x";"-"
"scale y";"-"
"set background size and position";"IRRE"
"when we are generating documentation config doesn t exist";"CODE"
"no mouse scrolling so the user is going to drag with this touch";"CODE"
"don t forget about grab event";"CODE"
"encoding utf 8";"-"
"x08 self delete word left alt backspace";"CODE"
"join the modifiers e g alt ctrl";"CODE"
"else e g ctrl alt or alt ctrl alt gr key";"-"
"look up mod and key";"-"
"clicking on a widget will activate focus and tab can now be used";"CODE"
"to cycle through";"CODE"
"when we are generating documentation config doesn t exist";"CODE"
"elf focus false this ll unbind";"IRRE"
"if self keyboard remove assigned keyboard from dict";"CODE"
"if next is value prevent infinite loop";"IRRE"
"old focus keyboards keyboard keyboard should be in dict";"-"
"keyboard shouldn t have been released here see keyboard warning";"-"
"if we hit a focusable walk through focus xxx";"-"
"return none make sure we don t loop forever";"CODE"
"hit unfocusable walk widget tree";"-"
"next itr current is returned first when walking forward";"CODE"
"why did we stop";"-"
"if keycode 1 tab deal with cycle";"-"
"track event history uid event grab list";"CODE"
"keep last 2 frames to compare grab states";"IRRE"
"elf event times me uid clock get time";"-"
"save original grab list for other event managers";"CODE"
"dispatch to widget tree";"-"
"store this frame s state for next frame comparison";"CODE"
"check for widgets that lost hover";"CODE"
"clean up completed events";"TASK"
"restore original grab list";"-"
"check registered collides handled";"-"
"not registered or didn t handle continue transparent for hover";"CODE"
"save current state";"CODE"
"restore previous frame s state";"-"
"find widgets that lost hover";"-"
"restore current state";"-"
"transform to widget space if needed";"CODE"
"set widget as grab target";"IRRE"
"dispatch with context handling";"-"
"restore grab state";"-"
"subscribe to hover motion events";"-"
"filter only process hover events with position";"-"
"handle based on hover mode";"CODE"
"standard children first then self";"CODE"
"handle hovermode all and hovermode self";"CODE"
"block children";"-"
"dispatch to children unless blocked";"-"
"always dispatch to self";"CODE"
"handle begin update hover active";"CODE"
"already grabbed by us";"CODE"
"disabled widgets block but don t dispatch";"META"
"hover within bounds grab and dispatch";"-"
"first time dispatch enter";"-"
"moved within bounds dispatch update";"CODE"
"handle end hover stopped or left";"CODE"
"we own this hover clean up and dispatch leave";"TASK"
"disabled widget blocking";"-"
"knspace my widget now points to widget";"CODE"
"knspace my widget now points to widget2";"CODE"
"return getattr parent name if parent doesn t have it";"CODE"
"needs to overwrite eventdispatcher property so kv lang will work";"TASK"
"we only get here if we never accessed our knspace";"CODE"
"etattr knspace name none reset old namespace";"IRRE"
"knspace self knspace get parents in case we haven t before";"CODE"
"if value is none if none first update the recursive knspace";"IRRE"
"elf knspace none cause a kv trigger";"-"
"elf set parent knspace update before trigger below";"CODE"
"process only grabbed events or events within bounds";"-"
"filter out non qualifying events";"-"
"grabbed events always pass through";"-"
"check collision and registration";"-"
"block if collides but not registered";"META"
"pass through to normal processing";"-"
"called as class method togglebuttonbehavior get group group id";"IRRE"
"called as instance method my button get group";"IRRE"
"group mygroup shared across entire application";"-"
"group root mygroup scoped to root widget";"-"
"these groups won t conflict with other mycomponent instances";"-"
"multiple instances won t interfere with each other";"CODE"
"each filterpanel instance has independent size groups";"CODE"
"active booleanproperty false internal storage for active state";"CODE"
"bind early so initial group value triggers handler";"IRRE"
"initialize parent behavior";"IRRE"
"override buttonbehavior do press hook";"CODE"
"override buttonbehavior do release hook";"CODE"
"block deactivation if this would leave the group with no active members";"CODE"
"return cancel deactivation cannot leave group empty";"IRRE"
"apply state change";"-"
"when activating ensure all other buttons in the same group are";"META"
"deactivated";"-"
"remove from previous group";"CODE"
"cleanup empty groups";"TASK"
"cleanup empty owner entries";"TASK"
"register in new group";"CODE"
"revalidate active state in new group";"CODE"
"defer on release until ripple fade has completed";"CODE"
"optimize layout by preventing looking at the same attribute in a loop";"IRRE"
"calculate maximum space used by size hint";"CODE"
"min size from all the none hint and from those with sh min";"CODE"
"do not move the w h get above it s likely to change on above line";"CODE"
"make sure the size hint min max are not violated";"CODE"
"there s no space so just set to min size or zero";"IRRE"
"hint i 0 everything else is zero";"CODE"
"hint gets updated in place";"CODE"
"background color 1 0 0 5 50 translucent red";"-"
"internal map that specifies the different parameters for fixed arrow";"CODE"
"position layouts the flex arrow pos uses these parameter sets";"IRRE"
"as a template";"-"
"0 orientation of the children of bubble content arrow";"-"
"1 order of widgets to add to the boxlayout default content arrow";"TASK"
"2 size hint of arrow image layout";"CODE"
"3 rotation of the arrow image";"-"
"4 pos hint of the arrow image layout";"CODE"
"bottom left vertical 1 1 none 0 top 1 0 x 0 05 noqa e201 e241 e501";"-"
"bottom mid vertical 1 1 none 0 top 1 0 center x 0 50 noqa e201 e241 e501";"-"
"bottom right vertical 1 1 none 0 top 1 0 right 0 95 noqa e201 e241 e501";"-"
"right bottom horizontal 1 none 1 90 left 0 0 y 0 05 noqa e201 e241 e501";"-"
"right mid horizontal 1 none 1 90 left 0 0 center y 0 50 noqa e201 e241 e501";"-"
"right top horizontal 1 none 1 90 left 0 0 top 0 95 noqa e201 e241 e501";"-"
"top left vertical 1 1 none 180 bottom 0 0 x 0 05 noqa e201 e241 e501";"-"
"top mid vertical 1 1 none 180 bottom 0 0 center x 0 50 noqa e201 e241 e501";"-"
"top right vertical 1 1 none 180 bottom 0 0 right 0 95 noqa e201 e241 e501";"-"
"left bottom horizontal 1 none 1 90 right 1 0 y 0 05 noqa e201 e241 e501";"-"
"left mid horizontal 1 none 1 90 right 1 0 center y 0 50 noqa e201 e241 e501";"-"
"left top horizontal 1 none 1 90 right 1 0 top 0 95 noqa e201 e241 e501";"-"
"the order of the following list defines the side that the arrow";"CODE"
"will be attached to in case of ambiguity same distances";"CODE"
"this function calculates the proper value for pos hint i e the";"CODE"
"arrow texture does not overflow and stays entirely connected to";"CODE"
"the side of the content";"-"
"remove the children of the bubble boxlayout as a first step";"-"
"find the layout parameters that define a specific bubble setup";"IRRE"
"rotate the arrow place it at the right pos and setup the size";"IRRE"
"of the widget so the boxlayout can do the rest";"CODE"
"set the orientation of the bubble boxlayout";"IRRE"
"add the updated children of the bubble boxlayout and update";"CODE"
"properties";"-"
"set the arrow margin so we can use this property for proper sizing";"IRRE"
"of the bubble widget";"-"
"determine whether to add the arrow image layout to the";"TASK"
"bubble boxlayout or not";"-"
"start the camera playing at creation";"-"
"create the camera and start later default";"IRRE"
"and later";"-"
"create a camera object with the best image available";"IRRE"
"create a camera object with an image of 320x240 if possible";"IRRE"
"if len slides 2 none or 1 slide";"-"
"if len self slides 2 none or 1 slide";"CODE"
"private properties for internal use only";"CODE"
"if first slide is moving to right with direction set to right";"IRRE"
"or toward left with direction set to left";"IRRE"
"put last slide before first slide";"CODE"
"if reached full offset switch index to next or prev";"IRRE"
"move to next slide";"-"
"move to previous slide";"-"
"compute target offset for ease back next or prev";"IRRE"
"if new offset is 0 it wasn t enough to go next prev";"IRRE"
"detect edge cases if not looping";"IRRE"
"don t forget about grab event";"CODE"
"xxx be careful the widget parent refer to the relativelayout";"-"
"added in add widget but it will break if relativelayout";"TASK"
"implementation change";"TASK"
"if we passed the real widget";"-"
"children must be a list of slides or none";"TASK"
"todo color chooser for keywords strings";"CODE"
"elf text color 000000";"-"
"use text color as foreground color";"IRRE"
"set foreground to white to allow text colors to show";"IRRE"
"use text color as the default color in bbcodes";"IRRE"
"create a label from a text using line options";"IRRE"
"if self password and not hint don t replace hint text with";"CODE"
"fixme right now we can t render very long line";"CODE"
"if we move on vbo version as fallback we won t need to";"TASK"
"do this";"CODE"
"try to find the maximum text we can handle";"CODE"
"ok we found it";"-"
"return the width of a text according to the current line options";"IRRE"
"get bbcoded text for python";"CODE"
"replace brackets with special chars that aren t highlighted";"CODE"
"by pygment can t use bl cause is highlighted";"-"
"replace special chars with bl and br";"CODE"
"remove possible extra highlight options";"-"
"overridden to prevent cursor position off screen";"-"
"kivy 1 0";"-"
"to monitor changes we can bind to color property changes";"-"
"print rgba str value or instance color";"IRRE"
"initialize list to hold all meshes";"IRRE"
"if its already zoomed all the way out cancel the inertial zoom";"CODE"
"if its already zoomed all the way in cancel the inertial zoom";"CODE"
"code is still set up to allow pinch to zoom but this is";"TASK"
"disabled for now since it was fiddly with small wheels";"CODE"
"comment out these lines and adjust on touch move to reenable";"-"
"this";"CODE"
"this is a pinch to zoom";"CODE"
"user was pinching and now both fingers are up return";"IRRE"
"to normal";"-"
"user was pinching and at least one finger remains we";"CODE"
"don t want to treat the remaining fingers as touches";"CODE"
"if touch up is outside the wheel ignore";"-"
"compute which colorarc is being touched they aren t";"-"
"widgets so we don t get collide point and set";"CODE"
"hsv based on the selected colorarc";"CODE"
"first calculate the distance between endpoints of the outer";"CODE"
"arc so we know how many steps to use when calculating";"-"
"vertices";"-"
"if not d outer 1 add a last point if d outer is even";"CODE"
"defaults to ffffffff";"CODE"
"now used only internally";"CODE"
"to prevent interaction between hsv rgba we work internally using rgba";"CODE"
"create a dropdown with 10 buttons";"IRRE"
"when adding widgets we need to specify the height manually";"TASK"
"disabling the size hint y so the dropdown can calculate";"CODE"
"the area it needs";"TASK"
"for each button attach a callback that will call the select method";"CODE"
"on the dropdown we ll pass the text of the button as the data of the";"META"
"selection";"CODE"
"then add the button inside the dropdown";"TASK"
"create a big main button";"IRRE"
"show the dropdown menu when the main button is released";"CODE"
"note all the bind calls pass the instance of the caller here the";"IRRE"
"mainbutton instance as the first argument of the callback here";"IRRE"
"dropdown open";"CODE"
"one last thing listen for the selection in the dropdown list and";"CODE"
"assign the data to the button text";"IRRE"
"kivy 1 4 0";"-"
"ensure we are not already attached";"CODE"
"we will attach ourself to the main window so ensure the";"CODE"
"widget we are looking for have a window";"CODE"
"attach ourself to the main window";"CODE"
"explicitly test for false as none occurs when shown by on touch down";"CODE"
"calculate the coordinate of the attached widget in the window";"CODE"
"coordinate system";"CODE"
"ensure the dropdown list doesn t get out on the x axis with a";"CODE"
"preference to 0 in case the list is too wide";"CODE"
"determine if we display the dropdown upper or lower to the widget";"CODE"
"none of both top bottom have enough place to display the";"-"
"widget at the current size take the best side and fit to";"-"
"import ew kivy uix effectwidget";"CODE"
"ifdef gl es";"CODE"
"endif";"CODE"
"make sure opengl context exists";"CODE"
"elf refresh background color in case this was changed in kwargs";"IRRE"
"add remove fbos until there is one per effect";"TASK"
"remove fbos from unused effects";"CODE"
"do resizing etc";"CODE"
"if there are no effects just draw our main fbo";"CODE"
"build effect shaders";"CODE"
"add the widget to our fbo instead of the normal canvas";"TASK"
"remove the widget from our fbo instead of the normal canvas";"CODE"
"clear widgets from our fbo instead of the normal canvas";"CODE"
"import that module here as it s not available on non windows machines";"CODE"
"see http bit ly i9klje except that the attributes are defined in";"CODE"
"win32file not win32com bug on page";"-"
"note for some reason this doesn t work after a os chdir no matter to";"CODE"
"what directory you change from where windows weirdness";"CODE"
"this error can occurred when a file is already accessed by";"CODE"
"someone else so don t return to true because we have lot";"CODE"
"of chances to not being able to do anything with it";"CODE"
"patterns";"-"
"callbacks";"IRRE"
"don t respond to touchs outside self";"CODE"
"don t respond to touchs outside self";"CODE"
"just check if we can list the directory this is also what";"IRRE"
"add file does so if it fails here it would also fail later";"TASK"
"on do the check here to prevent setting path to an invalid";"IRRE"
"directory that we cannot list";"-"
"if entry path is to jump to previous directory update path with";"CODE"
"parent directory";"-"
"trigger to start gathering the files in the new directory";"CODE"
"we ll start a timer that will do the job 10 times per frames";"TASK"
"default";"CODE"
"cancel any previous clock if exist";"-"
"show the progression screen";"-"
"not enough for creating all the entries all a clock to continue";"CODE"
"start a timer for the next 100 ms";"CODE"
"create maximum entries during 50ms max or 10 minimum slow system";"IRRE"
"on a fast system core i7 2700k we can create up to 40 entries";"IRRE"
"in 50 ms so 10 is fine for low system";"CODE"
"except typeerror in case gitems gen is none";"CODE"
"if this wasn t enough for creating all the entries show a progress";"CODE"
"bar and report the activity to the user";"IRRE"
"we created all the files now push them on the view";"IRRE"
"stop the progression creation";"-"
"if we cancel any action the path will be set same as the";"IRRE"
"previous one so we can safely cancel the update of the previous";"CODE"
"path";"-"
"generator that will create all the files entries";"IRRE"
"the generator is used via update files and create files entries";"IRRE"
"don t use it directly";"CODE"
"add the components that are always needed";"TASK"
"unknown fs just always add the entry but also log";"TASK"
"generate an entries to go back to previous";"-"
"generate all the entries for files";"CODE"
"in the following use fully qualified filenames";"CODE"
"apply filename filters";"-"
"sort the list of files";"-"
"use a closure for lazy loading here";"CODE"
"optimize layout by preventing looking at the same attribute in a loop";"IRRE"
"size";"-"
"pos";"-"
"size self trigger layout";"CODE"
"size hint self trigger layout";"CODE"
"size self trigger layout";"CODE"
"size hint self trigger layout";"CODE"
"clock undershoot margin fixme this is probably too high";"CODE"
"the color is applied to all canvas items of this gesture";"CODE"
"this is the touch uid of the oldest touch represented";"CODE"
"store various timestamps for decision making";"CODE"
"we can cache the candidate here to save zip vector instantiation";"CODE"
"key is touch uid value is a kivy graphics line it s used even";"IRRE"
"if line width is 0 i e not actually drawn anywhere";"CODE"
"make sure the bbox is up to date with the first touch position";"-"
"a list of gesturecontainer objects all gestures on the surface";"IRRE"
"touch events";"-"
"if the touch originates outside the surface ignore it";"-"
"add the stroke to existing gesture or make a new one";"TASK"
"we now belong to a gesture new or old start a new stroke";"CODE"
"retrieve the gesturecontainer object that handles this touch and";"CODE"
"test for colliding gestures if found merge them to one";"IRRE"
"add the new point to gesture stroke list and update the canvas line";"CODE"
"draw the gesture bounding box if it is a single press that";"-"
"does not trigger a move event we would miss it otherwise";"CODE"
"if this stroke hit the maximum limit dispatch immediately";"CODE"
"dispatch later only if we have a window";"CODE"
"gesture related methods";"-"
"create the bounding box rectangle for the gesture";"IRRE"
"update the bbox in case this will normally be done in on touch move";"CODE"
"but we want to update it also for a single press force that here";"CODE"
"register the stroke in gesturecontainer so we can look it up later";"-"
"swap order depending on gesture age the merged gesture gets";"TASK"
"the color from the oldest one of the two";"OUTD"
"apply the outer limits of bbox to the merged gesture";"-"
"now transfer the coordinates from old to new gesture";"CODE"
"fixme this can probably be copied more efficiently";"CODE"
"fixme can t figure out how to change group for existing line";"CODE"
"if draw bbox is changed while two gestures are active";"CODE"
"we might not have a bbrect member";"-"
"timeout callbacks";"IRRE"
"gesture is part of another gesture just delete it";"CODE"
"not active already handled or has active strokes it cannot";"CODE"
"possibly be complete proceed to next gesture on surface";"CODE"
"max strokes reached or temporal window has expired the gesture";"CODE"
"is complete need to dispatch complete or discard event";"CODE"
"merge into one list";"CODE"
"merge into one list";"CODE"
"if that makes impossible to construct things with deferred method";"CODE"
"migrate this test in do layout and or issue a warning";"CODE"
"the goal here is to calculate the minimum size of every cols rows";"-"
"and determine if they have stretch or not";"-"
"if no cols or rows are set we can t calculate minimum size";"IRRE"
"the grid must be constrained at least on one side";"TASK"
"elf cols min size none 0 min size from all the none hint";"CODE"
"elf rows min size none 0 min size from all the none hint";"CODE"
"update minimum size from the dicts";"CODE"
"calculate minimum size for each columns and rows";"CODE"
"compute minimum size maximum stretch needed";"-"
"calculate minimum width height needed starting from padding";"TASK"
"spacing";"CODE"
"we need to subtract for the sh max min the already guaranteed size";"CODE"
"due to having a none in the col so sh min gets smaller by that size";"CODE"
"since it s already covered similarly for sh max because if we";"CODE"
"already exceeded the max the subtracted max will be zero so";"CODE"
"it won t get larger";"-"
"finally set the minimum size";"CODE"
"resolve size for each column";"CODE"
"fix the hints to be within bounds";"CODE"
"if the col don t have stretch information nothing to do";"CODE"
"add to the min width whatever remains from size hint";"CODE"
"same algo for rows";"CODE"
"fix the hints to be within bounds";"CODE"
"if the row don t have stretch information nothing to do";"CODE"
"add to the min height whatever remains from size hint";"CODE"
"delayed imports";"CODE"
"calculate the appropriate height";"-"
"if the height is too higher take the height of the container";"-"
"and calculate appropriate width no need to test further";"TASK"
"note compatibility code due to deprecated properties";"TASK"
"update texture from core image";"CODE"
"do something";"CODE"
"image will be re loaded from disk";"CODE"
"hello world text";"-"
"unicode text can only display glyphs that are available in the font";"CODE"
"multiline text";"-"
"size";"-"
"define your background color template";"CODE"
"now you can simply mix the backgroundcolor class with almost";"IRRE"
"any other widget to give it a background";"-"
"default the background color for this label";"CODE"
"to r 0 g 0 b 0 a 0";"-"
"use the backgroundlabel any where in your kv code like below";"IRRE"
"hello world with world in bold";"-"
"hello in red world in blue";"-"
"color color color";"-"
"note the inversion of direction as y values start at the top of";"IRRE"
"the texture and increase downwards";"CODE"
"indicate the position of the anchors with a red top marker";"-"
"draw a green surround around the refs note the sizes y inversion";"TASK"
"bind all the property for recreating the texture";"CODE"
"note compatibility code due to deprecated properties";"TASK"
"force the texture creation";"CODE"
"create the core label class according to markup value";"IRRE"
"markup have change we need to change our rendering method";"TASK"
"check if the label core class need to be switch to a new one";"CODE"
"when disabled color or outline color changes should not get";"CODE"
"assigned or trigger updates";"CODE"
"note compatibility code due to deprecated properties";"TASK"
"must be removed along with padding x and padding y";"TASK"
"we must strip here otherwise if the last line is empty";"-"
"markup will retain the last empty line since it only strips";"CODE"
"line by line within markup";"CODE"
"force the rendering to get the references";"CODE"
"properties";"-"
"l texture is good";"-"
"l texture is not updated yet";"TASK"
"l texture is good now";"-"
"todo test when children have size hint max min of zero";"CODE"
"all divs are float denominator";"CODE"
"too small just set to min";"IRRE"
"hint i sh min stretch ratio set to min size";"IRRE"
"hint i 0 everything else is zero";"CODE"
"these dicts take i widget child as key";"-"
"not mined contrib all who s sh min sh or had no min sh";"-"
"not maxed contrib all who s sh max sh or had no max sh";"-"
"h mins avail the sh amt removable until we hit sh min";"-"
"h maxs avail the sh amt addable until we hit sh max";"TASK"
"first for all the items set them to be within their max min";"CODE"
"size hint bound also find how much their size hint can be reduced";"CODE"
"or increased";"-"
"diff sh min sh how much we are under the min";"-"
"hint i sh max how much we are over the max";"CODE"
"not mined contrib i max 0 diff how much got removed";"OUTD"
"not maxed contrib i max 0 diff how much got added";"TASK"
"if margin is zero the amount of the widgets that were made smaller";"-"
"magically equals the amount of the widgets that were made larger";"IRRE"
"so we re all good";"-"
"we need to redistribute the margin among all widgets";"TASK"
"if margin is positive then we have extra space because the widgets";"IRRE"
"that were larger and were reduced contributed more so increase";"IRRE"
"the size hint for those that are allowed to be larger by the";"CODE"
"most allowed proportionately to their size or inverse size hint";"CODE"
"similarly for the opposite case";"CODE"
"when reducing the size of widgets proportionately those with";"CODE"
"larger sh get reduced less and those with smaller more";"-"
"contrib amt is all the widgets that are not their max min and";"-"
"can afford to be made bigger smaller";"CODE"
"we only use the contrib amt indices from now on";"IRRE"
"assert mult 1 should only happen when all sh are zero";"CODE"
"n len items check when n 1";"-"
"if not sh available i all were under the margin";"-"
"noinspection pyargumentequaldefault";"CODE"
"internals properties used for graphical representation";"CODE"
"add view";"TASK"
"if not p it s first page";"-"
"elif p l children not first but there are post pages";"META"
"else not first and there are no post pages";"-"
"if not p second page no left margin";"-"
"else there s already a left margin";"IRRE"
"move next page up to right edge";"-"
"move current page until edge hits the right border";"-"
"move previous page left edge up to left border";"-"
"move current page up to left edge";"-"
"move next page until its edge hit the left border";"-"
"move second next page up to right border";"-"
"create content and add to the popup";"TASK"
"bind the on press event of the button to the dismiss function";"META"
"open the popup";"CODE"
"import factory kivy factory factory";"CODE"
"mypopup bad";"-"
"internal properties used for graphical representation";"CODE"
"add popup";"TASK"
"this will update the graphics automatically 75 done";"CODE"
"layout won t shouldn t change previous size if size hint is none";"CODE"
"which is what w h being none means";"-"
"get the total number of items";"-"
"limit index to valid range and handle negative indices";"-"
"get padding and spacing";"TASK"
"calculate total dimensions";"-"
"calculate total width including spacing and padding";"TASK"
"calculate position of target item";"-"
"calculate viewport width";"-"
"calculate scroll position 0 to 1";"-"
"we want the target item to be visible in the viewport";"CODE"
"if the item is wider than the viewport center it";"-"
"center the item";"-"
"make sure the item is visible";"-"
"apply scroll position";"-"
"else vertical orientation";"-"
"calculate total height including spacing and padding";"TASK"
"calculate position of target item";"-"
"calculate viewport height";"-"
"calculate scroll position 0 to 1";"-"
"we want the target item to be visible in the viewport";"CODE"
"if the item is taller than the viewport center it";"-"
"center the item";"-"
"make sure the item is visible";"-"
"apply scroll position";"-"
"calculate minimum size for each columns and rows";"CODE"
"compute minimum size maximum stretch needed";"-"
"this can be further improved to reduce re comp but whatever";"META"
"else size hint is none so check if it can be resized inplace";"IRRE"
"layout won t shouldn t change previous size if size hint is none";"CODE"
"which is what w h being none means";"-"
"visible area is one row column";"-"
"calculate grid dimensions";"-"
"limit index to valid range and handle negative indices";"-"
"if cols rows not set calculate them";"IRRE"
"calculate row and column of target index";"-"
"calculate total dimensions";"-"
"calculate total width and height of the grid";"-"
"right to left";"-"
"position at left of viewport";"-"
"calculate base position from bottom";"CODE"
"position at top of viewport";"-"
"adjust scroll position to center big widgets";"-"
"center wide widgets";"-"
"center tall widgets";"-"
"apply scroll positions";"-"
"at least one changed data unpredictably";"-"
"if f for f in flags if not f need to redo everything";"CODE"
"first update the sizing info so that when we update the size";"CODE"
"the widgets are not bound and won t trigger a re layout";"-"
"make sure widget is added first so that any sizing updates";"TASK"
"will be recorded";"-"
"then add all the visible widgets which binds size size hint";"TASK"
"add to the container if it s not already done";"CODE"
"finally make sure if the size has changed to cause a re layout";"CODE"
"we could use layoutchangeexception here but refresh views in rv";"CODE"
"needs to be updated to watch for it in the layout phase";"CODE"
"internals";"CODE"
"lm clear layout";"-"
"if data we were re triggered so finish in the next call";"TASK"
"otherwise go until fully laid out";"CODE"
"if flags data in case that happened meanwhile";"CODE"
"make sure if we were re triggered in the loop that we won t be";"IRRE"
"called needlessly later";"IRRE"
"todo make this also listen to layoutchangeexception";"CODE"
"dispatches the prop of this class when the";"CODE"
"view adapter layout manager property changes";"-"
"now convert the sv coordinates into the coordinates of the lm in";"CODE"
"case there s a relative layout type widget in the parent tree";"CODE"
"between the sv and the lm";"-"
"or easier way to use";"-"
"whatever too complicated don t try to compute it";"CODE"
"overwrite this method so that when data changes we update";"CODE"
"selectable nodes";"CODE"
"the indices of the data is used as the nodes";"-"
"the indices of the data is used as the nodes so node";"-"
"can be made more selective update than refresh from data which";"CODE"
"causes a full update but this likely affects most of the data";"CODE"
"implement this method in the subclass recycleboxlayout";"CODE"
"or recyclegridlayout";"-"
"resolve the real class if it was a string";"CODE"
"current number of unused classes in the class cache";"CODE"
"maximum number of items in the class cache";"CODE"
"all keys will be reduced to max size";"-"
"internals";"CODE"
"views current displayed items";"-"
"items whose attrs except for pos size is still accurate";"CODE"
"is it in the dirtied views";"CODE"
"if viewclass in dirty views get it first from dirty list";"CODE"
"we found ourself in the dirty list no need to update data";"CODE"
"global cache has this class update data";"CODE"
"random any dirty view element update data";"IRRE"
"elif cached views viewclass otherwise go directly to cache";"IRRE"
"global cache has this class update data";"CODE"
"iterate though the visible view";"-"
"add them into the container if not already done";"CODE"
"if view is not none was current view";"CODE"
"pos self pos incorrect";"CODE"
"handle some additional roles";"TASK"
"import parse color kivy parser parse color";"IRRE"
"rgb parse color cccccc";"IRRE"
"rgb parse color eeeeee";"IRRE"
"internals";"CODE"
"set the documentation root to the directory name of the";"IRRE"
"first tile";"-"
"parse the source";"IRRE"
"fill the current document node";"CODE"
"clear the current widgets";"-"
"parse the source";"IRRE"
"fill the current document node";"CODE"
"check if it s a file";"IRRE"
"whether it s a valid or invalid file let source deal with it";"CODE"
"get the association";"-"
"search into all the nodes containing anchors";"CODE"
"not found stop here";"-"
"found calculate the real coordinate";"-"
"get the anchor coordinate inside widget space";"-"
"ay node y";"-"
"what s the current coordinate for us";"CODE"
"ax ay self scatter to parent ax ay";"CODE"
"store refblock here while building";"CODE"
"store order for autonum sym footnotes refs";"TASK"
"last four default chars aren t in our roboto font";"CODE"
"those were replaced with something else";"-"
"u002a asterisk";"-"
"u2020 dagger";"-"
"u2021 doubledagger";"CODE"
"u00a7 section";"-"
"u00b6 pilcrow";"-"
"u0023 number";"-"
"u2206 cap delta";"-"
"u220f cap pi";"-"
"u0470 cap psi";"-"
"u0466 cap yus";"-"
"get foot cit refs manually because the output from";"IRRE"
"docutils parser doesn t contain any of these";"CODE"
"node s refid refname backref and or are just";"CODE"
"backref true is used in nodes footnote";"TASK"
"auto is either 1 int or";"CODE"
"these are unique and need to go first";"TASK"
"autonum autosym are unique";"-"
"it can be e g image or something else too";"IRRE"
"x footnote";"TASK"
"check if its autonumbered";"IRRE"
"auto is either 1 int or";"CODE"
"can have multiple refs";"-"
"8 1 2 footnote ref";"TASK"
"we can have a footnote without any link or ref";"TASK"
"1 empty footnote";"TASK"
"handle no refs";"-"
"colorize only with refs";"-"
"has no refs";"-"
"list of refs";"-"
"1 1 2 footnote";"TASK"
"single ref";"-"
"give it anchor event manually";"-"
"check if its autonumbered";"IRRE"
"auto is either 1 int or";"CODE"
"can have multiple refs";"-"
"8 1 2 footnote ref";"TASK"
"parser should trigger it when checking";"IRRE"
"for backlinks but we don t have any refs";"CODE"
"to work with so we have to trigger it manually";"-"
"has a single or no refs";"-"
"assert self text";"CODE"
"check if parent isn t a special directive";"IRRE"
"ref replace something";"-"
"ref";"-"
"comment";"-"
"rewrite it to handle autonum sym here";"TASK"
"close tags with departure";"IRRE"
"self do strip text false";"CODE"
"docutils parser breaks path with spaces";"CODE"
"e g c my path c mypath";"-"
"use user s size if defined";"CODE"
"todo";"TASK"
"img url";"-"
"img image img";"-"
"img needs refs and on ref press";"TASK"
"close opened footnote x";"CODE"
"self text ref";"CODE"
"self set text self current link";"CODE"
"self text color ref";"CODE"
"try to preload it";"CODE"
"if exist use the title of the first section found in the";"IRRE"
"document";"CODE"
"replace the text with a good reference";"CODE"
"search anchors";"-"
"force sandboxclock s scheduling";"CODE"
"import pdb pdb set trace";"CODE"
"raise exception fdfdfdfdfdfdfd";"CODE"
"raise exception";"CODE"
"invalid for testing";"IRRE"
"on touch up root d";"-"
"on touch down root f";"CODE"
"on press root args";"IRRE"
"this exception is within the with block but will be ignored by";"CODE"
"default because the sandbox on exception will return true";"CODE"
"the children are positioned relative to the scatter similarly to a";"-"
"the scatter size has no impact on the size of its children";"-"
"if you want to resize the scatter use scale not size read 2 scale";"CODE"
"the scatter is not a layout you must manage the size of the children";"-"
"x y lower left corner";"-"
"xxx float calculation are not accurate and then scale can be";"META"
"thrown again even with only the position change so to";"CODE"
"prevent anything wrong with scale just avoid to dispatch it";"META"
"if the scale visually didn t change 947";"-"
"remove this ugly hack when we ll be python 3 only";"CODE"
"just do a simple one finger drag";"CODE"
"last touch pos has last pos in correct parent space";"-"
"just like incoming touch";"-"
"we have more than one touch list of last known pos";"-"
"add current touch last";"TASK"
"we only want to transform if the touch is part of the two touches";"CODE"
"farthest apart so first we find anchor the point to transform";"CODE"
"around as another touch farthest away from current touch s pos";"CODE"
"now we find the touch farthest away from anchor if its not the";"CODE"
"same as touch touch is not one of the two touches used to transform";"OUTD"
"ok so we have touch and anchor so we can actually compute the";"META"
"transformation";"CODE"
"if not old line length div by zero";"-"
"auto bring to front";"-"
"if the touch isn t on the widget we do nothing";"CODE"
"let the child widgets handle the event if they want";"CODE"
"if our child didn t do anything and if we don t have any active";"CODE"
"interaction control then don t accept the touch";"CODE"
"grab the touch so we get all it later move events for sure";"CODE"
"let the child widgets handle the event if they want";"CODE"
"rotate scale translate";"CODE"
"stop propagating if its within our bounds";"-"
"if the touch isn t on the widget we do nothing just try children";"CODE"
"remove it from our saved touches";"CODE"
"stop propagating if its within our bounds";"-"
"create the manager";"IRRE"
"add few screens";"TASK"
"by default the first screen added into the screenmanager will be";"CODE"
"displayed you can then change to another screen";"-"
"let s display the screen named title 2";"CODE"
"a transition will automatically be used";"IRRE"
"create both screens please note the root manager current this is how";"TASK"
"you can control the screenmanager from kv each screen has by default a";"CODE"
"property manager that gives you the instance of the screenmanager used";"-"
"declare both screens";"-"
"create the screen manager";"IRRE"
"later";"-"
"privates";"CODE"
"create your own transition this shader implements a fading";"TASK"
"transition";"-"
"and create your transition";"IRRE"
"ensure that the correct widget is on top";"-"
"by default the first added screen will be shown if you want to";"TASK"
"show another one just set the current property";"IRRE"
"by default the first added screen will be shown if you want to";"TASK"
"show another one just set the current property";"IRRE"
"iterate over a copy of screens as self remove widget";"CODE"
"modifies self screens in place";"CODE"
"ensure screen is removed from its previous parent";"OUTD"
"later";"-"
"later";"-"
"stop any transition that might be happening already";"CODE"
"ensure the screen name will be unique";"-"
"change the transition if given explicitly";"-"
"change the transition options";"-"
"add and leave if we are set as the current screen";"TASK"
"d left up down right";"CODE"
"di d index self sm transition direction";"CODE"
"self sm transition direction d di 1 len d";"CODE"
"state machine enums";"-"
"touch intent detection state machine";"CODE"
"tracks whether a touch gesture is a tap click or a scroll gesture";"CODE"
"based on movement distance and timeout thresholds";"CODE"
"state transition diagram";"-"
"movement scroll distance";"-"
"unknown scroll";"-"
"timeout expires";"-"
"try simulate touch down";"CODE"
"child grabbed no child grabbed";"-"
"hand off to child transition to scroll mode";"CODE"
"button etc label empty space etc";"META"
"state descriptions";"META"
"unknown initial state accumulating movement to detect intent";"IRRE"
"scroll scroll gesture confirmed either movement exceeded threshold";"-"
"or timeout expired without child widget consuming the touch";"-"
"unknown unknown detecting intent accumulating movement";"CODE"
"scroll scroll confirmed scroll gesture movement exceeded threshold";"-"
"this addresses issues 1464 and 1499 for low performance devices";"CODE"
"then use the s as a widget";"IRRE"
"internal class not documented";"CODE"
"get current value in config";"IRRE"
"create popup layout";"IRRE"
"create the textinput used for numeric input";"CODE"
"construct the content widget are used as a spacer";"CODE"
"2 buttons are created for accept or cancel the current value";"IRRE"
"all done open the popup";"CODE"
"create popup layout";"IRRE"
"create the filechooser";"IRRE"
"construct the content";"CODE"
"2 buttons are created for accept or cancel the current value";"IRRE"
"all done open the popup";"CODE"
"create popup layout";"IRRE"
"2 buttons are created for accept or cancel the current value";"IRRE"
"all done open the popup";"CODE"
"we know the type just by checking if there is a in the original";"CODE"
"value";"IRRE"
"create the popup";"IRRE"
"add all the options";"TASK"
"finally add a cancel button to return on the previous panel";"CODE"
"and open the popup";"CODE"
"return false new uid doesn t exist";"CODE"
"determine the type and the class to use";"IRRE"
"create a instance of the class without the type attribute";"IRRE"
"instance created add to the panel";"TASK"
"internal class not documented";"CODE"
"config setdefaults colorselection testcolor ff0000";"IRRE"
"the following two methods constrain the slider s value";"CODE"
"to range min max otherwise it may happen that self value self min";"IRRE"
"at init";"IRRE"
"default value shown";"IRRE"
"available values";"IRRE"
"just for positioning in our example";"CODE"
"remove any previous binds";"-"
"optimize layout by preventing looking at the same attribute in a loop";"IRRE"
"determine which direction and in what order to place the widgets";"CODE"
"left to right";"-"
"bottom to top";"-"
"right to left";"-"
"top to bottom";"-"
"u ustart inner loop position variable";"IRRE"
"v vstart outer loop position variable";"IRRE"
"space calculation used for determining when a row or column is full";"CODE"
"v padding y size in v direction for minimum size property";"TASK"
"u padding x size in h direction";"TASK"
"v padding x size in v direction for minimum size property";"TASK"
"u padding y size in h direction";"TASK"
"space calculation row height or column width for arranging widgets";"CODE"
"does the widget fit in the row column";"CODE"
"no space left but we re trying to add another widget";"TASK"
"tiny value added in order to avoid issues with float precision";"CODE"
"causing unexpected children reordering when parent resizes";"-"
"e g if size is 101 and children size hint x is 1 5";"CODE"
"5 children would not fit in one line because 101 1 5 101 5";"-"
"even if there s no space we always add one widget to a row";"CODE"
"apply the sizes";"-"
"push the line";"-"
"v position is actually the top right side of the widget";"META"
"when going from high to low coordinate values";"IRRE"
"we need to subtract the height width from the position";"TASK"
"apply the sizes";"-"
"push the last incomplete line";"CODE"
"depending of the distance activate by norm pos or invert";"TASK"
"tp clear widgets to clear all the widgets in the content area";"CODE"
"tp clear tabs to remove the tabbedpanelheaders";"CODE"
"background color 1 0 0 5 50 translucent red";"-"
"rgba 0 1 0 1 green";"-"
"only allow selecting the tab if not already selected";"CODE"
"dispatch to children not to self";"CODE"
"tabbed panel header is a child of tab strib which has a";"CODE"
"tabbed panel property";"-"
"tab removed before we could switch to it switch back to";"CODE"
"previous tab";"-"
"other settings";"IRRE"
"these variables need to be initialized before the kv lang is";"IRRE"
"processed setup the base layout for the tabbed panel";"IRRE"
"https github com kivy kivy issues 3493 issuecomment 121567969";"CODE"
"if content has a previous parent remove it from that parent";"CODE"
"ensure canvas";"-"
"no need to instantiate if class is tabbedpanelheader";"CODE"
"cache variables for faster access";"CODE"
"update scrlv width when tab width changes depends on tab pos";"CODE"
"remove all widgets from the tab strip";"CODE"
"bottom or top positions";"-"
"one col containing the tab strip and the content";"-"
"tab layout contains the scrollview containing tabs and two blank";"-"
"dummy widgets for spacing";"CODE"
"bottom";"-"
"add two dummy widgets";"TASK"
"top";"-"
"left or right positions";"-"
"one row containing the tab strip and the content";"-"
"tab layout contains two blank dummy widgets for spacing";"CODE"
"vertically and the scatter containing scrollview";"IRRE"
"containing tabs";"-"
"rotate the scatter for vertical positions";"CODE"
"update scatter s top when its pos changes";"CODE"
"needed for repositioning scatter to the correct place after its";"CODE"
"added to the parent use clock schedule once to ensure top is";"TASK"
"calculated after the parent s pos on canvas has been calculated";"-"
"this is needed for when tab pos changes to correctly position";"CODE"
"scatter without clock schedule once the positions would look";"-"
"fine but touch won t translate to the correct position";"META"
"on positions left top and right top";"-"
"calculate top of scatter";"-"
"add widgets to tab layout";"TASK"
"add widgets to self";"TASK"
"tab width none";"-"
"size hint x x xyz";"CODE"
"drop to default tab width";"CODE"
"size hint x none";"CODE"
"bottom or top";"-"
"required for situations when scrl v s pos is calculated";"CODE"
"when it has no parent";"-"
"left or right";"-"
"or textinput instance selection color or app selection color";"CODE"
"late binding";"-"
"for reloading we need to keep a list of textinput to retrigger the rendering";"CODE"
"cache the result";"IRRE"
"when we are generating documentation config doesn t exist";"CODE"
"register an observer to clear the textinput cache when opengl will reload";"CODE"
"internal class used for showing the little bubble popup when";"CODE"
"copy cut paste happen";"-"
"this is a prevention to get the bubble staying on the screen if the";"CODE"
"attached textinput is not on the screen anymore";"OUTD"
"show only paste on long touch";"-"
"show only copy for read only text input";"CODE"
"normal mode";"-"
"from to range of lines being partially or fully rendered";"CODE"
"in textinput s viewport";"CODE"
"when the gl context is reloaded trigger the text rendering again";"CODE"
"avoid refreshing text on every keystroke";"CODE"
"allows for faster typing of text when the amount of text in";"CODE"
"textinput gets large";"CODE"
"calling trigger here could lead to wrong cursor positioning";"IRRE"
"and repeating of text when keys are added rapidly in a automated";"CODE"
"fashion from android keyboard for example";"CODE"
"handle undo and redo";"CODE"
"get current paragraph from cursor position";"CODE"
"handle undo and redo";"CODE"
"reset redo when undo is appended to";"CODE"
"delsel";"-"
"reached at top of undo list";"CODE"
"delsel";"-"
"reached at top of undo list";"CODE"
"ime system handles its own backspaces";"CODE"
"ch text col 1";"-"
"refresh just the current line instead of the whole text";"CODE"
"avoid trigger refresh leads to issue with";"CODE"
"keys text send rapidly through code";"CODE"
"handle undo and redo";"CODE"
"handle undo and redo for backspace";"CODE"
"reset redo when undo is appended to";"CODE"
"offset for horizontal text alignment";"IRRE"
"selection control";"CODE"
"handle undo and redo for delete selection";"CODE"
"handle undo and redo for backspace";"CODE"
"reset redo when undo is appended to";"CODE"
"update graphics only on new line";"CODE"
"allows smoother scrolling noticeably";"-"
"faster when dealing with large text";"CODE"
"self trigger update graphics";"CODE"
"touch control";"-"
"schedule long touch for paste";"CODE"
"check for scroll wheel";"CODE"
"todo implement scrollleft and scrollright";"TASK"
"stores the touch for later use";"CODE"
"is a new touch down so previous scroll states needs to be reset";"CODE"
"schedule long touch for paste";"CODE"
"cancel update existing selection after a single tap";"CODE"
"types of touch that will have higher priority in being recognized";"-"
"compared to single tap";"IRRE"
"is a single tap and did not scrolled";"CODE"
"selection needs to be canceled";"TASK"
"show bubble";"-"
"to be considered a scroll touch should travel more than";"-"
"scroll distance in less than the scroll timeout since touch down";"CODE"
"distance isn t enough yet to consider it as a scroll";"TASK"
"timeout is not reached scroll is still enabled";"TASK"
"we have a scroll";"-"
"ignore event if not triggered";"-"
"stop if cursor blink value changed right now";"IRRE"
"don t blink make cursor visible";"CODE"
"callback for blinking the cursor";"IRRE"
"start max 0 start";"-"
"with markup texture can be of height 1";"-"
"self line spacing 2";"CODE"
"now if the text change maybe the cursor is not at the same place as";"-"
"before so try to set the cursor on the good place";"CODE"
"if we back to a new line reset the scroll otherwise the effect is";"IRRE"
"ugly";"-"
"with the new text don t forget to update graphics again";"CODE"
"if not inserting at first line then";"CODE"
"make sure line flags restored for first line";"CODE"
"split smart assumes first line to be not a new line";"CODE"
"this is a little bit complex because we have to";"META"
"handle scroll x";"-"
"handle padding";"TASK"
"create rectangle for the lines matching the viewport";"IRRE"
"crop the texture coordinates to match the viewport";"IRRE"
"this is the first step of graphics the second is the selection";"CODE"
"adjust view if the cursor is going outside the bounds";"-"
"draw labels";"-"
"compute coordinate";"-"
"adjust size texcoord according to viewport";"-"
"cropping";"-"
"nothing to show";"-"
"horizontal alignment";"-"
"base dir self resolved base dir label find base direction value noqa";"IRRE"
"add rectangle";"TASK"
"useful to debug rectangle sizes";"-"
"self canvas add color 0 5 0 5 mode rgba";"TASK"
"self canvas add rectangle pos rect pos size rect size";"TASK"
"self canvas add color";"TASK"
"local references to avoid dot lookups later";"CODE"
"selection borders";"CODE"
"draw the current selection on the widget";"CODE"
"if the size change we might do invalid scrolling text split";"OUTD"
"size the text maybe be put after size hint have been resolved";"CODE"
"get the pixel width of the given row";"-"
"return the current cursor x y from the row col";"CODE"
"horizontal alignment";"-"
"return the height of the cursor s visible part";"IRRE"
"return the position of the cursor s top visible point";"CODE"
"get or create line options to be used for label creation";"IRRE"
"create a label from a text using line options";"IRRE"
"if self password and not hint don t replace hint text with";"CODE"
"fixme right now we can t render very long line";"CODE"
"if we move on vbo version as fallback we won t need to";"TASK"
"do this try to find the maximum text we can handle";"CODE"
"check for blank line";"CODE"
"exception happen when we tried to render the text";"CODE"
"reduce it";"-"
"ok we found it";"-"
"tokenize a text string from some delimiters";"CODE"
"depend of the options split the text on line or word";"CODE"
"no autosize do wordwrap";"CODE"
"try to add each word on current line";"CODE"
"if we have more than the width or if it s a newline";"CODE"
"push the current line and create a new one";"IRRE"
"split the word";"-"
"can t fit the word in give up";"-"
"handle deletion";"CODE"
"move cursor one char to the right if that was successful";"CODE"
"do a backspace effectively deleting char right of cursor";"CODE"
"handle action keys and text insertion";"CODE"
"this allows either ctrl or cmd but not both";"META"
"check for command modes";"CODE"
"we use x01info x02 to get info from ime on mobiles";"CODE"
"pygame seems to pass x01 as the unicode for ctrl a";"CODE"
"checking for modifiers ensures conflict resolution";"CODE"
"we expect to get managed key input via on textinput";"CODE"
"self recalc size";"CODE"
"if key 27 escape";"-"
"elif key 9 tab";"-"
"actions that can be done in readonly";"CODE"
"if key ord a select all";"CODE"
"elif key ord c copy selection";"CODE"
"actions that can be done only if editable";"CODE"
"if key ord x cut selection";"CODE"
"elif key ord v paste clipboard content";"CODE"
"elif key ord z undo";"CODE"
"elif key ord r redo";"CODE"
"current ime composition in progress by the ime system or if nothing";"CODE"
"cursor position of last ime event";"-"
"if text pcc len ime pcc self ime composition always";"CODE"
"remember to update graphics";"CODE"
"properties";"-"
"adjust scrollview to ensure that the cursor will be always inside our";"-"
"viewport";"-"
"if offset is outside the current bounds readjust";"IRRE"
"avoid right center horizontal alignment issues if the viewport is at";"CODE"
"the end of the line if not multiline";"CODE"
"do the same for y";"CODE"
"this algo try to center the cursor as much as possible";"CODE"
"set font size 20dp";"IRRE"
"check if the widget is ok for a node";"IRRE"
"create node";"IRRE"
"check if the widget is ok for a node";"IRRE"
"add nodes";"TASK"
"private";"CODE"
"display only the one who are is open";"CODE"
"now do layout";"CODE"
"now iterate for calculating minimum size";"CODE"
"toggle node or selection";"CODE"
"private properties";"CODE"
"properties";"-"
"start playing the video at creation";"-"
"create the video and start later";"IRRE"
"and later";"-"
"check if filename is not url";"IRRE"
"save the current volume and delta to it";"CODE"
"calculate delta";"-"
"convert to minutes seconds";"-"
"fix bubble label position";"-"
"start playing the video at creation";"-"
"create the video and start later";"IRRE"
"and later";"-"
"internals";"CODE"
"by default videoplayer should look for thumbnail and annotations";"CODE"
"with the same filename except extension of the source video file";"CODE"
"remove all window children";"CODE"
"put the video in fullscreen";"-"
"ensure the video widget is in 0 0 and the size will be";"-"
"readjusted";"CODE"
"f key";"-"
"capslock";"-"
"xxx internal variables";"CODE"
"xxx move to style kv";"-"
"load all the layouts found in the layout path directory";"CODE"
"ensure we have default layouts";"CODE"
"load the default layout from configuration";"CODE"
"ensure the current layout is found on the available layout";"CODE"
"update layout mode shift or normal";"CODE"
"create a top layer to draw active keys on";"IRRE"
"update mode according to capslock and shift key";"CODE"
"ensure new layouts are loaded first";"CODE"
"it s a filename try to load it directly";"CODE"
"first load available layouts from json files";"CODE"
"xxx fix to be able to reload layout when path is changing";"CODE"
"note all math will be done in window point of view";"CODE"
"determine rotation of the target";"-"
"determine the position of center top of the keyboard";"-"
"determine the position of center bottom of the target";"-"
"the goal now is to map both point calculate the diff between them";"CODE"
"we still have an issue self pos represent the bounding box";"TASK"
"not the 0 0 coordinate of the scatter we need to apply also";"TASK"
"the diff between them inside and outside coordinate matrix";"CODE"
"it s hard to explain but do a scheme on a paper write all";"META"
"the vector i m calculating and you ll understand";"-"
"now we have a good diff set it as a pos";"IRRE"
"xxx implement popup with all available layouts";"TASK"
"get relative efficient surface of the layout without external margins";"-"
"get relative unit surface";"-"
"calculate individual key relative surface and pos without key";"-"
"margin";"-"
"get line name";"-"
"go through the list of keys tuples of 4";"-"
"calculate relative pos size";"-"
"now adjust considering the key margin";"-"
"draw background";"-"
"xxx separate drawing the keys and the fonts to avoid";"CODE"
"xxx reloading the texture each time";"CODE"
"first draw keys without the font";"CODE"
"then draw the text";"CODE"
"retrieve the relative text";"-"
"focus on the surface without margins";"-"
"get the line of the layout";"-"
"e height h mbottom mtop h efficient height in pixels";"-"
"line height e height layout rows line height in px";"-"
"get the key within the line";"CODE"
"get the full character";"CODE"
"save pressed key on the touch";"CODE"
"for caps lock or shift only";"CODE"
"do not repeat special keys";"CODE"
"send info to the bus";"CODE"
"save key as an active key for drawing";"CODE"
"save pressed key on the touch";"CODE"
"send info to the bus";"CODE"
"do stuff here and kill the event";"CODE"
"references to all the widget destructors partial method with widget uid as";"CODE"
"key";"-"
"internal method called when a widget is deleted from memory the only";"CODE"
"thing we remember about it is its uid clear all the associated callbacks";"IRRE"
"created in kv language";"IRRE"
"base class used for widget that inherits from class eventdispatcher";"CODE"
"https docs python org 2 library gc html gc garbage";"CODE"
"before doing anything ensure the windows exist";"CODE"
"assign the default context of the widget creation";"IRRE"
"create the default canvas if it does not exist";"CODE"
"apply all the styles";"-"
"bind all the events";"-"
"proxy weakref proxy for more information";"CODE"
"only f should be enough here but it appears that is a very";"META"
"specific case the proxy destructor is not called if both f and";"IRRE"
"proxy ref are not together in a tuple";"-"
"collision";"-"
"default event handlers";"CODE"
"tree management";"-"
"check if the widget is already a child of another widget";"CODE"
"child will be disabled if added to a disabled parent";"TASK"
"we never want to insert widget before canvas before";"CODE"
"we pass index only when we are going on the parent";"-"
"so don t yield the parent as well";"CODE"
"if we want to continue with our parent just do it";"CODE"
"self is root if we want to loopback from the first element";"CODE"
"if we started with root i e index none then we have to";"-"
"start from root again so we return self again otherwise we";"CODE"
"never returned it so return it now starting with it";"IRRE"
"call walk on box with loopback true and restrict false";"IRRE"
"now with loopback false and restrict false";"IRRE"
"now with restrict true";"-"
"process is walk up level walk down its children tree then walk up";"CODE"
"next level etc";"CODE"
"default just walk down the children tree";"CODE"
"we need to go up a level before walking tree";"TASK"
"now walk children tree starting with last most child";"-"
"we need to return ourself last in all cases";"CODE"
"if going up continue walking up the parent tree";"CODE"
"call walk on box with loopback true";"IRRE"
"now with loopback false";"IRRE"
"in kv";"-"
"necessary to ensure a change between value of equal truthiness";"IRRE"
"doesn t mess up the count";"CODE"
"pylint disable w0611";"CODE"
"b str a return 12 54 68";"IRRE"
"c strtotuple b return 12 54 68";"IRRE"
"security";"-"
"fast syntax check";"-"
"if s startswith";"-"
"00ff00";"-"
"3fc4e57f";"-"
"return join 0 02x format int x 255 for x in color";"CODE"
"aliceblue f0f8ff";"-"
"antiquewhite faebd7";"-"
"aqua 00ffff";"-"
"aquamarine 7fffd4";"-"
"azure f0ffff";"-"
"beige f5f5dc";"-"
"bisque ffe4c4";"-"
"black 000000";"-"
"blanchedalmond ffebcd";"-"
"blue 0000ff";"-"
"blueviolet 8a2be2";"CODE"
"brown a52a2a";"-"
"burlywood deb887";"-"
"cadetblue 5f9ea0";"-"
"chartreuse 7fff00";"CODE"
"chocolate d2691e";"-"
"coral ff7f50";"-"
"cornflowerblue 6495ed";"-"
"cornsilk fff8dc";"-"
"crimson dc143c";"-"
"cyan 00ffff";"-"
"darkblue 00008b";"-"
"darkcyan 008b8b";"-"
"darkgoldenrod b8860b";"-"
"darkgray a9a9a9";"-"
"darkgrey a9a9a9";"-"
"darkgreen 006400";"-"
"darkkhaki bdb76b";"-"
"darkmagenta 8b008b";"-"
"darkolivegreen 556b2f";"-"
"darkorange ff8c00";"-"
"darkorchid 9932cc";"-"
"darkred 8b0000";"-"
"darksalmon e9967a";"-"
"darkseagreen 8fbc8f";"-"
"darkslateblue 483d8b";"-"
"darkslategray 2f4f4f";"-"
"darkslategrey 2f4f4f";"-"
"darkturquoise 00ced1";"-"
"darkviolet 9400d3";"CODE"
"deeppink ff1493";"-"
"deepskyblue 00bfff";"-"
"dimgray 696969";"-"
"dimgrey 696969";"-"
"dodgerblue 1e90ff";"CODE"
"firebrick b22222";"-"
"floralwhite fffaf0";"-"
"forestgreen 228b22";"CODE"
"fuchsia ff00ff";"-"
"gainsboro dcdcdc";"-"
"ghostwhite f8f8ff";"-"
"gold ffd700";"-"
"goldenrod daa520";"-"
"gray 808080";"-"
"grey 808080";"-"
"green 008000";"-"
"greenyellow adff2f";"-"
"honeydew f0fff0";"-"
"hotpink ff69b4";"-"
"indianred cd5c5c";"-"
"indigo 4b0082";"-"
"ivory fffff0";"-"
"khaki f0e68c";"-"
"lavender e6e6fa";"CODE"
"lavenderblush fff0f5";"CODE"
"lawngreen 7cfc00";"-"
"lemonchiffon fffacd";"-"
"lightblue add8e6";"TASK"
"lightcoral f08080";"-"
"lightcyan e0ffff";"-"
"lightgoldenrodyellow fafad2";"-"
"lightgreen 90ee90";"-"
"lightgray d3d3d3";"-"
"lightgrey d3d3d3";"-"
"lightpink ffb6c1";"-"
"lightsalmon ffa07a";"-"
"lightseagreen 20b2aa";"-"
"lightskyblue 87cefa";"-"
"lightslategray 778899";"-"
"lightslategrey 778899";"-"
"lightsteelblue b0c4de";"-"
"lightyellow ffffe0";"-"
"lime 00ff00";"-"
"limegreen 32cd32";"-"
"linen faf0e6";"-"
"magenta ff00ff";"-"
"maroon 800000";"-"
"mediumaquamarine 66cdaa";"-"
"mediumblue 0000cd";"-"
"mediumorchid ba55d3";"-"
"mediumpurple 9370db";"-"
"mediumseagreen 3cb371";"-"
"mediumslateblue 7b68ee";"-"
"mediumspringgreen 00fa9a";"-"
"mediumturquoise 48d1cc";"-"
"mediumvioletred c71585";"CODE"
"midnightblue 191970";"-"
"mintcream f5fffa";"CODE"
"mistyrose ffe4e1";"-"
"moccasin ffe4b5";"-"
"navajowhite ffdead";"-"
"navy 000080";"-"
"oldlace fdf5e6";"-"
"olive 808000";"-"
"olivedrab 6b8e23";"-"
"orange ffa500";"-"
"orangered ff4500";"-"
"orchid da70d6";"-"
"palegoldenrod eee8aa";"-"
"palegreen 98fb98";"-"
"paleturquoise afeeee";"CODE"
"palevioletred db7093";"CODE"
"papayawhip ffefd5";"-"
"peachpuff ffdab9";"-"
"peru cd853f";"-"
"pink ffc0cb";"-"
"plum dda0dd";"TASK"
"powderblue b0e0e6";"-"
"purple 800080";"-"
"red ff0000";"-"
"rosybrown bc8f8f";"-"
"royalblue 4169e1";"-"
"saddlebrown 8b4513";"TASK"
"salmon fa8072";"-"
"sandybrown f4a460";"-"
"seagreen 2e8b57";"-"
"seashell fff5ee";"CODE"
"sienna a0522d";"-"
"silver c0c0c0";"-"
"skyblue 87ceeb";"-"
"slateblue 6a5acd";"-"
"slategray 708090";"-"
"slategrey 708090";"-"
"snow fffafa";"-"
"springgreen 00ff7f";"-"
"steelblue 4682b4";"-"
"tan d2b48c";"-"
"teal 008080";"-"
"thistle d8bfd8";"CODE"
"tomato ff6347";"-"
"turquoise 40e0d0";"-"
"violet ee82ee";"CODE"
"wheat f5deb3";"-"
"white ffffff";"-"
"whitesmoke f5f5f5";"-"
"yellow ffff00";"-"
"yellowgreen 9acd32";"-"
"we want to print deprecated warnings only once";"CODE"
"create a key named toto with the value 1";"IRRE"
"it s the same as";"-"
"on android sys platform returns linux2 so prefer to check the";"CODE"
"existence of environ variables set during python initialization";"IRRE"
"we used to use this method to detect android platform";"CODE"
"leaving it here to be backwards compatible with pydroid3";"-"
"and similar tools outside kivy s ecosystem";"CODE"
"first time self lazy lazy is reify obj reify get runs";"CODE"
"econd time self lazy lazy is hard to compute int";"CODE"
"check if file exist";"IRRE"
"match a line like revision a01041";"META"
"couldn t find the hardware revision assume it is not a pi";"META"
"determine the pi version using the processor bits using the new style";"META"
"revision format";"META"
"if it is not using the new style revision format";"CODE"
"then it must be a raspberry pi 1";"TASK"
"construct a point at 82 34";"CODE"
"construct by giving a list of 2 values";"IRRE"
"optimized method";"-"
"non optimized method";"-"
"use the list getslice method and convert";"IRRE"
"result to vector";"IRRE"
"linear algebar sucks seriously";"IRRE"
"yaaay i love linear algebra applied within the realms of geometry";"CODE"
"this is mostly the same as the line intersection";"CODE"
"here are the new bits";"CODE"
"kivy cross platform ui framework";"CODE"
"https kivy org";"CODE"
"if p returncode if not returncode 0";"IRRE"
"sdl3 dev is installed before setup py is run when installing from";"CODE"
"source due to pyproject toml however it is installed to a";"-"
"pip isolated env which we need to add to compiler";"TASK"
"register an extension to ensure a compiler is created";"IRRE"
"disable building fake extensions";"-"
"run to populate self compiler";"CODE"
"create a temporary file which contains the code";"IRRE"
"determine on which platform we are";"CODE"
"detect python for android project http github com kivy python for android";"CODE"
"proprietary broadcom video core drivers";"-"
"force detected raspberry pi version for cross builds if needed";"CODE"
"the proprietary broadcom video core drivers are not available on the";"-"
"raspberry pi 4";"-"
"use mesa video core drivers";"-"
"needed when cross compiling";"-"
"if the user has specified a kivy deps root use that as the root for";"IRRE"
"atm only sdl dependencies otherwise use the default locations";"CODE"
"if kivy deps root is none and platform is linux or darwin show a warning";"CODE"
"message because using a system provided sdl3 is not recommended";"CODE"
"will be shown only in verbose mode";"IRRE"
"print";"CODE"
"print";"CODE"
"detect options";"-"
"now check if environ is changing the default values";"IRRE"
"we want to be able to install kivy as a wheel without a dependency";"CODE"
"on cython but we also want to use cython where possible as a setup";"IRRE"
"time dependency through pyproject toml if building from source";"CODE"
"there are issues with using cython at all on some platforms";"CODE"
"exclude them from using or declaring cython";"CODE"
"this determines whether cython specific functionality may be used";"CODE"
"never use or declare cython on these platforms";"CODE"
"setup classes";"IRRE"
"the build path where kivy is being compiled";"CODE"
"version is imported by exec but help linter not complain";"CODE"
"build the extensions in parallel if the options has not been set";"IRRE"
"use a maximum of 4 cores if cpu count returns none then parallel";"IRRE"
"build will be disabled";"-"
"build files";"-"
"generate headers";"CODE"
"config pxi autogenerated file for kivy cython configuration n";"CODE"
"config py autogenerated file for kivy configuration n";"CODE"
"generate content";"-"
"config h define 0 1 n format opt value";"CODE"
"extract version simulate doc generation kivy will be not imported";"CODE"
"cython check";"-"
"on python for android and kivy ios cython usage is external";"CODE"
"extra build commands go in the cmdclass dict command name commandclass";"CODE"
"see tools packaging platform build py for custom build commands for";"CODE"
"portable packages also e g we use build ext command from cython if its";"CODE"
"installed for c extensions";"CODE"
"add build rules for portable packages to cmdclass";"CODE"
"detect which opengl version headers to use";"CODE"
"check if we are in a kivy ios build";"IRRE"
"detect gstreamer only on desktop";"-"
"works if we forced the options or in autodetection";"CODE"
"check the existence of frameworks";"-"
"use pkg config approach instead";"-"
"detect sdl3 only on desktop and ios or android if explicitly enabled";"IRRE"
"works if we forced the options or in autodetection";"CODE"
"check the existence of frameworks";"-"
"use pkg config approach instead";"-"
"declare flags";"-"
"use xcode select to search on the right xcode path";"CODE"
"xxx use the best sdk available instead of a specific one";"IRRE"
"if darwin has already been configured with frameworks don t";"CODE"
"configure sdl3 via libs";"-"
"todo move framework configuration here";"TASK"
"no pkgconfig info or we want to use a specific sdl3 path so perform";"IRRE"
"manual configuration";"-"
"try to find sdl3 in default locations if we don t have a custom path";"CODE"
"if we have a custom path we need to add the rpath to the linker";"TASK"
"so that the libraries can be found and loaded without having to";"CODE"
"set ld library path every time";"IRRE"
"ensure headers for all the sdl3 and sub libraries are available";"CODE"
"sources to compile";"CODE"
"all the dependencies have been found manually with";"CODE"
"grep inr e cimport include kivy graphics context instructions pxd pyx";"CODE"
"activate imageio provider for our core image";"CODE"
"kivy graphics egl backend egl angle is always compiled";"CODE"
"but it only acts as a proxy to the real implementation";"TASK"
"dispmanx is only available on old versions of raspbian buster";"OUTD"
"for this reason we need to be sure that egl dispmanx is available";"CODE"
"before compiling the vidcore lite module even if we re on a rpi";"CODE"
"fixme add an option to depend on them but not compile them";"CODE"
"cause keytab is included in core and core is included in";"CODE"
"window x11";"CODE"
"depends";"CODE"
"core window window x11 keytab c";"CODE"
"core window window x11 core c";"CODE"
"extension modules";"CODE"
"the cythonized file can be either a c or cpp file";"-"
"depending on the language tag in the pyx file or the";"CODE"
"flag passed to the extension";"-"
"if the language tag or the flag is not set we assume";"IRRE"
"the file is a c file";"-"
"if line 0";"-"
"can t use cython so use the c or cpp files instead";"IRRE"
"automatically detect data files";"IRRE"
"setup";"IRRE"
"flasky extensions flasky pygments style based on tango style";"CODE"
"background color f8f8f8";"-"
"no corresponding class for the following";"CODE"
"text class";"IRRE"
"whitespace underline f8f8f8 class w";"IRRE"
"error a40000 border ef2929 class err";"IRRE"
"other 000000 class x";"IRRE"
"comment italic 8f5902 class c";"IRRE"
"comment preproc noitalic class cp";"IRRE"
"keyword bold 004461 class k";"IRRE"
"keyword constant bold 004461 class kc";"CODE"
"keyword declaration bold 004461 class kd";"IRRE"
"keyword namespace bold 004461 class kn";"IRRE"
"keyword pseudo bold 004461 class kp";"CODE"
"keyword reserved bold 004461 class kr";"IRRE"
"keyword type bold 004461 class kt";"IRRE"
"operator 582800 class o";"IRRE"
"operator word bold 004461 class ow like keywords";"IRRE"
"punctuation bold 000000 class p";"IRRE"
"because special names such as name class name function etc";"CODE"
"are not recognized as such later in the parsing we choose them";"CODE"
"to look the same as ordinary variables";"IRRE"
"name 000000 class n";"IRRE"
"name attribute c4a000 class na to be revised";"IRRE"
"name builtin 004461 class nb";"IRRE"
"name builtin pseudo 3465a4 class bp";"CODE"
"name class 000000 class nc to be revised";"IRRE"
"name constant 000000 class no to be revised";"CODE"
"name decorator 888 class nd to be revised";"CODE"
"name entity ce5c00 class ni";"IRRE"
"name exception bold cc0000 class ne";"CODE"
"name function 000000 class nf";"CODE"
"name property 000000 class py";"IRRE"
"name label f57900 class nl";"IRRE"
"name namespace 000000 class nn to be revised";"IRRE"
"name other 000000 class nx";"IRRE"
"name tag bold 004461 class nt like a keyword";"IRRE"
"name variable 000000 class nv to be revised";"IRRE"
"name variable class 000000 class vc to be revised";"IRRE"
"name variable global 000000 class vg to be revised";"IRRE"
"name variable instance 000000 class vi to be revised";"IRRE"
"number 990000 class m";"IRRE"
"literal 000000 class l";"IRRE"
"literal date 000000 class ld";"IRRE"
"string 4e9a06 class s";"CODE"
"string backtick 4e9a06 class sb";"CODE"
"string char 4e9a06 class sc";"CODE"
"string doc italic 8f5902 class sd like a comment";"CODE"
"string double 4e9a06 class s2";"CODE"
"string escape 4e9a06 class se";"CODE"
"string heredoc 4e9a06 class sh";"CODE"
"string interpol 4e9a06 class si";"CODE"
"string other 4e9a06 class sx";"CODE"
"string regex 4e9a06 class sr";"CODE"
"string single 4e9a06 class s1";"CODE"
"string symbol 4e9a06 class ss";"CODE"
"generic 000000 class g";"IRRE"
"generic deleted a40000 class gd";"CODE"
"generic emph italic 000000 class ge";"IRRE"
"generic error ef2929 class gr";"IRRE"
"generic heading bold 000080 class gh";"IRRE"
"generic inserted 00a000 class gi";"CODE"
"generic output 888 class go";"IRRE"
"generic prompt 745334 class gp";"IRRE"
"generic strong bold 000000 class gs";"IRRE"
"generic subheading bold 800080 class gu";"IRRE"
"generic traceback bold a40000 class gt";"IRRE"
"coding utf 8";"-"
"requests documentation build configuration file created by";"CODE"
"sphinx quickstart on fri feb 19 00 05 47 2016";"-"
"this file is execfile d with the current directory set to its";"IRRE"
"containing dir";"-"
"note that not all possible configuration values are present in this";"IRRE"
"autogenerated file";"-"
"all configuration values have a default values that are commented out";"IRRE"
"serve to show the default";"CODE"
"if extensions or modules to document with autodoc are in another directory";"CODE"
"add these directories to sys path here if the directory is relative to the";"TASK"
"documentation root use os path abspath to make it absolute like shown here";"CODE"
"sys path insert 0 os path abspath";"CODE"
"insert requests path into the system";"CODE"
"general configuration";"-"
"if your documentation needs a minimal sphinx version state it here";"TASK"
"needs sphinx 1 0";"TASK"
"add any sphinx extension module names here as strings they can be";"CODE"
"extensions coming with sphinx named sphinx ext or your custom";"-"
"ones";"-"
"add any paths that contain templates here relative to this directory";"CODE"
"the suffix es of source filenames";"CODE"
"you can specify multiple suffix as a list of string";"CODE"
"source suffix rst md";"-"
"the encoding of source files";"CODE"
"source encoding utf 8 sig";"-"
"the master toctree document";"CODE"
"general information about the project";"CODE"
"the version info for the project you re documenting acts as replacement for";"CODE"
"version and release also used in various other places throughout the";"META"
"built documents";"CODE"
"the short x y version";"META"
"the full version including alpha beta rc tags";"META"
"the language for content autogenerated by sphinx refer to documentation";"CODE"
"for a list of supported languages";"CODE"
"this is also used if you do content translation via gettext catalogs";"CODE"
"usually you set language from the command line for these cases";"CODE"
"there are two options for replacing today either you set today to some";"CODE"
"non false value then it is used";"IRRE"
"today";"-"
"else today fmt is used as the format for a strftime call";"IRRE"
"today fmt b d y";"-"
"list of patterns relative to source directory that match files and";"-"
"directories to ignore when looking for source files";"CODE"
"the rest default role used for this markup text to use for all";"CODE"
"documents";"CODE"
"default role none";"CODE"
"if true will be appended to func etc cross reference text";"CODE"
"if true the current module name will be prepended to all description";"CODE"
"unit titles such as function";"CODE"
"if true sectionauthor and moduleauthor directives will be shown in the";"META"
"output they are ignored by default";"IRRE"
"show authors false";"META"
"the name of the pygments syntax highlighting style to use";"CODE"
"a list of ignored prefixes for module index sorting";"CODE"
"modindex common prefix";"-"
"if true keep warnings as system message paragraphs in the built documents";"CODE"
"keep warnings false";"-"
"if true todo and todolist produce output else they produce nothing";"TASK"
"options for html output";"IRRE"
"the theme to use for html and html help pages see the documentation for";"CODE"
"a list of builtin themes";"CODE"
"theme options are theme specific and customize the look and feel of a theme";"-"
"further for a list of options available for each theme see the";"CODE"
"documentation";"CODE"
"note bg fff59c";"TASK"
"add any paths that contain custom themes here relative to this directory";"TASK"
"html theme path";"-"
"the name for this set of sphinx documents if none it defaults to";"CODE"
"project v release documentation";"CODE"
"html title none";"-"
"a shorter title for the navigation bar default is the same as html title";"CODE"
"html short title none";"-"
"the name of an image file relative to this directory to place at the top";"CODE"
"of the sidebar";"IRRE"
"html logo none";"-"
"the name of an image file within the static path to use as favicon of the";"CODE"
"docs this file should be a windows icon file ico being 16x16 or 32x32";"CODE"
"pixels large";"-"
"html favicon none";"-"
"add any paths that contain custom static files such as style sheets here";"TASK"
"relative to this directory they are copied after the builtin static files";"CODE"
"so a file named default css will overwrite the builtin default css";"CODE"
"add any extra paths that contain custom files such as robots txt or";"TASK"
"htaccess here relative to this directory these files are copied";"CODE"
"directly to the root of the documentation";"CODE"
"html extra path";"-"
"if not a last updated on timestamp is inserted at every page bottom";"CODE"
"using the given strftime format";"CODE"
"html last updated fmt b d y";"CODE"
"if true smartypants will be used to convert quotes and dashes to";"OUTD"
"typographically correct entities";"IRRE"
"custom sidebar templates maps document names to template names";"IRRE"
"additional templates that should be rendered to pages maps page names to";"TASK"
"template names";"-"
"html additional pages";"TASK"
"if false no module index is generated";"CODE"
"html domain indices true";"CODE"
"if false no index is generated";"-"
"html use index true";"-"
"if true the index is split into individual pages for each letter";"CODE"
"html split index false";"-"
"if true links to the rest sources are added to the pages";"TASK"
"if true created using sphinx is shown in the html footer default is true";"IRRE"
"if true c copyright is shown in the html footer default is true";"CODE"
"if true an opensearch description file will be output and all pages will";"CODE"
"contain a link tag referring to it the value of this option must be the";"TASK"
"base url from which the finished html is served";"TASK"
"html use opensearch";"CODE"
"this is the file name suffix for html files e g xhtml";"CODE"
"html file suffix none";"-"
"language to be used for generating the html full text search index";"CODE"
"sphinx supports the following languages";"-"
"da de en es fi fr hu it ja";"-"
"nl no pt ro ru sv tr";"-"
"html search language en";"-"
"a dictionary with options for the search language support empty by default";"CODE"
"now only ja uses this config value";"IRRE"
"html search options type default";"CODE"
"the name of a javascript file relative to the configuration directory that";"CODE"
"implements a search results scorer if empty the default will be used";"TASK"
"html search scorer scorer js";"-"
"output file base name for html help builder";"IRRE"
"options for latex output";"IRRE"
"the paper size letterpaper or a4paper";"CODE"
"papersize letterpaper";"CODE"
"the font size 10pt 11pt or 12pt";"-"
"pointsize 10pt";"CODE"
"additional stuff for the latex preamble";"TASK"
"preamble";"-"
"latex figure float alignment";"CODE"
"figure align htbp";"-"
"grouping the document tree into latex files list of tuples";"CODE"
"source start file target name title";"CODE"
"author documentclass howto manual or own class";"CODE"
"the name of an image file relative to this directory to place at the top of";"CODE"
"the title page";"-"
"latex logo none";"-"
"for manual documents if this is true then toplevel headings are parts";"CODE"
"not chapters";"-"
"latex use parts false";"-"
"if true show page references after internal links";"CODE"
"latex show pagerefs false";"-"
"if true show url addresses after external links";"TASK"
"latex show urls false";"-"
"documents to append as an appendix to all manuals";"CODE"
"latex appendices";"CODE"
"if false no module index is generated";"CODE"
"latex domain indices true";"CODE"
"options for manual page output";"IRRE"
"one entry per manual page list of tuples";"CODE"
"source start file name description authors manual section";"META"
"if true show url addresses after external links";"TASK"
"man show urls false";"-"
"options for texinfo output";"IRRE"
"grouping the document tree into texinfo files list of tuples";"CODE"
"source start file target name title author";"META"
"dir menu entry description category";"CODE"
"documents to append as an appendix to all manuals";"CODE"
"texinfo appendices";"CODE"
"if false no module index is generated";"CODE"
"texinfo domain indices true";"CODE"
"how to display url addresses footnote no or inline";"TASK"
"texinfo show urls footnote";"TASK"
"if true do not generate a detailmenu in the top node s menu";"CODE"
"texinfo no detailmenu false";"-"
"options for epub output";"IRRE"
"bibliographic dublin core info";"-"
"the basename for the epub file it defaults to the project name";"CODE"
"epub basename project";"-"
"the html theme for the epub output since the default themes are not";"CODE"
"optimized for small screen space using the same theme for html and epub";"CODE"
"output is usually not wise this defaults to epub a theme designed to save";"CODE"
"visual space";"-"
"epub theme epub";"-"
"the language of the text it defaults to the language option";"CODE"
"or en if the language is not set";"IRRE"
"epub language";"-"
"the scheme of the identifier typical schemes are isbn or url";"-"
"epub scheme";"-"
"the unique identifier of the text this can be a isbn number";"CODE"
"or the project homepage";"-"
"epub identifier";"-"
"a unique identification for the text";"CODE"
"epub uid";"-"
"a tuple containing the cover image and cover page html template filenames";"-"
"epub cover";"-"
"a sequence of type uri title tuples for the guide element of content opf";"CODE"
"epub guide";"-"
"html files that should be inserted before the pages created by sphinx";"CODE"
"the format is a list of tuples containing the path and title";"CODE"
"epub pre files";"-"
"html files that should be inserted after the pages created by sphinx";"IRRE"
"the format is a list of tuples containing the path and title";"CODE"
"epub post files";"-"
"a list of files that should not be packed into the epub file";"CODE"
"the depth of the table of contents in toc ncx";"CODE"
"epub tocdepth 3";"-"
"allow duplicate toc entries";"-"
"epub tocdup true";"-"
"choose between default and includehidden";"CODE"
"epub tocscope default";"CODE"
"fix unsupported image types using the pillow";"-"
"epub fix images false";"-"
"scale large images";"-"
"epub max image width 0";"-"
"how to display url addresses footnote no or inline";"TASK"
"epub show urls inline";"-"
"if false no index is generated";"-"
"epub use index true";"-"
"usr bin env python";"CODE"
"setup py publish shortcut";"IRRE"
"assert urllib3 version dev verify urllib3 isn t installed from git";"CODE"
"sometimes urllib3 only reports its version as 16 1";"META"
"check urllib3 for compatibility";"CODE"
"major minor patch urllib3 version noqa f811";"META"
"urllib3 1 21 1";"-"
"check charset normalizer for compatibility";"CODE"
"chardet version 3 0 2 6 0 0";"META"
"charset normalizer 2 0 0 4 0 0";"IRRE"
"cryptography 1 3 4";"-"
"check imported dependencies for compatibility";"CODE"
"attempt to enable urllib3 s fallback for sni support";"CODE"
"if the standard library doesn t support sni or the";"CODE"
"ssl library isn t available";"CODE"
"check cryptography version";"META"
"urllib3 s dependencywarnings should be silenced";"CODE"
"set default logging handler to avoid no handler found warnings";"CODE"
"filemodewarnings go off per the default";"CODE"
"import socket noqa f401";"CODE"
"according to our docs we allow users to specify just the client";"CODE"
"cert path";"-"
"can t handle by adding proxy manager to self attrs because";"TASK"
"self poolmanager uses a lambda function which isn t pickleable";"CODE"
"save these values for pickling";"IRRE"
"allow self specified cert location";"CODE"
"fallback to none if there s no status code for whatever reason";"CODE"
"make headers case insensitive";"CODE"
"set encoding";"IRRE"
"add new cookies from the server";"CODE"
"give the response some context";"CODE"
"only scheme should be lower case";"CODE"
"only scheme should be lower case";"CODE"
"if url startswith don t confuse urllib3";"META"
"todo remove this in 3 0 0 see 2811";"CODE"
"this branch is for urllib3 v1 22 and later";"CODE"
"this branch is for urllib3 versions earlier than v1 22";"CODE"
"by using the with statement we are sure the session is closed thus we";"CODE"
"avoid leaving sockets open which can trigger a resourcewarning in some";"CODE"
"cases and look like a memory leak in others";"CODE"
"if response is not 4xx do not auth";"CODE"
"see https github com psf requests issues 3772";"CODE"
"rewind the file position indicator of the body to where";"-"
"it was to resend the request";"CODE"
"consume content and release the original connection";"IRRE"
"to allow our new request to reuse the same one";"CODE"
"initialize per thread state if needed";"IRRE"
"if we have a saved nonce skip the 401";"CODE"
"in the case of httpdigestauth being reused and the body of";"CODE"
"the previous request was a file like object pos has the";"CODE"
"file position of the previous body ensure it s set to";"IRRE"
"none";"-"
"usr bin env python";"CODE"
"urllib3";"-"
"detect which major version of urllib3 is being used";"META"
"if we can t discern a version prefer old functionality";"META"
"character detection";"CODE"
"only return the response s url if the user hadn t set the host";"IRRE"
"header";"CODE"
"if they did set it retrieve it and reconstruct the expected domain";"CODE"
"reconstruct the url as we expect it";"CODE"
"if there are multiple cookies that meet passed in criteria";"-"
"we will eventually return this as long as no cookie conflict";"CODE"
"we re dealing with an instance of requestscookiejar";"CODE"
"we re dealing with a generic cookiejar instance";"-"
"warnings";"-"
"todo response is the only one";"CODE"
"import encoding now to avoid implicit import later";"CODE"
"implicit import within threads may cause lookuperror when standard library is in a zip";"CODE"
"such as in embedded python see https github com psf requests issues 3578";"CODE"
"import encodings idna noqa f401";"CODE"
"the set of http status codes that indicate an automatically";"IRRE"
"processable redirect";"-"
"codes moved 301";"-"
"codes found 302";"-"
"codes other 303";"-"
"codes temporary redirect 307";"-"
"codes permanent redirect 308";"-"
"default empty dicts for dict params";"CODE"
"note that prepare auth must be last to enable authentication schemes";"TASK"
"such as oauth to work on a fully prepared request";"CODE"
"this must go after prepare auth authenticators could add a hook";"TASK"
"accept objects that have string representations";"CODE"
"we re unable to blindly call unicode str functions";"CODE"
"as this will include the bytestring indicator b";"CODE"
"on python 3 x";"CODE"
"https github com psf requests pull 2238";"CODE"
"remove leading whitespaces from url";"CODE"
"don t do any url preparation for non http schemes like mailto";"CODE"
"data etc to work around exceptions from url parse which";"CODE"
"handles rfc 3986 only";"-"
"support for unicode domain names and paths";"CODE"
"in general we want to try idna encoding the hostname if the string contains";"CODE"
"non ascii characters this allows users to automatically get the correct idna";"CODE"
"behaviour for strings containing only ascii characters we need to also verify";"CODE"
"it doesn t start with a wildcard before allowing the unencoded hostname";"CODE"
"carefully reconstruct the network location";"CODE"
"bare domains aren t valid urls";"CODE"
"check if file fo generator iterator";"IRRE"
"if not run through normal process";"CODE"
"nottin on you";"-"
"urllib3 requires a bytes like body python 2 s json dumps";"CODE"
"provides this natively but python 3 gives a unicode string";"CODE"
"record the current file position before reading";"CODE"
"this will allow us to rewind a file in the event";"CODE"
"of a redirect";"-"
"this differentiates from none allowing us to catch";"CODE"
"a failed tell later when trying to rewind the body";"CODE"
"multi part file uploads";"CODE"
"add content type if it wasn t explicitly provided";"TASK"
"if no auth is explicitly provided extract it from the url first";"CODE"
"special case basic http auth";"CODE"
"allow auth to make its changes";"-"
"update self to reflect the auth changes";"CODE"
"recompute content length";"-"
"read the contents";"CODE"
"don t need to release the connection that s been handled by urllib3";"CODE"
"since we exhausted the data";"-"
"try charset from content type";"CODE"
"fallback to auto detected encoding";"-"
"decode unicode from given encoding";"CODE"
"a lookuperror is raised if the encoding was not found which could";"CODE"
"indicate a misspelling or similar mistake";"-"
"a typeerror can be raised if encoding is none";"CODE"
"so we try blindly encoding";"CODE"
"no encoding set json rfc 4627 section 3 states we should expect";"IRRE"
"utf 8 16 or 32 detect which one to use if the detection or";"CODE"
"decoding fails fall back to self text using charset normalizer to make";"CODE"
"a best guess";"-"
"wrong utf codec detected usually because it s not utf 8";"META"
"but some other 8 bit codec this is an rfc violation";"META"
"and the server didn t bother to tell us what codec was";"-"
"used";"-"
"catch json related errors and raise as requests jsondecodeerror";"CODE"
"this aliases json jsondecodeerror and simplejson jsondecodeerror";"CODE"
"we attempt to decode utf 8 first because some servers";"-"
"choose to localize their reason strings if the string";"CODE"
"isn t utf 8 we fall back to iso 8859 1 for all other";"CODE"
"encodings see pr 3538";"-"
"this code exists for backwards compatibility reasons";"CODE"
"i don t like it either just look the other way";"CODE"
"this traversal is apparently necessary such that the identities are";"CODE"
"preserved requests packages urllib3 is urllib3";"CODE"
"formerly defined here reexposed here for backward compatibility";"CODE"
"from models import noqa f401";"CODE"
"from utils import noqa f401";"CODE"
"preferred clock based on which one is more accurate on a given system";"CODE"
"bypass if not a dictionary e g verify";"-"
"remove keys that are set to none extract keys first to avoid altering";"IRRE"
"the dictionary during iteration";"-"
"special case allow http https redirect when using the standard";"CODE"
"ports this isn t specified by rfc 7235 but is kept to avoid";"CODE"
"breaking backwards compatibility with older versions of requests";"CODE"
"that allowed any redirects on the same host";"-"
"handle default port usage corresponding to scheme";"CODE"
"standard case root uri must match";"CODE"
"informational";"CODE"
"redirection";"-"
"resume and resume incomplete to be removed in 3 0";"OUTD"
"client error";"CODE"
"server error";"-"
"use the lowercased key for lookups but store the actual";"CODE"
"key alongside the value";"IRRE"
"we allow fall through here so values default to none";"IRRE"
"to native string is unused here but imported here for backwards compatibility";"CODE"
"from internal utils import noqa f401";"CODE"
"ensure that is used to preserve previous delimiter behavior";"OUTD"
"provide a proxy bypass version on windows without dns lookups";"META"
"proxyenable could be reg sz or reg dword normalizing it";"-"
"proxyoverride is almost always a string";"CODE"
"make a check value list from the registry entry replace the";"CODE"
"local string by the localhost entry and the corresponding";"CODE"
"canonical entry";"CODE"
"filter out empty strings to avoid re match return true in the following code";"CODE"
"now check if we match one of the registry values";"IRRE"
"test test replace r mask dots";"IRRE"
"test test replace r change glob sequence";"IRRE"
"test test replace r change glob char";"IRRE"
"def proxy bypass host noqa";"CODE"
"abort early if there isn t one";"-"
"return with login password";"IRRE"
"if there was a parsing error or a permissions issue reading the file";"CODE"
"we ll just skip netrc auth unless explicitly asked to raise errors";"CODE"
"app engine hackiness";"-"
"from mitsuhiko werkzeug used with permission";"CODE"
"from mitsuhiko werkzeug used with permission";"CODE"
"from mitsuhiko werkzeug used with permission";"CODE"
"this is not the real unquoting but fixing this so that the";"CODE"
"rfc is met will result in bugs with internet explorer and";"IRRE"
"probably some other browsers as well ie for example is";"CODE"
"uploading files with c foo bar txt as filename";"IRRE"
"if this is a filename and the starting characters look like";"CODE"
"a unc path then just return the value without quotes using the";"IRRE"
"replace sequence below on a unc path has the effect of turning";"-"
"the leading double slash into a single slash and then";"CODE"
"fix ie filename doesn t work correctly see 458";"CODE"
"more information please see the discussion on issue 2266 this";"CODE"
"assume utf 8 based on rfc 4627 https www ietf org rfc rfc4627 txt since the charset was unset";"CODE"
"more information please see the discussion on issue 2266 this";"CODE"
"try charset from content type";"CODE"
"fall back";"-"
"the unreserved uri characters rfc 3986";"CODE"
"afe with percent";"-"
"afe without percent";"-"
"unquote only the unreserved characters";"CODE"
"then quote only illegal characters do not quote reserved";"CODE"
"unreserved or";"CODE"
"we couldn t unquote the given uri so let s try quoting it but";"CODE"
"there may be unquoted s in the uri we need to make sure they re";"TASK"
"properly quoted so they do not cause issues elsewhere";"CODE"
"prioritize lowercase environment variables over uppercase";"CODE"
"to keep a consistent behaviour with other http projects curl wget";"CODE"
"first check whether no proxy is defined if it is check that the url";"CODE"
"we re getting isn t in the no proxy list";"CODE"
"urls don t always have hostnames e g file urls";"CODE"
"we need to check whether we match here we need to see if we match";"TASK"
"the end of the hostname both with and without the port";"CODE"
"if no proxy ip was defined in plain ip notation instead of cidr notation";"CODE"
"matches the ip of the index";"-"
"the url does match something in no proxy so we don t want";"CODE"
"to apply the proxies on this url";"CODE"
"parsed hostname can be none in cases such as a file uri";"IRRE"
"null bytes no need to recreate these on each call to guess json utf";"IRRE"
"null x00 encode ascii encoding to ascii for python 3";"CODE"
"json always starts with two ascii characters so detection is as";"CODE"
"easy as counting the nulls and from their location and count";"CODE"
"determine the encoding also detect a bom if present";"-"
"return utf 32 bom included";"CODE"
"return utf 8 sig bom included ms style discouraged";"CODE"
"return utf 16 bom included";"CODE"
"if sample 2 null2 1st and 3rd are null";"-"
"if sample 1 2 null2 2nd and 4th are null";"-"
"did not detect 2 valid utf 16 ascii range characters";"CODE"
"did not detect a valid utf 32 ascii range character";"CODE"
"a defect in urlparse determines that there isn t a netloc present in some";"IRRE"
"urls we previously assumed parsing was overly cautious and swapped the";"-"
"netloc and path due to a lack of tests on the original defect this is";"CODE"
"maintained with parse url for backwards compatibility";"CODE"
"parse url doesn t provide the netloc with auth";"IRRE"
"so we ll add it ourselves";"TASK"
"see func prepend scheme if needed";"CODE"
"issue 1483 make sure the url always has a trailing slash";"-"
"delay importing until the fixture in order to make it possible";"CODE"
"to deselect the test via command line when trustme is not available";"CODE"
"only commonname no subjectaltname";"-"
"close server set release server block";"IRRE"
"close server set release server block";"IRRE"
"url f http host port path to thing view edit token hunter2";"CODE"
"b location get relevant section r n r n";"-"
"url f http host port path to thing view edit token hunter2";"CODE"
"verify we haven t overwritten the location with our previous fragment";"-"
"assert r history 1 request url f http host port get relevant section";"CODE"
"verify previous fragment is used and not the original";"-"
"assert r url f http host port final url relevant section";"CODE"
"see gh 3579";"-"
"connecting to an unknown domain should raise a connectionerror";"CODE"
"connecting to an invalid port should raise a connectionerror";"CODE"
"inputing a url that cannot be parsed should raise an invalidurl error";"CODE"
"any proxy related error address resolution no route to host etc should result in a proxyerror";"TASK"
"should use netrc and work";"-"
"given auth should override and fail";"CODE"
"should use netrc and work";"-"
"given auth should override and fail";"CODE"
"should use netrc";"-"
"make sure that we don t use the example com credentails";"IRRE"
"for the request";"CODE"
"compat str is unicode";"-"
"from issue 6935";"CODE"
"prefix https example com trailing slash";"CODE"
"from issue 6935";"CODE"
"prefix https example com no trailing slash";"CODE"
"from issue 1321";"CODE"
"url http 9000 path query frag format";"CODE"
"this is testing that they are builtin strings a bit weird but there";"IRRE"
"we go";"-"
"per discussion in gh issue 3386";"-"
"make sure r content is none";"-"
"default behavior";"CODE"
"edge case check to see if location is in headers anyways";"CODE"
"the n 1 th request fails";"CODE"
"if the server thread fails to finish the test suite will hang";"TASK"
"and get killed by the jenkins timeout";"-"
"illegal bytes";"-"
"reserved characters";"CODE"
"xxx unsure whether this is reasonable behavior";"META"
"scikit learn documentation build configuration file created by";"IRRE"
"sphinx quickstart on fri jan 8 09 13 42 2010";"-"
"this file is execfile d with the current directory set to its containing";"IRRE"
"dir";"-"
"note that not all possible configuration values are present in this";"IRRE"
"autogenerated file";"-"
"all configuration values have a default values that are commented out";"IRRE"
"serve to show the default";"CODE"
"if extensions or modules to document with autodoc are in another";"CODE"
"directory add these directories to sys path here if the directory";"TASK"
"is relative to the documentation root use os path abspath to make it";"CODE"
"absolute like shown here";"-"
"configure plotly to integrate its output into the html pages generated by";"CODE"
"sphinx gallery";"-"
"make it possible to render the doc when not running the examples";"CODE"
"that need plotly";"-"
"general configuration";"-"
"add any sphinx extension module names here as strings they can be";"CODE"
"extensions coming with sphinx named sphinx ext or your custom ones";"-"
"see sphinxext";"-"
"specify how to identify the prompt when copying code snippets";"-"
"import jupyterlite sphinx noqa f401";"CODE"
"in some cases we don t want to require jupyterlite sphinx to be installed";"CODE"
"e g the doc min dependencies build";"CODE"
"produce plot directives for examples that contain import matplotlib or";"CODE"
"from matplotlib import";"CODE"
"options for the plot directive";"CODE"
"https matplotlib org stable api sphinxext plot directive api html";"CODE"
"we do not need the table of class members because sphinxext override pst pagetoc py";"CODE"
"will show them in the secondary sidebar";"IRRE"
"we want in page toc of class members instead of a separate page for each entry";"CODE"
"for maths use mathjax by default and svg if no mathjax env variable is set";"CODE"
"useful for viewing the doc offline";"CODE"
"add any paths that contain templates here relative to this directory";"CODE"
"generate autosummary even if no references";"CODE"
"the suffix of source filenames";"CODE"
"the encoding of source files";"CODE"
"the main toctree document";"CODE"
"general information about the project";"CODE"
"the version info for the project you re documenting acts as replacement for";"CODE"
"version and release also used in various other places throughout the";"META"
"built documents";"CODE"
"the short x y version";"META"
"the full version including alpha beta rc tags";"META"
"removes post from release name";"CODE"
"the language for content autogenerated by sphinx refer to documentation";"CODE"
"for a list of supported languages";"CODE"
"language none";"-"
"there are two options for replacing today either you set today to some";"CODE"
"non false value then it is used";"IRRE"
"today";"-"
"else today fmt is used as the format for a strftime call";"IRRE"
"today fmt b d y";"-"
"list of patterns relative to source directory that match files and";"-"
"directories to ignore when looking for source files";"CODE"
"the rest default role used for this markup text to use for all";"CODE"
"documents";"CODE"
"if true will be appended to func etc cross reference text";"CODE"
"if true the current module name will be prepended to all description";"CODE"
"unit titles such as function";"CODE"
"add module names true";"TASK"
"if true sectionauthor and moduleauthor directives will be shown in the";"META"
"output they are ignored by default";"IRRE"
"show authors false";"META"
"a list of ignored prefixes for module index sorting";"CODE"
"modindex common prefix";"-"
"options for html output";"IRRE"
"the theme to use for html and html help pages major themes that come with";"CODE"
"sphinx are currently default and sphinxdoc";"CODE"
"this config option is used to generate the canonical links in the header";"CODE"
"of every page the canonical link is needed to prevent search engines from";"IRRE"
"returning results pointing to old scikit learn versions";"IRRE"
"theme options are theme specific and customize the look and feel of a theme";"-"
"further for a list of options available for each theme see the";"CODE"
"documentation";"CODE"
"general configuration";"-"
"if prev next is included in article footer items then setting show prev next";"IRRE"
"to true would repeat prev and next links see";"-"
"https github com pydata pydata sphinx theme blob b731dc230bc26a3d1d1bb039c56c977a9b3d25d8 src pydata sphinx theme theme pydata sphinx theme layout html l118 l129";"TASK"
"the switcher requires a json file with the list of documentation versions which";"CODE"
"is generated by the script build tools circle list versions py and placed under";"META"
"the js static directory it will then be copied to the static directory in";"CODE"
"the built documentation";"CODE"
"check switcher may be set to false if docbuild pipeline fails see";"CODE"
"https pydata sphinx theme readthedocs io en stable user guide version dropdown html configure switcher json url";"CODE"
"template placement in theme layouts";"CODE"
"note that the alignment of navbar center is controlled by navbar align";"IRRE"
"navbar persistent is persistent right even when on mobiles";"IRRE"
"use html sidebars that map page patterns to list of sidebar templates";"IRRE"
"when specified as a dictionary the keys should follow glob style patterns as in";"-"
"https www sphinx doc org en master usage configuration html confval exclude patterns";"CODE"
"in particular specifies the default for all pages";"CODE"
"use html theme sidebar secondary remove for file wide removal";"IRRE"
"sphinx gallery specific sidebar components";"IRRE"
"https sphinx gallery github io stable advanced html using sphinx gallery sidebar components";"IRRE"
"add any paths that contain custom themes here relative to this directory";"TASK"
"html theme path themes";"-"
"the name for this set of sphinx documents if none it defaults to";"CODE"
"project v release documentation";"CODE"
"html title none";"-"
"a shorter title for the navigation bar default is the same as html title";"CODE"
"the name of an image file within the static path to use as favicon of the";"CODE"
"docs this file should be a windows icon file ico being 16x16 or 32x32";"CODE"
"pixels large";"-"
"add any paths that contain custom static files such as style sheets here";"TASK"
"relative to this directory they are copied after the builtin static files";"CODE"
"so a file named default css will overwrite the builtin default css";"CODE"
"if not a last updated on timestamp is inserted at every page bottom";"CODE"
"using the given strftime format";"CODE"
"html last updated fmt b d y";"CODE"
"custom sidebar templates maps document names to template names";"IRRE"
"workaround for removing the left sidebar on pages without toc";"IRRE"
"a better solution would be to follow the merge of";"-"
"https github com pydata pydata sphinx theme pull 1682";"CODE"
"additional templates that should be rendered to pages maps page names to";"TASK"
"template names";"-"
"additional files to copy";"TASK"
"html extra path";"-"
"additional js files";"TASK"
"compile scss files into css files using sphinxcontrib sass";"CODE"
"additional css files should be subset of the values of sass targets";"IRRE"
"external jquery and datatables";"CODE"
"internal api search initialization and styling";"CODE"
"if false no module index is generated";"CODE"
"if false no index is generated";"-"
"if true the index is split into individual pages for each letter";"CODE"
"html split index false";"-"
"if true links to the rest sources are added to the pages";"TASK"
"html show sourcelink true";"-"
"if true an opensearch description file will be output and all pages will";"CODE"
"contain a link tag referring to it the value of this option must be the";"TASK"
"base url from which the finished html is served";"TASK"
"html use opensearch";"CODE"
"if nonempty this is the file name suffix for html files e g xhtml";"CODE"
"html file suffix";"-"
"output file base name for html help builder";"IRRE"
"if true the rest sources are included in the html build as sources name";"CODE"
"adds variables into templates";"CODE"
"finds latest release highlights and places it into html context for";"CODE"
"index html";"-"
"finds the highlight with the latest version number";"IRRE"
"get version from highlight name assuming highlights have the form";"CODE"
"plot release highlights 0 22 0";"-"
"redirects dictionary maps from old links to new links";"CODE"
"see https github com scikit learn scikit learn pull 22550";"CODE"
"options for latex output";"IRRE"
"the paper size letterpaper or a4paper";"CODE"
"papersize letterpaper";"CODE"
"the font size 10pt 11pt or 12pt";"-"
"pointsize 10pt";"CODE"
"additional stuff for the latex preamble";"TASK"
"grouping the document tree into latex files list of tuples";"CODE"
"source start file target name title author documentclass";"CODE"
"howto manual";"-"
"the name of an image file relative to this directory to place at the top of";"CODE"
"the title page";"-"
"documents to append as an appendix to all manuals";"CODE"
"latex appendices";"CODE"
"if false no module index is generated";"CODE"
"intersphinx configuration";"CODE"
"forces release highlights to the top";"CODE"
"avoid generating too many cross links";"CODE"
"for the index page of the gallery and each nested section we hide the secondary";"CODE"
"sidebar by specifying an empty list no components because there is no meaningful";"IRRE"
"in page toc for these pages and they are generated so sourcelink is not useful";"CODE"
"either";"-"
"the following dictionary contains the information used to create the";"IRRE"
"thumbnails for the front page of the scikit learn home page";"CODE"
"key first image in set";"IRRE"
"values number of plot in set height of thumbnail";"IRRE"
"enable experimental module so that experimental estimators can be";"CODE"
"discovered properly by sphinx";"-"
"from sklearn experimental import noqa f401";"CODE"
"do not run the examples when using linkcheck by using a small priority";"CODE"
"default priority is 500 and sphinx gallery using builder inited event too";"IRRE"
"triggered just before the html for an individual page is created";"CODE"
"to hide show the prompt in code examples";"-"
"the following is used by sphinx ext linkcode to provide links to github";"-"
"package path l lineno";"-"
"todo 1 10 remove passiveaggressive";"TASK"
"maps functions with a class name that is indistinguishable when case is";"CODE"
"ignore to another filename";"-"
"config for sphinxext opengraph";"CODE"
"config for linkcheck that checks the documentation for broken links";"CODE"
"ignore all links in whats new to avoid doing many github requests and";"CODE"
"hitting the github rate threshold that makes linkcheck take a lot of time";"-"
"default timeout to make some sites links fail faster";"CODE"
"allow redirects from doi org";"CODE"
"ignore links to local html files e g in image directive target field";"-"
"ignore links to specific pdf pages because linkcheck does not handle them";"CODE"
"utf 8 codec can t decode byte error";"-"
"r http www utstat toronto edu rsalakhu sta4273 notes lecture2 pdf page";"TASK"
"the unseen labor behind our digital infrastructure pdf page";"CODE"
"links falsely flagged as broken";"-"
"broken links from testimonials";"IRRE"
"ignore some dynamically created anchors see";"IRRE"
"https github com sphinx doc sphinx issues 9016 for more details about";"CODE"
"the github example";"-"
"r https github com conda forge miniforge miniforge";"CODE"
"setting the maximum size of thread pools";"IRRE"
"consistently create same random numpy array 5837352 comment6712034 5837352";"IRRE"
"use a browser like user agent to avoid some 403 client error forbidden for";"CODE"
"url errors this is taken from the variable navigator useragent inside a";"CODE"
"browser console";"CODE"
"use github token from environment variable to avoid github rate limits when";"CODE"
"checking github links";"-"
"skip the test in rcv1 rst if the dataset is not already loaded";"IRRE"
"import pandas noqa f401";"CODE"
"checks sklearn skip network tests to see if test should run";"IRRE"
"import pandas noqa f401";"CODE"
"import pandas noqa f401";"CODE"
"import pandas noqa f401";"CODE"
"import pandas noqa f401";"CODE"
"import matplotlib noqa f401";"CODE"
"import cupy noqa f401";"CODE"
"normalize filename to use forward slashes on windows for easier handling";"CODE"
"later";"-"
"use matplotlib agg backend during the tests including doctests";"IRRE"
"todo configure numpy to output scalar arrays as regular python scalars";"CODE"
"once possible to improve readability of the tests docstrings";"CODE"
"https numpy org neps nep 0051 scalar representation html implementation";"TASK"
"normally doctest has the entire module s scope here we set globs to an empty dict";"IRRE"
"to remove the module s scope";"CODE"
"https docs python org 3 library doctest html what s the execution context";"CODE"
"here we generate the text only for one instance this directive";"CODE"
"should not be used for meta estimators where tags depend on the";"CODE"
"sub estimator";"-"
"get the first non empty line of the processed docstring this could lead";"CODE"
"to unexpected results if the object does not have a short summary line";"IRRE"
"default priority 9999 apply later than everything else";"CODE"
"unwrap the object to get the correct source";"IRRE"
"file in case that is wrapped by a decorator";"CODE"
"a class href obj a decompose";"IRRE"
"external repo regex re compile r w w";"CODE"
"return 0 format issue no";"CODE"
"if repo match external repo";"-"
"formatted issue self format text issue lstrip";"CODE"
"format template for issues uri";"CODE"
"e g https github com sloria marshmallow issues issue";"CODE"
"format template for pr uri";"CODE"
"e g https github com sloria marshmallow pull issue";"CODE"
"format template for commit uri";"CODE"
"e g https github com sloria marshmallow commits commit";"CODE"
"shortcut for github e g sloria marshmallow";"CODE"
"format template for user profile uri";"CODE"
"e g https github com user";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data exploration on the bike sharing demand dataset";"IRRE"
"we start by loading the data from the openml repository";"CODE"
"to get a quick understanding of the periodic patterns of the data let us";"CODE"
"have a look at the average demand per hour during a week";"-"
"note that the week starts on a sunday during the weekend we can clearly";"TASK"
"distinguish the commute patterns in the morning and evenings of the work days";"CODE"
"and the leisure use of the bikes on the weekends with a more spread peak";"CODE"
"demand around the middle of the days";"-"
"the target of the prediction problem is the absolute count of bike rentals on";"-"
"an hourly basis";"-"
"let us rescale the target variable number of hourly bike rentals to predict";"CODE"
"a relative demand so that the mean absolute error is more easily interpreted";"CODE"
"as a fraction of the maximum demand";"-"
"note";"TASK"
"the fit method of the models used in this notebook all minimizes the";"CODE"
"mean squared error to estimate the conditional mean";"-"
"the absolute error however would estimate the conditional median";"-"
"nevertheless when reporting performance measures on the test set in";"IRRE"
"the discussion we choose to focus on the mean absolute error instead";"IRRE"
"of the root mean squared error because it is more intuitive to";"CODE"
"interpret note however that in this study the best models for one";"CODE"
"metric are also the best ones in terms of the other metric";"CODE"
"the input feature data frame is a time annotated hourly log of variables";"CODE"
"describing the weather conditions it includes both numerical and categorical";"CODE"
"variables note that the time information has already been expanded into";"CODE"
"several complementary columns";"-"
"note";"TASK"
"if the time information was only present as a date or datetime column we";"CODE"
"could have expanded it into hour in the day day in the week";"CODE"
"day in the month month in the year using pandas";"CODE"
"https pandas pydata org pandas docs stable user guide timeseries html time date components";"CODE"
"we now introspect the distribution of the categorical variables starting";"CODE"
"with weather";"-"
"since there are only 3 heavy rain events we cannot use this category to";"IRRE"
"train machine learning models with cross validation instead we simplify the";"-"
"representation by collapsing those into the rain category";"CODE"
"as expected the season variable is well balanced";"IRRE"
"time based cross validation";"-"
"since the dataset is a time ordered event log hourly demand we will use a";"IRRE"
"time sensitive cross validation splitter to evaluate our demand forecasting";"CODE"
"model as realistically as possible we use a gap of 2 days between the train";"IRRE"
"and test side of the splits we also limit the training set size to make the";"IRRE"
"performance of the cv folds more stable";"CODE"
"1000 test datapoints should be enough to quantify the performance of the";"CODE"
"model this represents a bit less than a month and a half of contiguous test";"IRRE"
"data";"-"
"let us manually inspect the various splits to check that the";"CODE"
"timeseriessplit works as we expect starting with the first split";"-"
"we now inspect the last split";"-"
"all is well we are now ready to do some predictive modeling";"CODE"
"gradient boosting";"-"
"gradient boosting regression with decision trees is often flexible enough to";"-"
"efficiently handle heterogeneous tabular data with a mix of categorical and";"-"
"numerical features as long as the number of samples is large enough";"TASK"
"here we use the modern";"IRRE"
"class sklearn ensemble histgradientboostingregressor with native support";"IRRE"
"for categorical features therefore we only need to set";"TASK"
"categorical features from dtype such that features with categorical dtype";"TASK"
"are considered categorical features for reference we extract the categorical";"CODE"
"features from the dataframe based on the dtype the internal trees use a dedicated";"CODE"
"tree splitting rule for these features";"TASK"
"the numerical variables need no preprocessing and for the sake of simplicity";"CODE"
"we only try the default hyper parameters for this model";"CODE"
"let s evaluate our gradient boosting model with the mean absolute error of the";"CODE"
"relative demand averaged across our 5 time based cross validation splits";"-"
"we see that we set max iter large enough such that early stopping took place";"IRRE"
"this model has an average error around 4 to 5 of the maximum demand this is";"CODE"
"quite good for a first trial without any hyper parameter tuning we just had";"IRRE"
"to make the categorical variables explicit note that the time related";"TASK"
"features are passed as is i e without processing them but this is not much";"TASK"
"of a problem for tree based models as they can learn a non monotonic";"CODE"
"relationship between ordinal input features and the target";"TASK"
"this is not the case for linear regression models as we will see in the";"CODE"
"following";"-"
"naive linear regression";"-"
"as usual for linear models categorical variables need to be one hot encoded";"CODE"
"for consistency we scale the numerical features to the same 0 1 range using";"CODE"
"class sklearn preprocessing minmaxscaler although in this case it does not";"CODE"
"impact the results much because they are already on comparable scales";"IRRE"
"it is affirmative to see that the selected alpha is in our specified";"CODE"
"range";"-"
"the performance is not good the average error is around 14 of the maximum";"CODE"
"demand this is more than three times higher than the average error of the";"CODE"
"gradient boosting model we can suspect that the naive original encoding";"-"
"merely min max scaled of the periodic time related features might prevent";"TASK"
"the linear regression model to properly leverage the time information linear";"CODE"
"regression does not automatically model non monotonic relationships between";"IRRE"
"the input features and the target non linear terms have to be engineered in";"TASK"
"the input";"CODE"
"for example the raw numerical encoding of the hour feature prevents the";"CODE"
"linear model from recognizing that an increase of hour in the morning from 6";"CODE"
"to 8 should have a strong positive impact on the number of bike rentals while";"CODE"
"an increase of similar magnitude in the evening from 18 to 20 should have a";"CODE"
"strong negative impact on the predicted number of bike rentals";"-"
"time steps as categories";"-"
"since the time features are encoded in a discrete manner using integers 24";"TASK"
"unique values in the hours feature we could decide to treat those as";"IRRE"
"categorical variables using a one hot encoding and thereby ignore any";"IRRE"
"assumption implied by the ordering of the hour values";"IRRE"
"using one hot encoding for the time features gives the linear model a lot";"TASK"
"more flexibility as we introduce one additional feature per discrete time";"TASK"
"level";"-"
"the average error rate of this model is 10 which is much better than using";"CODE"
"the original ordinal encoding of the time feature confirming our intuition";"TASK"
"that the linear regression model benefits from the added flexibility to not";"TASK"
"treat time progression in a monotonic manner";"-"
"however this introduces a very large number of new features if the time of";"CODE"
"the day was represented in minutes since the start of the day instead of";"CODE"
"hours one hot encoding would have introduced 1440 features instead of 24";"CODE"
"this could cause some significant overfitting to avoid this we could use";"CODE"
"func sklearn preprocessing kbinsdiscretizer instead to re bin the number";"CODE"
"of levels of fine grained ordinal or numerical variables while still";"CODE"
"benefitting from the non monotonic expressivity advantages of one hot";"CODE"
"encoding";"-"
"finally we also observe that one hot encoding completely ignores the";"CODE"
"ordering of the hour levels while this could be an interesting inductive bias";"CODE"
"to preserve to some level in the following we try to explore smooth";"CODE"
"non monotonic encoding that locally preserves the relative ordering of time";"IRRE"
"features";"TASK"
"trigonometric features";"TASK"
"as a first attempt we can try to encode each of those periodic features";"TASK"
"using a sine and cosine transformation with the matching period";"CODE"
"each ordinal time feature is transformed into 2 features that together encode";"TASK"
"equivalent information in a non monotonic way and more importantly without";"CODE"
"any jump between the first and the last value of the periodic range";"IRRE"
"let us visualize the effect of this feature expansion on some synthetic hour";"CODE"
"data with a bit of extrapolation beyond hour 23";"-"
"let s use a 2d scatter plot with the hours encoded as colors to better see";"CODE"
"how this representation maps the 24 hours of the day to a 2d space akin to";"CODE"
"some sort of a 24 hour version of an analog clock note that the 25th hour";"TASK"
"is mapped back to the 1st hour because of the periodic nature of the";"-"
"sine cosine representation";"-"
"we can now build a feature extraction pipeline using this strategy";"CODE"
"the performance of our linear regression model with this simple feature";"CODE"
"engineering is a bit better than using the original ordinal time features but";"TASK"
"worse than using the one hot encoded time features we will further analyze";"TASK"
"possible reasons for this disappointing outcome at the end of this notebook";"CODE"
"periodic spline features";"TASK"
"we can try an alternative encoding of the periodic time related features";"TASK"
"using spline transformations with a large enough number of splines and as a";"CODE"
"result a larger number of expanded features compared to the sine cosine";"IRRE"
"transformation";"CODE"
"n knots n splines 1 periodic and include bias is true";"CODE"
"again let us visualize the effect of this feature expansion on some";"CODE"
"synthetic hour data with a bit of extrapolation beyond hour 23";"-"
"thanks to the use of the extrapolation periodic parameter we observe";"IRRE"
"that the feature encoding stays smooth when extrapolating beyond midnight";"TASK"
"we can now build a predictive pipeline using this alternative periodic";"CODE"
"feature engineering strategy";"TASK"
"it is possible to use fewer splines than discrete levels for those ordinal";"CODE"
"values this makes spline based encoding more efficient than one hot encoding";"IRRE"
"while preserving most of the expressivity";"CODE"
"spline features make it possible for the linear model to successfully";"TASK"
"leverage the periodic time related features and reduce the error from 14 to";"TASK"
"10 of the maximum demand which is similar to what we observed with the";"-"
"one hot encoded features";"TASK"
"qualitative analysis of the impact of features on linear model predictions";"TASK"
"here we want to visualize the impact of the feature engineering choices on";"TASK"
"the time related shape of the predictions";"-"
"to do so we consider an arbitrary time based split to compare the predictions";"TASK"
"on a range of held out data points";"CODE"
"we visualize those predictions by zooming on the last 96 hours 4 days of";"-"
"the test set to get some qualitative insights";"IRRE"
"we can draw the following conclusions from the above plot";"CODE"
"the raw ordinal time related features are problematic because they do";"TASK"
"not capture the natural periodicity we observe a big jump in the";"CODE"
"predictions at the end of each day when the hour features goes from 23 back";"CODE"
"to 0 we can expect similar artifacts at the end of each week or each year";"CODE"
"as expected the trigonometric features sine and cosine do not have";"TASK"
"these discontinuities at midnight but the linear regression model fails to";"META"
"leverage those features to properly model intra day variations";"CODE"
"using trigonometric features for higher harmonics or additional";"TASK"
"trigonometric features for the natural period with different phases could";"TASK"
"potentially fix this problem";"CODE"
"the periodic spline based features fix those two problems at once they";"TASK"
"give more expressivity to the linear model by making it possible to focus";"CODE"
"on specific hours thanks to the use of 12 splines furthermore the";"-"
"extrapolation periodic option enforces a smooth representation between";"CODE"
"hour 23 and hour 0";"IRRE"
"the one hot encoded features behave similarly to the periodic";"TASK"
"spline based features but are more spiky for instance they can better";"TASK"
"model the morning peak during the week days since this peak lasts shorter";"CODE"
"than an hour however we will see in the following that what can be an";"CODE"
"advantage for linear models is not necessarily one for more expressive";"CODE"
"models";"-"
"we can also compare the number of features extracted by each feature";"TASK"
"engineering pipeline";"CODE"
"this confirms that the one hot encoding and the spline encoding strategies";"CODE"
"create a lot more features for the time representation than the alternatives";"TASK"
"which in turn gives the downstream linear model more flexibility degrees of";"CODE"
"freedom to avoid underfitting";"CODE"
"finally we observe that none of the linear models can approximate the true";"CODE"
"bike rentals demand especially for the peaks that can be very sharp at rush";"CODE"
"hours during the working days but much flatter during the week ends the most";"META"
"accurate linear models based on splines or one hot encoding tend to forecast";"CODE"
"peaks of commuting related bike rentals even on the week ends and";"CODE"
"under estimate the commuting related events during the working days";"-"
"these systematic prediction errors reveal a form of under fitting and can be";"CODE"
"explained by the lack of interactions terms between features e g";"TASK"
"workingday and features derived from hours this issue will be addressed";"TASK"
"in the following section";"CODE"
"modeling pairwise interactions with splines and polynomial features";"TASK"
"linear models do not automatically capture interaction effects between input";"CODE"
"features it does not help that some features are marginally non linear as is";"TASK"
"the case with features constructed by splinetransformer or one hot";"CODE"
"encoding or binning";"-"
"however it is possible to use the polynomialfeatures class on coarse";"IRRE"
"grained spline encoded hours to model the workingday hours interaction";"CODE"
"explicitly without introducing too many new variables";"CODE"
"those features are then combined with the ones already computed in the";"CODE"
"previous spline base pipeline we can observe a nice performance improvement";"CODE"
"by modeling this pairwise interaction explicitly";"CODE"
"modeling non linear feature interactions with kernels";"TASK"
"the previous analysis highlighted the need to model the interactions between";"TASK"
"workingday and hours another example of a such a non linear";"-"
"interaction that we would like to model could be the impact of the rain that";"CODE"
"might not be the same during the working days and the week ends and holidays";"CODE"
"for instance";"CODE"
"to model all such interactions we could either use a polynomial expansion on";"CODE"
"all marginal features at once after their spline based expansion however";"TASK"
"this would create a quadratic number of features which can cause overfitting";"TASK"
"and computational tractability issues";"-"
"alternatively we can use the nystr m method to compute an approximate";"IRRE"
"polynomial kernel expansion let us try the latter";"CODE"
"we observe that this model can almost rival the performance of the gradient";"CODE"
"boosted trees with an average error around 5 of the maximum demand";"CODE"
"note that while the final step of this pipeline is a linear regression model";"CODE"
"the intermediate steps such as the spline feature extraction and the nystr m";"TASK"
"kernel approximation are highly non linear as a result the compound pipeline";"IRRE"
"is much more expressive than a simple linear regression model with raw features";"TASK"
"for the sake of completeness we also evaluate the combination of one hot";"CODE"
"encoding and kernel approximation";"-"
"while one hot encoded features were competitive with spline based features";"TASK"
"when using linear models this is no longer the case when using a low rank";"CODE"
"approximation of a non linear kernel this can be explained by the fact that";"CODE"
"spline features are smoother and allow the kernel approximation to find a";"TASK"
"more expressive decision function";"CODE"
"let us now have a qualitative look at the predictions of the kernel models";"CODE"
"and of the gradient boosted trees that should be able to better model";"CODE"
"non linear interactions between features";"TASK"
"again we zoom on the last 4 days of the test set";"IRRE"
"first note that trees can naturally model non linear feature interactions";"TASK"
"since by default decision trees are allowed to grow beyond a depth of 2";"CODE"
"levels";"-"
"here we can observe that the combinations of spline features and non linear";"TASK"
"kernels works quite well and can almost rival the accuracy of the gradient";"-"
"boosting regression trees";"-"
"on the contrary one hot encoded time features do not perform that well with";"CODE"
"the low rank kernel model in particular they significantly over estimate";"-"
"the low demand hours more than the competing models";"-"
"we also observe that none of the models can successfully predict some of the";"-"
"peak rentals at the rush hours during the working days it is possible that";"CODE"
"access to additional features would be required to further improve the";"TASK"
"accuracy of the predictions for instance it could be useful to have access";"CODE"
"to the geographical repartition of the fleet at any point in time or the";"CODE"
"fraction of bikes that are immobilized because they need servicing";"TASK"
"let us finally get a more quantitative look at the prediction errors of those";"CODE"
"three models using the true vs predicted demand scatter plots";"-"
"this visualization confirms the conclusions we draw on the previous plot";"CODE"
"all models under estimate the high demand events working day rush hours";"-"
"but gradient boosting a bit less so the low demand events are well predicted";"META"
"on average by gradient boosting while the one hot polynomial regression";"CODE"
"pipeline seems to systematically over estimate demand in that regime overall";"CODE"
"the predictions of the gradient boosted trees are closer to the diagonal than";"CODE"
"for the kernel models";"CODE"
"concluding remarks";"-"
"we note that we could have obtained slightly better results for kernel models";"TASK"
"by using more components higher rank kernel approximation at the cost of";"-"
"longer fit and prediction durations for large values of n components the";"IRRE"
"performance of the one hot encoded features would even match the spline";"TASK"
"features";"TASK"
"the nystroem ridgecv regressor could also have been replaced by";"OUTD"
"class sklearn neural network mlpregressor with one or two hidden layers";"IRRE"
"and we would have obtained quite similar results";"IRRE"
"the dataset we used in this case study is sampled on an hourly basis however";"CODE"
"cyclic spline based features could model time within day or time within week";"TASK"
"very efficiently with finer grained time resolutions for instance with";"CODE"
"measurements taken every minute instead of every hour without introducing";"CODE"
"more features one hot encoding time representations would not offer this";"TASK"
"flexibility";"-"
"finally in this notebook we used ridgecv because it is very efficient from";"CODE"
"a computational point of view however it models the target variable as a";"CODE"
"gaussian random variable with constant variance for positive regression";"CODE"
"problems it is likely that using a poisson or gamma distribution would make";"META"
"more sense this could be achieved by using";"IRRE"
"gridsearchcv tweedieregressor power 2 param grid alpha alphas";"-"
"instead of ridgecv";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the dataset via openml";"CODE"
"the usps digits datasets is available in openml we use";"IRRE"
"func sklearn datasets fetch openml to get this dataset in addition we";"IRRE"
"normalize the dataset such that all pixel values are in the range 0 1";"IRRE"
"the idea will be to learn a pca basis with and without a kernel on";"-"
"noisy images and then use these models to reconstruct and denoise these";"IRRE"
"images";"-"
"thus we split our dataset into a training and testing set composed of 1 000";"IRRE"
"samples for the training and 100 samples for testing these images are";"CODE"
"noise free and we will use them to evaluate the efficiency of the denoising";"IRRE"
"approaches in addition we create a copy of the original dataset and add a";"TASK"
"gaussian noise";"-"
"the idea of this application is to show that we can denoise corrupted images";"CODE"
"by learning a pca basis on some uncorrupted images we will use both a pca";"-"
"and a kernel based pca to solve this problem";"CODE"
"in addition we will create a helper function to qualitatively assess the";"TASK"
"image reconstruction by plotting the test images";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"download the data if not already on disk and load it as numpy arrays";"CODE"
"introspect the images arrays to find the shapes for plotting";"CODE"
"for machine learning we use the 2 data directly as relative pixel";"IRRE"
"positions info is ignored by this model";"CODE"
"the label to predict is the id of the person";"CODE"
"split into a training set and a test and keep 25 of the data for testing";"IRRE"
"compute a pca eigenfaces on the face dataset treated as unlabeled";"IRRE"
"dataset unsupervised feature extraction dimensionality reduction";"TASK"
"train a svm classification model";"IRRE"
"quantitative evaluation of the model quality on the test set";"IRRE"
"qualitative evaluation of the predictions using matplotlib";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"initialize random generator";"IRRE"
"load the data";"CODE"
"first we load both datasets";"IRRE"
"note we are using";"TASK"
"func sklearn datasets fetch 20newsgroups vectorized to download 20";"CODE"
"newsgroups dataset it returns ready to use features";"CODE"
"note x of the 20 newsgroups dataset is a sparse matrix while x";"IRRE"
"of diabetes dataset is a numpy array";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"hack to detect whether we are running by the sphinx builder";"CODE"
"reuters dataset related routines";"IRRE"
"the dataset used in this example is reuters 21578 as provided by the uci ml";"CODE"
"repository it will be automatically downloaded and uncompressed on first";"CODE"
"run";"CODE"
"check that the archive was not tampered";"-"
"main";"CODE"
"create the vectorizer and limit the number of features to a reasonable";"IRRE"
"maximum";"-"
"iterator over parsed reuters sgml files";"IRRE"
"we learn a binary classification between the acq class and all the others";"IRRE"
"acq was chosen as it is more or less evenly distributed in the reuters";"META"
"files for other datasets one should take care of creating a test set with";"IRRE"
"a realistic portion of positive instances";"-"
"here are some classifiers that support the partial fit method";"IRRE"
"discard test set";"IRRE"
"we will feed the classifier with mini batches of 1000 documents this means";"CODE"
"we have at most 1000 docs in memory at any time the smaller the document";"CODE"
"batch the bigger the relative overhead of the partial fit methods";"CODE"
"create the data stream that parses reuters sgml files and iterates on";"IRRE"
"documents as a stream";"CODE"
"main loop iterate on mini batches of examples";"IRRE"
"update estimator with examples in the current mini batch";"CODE"
"accumulate test accuracy stats";"IRRE"
"plot results";"IRRE"
"the plot represents the learning curve of the classifier the evolution";"IRRE"
"of classification accuracy over the course of the mini batches accuracy is";"IRRE"
"measured on the first 1000 samples held out as a validation set";"CODE"
"to limit the memory consumption we queue examples up to a fixed amount";"-"
"before feeding them to the learner";"CODE"
"plot prediction times";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"first example";"-"
"the first example illustrates how the minimum covariance determinant";"CODE"
"robust estimator can help concentrate on a relevant cluster when outlying";"IRRE"
"points exist here the empirical covariance estimation is skewed by points";"CODE"
"outside of the main cluster of course some screening tools would have pointed";"CODE"
"out the presence of two clusters support vector machines gaussian mixture";"-"
"models univariate outlier detection but had it been a high dimensional";"META"
"example none of these could be applied that easily";"-"
"x load wine data 1 2 two clusters";"CODE"
"learn a frontier for outlier detection with several classifiers";"CODE"
"second example";"-"
"the second example shows the ability of the minimum covariance determinant";"CODE"
"robust estimator of covariance to concentrate on the main mode of the data";"CODE"
"distribution the location seems to be well estimated although the";"META"
"covariance is hard to estimate due to the banana shaped distribution anyway";"META"
"we can get rid of some outlying observations the one class svm is able to";"CODE"
"capture the real data structure but the difficulty is to adjust its kernel";"META"
"bandwidth parameter so as to obtain a good compromise between the shape of";"IRRE"
"the data scatter matrix and the risk of over fitting the data";"-"
"x load wine data 6 9 banana shaped";"CODE"
"learn a frontier for outlier detection with several classifiers";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"hack to detect whether we are running by the sphinx builder";"CODE"
"benchmark and plot helper functions";"CODE"
"ax1 set title evolution of prediction time with features";"TASK"
"ax1 set xlabel features";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"if basemap is available we ll use it";"CODE"
"otherwise we ll improvise later";"-"
"x y coordinates for corner cells";"CODE"
"x coordinates of the grid cells";"-"
"y coordinates of the grid cells";"-"
"choose points associated with the desired species";"CODE"
"determine coverage values for each of the training testing points";"IRRE"
"load the compressed data";"CODE"
"set up the data grid";"IRRE"
"the grid in x y coordinates";"-"
"create a bunch for each species";"CODE"
"background points grid coordinates for evaluation";"CODE"
"we ll make use of the fact that coverages 6 has measurements at all";"-"
"land points this will help us decide between land and water";"CODE"
"fit predict and plot for each species";"CODE"
"standardize features";"TASK"
"fit oneclasssvm";"IRRE"
"plot map of south america";"-"
"predict species distribution using the training data";"META"
"we ll predict only for the land points";"CODE"
"plot contours of the prediction";"-"
"scatter training testing points";"IRRE"
"compute auc with regards to background points";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"retrieve the data from internet";"CODE"
"the data is from 2003 2008 this is reasonably calm not too long ago so";"CODE"
"that we get high tech firms and before the 2008 crash this kind of";"CODE"
"historical data can be obtained from apis like the";"CODE"
"data nasdaq com https data nasdaq com and";"CODE"
"alphavantage co https www alphavantage co";"CODE"
"the daily variations of the quotes are what carry the most information";"CODE"
"stock market";"-"
"learning a graph structure";"CODE"
"we use sparse inverse covariance estimation to find which quotes are";"IRRE"
"correlated conditionally on the others specifically sparse inverse";"IRRE"
"covariance gives us a graph that is a list of connections for each";"CODE"
"symbol the symbols that it is connected to are those useful to explain";"-"
"its fluctuations";"-"
"standardize the time series using correlations rather than covariance";"CODE"
"former is more efficient for structure recovery";"CODE"
"clustering using affinity propagation";"IRRE"
"we use clustering to group together quotes that behave similarly here";"IRRE"
"amongst the ref various clustering techniques clustering available";"CODE"
"in the scikit learn we use ref affinity propagation as it does";"CODE"
"not enforce equal size clusters and it can choose automatically the";"TASK"
"number of clusters from the data";"CODE"
"note that this gives us a different indication than the graph as the";"TASK"
"graph reflects conditional relations between variables while the";"CODE"
"clustering reflects marginal properties variables clustered together can";"IRRE"
"be considered as having a similar impact at the level of the full stock";"-"
"market";"-"
"embedding in 2d space";"-"
"for visualization purposes we need to lay out the different symbols on a";"TASK"
"2d canvas for this we use ref manifold techniques to retrieve 2d";"CODE"
"embedding";"-"
"we use a dense eigen solver to achieve reproducibility arpack is initiated";"IRRE"
"with the random vectors that we do not control in addition we use a large";"IRRE"
"number of neighbors to capture the large scale structure";"CODE"
"finding a low dimension embedding for visualization find the best position of";"CODE"
"the nodes the stocks on a 2d plane";"-"
"visualization";"-"
"the output of the 3 models are combined in a 2d graph where nodes";"IRRE"
"represent the stocks and edges the connections partial correlations";"CODE"
"cluster labels are used to define the color of the nodes";"CODE"
"the sparse covariance model is used to display the strength of the edges";"IRRE"
"the 2d embedding is used to position the nodes in the plan";"OUTD"
"this example has a fair amount of visualization related code as";"CODE"
"visualization is crucial here to display the graph one of the challenges";"-"
"is to position the labels minimizing overlap for this we use an";"CODE"
"heuristic based on the direction of the nearest neighbor along each";"CODE"
"axis";"-"
"plot the graph of partial correlations";"-"
"plot the nodes using the coordinates of our embedding";"-"
"plot the edges";"-"
"a sequence of line0 line1 line2 where";"CODE"
"linen x0 y0 x1 y1 xm ym";"-"
"add a label to each node the challenge here is that we want to";"TASK"
"position the labels to avoid overlap with other labels";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"analyzing the bike sharing demand dataset";"IRRE"
"we start by loading the data from the openml repository as a raw parquet file";"CODE"
"to illustrate how to work with an arbitrary parquet file instead of hiding this";"CODE"
"step in a convenience tool such as sklearn datasets fetch openml";"IRRE"
"the url of the parquet file can be found in the json description of the";"CODE"
"bike sharing demand dataset with id 44063 on openml org";"CODE"
"https openml org search type data status active id 44063";"CODE"
"the sha256 hash of the file is also provided to ensure the integrity of the";"CODE"
"downloaded file";"CODE"
"we load the parquet file with polars for feature engineering polars";"CODE"
"automatically caches common subexpressions which are reused in multiple";"IRRE"
"expressions like pl col count shift 1 below see";"-"
"https docs pola rs user guide lazy optimizations for more information";"CODE"
"next we take a look at the statistical summary of the dataset";"IRRE"
"so that we can better understand the data that we are working with";"CODE"
"let us look at the count of the seasons fall spring summer";"CODE"
"and winter present in the dataset to confirm they are balanced";"CODE"
"generating polars engineered lagged features";"TASK"
"let s consider the problem of predicting the demand at the";"CODE"
"next hour given past demands since the demand is a continuous";"CODE"
"variable one could intuitively use any regression model however we do";"CODE"
"not have the usual x train y train dataset instead we just have";"IRRE"
"the y train demand data sequentially organized by time";"-"
"watch out however the first lines have undefined values because their own";"IRRE"
"past is unknown this depends on how much lag we used";"CODE"
"we can now separate the lagged features in a matrix x and the target variable";"TASK"
"the counts to predict in an array of the same first dimension y";"CODE"
"naive evaluation of the next hour bike demand regression";"-"
"let s randomly split our tabularized dataset to train a gradient";"IRRE"
"boosting regression tree gbrt model and evaluate it using mean";"-"
"absolute percentage error mape if our model is aimed at forecasting";"CODE"
"i e predicting future data from past data we should not use training";"CODE"
"data that are ulterior to the testing data in time series machine learning";"IRRE"
"the i i d independent and identically distributed assumption does not";"CODE"
"hold true as the data points are not independent and have a temporal";"CODE"
"relationship";"-"
"taking a look at the performance of the model";"CODE"
"proper next hour forecasting evaluation";"CODE"
"let s use a proper evaluation splitting strategies that takes into account";"CODE"
"the temporal structure of the dataset to evaluate our model s ability to";"IRRE"
"predict data points in the future to avoid cheating by reading values from";"CODE"
"the lagged features in the training set";"TASK"
"n splits 3 to keep the notebook fast enough on common laptops";"TASK"
"gap 48 2 days data gap between train and test";"IRRE"
"max train size 10000 keep train sets of comparable sizes";"IRRE"
"test size 3000 for 2 or 3 digits of precision in scores";"IRRE"
"training the model and evaluating its performance based on mape";"CODE"
"the generalization error measured via a shuffled trained test split";"IRRE"
"is too optimistic the generalization via a time based split is likely to";"-"
"be more representative of the true performance of the regression model";"CODE"
"let s assess this variability of our error evaluation with proper";"CODE"
"cross validation";"-"
"the variability across splits is quite large in a real life setting";"IRRE"
"it would be advised to use more splits to better assess the variability";"CODE"
"let s report the mean cv scores and their standard deviation from now on";"CODE"
"we can compute several combinations of evaluation metrics and loss functions";"CODE"
"which are reported a bit below";"CODE"
"modeling predictive uncertainty via quantile regression";"META"
"instead of modeling the expected value of the distribution of";"IRRE"
"math y x like the least squares and poisson losses do one could try to";"CODE"
"estimate quantiles of the conditional distribution";"META"
"math y x x i is expected to be a random variable for a given data point";"CODE"
"math x i because we expect that the number of rentals cannot be 100";"-"
"accurately predicted from the features it can be influenced by other";"TASK"
"variables not properly captured by the existing lagged features for";"CODE"
"instance whether or not it will rain in the next hour cannot be fully";"CODE"
"anticipated from the past hours bike rental data this is what we";"CODE"
"call aleatoric uncertainty";"IRRE"
"quantile regression makes it possible to give a finer description of that";"CODE"
"distribution without making strong assumptions on its shape";"META"
"let us take a look at the losses that minimise each metric";"CODE"
"even if the score distributions overlap due to the variance in the dataset";"CODE"
"it is true that the average rmse is lower when loss squared error whereas";"-"
"the average mape is lower when loss absolute error as expected that is";"-"
"also the case for the mean pinball loss with the quantiles 5 and 95 the score";"CODE"
"corresponding to the 50 quantile loss is overlapping with the score obtained";"-"
"by minimizing other loss functions which is also the case for the mae";"CODE"
"a qualitative look at the predictions";"-"
"we can now visualize the performance of the model with regards";"CODE"
"to the 5th percentile median and the 95th percentile";"-"
"we can now take a look at the predictions made by the regression models";"-"
"here it s interesting to notice that the blue area between the 5 and 95";"CODE"
"percentile estimators has a width that varies with the time of the day";"CODE"
"at night the blue band is much narrower the pair of models is quite";"-"
"certain that there will be a small number of bike rentals and furthermore";"CODE"
"these seem correct in the sense that the actual demand stays in that blue";"CODE"
"band";"-"
"during the day the blue band is much wider the uncertainty grows probably";"META"
"because of the variability of the weather that can have a very large impact";"CODE"
"especially on week ends";"CODE"
"we can also see that during week days the commute pattern is still visible in";"TASK"
"the 5 and 95 estimations";"-"
"finally it is expected that 10 of the time the actual demand does not lie";"CODE"
"between the 5 and 95 percentile estimates on this test span the actual";"IRRE"
"demand seems to be higher especially during the rush hours it might reveal that";"-"
"our 95 percentile estimator underestimates the demand peaks this could be be";"CODE"
"quantitatively confirmed by computing empirical coverage numbers as done in";"CODE"
"the ref calibration of confidence intervals calibration section";"CODE"
"looking at the performance of non linear regression models vs";"CODE"
"the best models";"-"
"conclusion";"-"
"through this example we explored time series forecasting using lagged";"CODE"
"features we compared a naive regression using the standardized";"TASK"
"class sklearn model selection train test split with a proper time";"CODE"
"series evaluation strategy using";"-"
"class sklearn model selection timeseriessplit we observed that the";"CODE"
"model trained using class sklearn model selection train test split";"CODE"
"having a default value of shuffle set to true produced an overly";"IRRE"
"optimistic mean average percentage error mape the results";"IRRE"
"produced from the time based split better represent the performance";"CODE"
"of our time series regression model we also analyzed the predictive uncertainty";"META"
"of our model via quantile regression predictions based on the 5th and";"CODE"
"95th percentile using loss quantile provide us with a quantitative estimate";"-"
"of the uncertainty of the forecasts made by our time series regression model";"CODE"
"uncertainty estimation can also be performed";"CODE"
"using mapie https mapie readthedocs io en latest index html";"CODE"
"that provides an implementation based on recent work on conformal prediction";"TASK"
"methods and estimates both aleatoric and epistemic uncertainty at the same time";"META"
"furthermore functionalities provided";"CODE"
"by sktime https www sktime net en latest users html";"IRRE"
"can be used to extend scikit learn estimators by making use of recursive time";"OUTD"
"series forecasting that enables dynamic predictions of future values";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the 20 newsgroups dataset and vectorize it we use a few heuristics";"CODE"
"to filter out useless terms early on the posts are stripped of headers";"CODE"
"footers and quoted replies and common english words words occurring in";"IRRE"
"only one document or in at least 95 of the documents are removed";"CODE"
"use tf idf features for nmf";"TASK"
"use tf raw term count features for lda";"TASK"
"fit the nmf model";"-"
"fit the nmf model";"-"
"fit the minibatchnmf model";"-"
"fit the minibatchnmf model";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"download data if not already on disk";"CODE"
"loading the redirect files";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"return number if token 0 isdigit else token for token in tokens";"CODE"
"exclude comp os ms windows misc";"CODE"
"note the following is identical to x rows np newaxis";"TASK"
"cols sum but much faster in scipy 0 16";"META"
"categories";"-"
"words";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"we generate the sample data using the";"-"
"func sklearn datasets make checkerboard function each pixel within";"IRRE"
"shape 300 300 represents with its color a value from a uniform";"CODE"
"distribution the noise is added from a normal distribution where the value";"META"
"chosen for noise is the standard deviation";"CODE"
"as you can see the data is distributed over 12 cluster cells and is";"META"
"relatively well distinguishable";"-"
"we shuffle the data and the goal is to reconstruct it afterwards using";"CODE"
"class sklearn cluster spectralbiclustering";"IRRE"
"creating lists of shuffled row and column indices";"-"
"we redefine the shuffled data and plot it we observe that we lost the";"CODE"
"structure of original data matrix";"CODE"
"fitting spectralbiclustering";"-"
"we fit the model and compare the obtained clusters with the ground truth note";"TASK"
"that when creating the model we specify the same number of clusters that we";"-"
"used to create the dataset n clusters 4 3 which will contribute to";"IRRE"
"obtain a good result";"IRRE"
"compute the similarity of two sets of biclusters";"IRRE"
"the score is between 0 and 1 where 1 corresponds to a perfect matching it";"-"
"shows the quality of the biclustering";"-"
"plotting results";"IRRE"
"now we rearrange the data based on the row and column labels assigned by the";"CODE"
"class sklearn cluster spectralbiclustering model in ascending order and";"IRRE"
"plot again the row labels range from 0 to 3 while the column labels";"CODE"
"range from 0 to 2 representing a total of 4 clusters per row and 3 clusters";"CODE"
"per column";"-"
"reordering first the rows and then the columns";"-"
"as a last step we want to demonstrate the relationships between the row";"-"
"and column labels assigned by the model therefore we create a grid with";"IRRE"
"func numpy outer which takes the sorted row labels and column labels";"-"
"and adds 1 to each to ensure that the labels start from 1 instead of 0 for";"CODE"
"better visualization";"-"
"the outer product of the row and column label vectors shows a representation";"-"
"of the checkerboard structure where different combinations of row and column";"CODE"
"labels are represented by different shades of blue";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"shuffle clusters";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate synthetic dataset";"IRRE"
"generate 3 blobs with 2 classes where the second blob contains";"IRRE"
"half positive samples and half negative samples probability in this";"CODE"
"blob is therefore 0 5";"CODE"
"split train test for calibration";"CODE"
"gaussian naive bayes";"-"
"with no calibration";"-"
"clf fit x train y train gaussiannb itself does not support sample weights";"CODE"
"with isotonic calibration";"CODE"
"with sigmoid calibration";"-"
"plot data and the predicted probabilities";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset";"IRRE"
"we will use a synthetic binary classification dataset with 100 000 samples";"IRRE"
"and 20 features of the 20 features only 2 are informative 10 are";"TASK"
"redundant random combinations of the informative features and the";"IRRE"
"remaining 8 are uninformative random numbers of the 100 000 samples 1 000";"CODE"
"will be used for model fitting and the rest for testing";"CODE"
"calibration curves";"-"
"gaussian naive bayes";"-"
"first we will compare";"IRRE"
"class sklearn linear model logisticregression used as baseline";"IRRE"
"since very often properly regularized logistic regression is well";"-"
"calibrated by default thanks to the use of the log loss";"CODE"
"uncalibrated class sklearn naive bayes gaussiannb";"IRRE"
"class sklearn naive bayes gaussiannb with isotonic and sigmoid";"CODE"
"calibration see ref user guide calibration";"-"
"calibration curves for all 4 conditions are plotted below with the average";"CODE"
"predicted probability for each bin on the x axis and the fraction of positive";"CODE"
"classes in each bin on the y axis";"IRRE"
"add histogram";"TASK"
"uncalibrated class sklearn naive bayes gaussiannb is poorly calibrated";"IRRE"
"because of";"-"
"the redundant features which violate the assumption of feature independence";"TASK"
"and result in an overly confident classifier which is indicated by the";"IRRE"
"typical transposed sigmoid curve calibration of the probabilities of";"-"
"class sklearn naive bayes gaussiannb with ref isotonic can fix";"IRRE"
"this issue as can be seen from the nearly diagonal calibration curve";"CODE"
"ref sigmoid regression sigmoid regressor also improves calibration";"-"
"slightly";"-"
"albeit not as strongly as the non parametric isotonic regression this can be";"CODE"
"attributed to the fact that we have plenty of calibration data such that the";"META"
"greater flexibility of the non parametric model can be exploited";"-"
"below we will make a quantitative analysis considering several classification";"IRRE"
"metrics ref brier score loss ref log loss";"-"
"ref precision recall f1 score precision recall f measure metrics and";"IRRE"
"ref roc auc roc metrics";"-"
"notice that although calibration improves the ref brier score loss a";"-"
"metric composed";"-"
"of calibration term and refinement term and ref log loss it does not";"CODE"
"significantly alter the prediction accuracy measures precision recall and";"IRRE"
"f1 score";"-"
"this is because calibration should not significantly change prediction";"CODE"
"probabilities at the location of the decision threshold at x 0 5 on the";"-"
"graph calibration should however make the predicted probabilities more";"-"
"accurate and thus more useful for making allocation decisions under";"CODE"
"uncertainty";"META"
"further roc auc should not change at all because calibration is a";"-"
"monotonic transformation indeed no rank metrics are affected by";"CODE"
"calibration";"-"
"linear support vector classifier";"IRRE"
"next we will compare";"IRRE"
"class sklearn linear model logisticregression baseline";"IRRE"
"uncalibrated class sklearn svm linearsvc since svc does not output";"IRRE"
"probabilities by default we naively scale the output of the";"CODE"
"term decision function into 0 1 by applying min max scaling";"CODE"
"class sklearn svm linearsvc with isotonic and sigmoid";"CODE"
"calibration see ref user guide calibration";"-"
"data";"-"
"below we generate a classification dataset with 2000 samples 2 features";"IRRE"
"and 3 target classes we then split the data as follows";"IRRE"
"train 600 samples for training the classifier";"CODE"
"valid 400 samples for calibrating predicted probabilities";"CODE"
"test 1000 samples";"IRRE"
"note that we also create x train valid and y train valid which consists";"OUTD"
"of both the train and valid subsets this is used when we only want to train";"IRRE"
"the classifier but not calibrate the predicted probabilities";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"fitting and calibration";"-"
"first we will train a class sklearn ensemble randomforestclassifier";"IRRE"
"with 25 base estimators trees on the concatenated train and validation";"-"
"data 1000 samples this is the uncalibrated classifier";"CODE"
"to train the calibrated classifier we start with the same";"CODE"
"class sklearn ensemble randomforestclassifier but train it using only";"IRRE"
"the train data subset 600 samples then calibrate with method sigmoid";"IRRE"
"using the valid data subset 400 samples in a 2 stage process";"IRRE"
"compare probabilities";"IRRE"
"below we plot a 2 simplex with arrows showing the change in predicted";"-"
"probabilities of the test samples";"IRRE"
"plot arrows";"-"
"plot perfect predictions at each vertex";"-"
"plot boundaries of unit simplex";"-"
"annotate points 6 points around the simplex and mid point inside simplex";"CODE"
"add grid";"TASK"
"in the figure above each vertex of the simplex represents";"CODE"
"a perfectly predicted class e g 1 0 0 the mid point";"CODE"
"inside the simplex represents predicting the three classes with equal";"IRRE"
"probability i e 1 3 1 3 1 3 each arrow starts at the";"CODE"
"uncalibrated probabilities and end with the arrow head at the calibrated";"CODE"
"probability the color of the arrow represents the true class of that test";"IRRE"
"sample";"-"
"the uncalibrated classifier is overly confident in its predictions and";"IRRE"
"incurs a large ref log loss log loss the calibrated classifier incurs";"IRRE"
"a lower ref log loss log loss due to two factors first notice in the";"CODE"
"figure above that the arrows generally point away from the edges of the";"CODE"
"simplex where the probability of one class is 0 second a large proportion";"IRRE"
"of the arrows point towards the true class e g green arrows samples where";"CODE"
"the true class is green generally point towards the green vertex this";"CODE"
"results in fewer over confident 0 predicted probabilities and at the same";"IRRE"
"time an increase in the predicted probabilities of the correct class";"CODE"
"thus the calibrated classifier produces more accurate predicted probabilities";"IRRE"
"that incur a lower ref log loss log loss";"-"
"we can show this objectively by comparing the ref log loss log loss of";"CODE"
"the uncalibrated and calibrated classifiers on the predictions of the 1000";"IRRE"
"test samples note that an alternative would have been to increase the number";"IRRE"
"of base estimators trees of the";"-"
"class sklearn ensemble randomforestclassifier which would have resulted";"IRRE"
"in a similar decrease in ref log loss log loss";"-"
"we can also assess calibration with the brier score for probabilistics predictions";"CODE"
"lower is better possible range is 0 2";"-"
"according to the brier score the calibrated classifier is not better than";"IRRE"
"the original model";"-"
"finally we generate a grid of possible uncalibrated probabilities over";"CODE"
"the 2 simplex compute the corresponding calibrated probabilities and";"-"
"plot arrows for each the arrows are colored according the highest";"CODE"
"uncalibrated probability this illustrates the learned calibration map";"IRRE"
"generate grid of probability values";"IRRE"
"use the three class wise calibrators to compute calibrated probabilities";"IRRE"
"re normalize the calibrated predictions to make sure they stay inside the";"-"
"simplex this same renormalization step is performed internally by the";"CODE"
"predict method of calibratedclassifiercv on multiclass problems";"CODE"
"plot changes in predicted probabilities induced by the calibrators";"-"
"plot the boundaries of the unit simplex";"-"
"one can observe that on average the calibrator is pushing highly confident";"-"
"predictions away from the boundaries of the simplex while simultaneously";"CODE"
"moving uncertain predictions towards one of three modes one for each class";"CODE"
"we can also observe that the mapping is not symmetric furthermore some";"-"
"arrows seem to cross class assignment boundaries which is not necessarily";"IRRE"
"what one would expect from a calibration map as it means that some predicted";"CODE"
"classes will change after calibration";"IRRE"
"all in all the one vs rest multiclass calibration strategy implemented in";"TASK"
"calibratedclassifiercv should not be trusted blindly";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset";"IRRE"
"we will use a synthetic binary classification dataset with 100 000 samples";"IRRE"
"and 20 features of the 20 features only 2 are informative 2 are";"TASK"
"redundant random combinations of the informative features and the";"IRRE"
"remaining 16 are uninformative random numbers";"CODE"
"of the 100 000 samples 100 will be used for model fitting and the remaining";"CODE"
"for testing note that this split is quite unusual the goal is to obtain";"CODE"
"stable calibration curve estimates for models that are potentially prone to";"CODE"
"overfitting in practice one should rather use cross validation with more";"-"
"balanced splits but this would make the code of this example more complicated";"CODE"
"to follow";"-"
"train samples 100 samples used for training the models";"CODE"
"calibration curves";"-"
"below we train each of the four models with the small training dataset then";"IRRE"
"plot calibration curves also known as reliability diagrams using";"-"
"predicted probabilities of the test dataset calibration curves are created";"IRRE"
"by binning predicted probabilities then plotting the mean predicted";"-"
"probability in each bin against the observed frequency fraction of";"-"
"positives below the calibration curve we plot a histogram showing";"-"
"the distribution of the predicted probabilities or more specifically";"IRRE"
"the number of samples in each predicted probability bin";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data 2d projection of the iris dataset";"IRRE"
"x iris data 0 2 we only take the first two features for visualization";"TASK"
"probabilistic classifiers";"IRRE"
"we will plot the decision boundaries of several classifiers that have a";"IRRE"
"predict proba method this will allow us to visualize the uncertainty of";"CODE"
"the classifier in regions where it is not certain of its prediction";"IRRE"
"plotting the decision boundaries";"-"
"for each classifier we plot the per class probabilities on the first three";"CODE"
"columns and the probabilities of the most likely class on the last column";"IRRE"
"ensure legend not cut off";"CODE"
"plot the probability estimate provided by the classifier";"IRRE"
"plot data predicted to belong to given class";"IRRE"
"add column that shows all classes by plotting class with max predict proba";"IRRE"
"colorbar for single class plots";"IRRE"
"colorbars for max probability class column";"IRRE"
"quantitative evaluation";"-"
"analysis";"-"
"the two logistic regression models fitted on the original features display";"TASK"
"linear decision boundaries as expected for this particular problem this";"CODE"
"does not seem to be detrimental as both models are competitive with the";"CODE"
"non linear models when quantitatively evaluated on the test set we can";"IRRE"
"observe that the amount of regularization influences the model confidence";"-"
"lighter colors for the strongly regularized model with a lower value of c";"IRRE"
"regularization also impacts the orientation of decision boundary leading to";"-"
"slightly different roc auc";"-"
"the log loss on the other hand evaluates both sharpness and calibration and";"-"
"as a result strongly favors the weakly regularized logistic regression model";"IRRE"
"probably because the strongly regularized model is under confident this";"IRRE"
"could be confirmed by looking at the calibration curve using";"-"
"class sklearn calibration calibrationdisplay";"IRRE"
"the logistic regression model with rbf features has a blobby decision";"TASK"
"boundary that is non linear in the original feature space and is quite";"TASK"
"similar to the decision boundary of the gaussian process classifier which is";"IRRE"
"configured to use an rbf kernel";"-"
"the logistic regression model fitted on binned features with interactions has";"CODE"
"a decision boundary that is non linear in the original feature space and is";"TASK"
"quite similar to the decision boundary of the gradient boosting classifier";"IRRE"
"both models favor axis aligned decisions when extrapolating to unseen region";"-"
"of the feature space";"TASK"
"the logistic regression model fitted on spline features with interactions";"CODE"
"has a similar axis aligned extrapolation behavior but a smoother decision";"META"
"boundary in the dense region of the feature space than the two previous";"TASK"
"models";"-"
"to conclude it is interesting to observe that feature engineering for";"CODE"
"logistic regression models can be used to mimic some of the inductive bias of";"OUTD"
"various non linear models however for this particular dataset using the";"CODE"
"raw features is enough to train a competitive model this would not";"TASK"
"necessarily the case for other datasets";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"iterate over datasets";"IRRE"
"preprocess dataset split into training and test part";"IRRE"
"just plot the dataset first";"IRRE"
"cm bright listedcolormap ff0000 0000ff";"-"
"plot the training points";"CODE"
"plot the testing points";"IRRE"
"iterate over classifiers";"IRRE"
"plot the training points";"CODE"
"plot the testing points";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"standard scientific python imports";"CODE"
"import datasets classifiers and performance metrics";"CODE"
"digits dataset";"IRRE"
"the digits dataset consists of 8x8";"IRRE"
"pixel images of digits the images attribute of the dataset stores";"IRRE"
"8x8 arrays of grayscale values for each image we will use these arrays to";"IRRE"
"visualize the first 4 images the target attribute of the dataset stores";"IRRE"
"the digit each image represents and this is included in the title of the 4";"CODE"
"plots below";"-"
"note if we were working from image files e g png files we would load";"CODE"
"them using func matplotlib pyplot imread";"CODE"
"classification";"IRRE"
"to apply a classifier on this data we need to flatten the images turning";"CODE"
"each 2 d array of grayscale values from shape 8 8 into shape";"IRRE"
"64 subsequently the entire dataset will be of shape";"IRRE"
"n samples n features where n samples is the number of images and";"TASK"
"n features is the total number of pixels in each image";"TASK"
"we can then split the data into train and test subsets and fit a support";"IRRE"
"vector classifier on the train samples the fitted classifier can";"IRRE"
"subsequently be used to predict the value of the digit for the samples";"IRRE"
"in the test subset";"IRRE"
"flatten the images";"-"
"create a classifier a support vector classifier";"IRRE"
"split data into 50 train and 50 test subsets";"IRRE"
"learn the digits on the train subset";"IRRE"
"predict the value of the digit on the test subset";"IRRE"
"below we visualize the first 4 test samples and show their predicted";"IRRE"
"digit value in the title";"IRRE"
"func sklearn metrics classification report builds a text report showing";"IRRE"
"the main classification metrics";"CODE"
"we can also plot a ref confusion matrix confusion matrix of the";"-"
"true digit values and the predicted digit values";"IRRE"
"if the results from evaluating a classifier are stored in the form of a";"CODE"
"ref confusion matrix confusion matrix and not in terms of y true and";"CODE"
"y pred one can still build a func sklearn metrics classification report";"CODE"
"as follows";"-"
"the ground truth and predicted lists";"-"
"for each cell in the confusion matrix add the corresponding ground truths";"CODE"
"and predictions to the lists";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"n train 20 samples for training";"CODE"
"n test 200 samples for testing";"IRRE"
"n averages 50 how often to repeat classification";"IRRE"
"n features max 75 maximum number of features";"TASK"
"tep 4 step size for the calculation";"CODE"
"add non discriminative features";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation";"-"
"first we define a function to generate synthetic data it creates two blobs centered";"IRRE"
"at 0 0 and 1 1 each blob is assigned a specific class the dispersion of";"IRRE"
"the blob is controlled by the parameters cov class 1 and cov class 2 that are the";"IRRE"
"covariance matrices used when generating the samples from the gaussian distributions";"CODE"
"we generate three datasets in the first dataset the two classes share the same";"IRRE"
"covariance matrix and this covariance matrix has the specificity of being spherical";"CODE"
"isotropic the second dataset is similar to the first one but does not enforce the";"CODE"
"covariance to be spherical finally the third dataset has a non spherical covariance";"CODE"
"matrix for each class";"CODE"
"plotting functions";"CODE"
"the code below is used to plot several pieces of information from the estimators used";"CODE"
"i e class sklearn discriminant analysis lineardiscriminantanalysis lda and";"IRRE"
"class sklearn discriminant analysis quadraticdiscriminantanalysis qda the";"IRRE"
"displayed information includes";"CODE"
"the decision boundary based on the probability estimate of the estimator";"CODE"
"a scatter plot with circles representing the well classified samples";"IRRE"
"a scatter plot with crosses representing the misclassified samples";"IRRE"
"the mean of each class estimated by the estimator marked with a star";"IRRE"
"the estimated covariance represented by an ellipse at 2 standard deviations from the";"CODE"
"mean";"-"
"angle 180 angle np pi convert to degrees";"-"
"filled gaussian at 2 standard deviation";"-"
"comparison of lda and qda";"-"
"we compare the two estimators lda and qda on all three datasets";"IRRE"
"the first important thing to notice is that lda and qda are equivalent for the";"CODE"
"first and second datasets indeed the major difference is that lda assumes";"IRRE"
"that the covariance matrix of each class is equal while qda estimates a";"CODE"
"covariance matrix per class since in these cases the data generative process";"CODE"
"has the same covariance matrix for both classes qda estimates two covariance";"CODE"
"matrices that are almost equal and therefore equivalent to the covariance";"CODE"
"matrix estimated by lda";"-"
"in the first dataset the covariance matrix used to generate the dataset is";"CODE"
"spherical which results in a discriminant boundary that aligns with the";"IRRE"
"perpendicular bisector between the two means this is no longer the case for";"CODE"
"the second dataset the discriminant boundary only passes through the middle";"IRRE"
"of the two means";"-"
"finally in the third dataset we observe the real difference between lda and";"CODE"
"qda qda fits two covariance matrices and provides a non linear discriminant";"CODE"
"boundary whereas lda underfits since it assumes that both classes share a";"IRRE"
"single covariance matrix";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"defining the list of metrics to evaluate";"CODE"
"clustering algorithms are fundamentally unsupervised learning methods";"CODE"
"however since we assign class labels for the synthetic clusters in this";"CODE"
"example it is possible to use evaluation metrics that leverage this";"CODE"
"supervised ground truth information to quantify the quality of the resulting";"CODE"
"clusters examples of such metrics are the following";"-"
"v measure the harmonic mean of completeness and homogeneity";"CODE"
"rand index which measures how frequently pairs of data points are grouped";"IRRE"
"consistently according to the result of the clustering algorithm and the";"IRRE"
"ground truth class assignment";"IRRE"
"adjusted rand index ari a chance adjusted rand index such that a random";"IRRE"
"cluster assignment has an ari of 0 0 in expectation";"IRRE"
"mutual information mi is an information theoretic measure that quantifies";"CODE"
"how dependent are the two labelings note that the maximum value of mi for";"CODE"
"perfect labelings depends on the number of clusters and samples";"CODE"
"normalized mutual information nmi a mutual information defined between 0";"CODE"
"no mutual information in the limit of large number of data points and 1";"CODE"
"perfectly matching label assignments up to a permutation of the labels";"IRRE"
"it is not adjusted for chance then the number of clustered data points is";"CODE"
"not large enough the expected values of mi or nmi for random labelings can";"IRRE"
"be significantly non zero";"-"
"adjusted mutual information ami a chance adjusted mutual information";"CODE"
"similarly to ari random cluster assignment has an ami of 0 0 in";"IRRE"
"expectation";"-"
"for more information see the ref clustering evaluation module";"CODE"
"first experiment fixed ground truth labels and growing number of clusters";"-"
"we first define a function that creates uniformly distributed random labeling";"CODE"
"another function will use the random labels function to create a fixed set";"IRRE"
"of ground truth labels labels a distributed in n classes and then score";"IRRE"
"several sets of randomly predicted labels labels b to assess the";"IRRE"
"variability of a given metric at a given n clusters";"CODE"
"in this first example we set the number of classes true number of clusters to";"CODE"
"n classes 10 the number of clusters varies over the values provided by";"IRRE"
"n clusters range";"-"
"the rand index saturates for n clusters n classes other non adjusted";"IRRE"
"measures such as the v measure show a linear dependency between the number of";"CODE"
"clusters and the number of samples";"-"
"adjusted for chance measure such as ari and ami display some random";"IRRE"
"variations centered around a mean score of 0 0 independently of the number of";"CODE"
"samples and clusters";"-"
"second experiment varying number of classes and clusters";"CODE"
"in this section we define a similar function that uses several metrics to";"CODE"
"score 2 uniformly distributed random labelings in this case the number of";"CODE"
"classes and assigned number of clusters are matched for each possible value in";"IRRE"
"n clusters range";"-"
"in this case we use n samples 100 to show the effect of having a number of";"CODE"
"clusters similar or equal to the number of samples";"-"
"we observe similar results as for the first experiment adjusted for chance";"CODE"
"metrics stay constantly near zero while other metrics tend to get larger with";"CODE"
"finer grained labelings the mean v measure of random labeling increases";"IRRE"
"significantly as the number of clusters is closer to the total number of";"CODE"
"samples used to compute the measure furthermore raw mutual information is";"OUTD"
"unbounded from above and its scale depends on the dimensions of the clustering";"CODE"
"problem and the cardinality of the ground truth classes this is why the";"CODE"
"curve goes off the chart";"CODE"
"only adjusted measures can hence be safely used as a consensus index to";"-"
"evaluate the average stability of clustering algorithms for a given value of k";"IRRE"
"on various overlapping sub samples of the dataset";"IRRE"
"non adjusted clustering evaluation metric can therefore be misleading as they";"META"
"output large values for fine grained labelings one could be lead to think";"IRRE"
"that the labeling has captured meaningful groups while they can be totally";"CODE"
"random in particular such non adjusted metrics should not be used to compare";"IRRE"
"the results of different clustering algorithms that output a different number";"IRRE"
"of clusters";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"compute affinity propagation";"IRRE"
"plot result";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate waveform data";"CODE"
"make the noise sparse";"IRRE"
"colors f7bd01 377eb8 f781bf";"-"
"plot the ground truth labelling";"-"
"plot the distances";"-"
"plot clustering results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"create linkage matrix and then plot the dendrogram";"IRRE"
"create the counts of samples under each node";"IRRE"
"current count 1 leaf node";"-"
"plot the corresponding dendrogram";"CODE"
"setting distance threshold 0 ensures we compute the full tree";"IRRE"
"plot the top three levels of the dendrogram";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate centers for the blobs so that it forms a 10 x 10 grid";"CODE"
"generate blobs to do a comparison between minibatchkmeans and birch";"TASK"
"use all colors that matplotlib provides by default";"CODE"
"compute clustering with birch with and without the final clustering step";"CODE"
"and plot";"-"
"plot result";"IRRE"
"compute clustering with minibatchkmeans";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"number of cluster centers for kmeans and bisectingkmeans";"CODE"
"algorithms to compare";"IRRE"
"make subplots for each variant";"CODE"
"hide x labels and tick labels for top plots and y ticks for right plots";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate datasets we choose the size big enough to see the scalability";"IRRE"
"of the algorithms but not too big to avoid too long running times";"CODE"
"anisotropicly distributed data";"META"
"blobs with varied variances";"CODE"
"set up cluster parameters";"IRRE"
"update parameters with dataset specific values";"IRRE"
"normalize dataset for easier parameter selection";"IRRE"
"estimate bandwidth for mean shift";"CODE"
"connectivity matrix for structured ward";"CODE"
"make connectivity symmetric";"-"
"create cluster objects";"IRRE"
"catch warnings related to kneighbors graph";"CODE"
"377eb8";"-"
"ff7f00";"-"
"4daf4a";"-"
"f781bf";"-"
"a65628";"-"
"984ea3";"-"
"999999";"-"
"e41a1c";"-"
"dede00";"-"
"add black color for outliers if any";"TASK"
"colors np append colors 000000";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the coins as a numpy array";"CODE"
"resize it to 20 of the original size to speed up the processing";"-"
"applying a gaussian filter for smoothing prior to down scaling";"CODE"
"reduces aliasing artifacts";"-"
"convert the image into a graph with the value of the gradient on the";"IRRE"
"edges";"-"
"take a decreasing function of the gradient an exponential";"CODE"
"the smaller beta is the more independent the segmentation is of the";"CODE"
"actual image for beta 1 the segmentation is close to a voronoi";"CODE"
"the number of segmented regions to display needs to be chosen manually";"TASK"
"the current version of spectral clustering does not support determining";"META"
"the number of good quality clusters automatically";"IRRE"
"compute and visualize the resulting regions";"IRRE"
"computing a few extra eigenvectors may speed up the eigen solver";"-"
"the spectral clustering quality may also benefit from requesting";"CODE"
"extra regions for segmentation";"CODE"
"apply spectral clustering using the default eigen solver arpack";"CODE"
"any implemented solver can be used eigen solver arpack lobpcg or amg";"TASK"
"choosing eigen solver amg requires an extra package called pyamg";"IRRE"
"the quality of segmentation and the speed of calculations is mostly determined";"CODE"
"by the choice of the solver and the value of the tolerance eigen tol";"IRRE"
"todo varying eigen tol seems to have no effect for lobpcg and amg 21243";"CODE"
"to view individual segments as appear comment in plt pause 0 5";"-"
"todo after 21194 is merged and 21243 is fixed check which eigen solver";"TASK"
"is the best and set eigen solver arpack lobpcg or amg and eigen tol";"IRRE"
"explicitly in this example";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate data";"-"
"resize it to 20 of the original size to speed up the processing";"-"
"applying a gaussian filter for smoothing prior to down scaling";"CODE"
"reduces aliasing artifacts";"-"
"define structure of the data";"CODE"
"pixels are connected to their neighbors";"-"
"compute clustering";"-"
"n clusters 27 number of regions";"-"
"plot the results on an image";"IRRE"
"agglomerative clustering is able to segment each coin however we have had to";"CODE"
"use a n cluster larger than the number of coins because the segmentation";"IRRE"
"is finding a large in the background";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation";"-"
"we use class sklearn datasets make blobs to create 3 synthetic clusters";"IRRE"
"we can visualize the resulting data";"IRRE"
"compute dbscan";"-"
"one can access the labels assigned by class sklearn cluster dbscan using";"IRRE"
"the labels attribute noisy samples are given the label math 1";"META"
"number of clusters in labels ignoring noise if present";"-"
"clustering algorithms are fundamentally unsupervised learning methods";"CODE"
"however since class sklearn datasets make blobs gives access to the true";"IRRE"
"labels of the synthetic clusters it is possible to use evaluation metrics";"CODE"
"that leverage this supervised ground truth information to quantify the";"CODE"
"quality of the resulting clusters examples of such metrics are the";"IRRE"
"homogeneity completeness v measure rand index adjusted rand index and";"IRRE"
"adjusted mutual information ami";"CODE"
"if the ground truth labels are not known evaluation can only be performed";"CODE"
"using the model results itself in that case the silhouette coefficient comes";"CODE"
"in handy";"-"
"for more information see the";"CODE"
"ref sphx glr auto examples cluster plot adjusted for chance measures py";"CODE"
"example or the ref clustering evaluation module";"CODE"
"plot results";"IRRE"
"core samples large dots and non core samples small dots are color coded";"CODE"
"according to the assigned cluster samples tagged as noise are represented in";"IRRE"
"black";"-"
"black used for noise";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the data";"CODE"
"learn the dictionary of images";"-"
"the online learning part cycle over the whole dataset 6 times";"IRRE"
"plot the results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"visualize the clustering";"-"
"2d embedding of the digits dataset";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"original image";"-"
"we start by loading the raccoon face image from scipy we will additionally check";"CODE"
"a couple of information regarding the image such as the shape and data type used";"CODE"
"to store the image";"-"
"thus the image is a 2d array of 768 pixels in height and 1024 pixels in width each";"-"
"value is a 8 bit unsigned integer which means that the image is encoded using 8";"IRRE"
"bits per pixel the total memory usage of the image is 786 kilobytes 1 byte equals";"-"
"8 bits";"-"
"using 8 bit unsigned integer means that the image is encoded using 256 different";"CODE"
"shades of gray at most we can check the distribution of these values";"IRRE"
"compression via vector quantization";"-"
"the idea behind compression via vector quantization is to reduce the number of";"-"
"gray levels to represent an image for instance we can use 8 values instead";"IRRE"
"of 256 values therefore it means that we could efficiently use 3 bits instead";"IRRE"
"of 8 bits to encode a single pixel and therefore reduce the memory usage by a";"CODE"
"factor of approximately 2 5 we will later discuss about this memory usage";"CODE"
"encoding strategy";"-"
"we previously stated that we should save 8 times less memory let s verify it";"CODE"
"it is quite surprising to see that our compressed image is taking x8 more";"-"
"memory than the original image this is indeed the opposite of what we";"CODE"
"expected the reason is mainly due to the type of data used to encode the";"OUTD"
"image";"-"
"indeed the output of the class sklearn preprocessing kbinsdiscretizer is";"IRRE"
"an array of 64 bit float it means that it takes x8 more memory however we";"CODE"
"use this 64 bit float representation to encode 8 values indeed we will save";"IRRE"
"memory only if we cast the compressed image into an array of 3 bits integers we";"CODE"
"could use the method numpy ndarray astype however a 3 bits integer";"IRRE"
"representation does not exist and to encode the 8 values we would need to use";"TASK"
"the 8 bit unsigned integer representation as well";"CODE"
"in practice observing a memory gain would require the original image to be in";"CODE"
"a 64 bit float representation";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"set parameters";"IRRE"
"ize 40 image size";"-"
"generate data";"-"
"for x in x smooth data";"CODE"
"add noise";"TASK"
"compute the coefs of a bayesian ridge with gridsearch";"-"
"cv kfold 2 cross validation generator for model selection";"CODE"
"ward agglomeration followed by bayesianridge";"-"
"select the optimal number of parcels with grid search";"CODE"
"clf fit x y set the best parameters";"IRRE"
"anova univariate feature selection followed by bayesianridge";"CODE"
"f regression mem cache feature selection f regression caching function";"CODE"
"select the optimal percentage of features with grid search";"TASK"
"clf fit x y set the best parameters";"IRRE"
"inverse the transformation to plot the results on an image";"IRRE"
"attempt to remove the temporary cachedir but don t worry if it fails";"META"
"coding utf 8";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"black removed and is used for noise instead";"OUTD"
"the probability of a point belonging to its labeled cluster determines";"CODE"
"the size of its marker";"-"
"black used for noise";"CODE"
"generate sample data";"-"
"one of the greatest advantages of hdbscan over dbscan is its out of the box";"IRRE"
"robustness it s especially remarkable on heterogeneous mixtures of data";"-"
"like dbscan it can model arbitrary shapes and distributions however unlike";"META"
"dbscan it does not require specification of an arbitrary and sensitive";"CODE"
"eps hyperparameter";"IRRE"
"for example below we generate a dataset from a mixture of three bi dimensional";"CODE"
"and isotropic gaussian distributions";"META"
"scale invariance";"CODE"
"it s worth remembering that while dbscan provides a default value for eps";"CODE"
"parameter it hardly has a proper default value and must be tuned for the";"IRRE"
"specific dataset at use";"IRRE"
"as a simple demonstration consider the clustering for a eps value tuned";"IRRE"
"for one dataset and clustering obtained with the same value but applied to";"IRRE"
"rescaled versions of the dataset";"IRRE"
"indeed in order to maintain the same results we would have to scale eps by";"CODE"
"the same factor";"-"
"while standardizing data e g using";"CODE"
"class sklearn preprocessing standardscaler helps mitigate this problem";"CODE"
"great care must be taken to select the appropriate value for eps";"CODE"
"hdbscan is much more robust in this sense hdbscan can be seen as";"CODE"
"clustering over all possible values of eps and extracting the best";"IRRE"
"clusters from all possible clusters see ref user guide hdbscan";"CODE"
"one immediate advantage is that hdbscan is scale invariant";"CODE"
"multi scale clustering";"-"
"hdbscan is much more than scale invariant though it is capable of";"CODE"
"multi scale clustering which accounts for clusters with varying density";"CODE"
"traditional dbscan assumes that any potential clusters are homogeneous in";"-"
"density hdbscan is free from such constraints to demonstrate this we";"CODE"
"consider the following dataset";"IRRE"
"this dataset is more difficult for dbscan due to the varying densities and";"CODE"
"spatial separation";"-"
"if eps is too large then we risk falsely clustering the two dense";"-"
"clusters as one since their mutual reachability will extend";"CODE"
"clusters";"-"
"if eps is too small then we risk fragmenting the sparser clusters";"IRRE"
"into many false clusters";"CODE"
"not to mention this requires manually tuning choices of eps until we";"CODE"
"find a tradeoff that we are comfortable with";"CODE"
"to properly cluster the two dense clusters we would need a smaller value of";"IRRE"
"epsilon however at eps 0 3 we are already fragmenting the sparse clusters";"IRRE"
"which would only become more severe as we decrease epsilon indeed it seems";"CODE"
"that dbscan is incapable of simultaneously separating the two dense clusters";"-"
"while preventing the sparse clusters from fragmenting let s compare with";"CODE"
"hdbscan";"-"
"hdbscan is able to adapt to the multi scale structure of the dataset without";"CODE"
"requiring parameter tuning while any sufficiently interesting dataset will";"IRRE"
"require tuning this case demonstrates that hdbscan can yield qualitatively";"CODE"
"better classes of clusterings without users intervention which are";"CODE"
"inaccessible via dbscan";"-"
"hyperparameter robustness";"IRRE"
"ultimately tuning will be an important step in any real world application so";"CODE"
"let s take a look at some of the most important hyperparameters for hdbscan";"CODE"
"while hdbscan is free from the eps parameter of dbscan it does still have";"CODE"
"some hyperparameters like min cluster size and min samples which tune its";"IRRE"
"results regarding density we will however see that hdbscan is relatively robust";"IRRE"
"to various real world examples thanks to those parameters whose clear meaning";"IRRE"
"helps tuning them";"-"
"min cluster size";"-"
"min cluster size is the minimum number of samples in a group for that";"CODE"
"group to be considered a cluster";"-"
"clusters smaller than the ones of this size will be left as noise";"CODE"
"the default value is 5 this parameter is generally tuned to";"IRRE"
"larger values as needed smaller values will likely to lead to results with";"IRRE"
"fewer points labeled as noise however values which too small will lead to";"IRRE"
"false sub clusters being picked up and preferred larger values tend to be";"IRRE"
"more robust with respect to noisy datasets e g high variance clusters with";"IRRE"
"significant overlap";"-"
"min samples";"-"
"min samples is the number of samples in a neighborhood for a point to";"CODE"
"be considered as a core point including the point itself";"CODE"
"min samples defaults to min cluster size";"CODE"
"similarly to min cluster size larger values for min samples increase";"IRRE"
"the model s robustness to noise but risks ignoring or discarding";"META"
"potentially valid but small clusters";"META"
"min samples better be tuned after finding a good value for min cluster size";"IRRE"
"dbscan clustering";"-"
"during fit hdbscan builds a single linkage tree which encodes the";"-"
"clustering of all points across all values of class cluster dbscan s";"IRRE"
"eps parameter";"IRRE"
"we can thus plot and evaluate these clusterings efficiently without fully";"-"
"recomputing intermediate values such as core distances mutual reachability";"IRRE"
"and the minimum spanning tree all we need to do is specify the cut distance";"TASK"
"equivalent to eps we want to cluster with";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate some training data from clustering";"CODE"
"train a clustering algorithm on the training data and get the cluster labels";"-"
"generate new samples and plot them along with the original dataset";"IRRE"
"declare the inductive learning model that it will be used to";"OUTD"
"predict cluster membership for unknown instances";"CODE"
"plotting decision regions";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation";"-"
"the function func sklearn datasets make blobs generates isotropic";"IRRE"
"spherical gaussian blobs to obtain anisotropic elliptical gaussian blobs";"-"
"one has to define a linear transformation";"CODE"
"x aniso np dot x transformation anisotropic blobs";"CODE"
"unequal variance";"CODE"
"unevenly sized blobs";"-"
"we can visualize the resulting data";"IRRE"
"fit models and plot results";"IRRE"
"the previously generated data is now used to show how";"OUTD"
"class sklearn cluster kmeans behaves in the following scenarios";"CODE"
"non optimal number of clusters in a real setting there is no uniquely";"IRRE"
"defined true number of clusters an appropriate number of clusters has";"CODE"
"to be decided from data based criteria and knowledge of the intended goal";"CODE"
"anisotropically distributed blobs k means consists of minimizing sample s";"IRRE"
"euclidean distances to the centroid of the cluster they are assigned to as";"CODE"
"a consequence k means is more appropriate for clusters that are isotropic";"CODE"
"and normally distributed i e spherical gaussians";"META"
"unequal variance k means is equivalent to taking the maximum likelihood";"CODE"
"estimator for a mixture of k gaussian distributions with the same";"META"
"variances but with possibly different means";"META"
"unevenly sized blobs there is no theoretical result about k means that";"IRRE"
"states that it requires similar cluster sizes to perform well yet";"CODE"
"minimizing euclidean distances does mean that the more sparse and";"CODE"
"high dimensional the problem is the higher is the need to run the algorithm";"TASK"
"with different centroid seeds to ensure a global minimal inertia";"-"
"possible solutions";"-"
"for an example on how to find a correct number of blobs see";"CODE"
"ref sphx glr auto examples cluster plot kmeans silhouette analysis py";"-"
"in this case it suffices to set n clusters 3";"CODE"
"to deal with unevenly sized blobs one can increase the number of random";"IRRE"
"initializations in this case we set n init 10 to avoid finding a";"CODE"
"sub optimal local minimum for more details see ref kmeans sparse high dim";"IRRE"
"as anisotropic and unequal variances are real limitations of the k means";"CODE"
"algorithm here we propose instead the use of";"-"
"class sklearn mixture gaussianmixture which also assumes gaussian";"IRRE"
"clusters but does not impose any constraints on their variances notice that";"CODE"
"one still has to find the correct number of blobs see";"TASK"
"ref sphx glr auto examples mixture plot gmm selection py";"CODE"
"for an example on how other clustering methods deal with anisotropic or";"CODE"
"unequal variance blobs see the example";"CODE"
"ref sphx glr auto examples cluster plot cluster comparison py";"-"
"final remarks";"CODE"
"in high dimensional spaces euclidean distances tend to become inflated";"CODE"
"not shown in this example running a dimensionality reduction algorithm";"CODE"
"prior to k means clustering can alleviate this problem and speed up the";"CODE"
"computations see the example";"-"
"ref sphx glr auto examples text plot document clustering py";"CODE"
"in the case where clusters are known to be isotropic have similar variance";"CODE"
"and are not too sparse the k means algorithm is quite effective and is one of";"IRRE"
"the fastest clustering algorithms available this advantage is lost if one has";"CODE"
"to restart it several times to avoid convergence to a local minimum";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the dataset";"IRRE"
"we will start by loading the digits dataset this dataset contains";"IRRE"
"handwritten digits from 0 to 9 in the context of clustering one would like";"CODE"
"to group images such that the handwritten digits on the image are the same";"CODE"
"print f digits n digits samples n samples features n features";"CODE"
"define our evaluation benchmark";"CODE"
"we will first our evaluation benchmark during this benchmark we intend to";"CODE"
"compare different initialization methods for kmeans our benchmark will";"IRRE"
"create a pipeline which will scale the data using a";"CODE"
"class sklearn preprocessing standardscaler";"IRRE"
"train and time the pipeline fitting";"CODE"
"measure the performance of the clustering obtained via different metrics";"CODE"
"define the metrics which require only the true labels and estimator";"CODE"
"labels";"-"
"the silhouette score requires the full dataset";"IRRE"
"show the results";"IRRE"
"run the benchmark";"CODE"
"we will compare three approaches";"IRRE"
"an initialization using k means this method is stochastic and we will";"IRRE"
"run the initialization 4 times";"IRRE"
"a random initialization this method is stochastic as well and we will run";"IRRE"
"the initialization 4 times";"IRRE"
"an initialization based on a class sklearn decomposition pca";"IRRE"
"projection indeed we will use the components of the";"IRRE"
"class sklearn decomposition pca to initialize kmeans this method is";"IRRE"
"deterministic and a single initialization suffice";"IRRE"
"visualize the results on pca reduced data";"IRRE"
"class sklearn decomposition pca allows to project the data from the";"CODE"
"original 64 dimensional space into a lower dimensional space subsequently";"CODE"
"we can use class sklearn decomposition pca to project into a";"CODE"
"2 dimensional space and plot the data and the clusters in this new space";"CODE"
"step size of the mesh decrease to increase the quality of the vq";"IRRE"
"h 0 02 point in the mesh x min x max x y min y max";"CODE"
"plot the decision boundary for that we will assign a color to each";"IRRE"
"obtain labels for each point in mesh use last trained model";"CODE"
"put the result into a color plot";"IRRE"
"plot the centroids as a white x";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"calculate seeds from k means";"CODE"
"plot init seeds along side sample data";"IRRE"
"colors 4eacc5 ff9c34 4e9a06 m";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generating the sample data from make blobs";"CODE"
"this particular setting has one distinct cluster and 3 clusters placed close";"IRRE"
"together";"-"
"for reproducibility";"CODE"
"create a subplot with 1 row and 2 columns";"IRRE"
"the 1st subplot is the silhouette plot";"-"
"the silhouette coefficient can range from 1 1 but in this example all";"CODE"
"lie within 0 1 1";"-"
"the n clusters 1 10 is for inserting blank space between silhouette";"CODE"
"plots of individual clusters to demarcate them clearly";"-"
"initialize the clusterer with n clusters value and a random generator";"IRRE"
"seed of 10 for reproducibility";"CODE"
"the silhouette score gives the average value for all the samples";"IRRE"
"this gives a perspective into the density and separation of the formed";"CODE"
"clusters";"-"
"compute the silhouette scores for each sample";"CODE"
"aggregate the silhouette scores for samples belonging to";"CODE"
"cluster i and sort them";"-"
"label the silhouette plots with their cluster numbers at the middle";"-"
"compute the new y lower for next plot";"CODE"
"y lower y upper 10 10 for the 0 samples";"CODE"
"the vertical line for average silhouette score of all the values";"IRRE"
"ax1 set yticks clear the yaxis labels ticks";"IRRE"
"2nd plot showing the actual clusters formed";"CODE"
"labeling the clusters";"-"
"draw white circles at cluster centers";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"number of run with randomly generated dataset for each strategy so as";"CODE"
"to be able to compute an estimate of the standard deviation";"CODE"
"k means models can do several random inits so as to be able to trade";"IRRE"
"cpu time for convergence robustness";"CODE"
"datasets generation parameters";"IRRE"
"part 1 quantitative evaluation of various init methods";"IRRE"
"part 2 qualitative visual inspection of the convergence";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate datasets we choose the size big enough to see the scalability";"IRRE"
"of the algorithms but not too big to avoid too long running times";"CODE"
"anisotropicly distributed data";"META"
"blobs with varied variances";"CODE"
"run the clustering and plot";"CODE"
"set up cluster parameters";"IRRE"
"update parameters with dataset specific values";"IRRE"
"normalize dataset for easier parameter selection";"IRRE"
"create cluster objects";"IRRE"
"catch warnings related to kneighbors graph";"CODE"
"377eb8";"-"
"ff7f00";"-"
"4daf4a";"-"
"f781bf";"-"
"a65628";"-"
"984ea3";"-"
"999999";"-"
"e41a1c";"-"
"dede00";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"compute clustering with meanshift";"-"
"the following bandwidth can be automatically detected using";"IRRE"
"plot result";"IRRE"
"colors dede00 377eb8 f781bf";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate the data";"-"
"we start by generating the blobs of data to be clustered";"-"
"compute clustering with kmeans";"-"
"compute clustering with minibatchkmeans";"-"
"establishing parity between clusters";"-"
"we want to have the same color for the same cluster from both the";"CODE"
"minibatchkmeans and the kmeans algorithm let s pair the cluster centers per";"CODE"
"closest one";"CODE"
"plotting the results";"IRRE"
"colors 4eacc5 ff9c34 4e9a06";"-"
"kmeans";"-"
"minibatchkmeans";"-"
"initialize the different array to all false";"IRRE"
"ax plot x identical 0 x identical 1 w markerfacecolor bbbbbb marker";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"run the fit";"CODE"
"reachability plot";"-"
"optics";"-"
"dbscan at 0 5";"-"
"dbscan at 2";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate the data";"-"
"plotting four circles";"-"
"we use a mask that limits to the foreground the problem that we are";"CODE"
"interested in here is not separating the objects from the background";"CODE"
"but separating them one from the other";"META"
"convert the image into a graph with the value of the gradient on the";"IRRE"
"edges";"-"
"take a decreasing function of the gradient resulting in a segmentation";"IRRE"
"that is close to a voronoi partition";"IRRE"
"here we perform spectral clustering using the arpack solver since amg is";"CODE"
"numerically unstable on this example we then plot the results";"IRRE"
"plotting two circles";"-"
"here we repeat the above process but only consider the first two circles";"META"
"we generated note that this results in a cleaner separation between the";"TASK"
"circles as the region sizes are easier to balance in this case";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate the swiss roll dataset";"IRRE"
"x1 1 0 5 make the roll thinner";"-"
"compute clustering without connectivity constraints";"CODE"
"plot unstructured clustering result";"IRRE"
"compute clustering with connectivity constraints";"CODE"
"plot structured clustering result";"IRRE"
"generate 2d spiral dataset";"IRRE"
"capture local connectivity using a graph";"-"
"larger number of neighbors will give more homogeneous clusters to";"-"
"the cost of computation time a very large number of neighbors gives";"-"
"more evenly distributed cluster sizes but may not impose the local";"META"
"manifold structure of the data";"CODE"
"plot clustering with and without structure";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"20 newsgroups dataset";"IRRE"
"we will use the ref 20 newsgroups dataset 20newsgroups dataset which";"IRRE"
"comprises posts from newsgroups on 20 topics this dataset is split";"CODE"
"into train and test subsets based on messages posted before and after";"CODE"
"a specific date we will only use posts from 2 categories to speed up running";"CODE"
"time";"-"
"each feature comprises meta information about that post such as the subject";"TASK"
"and the body of the news post";"CODE"
"creating transformers";"CODE"
"first we would like a transformer that extracts the subject and";"CODE"
"body of each post since this is a stateless transformation does not";"CODE"
"require state information from training data we can define a function that";"CODE"
"performs the data transformation then use";"CODE"
"class sklearn preprocessing functiontransformer to create a scikit learn";"CODE"
"transformer";"CODE"
"construct object dtype array with two columns";"CODE"
"first column subject and second column body";"-"
"temporary variable stores n n";"IRRE"
"store body text in second column";"-"
"save text after subject in first column";"CODE"
"we will also create a transformer that extracts the";"IRRE"
"length of the text and the number of sentences";"-"
"classification pipeline";"CODE"
"the pipeline below extracts the subject and body from each post using";"CODE"
"subjectbodyextractor producing a n samples 2 array this array is";"CODE"
"then used to compute standard bag of words features for the subject and body";"TASK"
"as well as text length and number of sentences on the body using";"-"
"columntransformer we combine them with weights then train a";"CODE"
"classifier on the combined set of features";"IRRE"
"extract subject body";"-"
"use columntransformer to combine the subject and body features";"TASK"
"bag of words for subject col 0";"CODE"
"bag of words with decomposition for body col 1";"CODE"
"pipeline for pulling text stats from post s body";"CODE"
"returns a list of dicts";"IRRE"
"list of dicts feature matrix";"TASK"
"weight above columntransformer features";"TASK"
"use a svc classifier on the combined features";"TASK"
"finally we fit our pipeline on the training data and use it to predict";"CODE"
"topics for x test performance metrics of our pipeline are then printed";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load data from https www openml org d 40945";"CODE"
"alternatively x and y can be obtained directly from the frame attribute";"META"
"x titanic frame drop survived axis 1";"-"
"y titanic frame survived";"-"
"use columntransformer by selecting column by names";"CODE"
"we will train our classifier with the following features";"TASK"
"numeric features";"TASK"
"age float";"CODE"
"fare float";"CODE"
"categorical features";"TASK"
"embarked categories encoded as strings c s q";"IRRE"
"sex categories encoded as strings female male";"CODE"
"pclass ordinal integers 1 2 3";"CODE"
"we create the preprocessing pipelines for both numeric and categorical data";"CODE"
"note that pclass could either be treated as a categorical or numeric";"TASK"
"feature";"TASK"
"append classifier to preprocessing pipeline";"CODE"
"now we have a full prediction pipeline";"CODE"
"html representation of pipeline display diagram";"CODE"
"when the pipeline is printed out in a jupyter notebook an html";"CODE"
"representation of the estimator is displayed";"-"
"use columntransformer by selecting column by data types";"CODE"
"when dealing with a cleaned dataset the preprocessing can be automatic by";"TASK"
"using the data types of the column to decide whether to treat a column as a";"-"
"numerical or categorical feature";"TASK"
"func sklearn compose make column selector gives this possibility";"CODE"
"first let s only select a subset of columns to simplify our";"CODE"
"example";"-"
"then we introspect the information regarding each column data type";"CODE"
"we can observe that the embarked and sex columns were tagged as";"IRRE"
"category columns when loading the data with fetch openml therefore we";"CODE"
"can use this information to dispatch the categorical columns to the";"CODE"
"categorical transformer and the remaining columns to the";"CODE"
"numerical transformer";"CODE"
"note in practice you will have to handle yourself the column data type";"TASK"
"if you want some columns to be considered as category you will have to";"-"
"convert them into categorical columns if you are using pandas you can";"CODE"
"refer to their documentation regarding categorical data";"CODE"
"https pandas pydata org pandas docs stable user guide categorical html";"CODE"
"the resulting score is not exactly the same as the one from the previous";"TASK"
"pipeline because the dtype based selector treats the pclass column as";"CODE"
"a numeric feature instead of a categorical feature as previously";"TASK"
"using the prediction pipeline in a grid search";"CODE"
"grid search can also be performed on the different preprocessing steps";"CODE"
"defined in the columntransformer object together with the classifier s";"CODE"
"hyperparameters as part of the pipeline";"IRRE"
"we will search for both the imputer strategy of the numeric preprocessing";"CODE"
"and the regularization parameter of the logistic regression using";"IRRE"
"class sklearn model selection randomizedsearchcv this";"CODE"
"hyperparameter search randomly selects a fixed number of parameter";"IRRE"
"settings configured by n iter alternatively one can use";"IRRE"
"class sklearn model selection gridsearchcv but the cartesian product of";"CODE"
"the parameter space will be evaluated";"IRRE"
"calling fit triggers the cross validated search for the best";"IRRE"
"hyper parameters combination";"IRRE"
"the internal cross validation scores obtained by those parameters is";"IRRE"
"we can also introspect the top grid search results as a pandas dataframe";"IRRE"
"the best hyper parameters have be used to re fit a final model on the full";"IRRE"
"training set we can evaluate that final model on held out test data that was";"CODE"
"not used for hyperparameter tuning";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"illustration of pipeline and gridsearchcv";"CODE"
"the reduce dim stage is populated by the param grid";"-"
"scores are in the order of param grid iteration which is alphabetical";"CODE"
"select score for best c";"CODE"
"create a dataframe to ease plotting";"IRRE"
"caching transformers within a pipeline";"CODE"
"it is sometimes worthwhile storing the state of a specific transformer";"CODE"
"since it could be used again using a pipeline in gridsearchcv triggers";"CODE"
"such situations therefore we use the argument memory to enable caching";"IRRE"
"warning";"-"
"note that this example is however only an illustration since for this";"CODE"
"specific case fitting pca is not necessarily slower than loading the";"CODE"
"cache hence use the memory constructor parameter when the fitting";"IRRE"
"of a transformer is costly";"CODE"
"create a temporary folder to store the transformers of the pipeline";"CODE"
"this time a cached pipeline will be used within the grid search";"CODE"
"delete the temporary cache before exiting";"CODE"
"the pca fitting is only computed at the evaluation of the first";"-"
"configuration of the c parameter of the linearsvc classifier the";"IRRE"
"other configurations of c will trigger the loading of the cached pca";"CODE"
"estimator data leading to save processing time therefore the use of";"CODE"
"caching the pipeline using memory is highly beneficial when fitting";"CODE"
"a transformer is costly";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"define a pipeline to search for the best combination of pca truncation";"CODE"
"and classifier regularization";"IRRE"
"define a standard scaler to normalize inputs";"CODE"
"set the tolerance to a large value to make the example faster";"IRRE"
"parameters of pipelines can be set using separated parameter names";"IRRE"
"plot the pca spectrum";"-"
"for each number of components find the best classifier results";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"this dataset is way too high dimensional better do pca";"CODE"
"maybe some original features were good too";"TASK"
"build estimator from pca and univariate selection";"CODE"
"use combined features to transform dataset";"TASK"
"do grid search over k n components and c";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"synthetic example";"-"
"a synthetic random regression dataset is generated the targets y are";"IRRE"
"modified by";"-"
"1 translating all targets such that all entries are";"-"
"non negative by adding the absolute value of the lowest y and";"TASK"
"2 applying an exponential function to obtain non linear";"CODE"
"targets which cannot be fitted using a simple linear model";"-"
"therefore a logarithmic np log1p and an exponential function";"CODE"
"np expm1 will be used to transform the targets before training a linear";"CODE"
"regression model and using it for prediction";"CODE"
"below we plot the probability density functions of the target";"CODE"
"before and after applying the logarithmic functions";"CODE"
"at first a linear model will be applied on the original targets due to the";"CODE"
"non linearity the model trained will not be precise during";"-"
"prediction subsequently a logarithmic function is used to linearize the";"OUTD"
"targets allowing better prediction even with a similar linear model as";"-"
"reported by the median absolute error medae";"-"
"add the score in the legend of each axis";"CODE"
"real world data set";"IRRE"
"in a similar manner the ames housing data set is used to show the impact";"IRRE"
"of transforming the targets before learning a model in this example the";"CODE"
"target to be predicted is the selling price of each house";"-"
"keep only numeric columns";"-"
"remove columns with nan or inf values";"IRRE"
"let the price be in k";"CODE"
"a class sklearn preprocessing quantiletransformer is used to normalize";"CODE"
"the target distribution before applying a";"META"
"class sklearn linear model ridgecv model";"IRRE"
"the effect of the transformer is weaker than on the synthetic data however";"CODE"
"the transformation results in an increase in math r 2 and large decrease";"IRRE"
"of the medae the residual plot predicted target true target vs predicted";"-"
"target without target transformation takes on a curved reverse smile";"CODE"
"shape due to residual values that vary depending on the value of predicted";"IRRE"
"target with target transformation the shape is more linear indicating";"CODE"
"better model fit";"-"
"plot the actual vs predicted values";"IRRE"
"add the score in the legend of each axis";"CODE"
"plot the residuals vs the predicted values";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"color samples";"-"
"compute the likelihood on test data";"CODE"
"spanning a range of possible shrinkage coefficient values";"IRRE"
"under the ground truth model which we would not have access to in real";"-"
"settings";"IRRE"
"compare different approaches to setting the regularization parameter";"IRRE"
"here we compare 3 approaches";"IRRE"
"setting the parameter by cross validating the likelihood on three folds";"IRRE"
"according to a grid of potential shrinkage parameters";"IRRE"
"a close formula proposed by ledoit and wolf to compute";"CODE"
"the asymptotically optimal regularization parameter minimizing a mse";"IRRE"
"criterion yielding the class sklearn covariance ledoitwolf";"CODE"
"covariance estimate";"CODE"
"an improvement of the ledoit wolf shrinkage the";"TASK"
"class sklearn covariance oas proposed by chen et al 1 its";"CODE"
"convergence is significantly better under the assumption that the data";"CODE"
"are gaussian in particular for small samples";"CODE"
"gridsearch for an optimal shrinkage coefficient";"CODE"
"ledoit wolf optimal shrinkage coefficient estimate";"IRRE"
"oas coefficient estimate";"IRRE"
"plot results";"IRRE"
"to quantify estimation error we plot the likelihood of unseen data for";"CODE"
"different values of the shrinkage parameter we also show the choices by";"IRRE"
"cross validation or with the ledoitwolf and oas estimates";"CODE"
"range shrinkage curve";"-"
"adjust view";"-"
"lw likelihood";"-"
"oas likelihood";"-"
"best cv estimator likelihood";"-"
"note";"TASK"
"the maximum likelihood estimate corresponds to no shrinkage";"-"
"and thus performs poorly the ledoit wolf estimate performs really well";"CODE"
"as it is close to the optimal and is not computationally costly in this";"CODE"
"example the oas estimate is a bit further away interestingly both";"CODE"
"approaches outperform cross validation which is significantly most";"CODE"
"computationally costly";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"simulation covariance matrix ar 1 process";"CODE"
"plot mse";"-"
"plot shrinkage coefficient";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate data";"-"
"first we generate a dataset of 125 samples and 2 features both features";"TASK"
"are gaussian distributed with mean of 0 but feature 1 has a standard";"META"
"deviation equal to 2 and feature 2 has a standard deviation equal to 1 next";"TASK"
"25 samples are replaced with gaussian outlier samples where feature 1 has";"TASK"
"a standard deviation equal to 1 and feature 2 has a standard deviation equal";"TASK"
"to 7";"-"
"for consistent results";"IRRE"
"generate gaussian data of shape 125 2";"-"
"add some outliers";"TASK"
"comparison of results";"IRRE"
"below we fit mcd and mle based covariance estimators to our data and print";"CODE"
"the estimated covariance matrices note that the estimated variance of";"CODE"
"feature 2 is much higher with the mle based estimator 7 5 than";"TASK"
"that of the mcd robust estimator 1 2 this shows that the mcd based";"IRRE"
"robust estimator is much more resistant to the outlier samples which were";"IRRE"
"designed to have a much larger variance in feature 2";"TASK"
"fit a mcd robust estimator to data";"IRRE"
"fit a mle estimator to data";"-"
"to better visualize the difference we plot contours of the";"-"
"mahalanobis distances calculated by both methods notice that the robust";"-"
"mcd based mahalanobis distances fit the inlier black points much better";"CODE"
"whereas the mle based distances are more influenced by the outlier";"-"
"red points";"CODE"
"plot data set";"IRRE"
"create meshgrid of feature 1 and feature 2 values";"IRRE"
"calculate the mle based mahalanobis distances of the meshgrid";"-"
"calculate the mcd based mahalanobis distances";"-"
"add legend";"TASK"
"finally we highlight the ability of mcd based mahalanobis distances to";"CODE"
"distinguish outliers we take the cubic root of the mahalanobis distances";"-"
"yielding approximately normal distributions as suggested by wilson and";"META"
"hilferty 2 then plot the values of inlier and outlier samples with";"IRRE"
"boxplots the distribution of outlier samples is more separated from the";"META"
"distribution of inlier samples for robust mcd based mahalanobis distances";"META"
"calculate cubic root of mle mahalanobis distances for samples";"CODE"
"plot boxplots";"-"
"plot individual samples";"-"
"calculate cubic root of mcd mahalanobis distances for samples";"CODE"
"plot boxplots";"-"
"plot individual samples";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"example settings";"IRRE"
"definition of arrays to store results";"IRRE"
"computation";"-"
"generate data";"-"
"add some outliers";"TASK"
"fit a minimum covariance determinant mcd robust estimator to data";"IRRE"
"compare raw robust estimates with the true location and covariance";"IRRE"
"compare estimators learned from the full data set with true";"IRRE"
"parameters";"IRRE"
"compare with an empirical covariance learned from a pure data set";"IRRE"
"i e perfect mcd";"-"
"display results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate the data";"-"
"estimate the covariance";"CODE"
"plot the results";"IRRE"
"plot the covariances";"CODE"
"plot the precisions";"-"
"plot the model selection metric";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset based latent variables model";"IRRE"
"2 latents vars";"CODE"
"canonical symmetric pls";"-"
"transform data";"CODE"
"scatter plot of scores";"-"
"on diagonal plot x vs y scores on each components";"-"
"off diagonal plot components 1 vs 2 for x and y";"CODE"
"pls regression with multivariate response a k a pls2";"CODE"
"each yj 1 x1 2 x2 noize";"-"
"compare pls2 coef with b";"IRRE"
"pls regression with univariate response a k a pls1";"CODE"
"note that the number of components exceeds 1 the dimension of y";"TASK"
"cca pls mode b with symmetric deflation";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the data";"-"
"we start by creating a simple dataset with two features before we even dive";"CODE"
"into pcr and pls we fit a pca estimator to display the two principal";"IRRE"
"components of this dataset i e the two directions that explain the most";"CODE"
"variance in the data";"CODE"
"comp comp var scale component by its variance explanation power";"CODE"
"for the purpose of this example we now define the target y such that it is";"CODE"
"strongly correlated with a direction that has a small variance to this end";"CODE"
"we will project x onto the second component and add some noise to it";"TASK"
"projection on one component and predictive power";"-"
"we now create two regressors pcr and pls and for our illustration purposes";"IRRE"
"we set the number of components to 1 before feeding the data to the pca step";"IRRE"
"of pcr we first standardize it as recommended by good practice the pls";"CODE"
"estimator has built in scaling capabilities";"-"
"for both models we plot the projected data onto the first component against";"CODE"
"the target in both cases this projected data is what the regressors will";"CODE"
"use as training data";"-"
"pca pcr named steps pca retrieve the pca step of the pipeline";"CODE"
"as expected the unsupervised pca transformation of pcr has dropped the";"CODE"
"second component i e the direction with the lowest variance despite";"CODE"
"it being the most predictive direction this is because pca is a completely";"CODE"
"unsupervised transformation and results in the projected data having a low";"CODE"
"predictive power on the target";"-"
"on the other hand the pls regressor manages to capture the effect of the";"-"
"direction with the lowest variance thanks to its use of target information";"CODE"
"during the transformation it can recognize that this direction is actually";"CODE"
"the most predictive we note that the first pls component is negatively";"TASK"
"correlated with the target which comes from the fact that the signs of";"CODE"
"eigenvectors are arbitrary";"-"
"we also print the r squared scores of both estimators which further confirms";"CODE"
"that pls is a better alternative than pcr in this case a negative r squared";"CODE"
"indicates that pcr performs worse than a regressor that would simply predict";"IRRE"
"the mean of the target";"-"
"as a final remark we note that pcr with 2 components performs as well as";"CODE"
"pls this is because in this case pcr was able to leverage the second";"CODE"
"component which has the most preditive power on the target";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"ff3333 red";"-"
"0198e1 blue";"-"
"bf5fff purple";"-"
"fcd116 yellow";"-"
"ff7216 orange";"CODE"
"4dbd33 green";"-"
"87421f brown";"-"
"use same random seed for multiple calls to make multilabel classification to";"IRRE"
"ensure same distributions";"META"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset preparation";"IRRE"
"loading and preprocessing the olivetti faces dataset";"IRRE"
"display progress logs on stdout";"CODE"
"global centering focus on one feature centering all samples";"TASK"
"local centering focus on one sample centering all features";"TASK"
"define a base function to plot the gallery of faces";"CODE"
"let s take a look at our data gray color indicates negative values";"IRRE"
"white indicates positive values";"IRRE"
"decomposition";"-"
"initialise different estimators for decomposition and fit each";"IRRE"
"of them on all images and plot some results each estimator extracts";"IRRE"
"6 components as vectors math h in mathbb r 4096";"-"
"we just displayed these vectors in human friendly visualisation as 64x64 pixel images";"CODE"
"read more in the ref user guide decompositions";"CODE"
"eigenfaces pca using randomized svd";"IRRE"
"linear dimensionality reduction using singular value decomposition svd of the data";"IRRE"
"to project it to a lower dimensional space";"-"
"note";"TASK"
"the eigenfaces estimator via the py mod sklearn decomposition pca";"-"
"also provides a scalar noise variance the mean of pixelwise variance";"CODE"
"that cannot be displayed as an image";"-"
"non negative components nmf";"-"
"estimate non negative original data as production of two non negative matrices";"-"
"nmf estimator fit faces original non negative dataset";"IRRE"
"independent components fastica";"CODE"
"independent component analysis separates a multivariate vectors into additive";"CODE"
"subcomponents that are maximally independent";"CODE"
"sparse components minibatchsparsepca";"IRRE"
"mini batch sparse pca class sklearn decomposition minibatchsparsepca";"IRRE"
"extracts the set of sparse components that best reconstruct the data this";"CODE"
"variant is faster but less accurate than the similar";"META"
"class sklearn decomposition sparsepca";"IRRE"
"dictionary learning";"-"
"by default class sklearn decomposition minibatchdictionarylearning";"CODE"
"divides the data into mini batches and optimizes in an online manner by";"CODE"
"cycling over the mini batches for the specified number of iterations";"CODE"
"cluster centers minibatchkmeans";"-"
"class sklearn cluster minibatchkmeans is computationally efficient and";"IRRE"
"implements on line learning with a";"TASK"
"meth sklearn cluster minibatchkmeans partial fit method that is";"-"
"why it could be beneficial to enhance some time consuming algorithms with";"-"
"class sklearn cluster minibatchkmeans";"IRRE"
"factor analysis components fa";"-"
"class sklearn decomposition factoranalysis is similar to";"IRRE"
"class sklearn decomposition pca but has the advantage of modelling the";"IRRE"
"variance in every direction of the input space independently heteroscedastic";"CODE"
"noise read more in the ref user guide fa";"CODE"
"pixelwise variance";"CODE"
"decomposition dictionary learning";"-"
"in the further section let s consider ref dictionarylearning more precisely";"CODE"
"dictionary learning is a problem that amounts to finding a sparse representation";"IRRE"
"of the input data as a combination of simple elements these simple elements form";"CODE"
"a dictionary it is possible to constrain the dictionary and or coding coefficients";"CODE"
"to be positive to match constraints that may be present in the data";"CODE"
"class sklearn decomposition minibatchdictionarylearning implements a";"TASK"
"faster but less accurate version of the dictionary learning algorithm that";"META"
"is better suited for large datasets read more in the ref user guide";"CODE"
"minibatchdictionarylearning";"-"
"plot the same samples from our dataset but with another colormap";"IRRE"
"red indicates negative values blue indicates positive values";"IRRE"
"and white represents zeros";"-"
"similar to the previous examples we change parameters and train";"IRRE"
"class sklearn decomposition minibatchdictionarylearning estimator on all";"IRRE"
"images generally the dictionary learning and sparse encoding decompose";"IRRE"
"input data into the dictionary and the coding coefficients matrices math x";"CODE"
"approx uv where math x x 1 x n math x in";"-"
"mathbb r m n dictionary math u in mathbb r m k coding";"-"
"coefficients math v in mathbb r k n";"-"
"also below are the results when the dictionary and coding";"IRRE"
"coefficients are positively constrained";"CODE"
"dictionary learning positive dictionary";"-"
"in the following section we enforce positivity when finding the dictionary";"CODE"
"dictionary learning positive code";"-"
"below we constrain the coding coefficients as a positive matrix";"CODE"
"dictionary learning positive dictionary code";"-"
"also below are the results if the dictionary values and coding";"IRRE"
"coefficients are positively constrained";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"1 np sin 2 time signal 1 sinusoidal signal";"CODE"
"2 np sign np sin 3 time signal 2 square signal";"CODE"
"3 signal sawtooth 2 np pi time signal 3 saw tooth signal";"-"
"s 0 2 np random normal size s shape add noise";"IRRE"
"s s std axis 0 standardize data";"CODE"
"mix data";"-"
"a np array 1 1 1 0 5 2 1 0 1 5 1 0 2 0 mixing matrix";"-"
"x np dot s a t generate observations";"CODE"
"fit ica and pca models";"-"
"compute ica";"-"
"s ica fit transform x reconstruct signals";"CODE"
"a ica mixing get estimated mixing matrix";"IRRE"
"we can prove that the ica model applies by reverting the unmixing";"META"
"for comparison compute pca";"CODE"
"h pca fit transform x reconstruct signals based on orthogonal components";"CODE"
"plot results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"mix data";"-"
"a np array 1 1 0 2 mixing matrix";"-"
"x np dot s a t generate observations";"CODE"
"s ica ica fit x transform x estimate the sources";"CODE"
"plot results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate distorted image";"-"
"convert from uint8 representation with values between 0 and 255 to";"IRRE"
"a floating point representation with values between 0 and 1";"IRRE"
"downsample for higher speed";"CODE"
"distort the right half of the image";"-"
"display the distorted image";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"projecting data pca vs kernelpca";"-"
"in this section we show the advantages of using a kernel when";"CODE"
"projecting data using a principal component analysis pca we create a";"IRRE"
"dataset made of two nested circles";"IRRE"
"let s have a quick first look at the generated dataset";"IRRE"
"train ax set ylabel feature 1";"TASK"
"train ax set xlabel feature 0";"TASK"
"test ax set xlabel feature 0";"IRRE"
"the samples from each class cannot be linearly separated there is no";"CODE"
"straight line that can split the samples of the inner set from the outer";"IRRE"
"set";"IRRE"
"now we will use pca with and without a kernel to see what is the effect of";"-"
"using such a kernel the kernel used here is a radial basis function rbf";"CODE"
"kernel";"-"
"orig data ax set ylabel feature 1";"TASK"
"orig data ax set xlabel feature 0";"TASK"
"pca proj ax set ylabel principal component 1";"IRRE"
"pca proj ax set xlabel principal component 0";"IRRE"
"kernel pca proj ax set ylabel principal component 1";"IRRE"
"kernel pca proj ax set xlabel principal component 0";"IRRE"
"we recall that pca transforms the data linearly intuitively it means that";"CODE"
"the coordinate system will be centered rescaled on each component";"CODE"
"with respected to its variance and finally be rotated";"CODE"
"the obtained data from this transformation is isotropic and can now be";"CODE"
"projected on its principal components";"CODE"
"thus looking at the projection made using pca i e the middle figure we";"-"
"see that there is no change regarding the scaling indeed the data being two";"-"
"concentric circles centered in zero the original data is already isotropic";"CODE"
"however we can see that the data have been rotated as a";"-"
"conclusion we see that such a projection would not help if define a linear";"CODE"
"classifier to distinguish samples from both classes";"CODE"
"using a kernel allows to make a non linear projection here by using an rbf";"-"
"kernel we expect that the projection will unfold the dataset while keeping";"IRRE"
"approximately preserving the relative distances of pairs of data points that";"CODE"
"are close to one another in the original space";"CODE"
"we observe such behaviour in the figure on the right the samples of a given";"CODE"
"class are closer to each other than the samples from the opposite class";"CODE"
"untangling both sample sets now we can use a linear classifier to separate";"IRRE"
"the samples from the two classes";"CODE"
"projecting into the original feature space";"TASK"
"one particularity to have in mind when using";"-"
"class sklearn decomposition kernelpca is related to the reconstruction";"CODE"
"i e the back projection in the original feature space with";"TASK"
"class sklearn decomposition pca the reconstruction will be exact if";"CODE"
"n components is the same than the number of original features";"TASK"
"this is the case in this example";"CODE"
"we can investigate if we get the original dataset when back projecting with";"IRRE"
"class sklearn decomposition kernelpca";"IRRE"
"orig data ax set ylabel feature 1";"TASK"
"orig data ax set xlabel feature 0";"TASK"
"pca back proj ax set xlabel feature 0";"TASK"
"kernel pca back proj ax set xlabel feature 0";"TASK"
"while we see a perfect reconstruction with";"CODE"
"class sklearn decomposition pca we observe a different result for";"IRRE"
"class sklearn decomposition kernelpca";"IRRE"
"indeed meth sklearn decomposition kernelpca inverse transform cannot";"IRRE"
"rely on an analytical back projection and thus an exact reconstruction";"CODE"
"instead a class sklearn kernel ridge kernelridge is internally trained";"CODE"
"to learn a mapping from the kernalized pca basis to the original feature";"TASK"
"space this method therefore comes with an approximation introducing small";"CODE"
"differences when back projecting in the original feature space";"TASK"
"to improve the reconstruction using";"CODE"
"meth sklearn decomposition kernelpca inverse transform one can tune";"IRRE"
"alpha in class sklearn decomposition kernelpca the regularization term";"IRRE"
"which controls the reliance on the training data during the training of";"-"
"the mapping";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"loading the iris dataset";"IRRE"
"the iris dataset is directly available as part of scikit learn it can be loaded";"IRRE"
"using the func sklearn datasets load iris function with the default parameters";"CODE"
"a class sklearn utils bunch object is returned containing the data the";"IRRE"
"target values the feature names and the target names";"IRRE"
"plot of pairs of features of the iris dataset";"TASK"
"let s first plot the pairs of features of the iris dataset";"TASK"
"rename classes using the iris target names";"IRRE"
"each data point on each scatter plot refers to one of the 150 iris flowers";"CODE"
"in the dataset with the color indicating their respective type";"IRRE"
"setosa versicolor and virginica";"IRRE"
"you can already see a pattern regarding the setosa type which is";"IRRE"
"easily identifiable based on its short and wide sepal only";"CODE"
"considering these two dimensions sepal width and length there s still";"TASK"
"overlap between the versicolor and virginica types";"IRRE"
"the diagonal of the plot shows the distribution of each feature we observe";"TASK"
"that the petal width and the petal length are the most discriminant features";"TASK"
"for the three types";"CODE"
"plot a pca representation";"-"
"let s apply a principal component analysis pca to the iris dataset";"IRRE"
"and then plot the irises across the first three principal components";"-"
"this will allow us to better differentiate among the three types";"CODE"
"unused but required import for doing 3d projections with matplotlib 3 2";"CODE"
"import mpl toolkits mplot3d noqa f401";"CODE"
"add a legend";"TASK"
"pca will create 3 new features that are a linear combination of the 4 original";"TASK"
"features in addition this transformation maximizes the variance with this";"CODE"
"transformation we can identify each species using only the first principal component";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"create the data";"IRRE"
"adding homoscedastic noise";"TASK"
"adding heteroscedastic noise";"TASK"
"fit the models";"-"
"n components np arange 0 n features 5 options for n components";"TASK"
"compare with other covariance estimators";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"percentage of variance explained for each components";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"ubsampling 3 subsampling factor";"-"
"compute a wavelet dictionary";"CODE"
"generate a signal";"-"
"list the different sparse coding methods in the following format";"CODE"
"title transform algorithm transform alpha";"CODE"
"transform n nozero coefs color";"CODE"
"do a wavelet approximation";"CODE"
"soft thresholding debiasing";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load iris data";"CODE"
"plot covariance of iris features";"TASK"
"run factor analysis with varimax rotation";"CODE"
"an example custom estimator implementing a simple classifier";"TASK"
"this code snippet defines a custom estimator class called customestimator";"CODE"
"that extends both the baseestimator and classifiermixin classes from";"CODE"
"scikit learn and showcases the usage of the sklearn is fitted method";"CODE"
"and the check is fitted utility function";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"custom attribute to track if the estimator is fitted";"META"
"perform prediction logic";"CODE"
"perform scoring logic";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"creating the dataset";"IRRE"
"the classification dataset is constructed by taking a ten dimensional standard";"CODE"
"normal distribution math x in math r 10 and defining three classes";"IRRE"
"separated by nested concentric ten dimensional spheres such that roughly equal";"CODE"
"numbers of samples are in each class quantiles of the math chi 2";"IRRE"
"distribution";"META"
"we split the dataset into 2 sets 70 percent of the samples are used for";"IRRE"
"training and the remaining 30 percent for testing";"CODE"
"training the adaboostclassifier";"IRRE"
"we train the class sklearn ensemble adaboostclassifier the estimator";"CODE"
"utilizes boosting to improve the classification accuracy boosting is a method";"IRRE"
"designed to train weak learners i e estimator that learn from their";"CODE"
"predecessor s mistakes";"-"
"here we define the weak learner as a";"CODE"
"class sklearn tree decisiontreeclassifier and set the maximum number of";"IRRE"
"leaves to 8 in a real setting this parameter should be tuned we set it to a";"IRRE"
"rather low value to limit the runtime of the example";"IRRE"
"the samme algorithm build into the";"CODE"
"class sklearn ensemble adaboostclassifier then uses the correct or";"IRRE"
"incorrect predictions made be the current weak learner to update the sample";"CODE"
"weights used for training the consecutive weak learners also the weight of";"CODE"
"the weak learner itself is calculated based on its accuracy in classifying the";"CODE"
"training examples the weight of the weak learner determines its influence on";"-"
"the final ensemble prediction";"CODE"
"analysis";"-"
"convergence of the adaboostclassifier";"IRRE"
"to demonstrate the effectiveness of boosting in improving accuracy we";"-"
"evaluate the misclassification error of the boosted trees in comparison to two";"IRRE"
"baseline scores the first baseline score is the misclassification error";"IRRE"
"obtained from a single weak learner i e";"CODE"
"class sklearn tree decisiontreeclassifier which serves as a reference";"CODE"
"point the second baseline score is obtained from the";"CODE"
"class sklearn dummy dummyclassifier which predicts the most prevalent";"IRRE"
"class in a dataset";"IRRE"
"after training the class sklearn tree decisiontreeclassifier model the";"IRRE"
"achieved error surpasses the expected value that would have been obtained by";"IRRE"
"guessing the most frequent class label as the";"IRRE"
"class sklearn dummy dummyclassifier does";"CODE"
"now we calculate the misclassification error i e 1 accuracy of the";"IRRE"
"additive model class sklearn tree decisiontreeclassifier at each";"IRRE"
"boosting iteration on the test set to assess its performance";"IRRE"
"we use meth sklearn ensemble adaboostclassifier staged predict that makes";"IRRE"
"as many iterations as the number of fitted estimator i e corresponding to";"-"
"n estimators at iteration n the predictions of adaboost only use the";"IRRE"
"n first weak learners we compare these predictions with the true";"IRRE"
"predictions y test and we therefore conclude on the benefit or not of adding a";"TASK"
"new weak learner into the chain";"CODE"
"we plot the misclassification error for the different stages";"CODE"
"the plot shows the missclassification error on the test set after each";"IRRE"
"boosting iteration we see that the error of the boosted trees converges to an";"-"
"error of around 0 3 after 50 iterations indicating a significantly higher";"-"
"accuracy compared to a single tree as illustrated by the dashed line in the";"CODE"
"plot";"-"
"the misclassification error jitters because the samme algorithm uses the";"IRRE"
"discrete outputs of the weak learners to train the boosted model";"IRRE"
"the convergence of class sklearn ensemble adaboostclassifier is mainly";"CODE"
"influenced by the learning rate i e learning rate the number of weak";"-"
"learners used n estimators and the expressivity of the weak learners";"-"
"e g max leaf nodes";"-"
"errors and weights of the weak learners";"-"
"as previously mentioned adaboost is a forward stagewise additive model we";"TASK"
"now focus on understanding the relationship between the attributed weights of";"META"
"the weak learners and their statistical performance";"CODE"
"we use the fitted class sklearn ensemble adaboostclassifier s attributes";"IRRE"
"estimator errors and estimator weights to investigate this link";"CODE"
"on the left plot we show the weighted error of each weak learner on the";"-"
"reweighted training set at each boosting iteration on the right plot we show";"IRRE"
"the weights associated with each weak learner later used to make the";"OUTD"
"predictions of the final additive model";"TASK"
"we see that the error of the weak learner is the inverse of the weights it";"-"
"means that our additive model will trust more a weak learner that makes";"TASK"
"smaller errors on the training set by increasing its impact on the final";"IRRE"
"decision indeed this exactly is the formulation of updating the base";"CODE"
"estimators weights after each iteration in adaboost";"-"
"dropdown mathematical details";"CODE"
"the weight associated with a weak learner trained at the stage math m is";"-"
"inversely associated with its misclassification error such that";"IRRE"
"math alpha m log frac 1 err m err m log k 1";"-"
"where math alpha m and math err m are the weight and the error";"-"
"of the math m th weak learner respectively and math k is the number of";"-"
"classes in our classification problem";"IRRE"
"another interesting observation boils down to the fact that the first weak";"CODE"
"learners of the model make fewer errors than later weak learners of the";"-"
"boosting chain";"-"
"the intuition behind this observation is the following due to the sample";"CODE"
"reweighting later classifiers are forced to try to classify more difficult or";"CODE"
"noisy samples and to ignore already well classified samples therefore the";"CODE"
"overall error on the training set will increase that s why the weak learner s";"IRRE"
"weights are built to counter balance the worse performing weak learners";"CODE"
"preparing the data";"-"
"first we prepare dummy data with a sinusoidal relationship and some gaussian noise";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"training and prediction with decisiontree and adaboost regressors";"-"
"now we define the classifiers and fit them to the data";"CODE"
"then we predict on that same data to see how well they could fit it";"-"
"the first regressor is a decisiontreeregressor with max depth 4";"-"
"the second regressor is an adaboostregressor with a decisiontreeregressor";"-"
"of max depth 4 as base learner and will be built with n estimators 300";"IRRE"
"of those base learners";"-"
"plotting the results";"IRRE"
"finally we plot how well our two regressors";"CODE"
"single decision tree regressor and adaboost regressor could fit the data";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"construct dataset";"CODE"
"create and fit an adaboosted decision tree";"IRRE"
"plot the decision boundaries";"-"
"plot the training points";"CODE"
"plot the two class decision scores";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"settings";"IRRE"
"n repeat 50 number of iterations for computing expectations";"CODE"
"n train 50 size of the training set";"IRRE"
"n test 1000 size of the test set";"IRRE"
"noise 0 1 standard deviation of the noise";"-"
"change this for exploring the bias variance decomposition of other";"CODE"
"estimators this should work well for estimators with high variance e g";"CODE"
"decision trees or knn but poorly for estimators with low variance e g";"CODE"
"linear models";"-"
"generate data";"-"
"loop over estimators to compare";"IRRE"
"compute predictions";"-"
"bias 2 variance noise decomposition of the mean squared error";"CODE"
"plot figures";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate a binary classification dataset";"IRRE"
"note setting the warm start construction parameter to true disables";"IRRE"
"support for parallelized ensembles but is necessary for tracking the oob";"CODE"
"error trajectory during training";"-"
"map a classifier name to a list of n estimators error rate pairs";"IRRE"
"range of n estimators values to explore";"IRRE"
"record the oob error for each n estimators i setting";"CODE"
"generate the oob error rate vs n estimators plot";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"first we will create a large dataset and split it into three sets";"IRRE"
"a set to train the ensemble methods which are later used to as a feature";"CODE"
"engineering transformer";"CODE"
"a set to train the linear model";"IRRE"
"a set to test the linear model";"IRRE"
"it is important to split the data in such way to avoid overfitting by leaking";"CODE"
"data";"-"
"for each of the ensemble methods we will use 10 estimators and a maximum";"CODE"
"depth of 3 levels";"-"
"first we will start by training the random forest and gradient boosting on";"IRRE"
"the separated training set";"IRRE"
"notice that class sklearn ensemble histgradientboostingclassifier is much";"IRRE"
"faster than class sklearn ensemble gradientboostingclassifier starting";"IRRE"
"with intermediate datasets n samples 10 000 which is not the case of";"CODE"
"the present example";"-"
"the class sklearn ensemble randomtreesembedding is an unsupervised method";"IRRE"
"and thus does not required to be trained independently";"CODE"
"now we will create three pipelines that will use the above embedding as";"IRRE"
"a preprocessing stage";"-"
"the random trees embedding can be directly pipelined with the logistic";"IRRE"
"regression because it is a standard scikit learn transformer";"CODE"
"then we can pipeline random forest or gradient boosting with a logistic";"CODE"
"regression however the feature transformation will happen by calling the";"TASK"
"method apply the pipeline in scikit learn expects a call to transform";"CODE"
"therefore we wrapped the call to apply within a functiontransformer";"CODE"
"we can finally show the different roc curves for all the models";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load dataset";"TASK"
"hgbt uses a histogram based algorithm on binned feature values that can";"IRRE"
"efficiently handle large datasets tens of thousands of samples or more with";"IRRE"
"a high number of features see ref why it s faster the scikit learn";"TASK"
"implementation of rf does not use binning and relies on exact splitting which";"TASK"
"can be computationally expensive";"-"
"compute score and computation times";"-"
"notice that many parts of the implementation of";"TASK"
"class sklearn ensemble histgradientboostingclassifier and";"IRRE"
"class sklearn ensemble histgradientboostingregressor are parallelized by";"IRRE"
"default";"CODE"
"the implementation of class sklearn ensemble randomforestregressor and";"IRRE"
"class sklearn ensemble randomforestclassifier can also be run on multiple";"CODE"
"cores by using the n jobs parameter here set to match the number of";"IRRE"
"physical cores on the host machine see ref parallelism for more";"CODE"
"information";"CODE"
"unlike rf hgbt models offer an early stopping option see";"-"
"ref sphx glr auto examples ensemble plot gradient boosting early stopping py";"-"
"to avoid adding new unnecessary trees internally the algorithm uses an";"CODE"
"out of sample set to compute the generalization performance of the model at";"IRRE"
"each addition of a tree thus if the generalization performance is not";"TASK"
"improving for more than n iter no change iterations it stops adding trees";"TASK"
"the other parameters of both models were tuned but the procedure is not shown";"IRRE"
"here to keep the example simple";"-"
"note";"TASK"
"tuning the n estimators for rf generally results in a waste of computer";"IRRE"
"power in practice one just needs to ensure that it is large enough so that";"TASK"
"doubling its value does not lead to a significant improvement of the testing";"IRRE"
"score";"-"
"plot results";"IRRE"
"we can use a plotly express scatter";"-"
"https plotly com python api reference generated plotly express scatter html";"CODE"
"to visualize the trade off between elapsed computing time and mean test score";"IRRE"
"passing the cursor over a given point displays the corresponding parameters";"IRRE"
"error bars correspond to one standard deviation as computed in the different";"IRRE"
"folds of the cross validation";"-"
"both hgbt and rf models improve when increasing the number of trees in the";"CODE"
"ensemble however the scores reach a plateau where adding new trees just";"CODE"
"makes fitting and scoring slower the rf model reaches such plateau earlier";"-"
"and can never reach the test score of the largest hgbdt model";"IRRE"
"note that the results shown on the above plot can change slightly across runs";"TASK"
"and even more significantly when running on other machines try to run this";"CODE"
"example on your own local machine";"-"
"overall one should often observe that the histogram based gradient boosting";"CODE"
"models uniformly dominate the random forest models in the test score vs";"CODE"
"training speed trade off the hgbdt curve should be on the top left of the rf";"-"
"curve without ever crossing the test score vs prediction speed trade off";"IRRE"
"can also be more disputed but it s most often favorable to hgbdt it s always";"META"
"a good idea to check both kinds of model with hyper parameter tuning and";"IRRE"
"compare their performance on your specific problem to determine which model is";"CODE"
"the best fit but hgbt almost always offers a more favorable speed accuracy";"META"
"trade off than rf either with the default hyper parameters or including the";"IRRE"
"hyper parameter tuning cost";"IRRE"
"there is one exception to this rule of thumb though when training a";"CODE"
"multiclass classification model with a large number of possible classes hgbdt";"IRRE"
"fits internally one tree per class at each boosting iteration while the trees";"CODE"
"used by the rf models are naturally multiclass which should improve the speed";"IRRE"
"accuracy trade off of the rf models in this case";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation and model fitting";"-"
"we generate a synthetic dataset with only 3 informative features we will";"TASK"
"explicitly not shuffle the dataset to ensure that the informative features";"CODE"
"will correspond to the three first columns of x in addition we will split";"TASK"
"our dataset into training and testing subsets";"IRRE"
"a random forest classifier will be fitted to compute the feature importances";"CODE"
"feature importance based on mean decrease in impurity";"CODE"
"feature importances are provided by the fitted attribute";"CODE"
"feature importances and they are computed as the mean and standard";"CODE"
"deviation of accumulation of the impurity decrease within each tree";"-"
"warning";"-"
"impurity based feature importances can be misleading for high";"CODE"
"cardinality features many unique values see";"IRRE"
"ref permutation importance as an alternative below";"CODE"
"let s plot the impurity based importance";"CODE"
"we observe that as expected the three first features are found important";"CODE"
"feature importance based on feature permutation";"CODE"
"permutation feature importance overcomes limitations of the impurity based";"CODE"
"feature importance they do not have a bias toward high cardinality features";"CODE"
"and can be computed on a left out test set";"IRRE"
"the computation for full permutation importance is more costly each feature is";"CODE"
"shuffled n times and the model is used to make predictions on the permuted data to see";"OUTD"
"the drop in performance please see ref permutation importance for more details";"CODE"
"we can now plot the importance ranking";"CODE"
"the same features are detected as most important using both methods although";"CODE"
"the relative importances vary as seen on the plots mdi is less likely than";"CODE"
"permutation importance to fully omit a feature";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"parameters";"IRRE"
"plot step 0 02 fine step width for decision surface contours";"CODE"
"plot step coarser 0 5 step widths for coarse classifier guesses";"CODE"
"random seed 13 fix the seed on each iteration";"IRRE"
"load data";"TASK"
"we only take the two corresponding features";"TASK"
"shuffle";"-"
"standardize";"-"
"train";"-"
"create a title for each column and the console by using str and";"CODE"
"slicing away useless parts of the string";"CODE"
"add a title at the top of each column";"TASK"
"now plot the decision boundary using a fine mesh as input to a";"CODE"
"filled contour plot";"-"
"plot either a single decisiontreeclassifier or alpha blend the";"CODE"
"decision surfaces of the ensemble of classifiers";"IRRE"
"choose alpha blend level with respect to the number";"CODE"
"of estimators";"-"
"that are in use noting that adaboost can use fewer estimators";"-"
"than its maximum if it achieves a good enough fit early on";"-"
"build a coarser grid to plot a set of ensemble classifications";"IRRE"
"to show how these are different to what we see in the decision";"CODE"
"surfaces these points are regularly space and do not have a";"CODE"
"black outline";"-"
"plot the training points these are clustered together and have a";"IRRE"
"black outline";"-"
"plot idx 1 move on to the next plot in sequence";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load ames housing dataset";"IRRE"
"first we load the ames housing data as a pandas dataframe the features";"TASK"
"are either categorical or numerical";"-"
"select only a subset of features of x to make the example faster to run";"CODE"
"gradient boosting estimator with dropped categorical features";"TASK"
"as a baseline we create an estimator where the categorical features are";"TASK"
"dropped";"-"
"gradient boosting estimator with one hot encoding";"-"
"next we create a pipeline to one hot encode the categorical features";"CODE"
"while letting the remaining features passthrough unchanged";"CODE"
"gradient boosting estimator with ordinal encoding";"-"
"next we create a pipeline that treats categorical features as ordered";"CODE"
"quantities i e the categories are encoded as 0 1 2 etc and treated as";"IRRE"
"continuous features";"TASK"
"gradient boosting estimator with target encoding";"-"
"another possibility is to use the class preprocessing targetencoder which";"IRRE"
"encodes the categories computed from the mean of the training target";"CODE"
"variable as computed using a smoothed np mean y axis 0 i e";"CODE"
"in regression it uses the mean of y";"-"
"in binary classification the positive class rate";"IRRE"
"in multiclass a vector of class rates one per class";"CODE"
"for each category it computes these target averages using term cross";"IRRE"
"fitting meaning that the training data are split into folds in each fold";"CODE"
"the averages are calculated only on a subset of data and then applied to the";"IRRE"
"held out part this way each sample is encoded using statistics from data it";"CODE"
"was not part of preventing information leakage from the target";"CODE"
"gradient boosting estimator with native categorical support";"-"
"we now create a class ensemble histgradientboostingregressor estimator";"IRRE"
"that can natively handle categorical features without explicit encoding such";"TASK"
"functionality can be enabled by setting categorical features from dtype";"CODE"
"which automatically detects features with categorical dtypes or more explicitly";"TASK"
"by categorical features categorical columns subset";"TASK"
"unlike previous encoding approaches the estimator natively deals with the";"-"
"categorical features at each split it partitions the categories of such a";"TASK"
"feature into disjoint sets using a heuristic that sorts them by their effect";"CODE"
"on the target variable see split finding with categorical features";"TASK"
"https scikit learn org stable modules ensemble html split finding with categorical features";"CODE"
"for details";"CODE"
"while ordinal encoding may work well for low cardinality features even if";"CODE"
"categories have no natural order reaching meaningful splits requires deeper";"CODE"
"trees as the cardinality increases the native categorical support avoids this";"CODE"
"by directly working with unordered categories the advantage over one hot";"-"
"encoding is the omitted preprocessing and faster fit and predict time";"-"
"model comparison";"-"
"here we use term cross validation to compare the models performance in";"IRRE"
"terms of func metrics mean absolute percentage error and fit times in the";"IRRE"
"upcoming plots error bars represent 1 standard deviation as computed across";"IRRE"
"cross validation splits";"-"
"in the plot above the best models are those that are closer to the";"CODE"
"down left corner as indicated by the arrow those models would indeed";"CODE"
"correspond to faster fitting and lower error";"-"
"the model using one hot encoded data is the slowest this is to be expected";"CODE"
"as one hot encoding creates an additional feature for each category value of";"TASK"
"every categorical feature greatly increasing the number of split candidates";"TASK"
"during training in theory we expect the native handling of categorical";"CODE"
"features to be slightly slower than treating categories as ordered quantities";"TASK"
"ordinal since native handling requires ref sorting categories";"CODE"
"categorical support gbdt fitting times should however be close when the";"CODE"
"number of categories is small and this may not always be reflected in";"CODE"
"practice";"-"
"the time required to fit when using the targetencoder depends on the";"CODE"
"cross fitting parameter cv as adding splits come at a computational cost";"TASK"
"in terms of prediction performance dropping the categorical features leads to";"CODE"
"the worst performance the four models that make use of the categorical";"CODE"
"features have comparable error rates with a slight edge for the native";"TASK"
"handling";"-"
"limiting the number of splits";"-"
"in general one can expect poorer predictions from one hot encoded data";"CODE"
"especially when the tree depths or the number of nodes are limited with";"-"
"one hot encoded data one needs more split points i e more depth in order";"TASK"
"to recover an equivalent split that could be obtained in one single split";"-"
"point with native handling";"CODE"
"this is also true when categories are treated as ordinal quantities if";"CODE"
"categories are a f and the best split is acf bde the one hot encoder";"-"
"model would need 3 split points one per category in the left node and the";"CODE"
"ordinal non native model would need 4 splits 1 split to isolate a 1 split";"TASK"
"to isolate f and 2 splits to isolate c from bcde";"CODE"
"how strongly the models performances differ in practice depends on the";"CODE"
"dataset and on the flexibility of the trees";"IRRE"
"to see this let us re run the same analysis with under fitting models where";"CODE"
"we artificially limit the total number of splits by both limiting the number";"-"
"of trees and the depth of each tree";"-"
"the native model does not use a pipeline so we can set the parameters";"CODE"
"directly";"-"
"the results for these underfitting models confirm our previous intuition the";"CODE"
"native category handling strategy performs the best when the splitting budget";"CODE"
"is constrained the three explicit encoding strategies one hot ordinal and";"CODE"
"target encoding lead to slightly larger errors than the estimator s native";"-"
"handling but still perform better than the baseline model that just dropped";"CODE"
"the categorical features altogether";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data preparation";"-"
"first we load and prepares the california housing prices dataset for";"CODE"
"training and evaluation it subsets the dataset splits it into training";"IRRE"
"and validation sets";"IRRE"
"model training and comparison";"-"
"two class sklearn ensemble gradientboostingregressor models are trained";"IRRE"
"one with and another without early stopping the purpose is to compare their";"IRRE"
"performance it also calculates the training time and the n estimators";"IRRE"
"used by both models";"-"
"error calculation";"-"
"the code calculates the func sklearn metrics mean squared error for both";"IRRE"
"training and validation datasets for the models trained in the previous";"CODE"
"section it computes the errors for each boosting iteration the purpose is";"CODE"
"to assess the performance and convergence of the models";"CODE"
"visualize comparison";"-"
"it includes three subplots";"CODE"
"1 plotting training errors of both models over boosting iterations";"-"
"2 plotting validation errors of both models over boosting iterations";"-"
"3 creating a bar chart to compare the training times and the estimator used";"IRRE"
"of the models with and without early stopping";"-"
"the difference in training error between the gbm full and the";"CODE"
"gbm early stopping stems from the fact that gbm early stopping sets";"IRRE"
"aside validation fraction of the training data as internal validation set";"IRRE"
"early stopping is decided based on this internal validation score";"CODE"
"summary";"-"
"in our example with the class sklearn ensemble gradientboostingregressor";"IRRE"
"model on the california housing prices dataset we have demonstrated the";"IRRE"
"practical benefits of early stopping";"-"
"preventing overfitting we showed how the validation error stabilizes";"-"
"or starts to increase after a certain point indicating that the model";"CODE"
"generalizes better to unseen data this is achieved by stopping the training";"CODE"
"process before overfitting occurs";"CODE"
"improving training efficiency we compared training times between";"IRRE"
"models with and without early stopping the model with early stopping";"-"
"achieved comparable accuracy while requiring significantly fewer";"CODE"
"estimators resulting in faster training";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate data adapted from g ridgeway s gbm example";"CODE"
"fit classifier with out of bag estimates";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate some data for a synthetic regression problem by applying the";"CODE"
"function f to uniformly sampled random inputs";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the data";"CODE"
"first we need to load the data";"TASK"
"data preprocessing";"-"
"next we will split our dataset to use 90 for training and leave the rest";"IRRE"
"for testing we will also set the regression model parameters you can play";"IRRE"
"with these parameters to see how the results change";"IRRE"
"n estimators the number of boosting stages that will be performed";"CODE"
"later we will plot deviance against boosting iterations";"-"
"max depth limits the number of nodes in the tree";"CODE"
"the best value depends on the interaction of the input variables";"CODE"
"min samples split the minimum number of samples required to split an";"CODE"
"internal node";"CODE"
"learning rate how much the contribution of each tree will shrink";"META"
"loss loss function to optimize the least squares function is used in";"CODE"
"this case however there are many other options see";"CODE"
"class sklearn ensemble gradientboostingregressor";"IRRE"
"fit regression model";"-"
"now we will initiate the gradient boosting regressors and fit it with our";"IRRE"
"training data let s also look and the mean squared error on the test data";"CODE"
"plot training deviance";"-"
"finally we will visualize the results to do that we will first compute the";"CODE"
"test set deviance and then plot it against boosting iterations";"IRRE"
"plot feature importance";"CODE"
"warning";"-"
"careful impurity based feature importances can be misleading for";"CODE"
"high cardinality features many unique values as an alternative";"IRRE"
"the permutation importances of reg can be computed on a";"CODE"
"held out test set see ref permutation importance for more details";"CODE"
"for this example the impurity based and permutation methods identify the";"CODE"
"same 2 strongly predictive features but not in the same order the third most";"TASK"
"predictive feature bp is also the same for the 2 methods the remaining";"CODE"
"features are less predictive and the error bars of the permutation plot";"TASK"
"show that they overlap with 0";"-"
"labels argument in boxplot is deprecated in matplotlib 3 9 and has been";"OUTD"
"renamed to tick labels the following code handles this but as a";"META"
"scikit learn user you probably can write simpler code by using labels";"TASK"
"matplotlib 3 9 or tick labels matplotlib 3 9";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"map labels from 1 1 to 0 1";"CODE"
"compute test set deviance";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"preparing the data";"-"
"the electricity dataset http www openml org d 151 consists of data";"CODE"
"collected from the australian new south wales electricity market in this";"CODE"
"market prices are not fixed and are affected by supply and demand they are";"-"
"set every five minutes electricity transfers to from the neighboring state of";"CODE"
"victoria were done to alleviate fluctuations";"CODE"
"the dataset originally named elec2 contains 45 312 instances dated from 7";"IRRE"
"may 1996 to 5 december 1998 each sample of the dataset refers to a period of";"IRRE"
"30 minutes i e there are 48 instances for each time period of one day each";"CODE"
"sample on the dataset has 7 columns";"IRRE"
"date between 7 may 1996 to 5 december 1998 normalized between 0 and 1";"-"
"day day of week 1 7";"-"
"period half hour intervals over 24 hours normalized between 0 and 1";"CODE"
"nswprice nswdemand electricity price demand of new south wales";"CODE"
"vicprice vicdemand electricity price demand of victoria";"CODE"
"originally it is a classification task but here we use it for the regression";"CODE"
"task to predict the scheduled electricity transfer between states";"TASK"
"this particular dataset has a stepwise constant target for the first 17 760";"CODE"
"samples";"-"
"let us drop those entries and explore the hourly electricity transfer over";"CODE"
"different days of the week";"-"
"notice that energy transfer increases systematically during weekends";"CODE"
"effect of number of trees and early stopping";"-"
"for the sake of illustrating the effect of the maximum number of trees we";"CODE"
"train a class sklearn ensemble histgradientboostingregressor over the";"IRRE"
"daily electricity transfer using the whole dataset then we visualize its";"IRRE"
"predictions depending on the max iter parameter here we don t try to";"CODE"
"evaluate the performance of the model and its capacity to generalize but";"META"
"rather its capability to learn from the training data";"CODE"
"with just a few iterations hgbt models can achieve convergence see";"-"
"ref sphx glr auto examples ensemble plot forest hist grad boosting comparison py";"CODE"
"meaning that adding more trees does not improve the model anymore in the";"CODE"
"figure above 5 iterations are not enough to get good predictions with 50";"TASK"
"iterations we are already able to do a good job";"CODE"
"setting max iter too high might degrade the prediction quality and cost a lot of";"IRRE"
"avoidable computing resources therefore the hgbt implementation in scikit learn";"TASK"
"provides an automatic early stopping strategy with it the model";"-"
"uses a fraction of the training data as internal validation set";"IRRE"
"validation fraction and stops training if the validation score does not";"CODE"
"improve or degrades after n iter no change iterations up to a certain";"-"
"tolerance tol";"-"
"notice that there is a trade off between learning rate and max iter";"-"
"generally smaller learning rates are preferable but require more iterations";"META"
"to converge to the minimum loss while larger learning rates converge faster";"CODE"
"less iterations trees needed but at the cost of a larger minimum loss";"META"
"because of this high correlation between the learning rate the number of iterations";"CODE"
"a good practice is to tune the learning rate along with all important other";"CODE"
"hyperparameters fit the hbgt on the training set with a large enough value";"IRRE"
"for max iter and determine the best max iter via early stopping and some";"IRRE"
"explicit validation fraction";"-"
"we can then overwrite the value for max iter to a reasonable value and avoid";"CODE"
"the extra computational cost of the inner validation rounding up the number";"-"
"of iterations may account for variability of the training set";"CODE"
"note the inner validation done during early stopping is not optimal for";"CODE"
"time series";"-"
"support for missing values";"IRRE"
"hgbt models have native support of missing values during training the tree";"IRRE"
"grower decides where samples with missing values should go left or right";"IRRE"
"child at each split based on the potential gain when predicting these";"CODE"
"samples are sent to the learnt child accordingly if a feature had no missing";"TASK"
"values during training then for prediction samples with missing values for that";"IRRE"
"feature are sent to the child with the most samples as seen during fit";"TASK"
"the present example shows how hgbt regressions deal with values missing";"IRRE"
"completely at random mcar i e the missingness does not depend on the";"CODE"
"observed data or the unobserved data we can simulate such scenario by";"-"
"randomly replacing values from randomly selected features with nan values";"IRRE"
"first week slice 0 336 first week in the test set as 7 48 336";"IRRE"
"as expected the model degrades as the proportion of missing values increases";"IRRE"
"support for quantile loss";"CODE"
"the quantile loss in regression enables a view of the variability or";"CODE"
"uncertainty of the target variable for instance predicting the 5th and 95th";"CODE"
"percentiles can provide a 90 prediction interval i e the range within which";"CODE"
"we expect a new observed value to fall with 90 probability";"IRRE"
"we observe a tendence to over estimate the energy transfer this could be be";"CODE"
"quantitatively confirmed by computing empirical coverage numbers as done in";"CODE"
"the ref calibration of confidence intervals section calibration section";"CODE"
"keep in mind that those predicted percentiles are just estimations from a";"IRRE"
"model one can still improve the quality of such estimations by";"TASK"
"collecting more data points";"CODE"
"better tuning of the model hyperparameters see";"IRRE"
"ref sphx glr auto examples ensemble plot gradient boosting quantile py";"-"
"engineering more predictive features from the same data see";"TASK"
"ref sphx glr auto examples applications plot cyclical feature engineering py";"TASK"
"monotonic constraints";"CODE"
"given specific domain knowledge that requires the relationship between a";"CODE"
"feature and the target to be monotonically increasing or decreasing one can";"TASK"
"enforce such behaviour in the predictions of a hgbt model using monotonic";"CODE"
"constraints this makes the model more interpretable and can reduce its";"CODE"
"variance and potentially mitigate overfitting at the risk of increasing";"CODE"
"bias monotonic constraints can also be used to enforce specific regulatory";"CODE"
"requirements ensure compliance and align with ethical considerations";"CODE"
"in the present example the policy of transferring energy from victoria to new";"CODE"
"south wales is meant to alleviate price fluctuations meaning that the model";"-"
"predictions have to enforce such goal i e transfer should increase with";"CODE"
"price and demand in new south wales but also decrease with price and demand";"META"
"in victoria in order to benefit both populations";"-"
"if the training data has feature names it s possible to specify the monotonic";"TASK"
"constraints by passing a dictionary with the convention";"CODE"
"1 monotonic increase";"CODE"
"0 no constraint";"CODE"
"1 monotonic decrease";"-"
"alternatively one can pass an array like object encoding the above convention by";"IRRE"
"position";"-"
"observe that nswdemand and vicdemand seem already monotonic without constraint";"CODE"
"this is a good example to show that the model with monotonicity constraints is";"CODE"
"overconstraining";"CODE"
"additionally we can verify that the predictive quality of the model is not";"TASK"
"significantly degraded by introducing the monotonic constraints for such";"CODE"
"purpose we use class sklearn model selection timeseriessplit";"CODE"
"cross validation to estimate the variance of the test score by doing so we";"CODE"
"guarantee that the training data does not succeed the testing data which is";"IRRE"
"crucial when dealing with data that have a temporal relationship";"CODE"
"ts cv timeseriessplit n splits 5 gap 48 test size 336 a week has 336 samples";"IRRE"
"that being said notice the comparison is between two different models that";"-"
"may be optimized by a different combination of hyperparameters that is the";"IRRE"
"reason why we do no use the common params in this section as done before";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation";"-"
"we generate two clusters each one containing n samples by randomly";"IRRE"
"sampling the standard normal distribution as returned by";"IRRE"
"func numpy random randn one of them is spherical and the other one is";"IRRE"
"slightly deformed";"CODE"
"for consistency with the class sklearn ensemble isolationforest notation";"CODE"
"the inliers i e the gaussian clusters are assigned a ground truth label 1";"IRRE"
"whereas the outliers created with func numpy random uniform are assigned";"IRRE"
"the label 1";"-"
"cluster 1 0 4 rng randn n samples 2 covariance np array 2 2 general";"IRRE"
"cluster 2 0 3 rng randn n samples 2 np array 2 2 spherical";"IRRE"
"we can visualize the resulting clusters";"IRRE"
"training of the model";"-"
"plot discrete decision boundary";"-"
"we use the class class sklearn inspection decisionboundarydisplay to";"IRRE"
"visualize a discrete decision boundary the background color represents";"-"
"whether a sample in that given area is predicted to be an outlier";"CODE"
"or not the scatter plot displays the true labels";"-"
"plot path length decision boundary";"-"
"by setting the response method decision function the background of the";"CODE"
"class sklearn inspection decisionboundarydisplay represents the measure of";"IRRE"
"normality of an observation such score is given by the path length averaged";"-"
"over a forest of random trees which itself is given by the depth of the leaf";"CODE"
"or equivalently the number of splits required to isolate a given sample";"CODE"
"when a forest of random trees collectively produce short path lengths for";"CODE"
"isolating some particular samples they are highly likely to be anomalies and";"META"
"the measure of normality is close to 0 similarly large paths correspond to";"IRRE"
"values close to 1 and are more likely to be inliers";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"y is positively correlated with f 0 and negatively correlated with f 1";"-"
"fit a first model on this dataset without any constraints";"CODE"
"fit a second model on this dataset with monotonic increase 1";"CODE"
"and a monotonic decrease 1 constraints respectively";"CODE"
"let s display the partial dependence of the predictions on the two features";"CODE"
"we can see that the predictions of the unconstrained model capture the";"CODE"
"oscillations of the data while the constrained model follows the general";"CODE"
"trend and ignores the local variations";"CODE"
"monotonic cst features names";"TASK"
"using feature names to specify monotonic constraints";"CODE"
"note that if the training data has feature names it s possible to specify the";"TASK"
"monotonic constraints by passing a dictionary";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"make a synthetic dataset";"IRRE"
"use randomtreesembedding to transform data";"IRRE"
"visualize result after dimensionality reduction using truncated svd";"IRRE"
"learn a naive bayes classifier on the transformed data";"CODE"
"learn an extratreesclassifier for comparison";"CODE"
"scatter plot of original and reduced data";"CODE"
"plot the decision in original space for that we will assign a color";"IRRE"
"to each point in the mesh x min x max x y min y max";"CODE"
"transform grid using randomtreesembedding";"IRRE"
"transform grid using extratreesclassifier";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"create a random dataset";"IRRE"
"predict on new data";"CODE"
"plot the results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"download the dataset";"CODE"
"we will use the ames housing dataset which was first compiled by dean de cock";"IRRE"
"and became better known after it was used in kaggle challenge it is a set";"IRRE"
"of 1460 residential homes in ames iowa each described by 80 features we";"TASK"
"will use it to predict the final logarithmic price of the houses in this";"CODE"
"example we will use only 20 most interesting features chosen using";"TASK"
"gradientboostingregressor and limit number of entries here we won t go";"IRRE"
"into the details on how to select the most interesting features";"CODE"
"the ames housing dataset is not shipped with scikit learn and therefore we";"IRRE"
"will fetch it from openml";"CODE"
"ames housing http jse amstat org v19n3 decock pdf";"CODE"
"openml https www openml org d 42165";"CODE"
"make pipeline to preprocess the data";"CODE"
"before we can use ames dataset we still need to do some preprocessing";"TASK"
"first we will select the categorical and numerical columns of the dataset to";"IRRE"
"construct the first step of the pipeline";"CODE"
"then we will need to design preprocessing pipelines which depends on the";"CODE"
"ending regressor if the ending regressor is a linear model one needs to";"CODE"
"one hot encode the categories if the ending regressor is a tree based model";"CODE"
"an ordinal encoder will be sufficient besides numerical values need to be";"IRRE"
"standardized for a linear model while the raw numerical data can be treated";"CODE"
"as is by a tree based model however both models need an imputer to";"-"
"handle missing values";"IRRE"
"we will first design the pipeline required for the tree based models";"CODE"
"then we will now define the preprocessor used when the ending regressor";"CODE"
"is a linear model";"-"
"stack of predictors on a single data set";"IRRE"
"it is sometimes tedious to find the model which will best perform on a given";"CODE"
"dataset stacking provide an alternative by combining the outputs of several";"IRRE"
"learners without the need to choose a model specifically the performance of";"TASK"
"stacking is usually close to the best model and sometimes it can outperform";"CODE"
"the prediction performance of each individual model";"CODE"
"here we combine 3 learners linear and non linear and use a ridge regressor";"IRRE"
"to combine their outputs together";"IRRE"
"note";"TASK"
"although we will make new pipelines with the processors which we wrote in";"CODE"
"the previous section for the 3 learners the final estimator";"CODE"
"class sklearn linear model ridgecv does not need preprocessing of";"CODE"
"the data as it will be fed with the already preprocessed output from the 3";"CODE"
"learners";"-"
"measure and plot the results";"IRRE"
"now we can use ames housing dataset to make the predictions we check the";"IRRE"
"performance of each individual predictor as well as of the stack of the";"CODE"
"regressors";"-"
"the stacked regressor will combine the strengths of the different regressors";"-"
"however we also see that training the stacked regressor is much more";"-"
"computationally expensive";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we first generate a noisy xor dataset which is a binary classification task";"IRRE"
"feature names feature 0 feature 1";"TASK"
"xor feature 0 noise 0 0 xor feature 1 noise 1 0";"TASK"
"ax scatter x feature 0 x feature 1 c y common scatter plot params";"TASK"
"due to the inherent non linear separability of the xor dataset tree based";"IRRE"
"models would often be preferred however appropriate feature engineering";"TASK"
"combined with a linear model can yield effective results with the added";"TASK"
"benefit of producing better calibrated probabilities for samples located in";"CODE"
"the transition regions affected by noise";"-"
"we define and fit the models on the whole dataset";"IRRE"
"finally we use class inspection decisionboundarydisplay to plot the";"CODE"
"predicted probabilities by using a diverging colormap such as rdbu we";"-"
"can ensure that darker colors correspond to predict proba close to either 0";"IRRE"
"or 1 and white corresponds to predict proba of 0 5";"IRRE"
"x feature 0";"TASK"
"x feature 1";"TASK"
"as a sanity check we can verify for a given sample that the probability";"CODE"
"predicted by the class ensemble votingclassifier is indeed the weighted";"IRRE"
"average of the individual classifiers soft predictions";"IRRE"
"in the case of binary classification such as in the present example the";"CODE"
"term predict proba arrays contain the probability of belonging to class 0";"CODE"
"here in red as the first entry and the probability of belonging to class 1";"CODE"
"here in blue as the second entry";"CODE"
"test sample pd dataframe feature 0 0 5 feature 1 1 5";"TASK"
"we can see that manual calculation of predicted probabilities above is";"-"
"equivalent to that produced by the votingclassifier";"IRRE"
"to convert soft predictions into hard predictions when weights are provided";"CODE"
"the weighted average predicted probabilities are computed for each class";"CODE"
"then the final class label is then derived from the class label with the";"CODE"
"highest average probability which corresponds to the default threshold at";"CODE"
"predict proba 0 5 in the case of binary classification";"CODE"
"this is equivalent to the output of votingclassifier s predict method";"IRRE"
"soft votes can be thresholded as for any other probabilistic classifier this";"CODE"
"allows you to set a threshold probability at which the positive class will be";"IRRE"
"predicted instead of simply selecting the class with the highest predicted";"CODE"
"probability";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"training classifiers";"IRRE"
"first we will load the diabetes dataset and initiate a gradient boosting";"IRRE"
"regressor a random forest regressor and a linear regression next we will";"IRRE"
"use the 3 regressors to build the voting regressor";"IRRE"
"train classifiers";"IRRE"
"making predictions";"-"
"now we will use each of the regressors to make the 20 first predictions";"-"
"plot the results";"IRRE"
"finally we will visualize the 20 predictions the red stars show the average";"CODE"
"prediction made by class ensemble votingregressor";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"the iris dataset";"IRRE"
"some noisy data not correlated";"-"
"add the noisy data to the informative features";"TASK"
"split dataset to select feature and evaluate the classifier";"IRRE"
"univariate feature selection";"CODE"
"univariate feature selection with f test for feature scoring";"CODE"
"we use the default selection function to select";"CODE"
"the four most significant features";"TASK"
"in the total set of features only the 4 of the original features are significant";"TASK"
"we can see that they have the highest score with univariate feature";"TASK"
"selection";"CODE"
"compare with svms";"IRRE"
"without univariate feature selection";"CODE"
"after univariate feature selection";"CODE"
"without univariate feature selection the svm assigns a large weight";"CODE"
"to the first 4 original significant features but also selects many of the";"TASK"
"non informative features applying univariate feature selection before";"CODE"
"the svm increases the svm weight attributed to the significant features";"TASK"
"and will thus improve classification";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we will start by generating a binary classification dataset subsequently we";"IRRE"
"will divide the dataset into two subsets";"IRRE"
"a common mistake done with feature selection is to search a subset of";"CODE"
"discriminative features on the full dataset instead of only using the";"TASK"
"training set the usage of scikit learn func sklearn pipeline pipeline";"CODE"
"prevents to make such mistake";"-"
"here we will demonstrate how to build a pipeline where the first step will";"CODE"
"be the feature selection";"TASK"
"when calling fit on the training data a subset of feature will be selected";"IRRE"
"and the index of these selected features will be stored the feature selector";"TASK"
"will subsequently reduce the number of features and pass this subset to the";"TASK"
"classifier which will be trained";"IRRE"
"once the training is complete we can predict on new unseen samples in this";"CODE"
"case the feature selector will only select the most discriminative features";"CODE"
"based on the information stored during training then the data will be";"CODE"
"passed to the classifier which will make the prediction";"IRRE"
"here we show the final metrics via a classification report";"CODE"
"be aware that you can inspect a step in the pipeline for instance we might";"CODE"
"be interested about the parameters of the classifier since we selected";"CODE"
"three features we expect to have three coefficients";"TASK"
"however we do not know which features were selected from the original";"CODE"
"dataset we could proceed by several manners here we will invert the";"IRRE"
"transformation of these coefficients to get information about the original";"CODE"
"space";"-"
"we can see that the features with non zero coefficients are the selected";"TASK"
"features by the first step";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the digits dataset";"IRRE"
"plot pixel ranking";"-"
"add annotations for pixel numbers";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation";"-"
"we build a classification task using 3 informative features the introduction";"CODE"
"of 2 additional redundant i e correlated features has the effect that the";"TASK"
"selected features vary depending on the cross validation fold the remaining";"CODE"
"features are non informative as they are drawn at random";"IRRE"
"model training and selection";"CODE"
"we create the rfe object and compute the cross validated scores the scoring";"IRRE"
"strategy accuracy optimizes the proportion of correctly classified samples";"IRRE"
"min features to select 1 minimum number of features to consider";"TASK"
"in the present case the model with 3 features which corresponds to the true";"CODE"
"generative model is found to be the most optimal";"-"
"plot number of features vs cross validation scores";"TASK"
"from the plot above one can further notice a plateau of equivalent scores";"CODE"
"similar mean value and overlapping errorbars for 3 to 5 selected features";"CODE"
"this is the result of introducing correlated features indeed the optimal";"CODE"
"model selected by the rfe can lie within this range depending on the";"CODE"
"cross validation technique the test accuracy decreases above 5 selected";"IRRE"
"features this is keeping non informative features leads to over fitting and";"TASK"
"is therefore detrimental for the statistical performance of the models";"CODE"
"mask of features selected by the rfe";"TASK"
"in the five folds the selected features are consistent this is good news";"CODE"
"it means that the selection is stable across folds and it confirms that";"CODE"
"these features are the most informative ones";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"loading the data";"CODE"
"we first load the diabetes dataset which is available from within";"CODE"
"scikit learn and print its description";"CODE"
"feature importance from coefficients";"CODE"
"to get an idea of the importance of the features we are going to use the";"CODE"
"class sklearn linear model ridgecv estimator the features with the";"TASK"
"highest absolute coef value are considered the most important";"CODE"
"we can observe the coefficients directly without needing to scale them or";"CODE"
"scale the data because from the description above we know that the features";"CODE"
"were already standardized";"CODE"
"for a more complete example on the interpretations of the coefficients of";"CODE"
"linear models you may refer to";"-"
"ref sphx glr auto examples inspection plot linear model coefficient interpretation py noqa e501";"CODE"
"selecting features based on importance";"CODE"
"now we want to select the two features which are the most important according";"CODE"
"to the coefficients the class sklearn feature selection selectfrommodel";"CODE"
"is meant just for that class sklearn feature selection selectfrommodel";"CODE"
"accepts a threshold parameter and will select the features whose importance";"CODE"
"defined by the coefficients are above this threshold";"CODE"
"since we want to select only 2 features we will set this threshold slightly";"CODE"
"above the coefficient of third most important feature";"CODE"
"selecting features with sequential feature selection";"TASK"
"another way of selecting features is to use";"TASK"
"class sklearn feature selection sequentialfeatureselector";"CODE"
"sfs sfs is a greedy procedure where at each iteration we choose the best";"IRRE"
"new feature to add to our selected features based a cross validation score";"TASK"
"that is we start with 0 features and choose the best single feature with the";"TASK"
"highest score the procedure is repeated until we reach the desired number of";"-"
"selected features";"TASK"
"we can also go in the reverse direction backward sfs i e start with all";"CODE"
"the features and greedily choose features to remove one by one we illustrate";"TASK"
"both approaches here";"-"
"interestingly forward and backward selection have selected the same set of";"CODE"
"features in general this isn t the case and the two methods would lead to";"CODE"
"different results";"IRRE"
"we also note that the features selected by sfs differ from those selected by";"CODE"
"feature importance sfs selects bmi instead of s1 this does sound";"CODE"
"reasonable though since bmi corresponds to the third most important";"CODE"
"feature according to the coefficients it is quite remarkable considering";"TASK"
"that sfs makes no use of the coefficients at all";"-"
"to finish with we should note that";"TASK"
"class sklearn feature selection selectfrommodel is significantly faster";"CODE"
"than sfs indeed class sklearn feature selection selectfrommodel only";"CODE"
"needs to fit a model once while sfs needs to cross validate many different";"TASK"
"models for each of the iterations sfs however works with any model while";"CODE"
"class sklearn feature selection selectfrommodel requires the underlying";"CODE"
"estimator to expose a coef attribute or a feature importances";"CODE"
"attribute the forward sfs is faster than the backward sfs because it only";"META"
"needs to perform n features to select 2 iterations while the backward";"CODE"
"sfs needs to perform n features n features to select 8 iterations";"TASK"
"using negative tolerance values";"IRRE"
"class sklearn feature selection sequentialfeatureselector can be used";"CODE"
"to remove features present in the dataset and return a";"IRRE"
"smaller subset of the original features with direction backward";"TASK"
"and a negative value of tol";"IRRE"
"we begin by loading the breast cancer dataset consisting of 30 different";"IRRE"
"features and 569 samples";"TASK"
"we will make use of the class sklearn linear model logisticregression";"IRRE"
"estimator with class sklearn feature selection sequentialfeatureselector";"CODE"
"to perform the feature selection";"CODE"
"we can see that the number of features selected tend to increase as negative";"CODE"
"values of tol approach to zero the time taken for feature selection also";"IRRE"
"decreases as the values of tol come closer to zero";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"setting a decision threshold for a pre fitted classifier";"IRRE"
"fitted classifiers in scikit learn use an arbitrary decision threshold to decide";"IRRE"
"which class the given sample belongs to the decision threshold is either 0 0 on the";"IRRE"
"value returned by term decision function or 0 5 on the probability returned by";"IRRE"
"term predict proba";"-"
"however one might want to set a custom decision threshold we can do this by";"CODE"
"using class sklearn model selection fixedthresholdclassifier and wrapping the";"IRRE"
"classifier with class sklearn frozen frozenestimator";"IRRE"
"now imagine you d want to set a different decision threshold on the probability";"IRRE"
"estimates we can do this by wrapping the classifier with";"CODE"
"class sklearn frozen frozenestimator and passing it to";"IRRE"
"class sklearn model selection fixedthresholdclassifier";"CODE"
"note that in the above piece of code calling fit on";"TASK"
"class sklearn model selection fixedthresholdclassifier does not refit the";"CODE"
"underlying classifier";"IRRE"
"now let s see how the predictions changed with respect to the probability";"CODE"
"threshold";"-"
"we see that the probability estimates stay the same but since a different decision";"META"
"threshold is used the predicted classes are different";"IRRE"
"please refer to";"-"
"ref sphx glr auto examples model selection plot cost sensitive learning py";"CODE"
"to learn about cost sensitive learning and decision threshold tuning";"-"
"calibration of a pre fitted classifier";"IRRE"
"you can use class sklearn frozen frozenestimator to calibrate a pre fitted";"IRRE"
"classifier using class sklearn calibration calibratedclassifiercv";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generating a dataset";"IRRE"
"we create a synthetic dataset the true generative process will take a 1 d";"IRRE"
"vector and compute its sine note that the period of this sine is thus";"CODE"
"math 2 pi we will reuse this information later in this example";"CODE"
"now we can imagine a scenario where we get observations from this true";"CODE"
"process however we will add some challenges";"TASK"
"the measurements will be noisy";"-"
"only samples from the beginning of the signal will be available";"CODE"
"let s plot the true signal and the noisy measurements available for training";"CODE"
"limitations of a simple linear model";"-"
"first we would like to highlight the limitations of a linear model given";"-"
"our dataset we fit a class sklearn linear model ridge and check the";"IRRE"
"predictions of this model on our dataset";"IRRE"
"such a ridge regressor underfits data since it is not expressive enough";"CODE"
"kernel methods kernel ridge and gaussian process";"-"
"kernel ridge";"-"
"we can make the previous linear model more expressive by using a so called";"IRRE"
"kernel a kernel is an embedding from the original feature space to another";"TASK"
"one simply put it is used to map our original data into a newer and more";"CODE"
"complex feature space this new space is explicitly defined by the choice of";"CODE"
"kernel";"-"
"in our case we know that the true generative process is a periodic function";"CODE"
"we can use a class sklearn gaussian process kernels expsinesquared kernel";"IRRE"
"which allows recovering the periodicity the class";"IRRE"
"class sklearn kernel ridge kernelridge will accept such a kernel";"IRRE"
"using this model together with a kernel is equivalent to embed the data";"CODE"
"using the mapping function of the kernel and then apply a ridge regression";"CODE"
"in practice the data are not mapped explicitly instead the dot product";"CODE"
"between samples in the higher dimensional feature space is computed using the";"TASK"
"kernel trick";"-"
"thus let s use such a class sklearn kernel ridge kernelridge";"CODE"
"this fitted model is not accurate indeed we did not set the parameters of";"IRRE"
"the kernel and instead used the default ones we can inspect them";"CODE"
"our kernel has two parameters the length scale and the periodicity for our";"IRRE"
"dataset we use sin as the generative process implying a";"IRRE"
"math 2 pi periodicity for the signal the default value of the parameter";"CODE"
"being math 1 it explains the high frequency observed in the predictions of";"CODE"
"our model";"-"
"similar conclusions could be drawn with the length scale parameter thus it";"IRRE"
"tells us that the kernel parameters need to be tuned we will use a randomized";"IRRE"
"search to tune the different parameters the kernel ridge model the alpha";"IRRE"
"parameter and the kernel parameters";"IRRE"
"fitting the model is now more computationally expensive since we have to try";"CODE"
"several combinations of hyperparameters we can have a look at the";"IRRE"
"hyperparameters found to get some intuitions";"IRRE"
"looking at the best parameters we see that they are different from the";"IRRE"
"defaults we also see that the periodicity is closer to the expected value";"CODE"
"math 2 pi we can now inspect the predictions of our tuned kernel ridge";"-"
"we get a much more accurate model we still observe some errors mainly due to";"TASK"
"the noise added to the dataset";"TASK"
"gaussian process regression";"-"
"now we will use a";"-"
"class sklearn gaussian process gaussianprocessregressor to fit the same";"IRRE"
"dataset when training a gaussian process the hyperparameters of the kernel";"IRRE"
"are optimized during the fitting process there is no need for an external";"CODE"
"hyperparameter search here we create a slightly more complex kernel than";"IRRE"
"for the kernel ridge regressor we add a";"TASK"
"class sklearn gaussian process kernels whitekernel that is used to";"IRRE"
"estimate the noise in the dataset";"IRRE"
"the computation cost of training a gaussian process is much less than the";"-"
"kernel ridge that uses a randomized search we can check the parameters of";"IRRE"
"the kernels that we computed";"-"
"indeed we see that the parameters have been optimized looking at the";"IRRE"
"periodicity parameter we see that we found a period close to the";"IRRE"
"theoretical value math 2 pi we can have a look now at the predictions of";"IRRE"
"our model";"-"
"plot the predictions of the kernel ridge";"-"
"plot the predictions of the gaussian process regressor";"-"
"we observe that the results of the kernel ridge and the gaussian process";"IRRE"
"regressor are close however the gaussian process regressor also provide";"CODE"
"an uncertainty information that is not available with a kernel ridge";"CODE"
"due to the probabilistic formulation of the target functions the";"CODE"
"gaussian process can output the standard deviation or the covariance";"IRRE"
"together with the mean predictions of the target functions";"CODE"
"however it comes at a cost the time to compute the predictions is higher";"-"
"with a gaussian process";"-"
"final conclusion";"CODE"
"we can give a final word regarding the possibility of the two models to";"CODE"
"extrapolate indeed we only provided the beginning of the signal as a";"-"
"training set using a periodic kernel forces our model to repeat the pattern";"IRRE"
"found on the training set using this kernel information together with the";"CODE"
"capacity of the both models to extrapolate we observe that the models will";"-"
"continue to predict the sine pattern";"CODE"
"gaussian process allows to combine kernels together thus we could associate";"-"
"the exponential sine squared kernel together with a radial basis function";"CODE"
"kernel";"-"
"plot the predictions of the kernel ridge";"-"
"plot the predictions of the gaussian process regressor";"-"
"the effect of using a radial basis function kernel will attenuate the";"CODE"
"periodicity effect once that no sample are available in the training";"CODE"
"as testing samples get further away from the training ones predictions";"IRRE"
"are converging towards their mean and their standard deviation";"-"
"also increases";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate data";"-"
"specify gaussian processes with fixed and optimized hyperparameters";"IRRE"
"plot posteriors";"-"
"plot lml landscape";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"import some data to play with";"CODE"
"x iris data 2 we only take the first two features";"TASK"
"h 0 02 step size in the mesh";"CODE"
"create a mesh to plot in";"IRRE"
"plot the predicted probabilities for that we will assign a color to";"IRRE"
"each point in the mesh x min m max x y min y max";"CODE"
"put the result into a color plot";"IRRE"
"plot also the training points";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"a few constants";"CODE"
"design of experiments";"-"
"observations";"-"
"instantiate and fit gaussian process model";"-"
"evaluate real function and the predicted probability";"CODE"
"plot the probabilistic classification iso values";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"fit the model";"-"
"plot the decision function for each datapoint on the grid";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"build the dataset";"IRRE"
"we will derive a dataset from the mauna loa observatory that collected air";"IRRE"
"samples we are interested in estimating the concentration of co2 and";"CODE"
"extrapolate it for further years first we load the original dataset available";"CODE"
"in openml as a pandas dataframe this will be replaced with polars";"CODE"
"once fetch openml adds a native support for it";"CODE"
"first we process the original dataframe to create a date column and select";"IRRE"
"it along with the co2 column";"-"
"we see that we get co2 concentration for some days from march 1958 to";"CODE"
"december 2001 we can plot the raw information to have a better";"CODE"
"understanding";"-"
"we will preprocess the dataset by taking a monthly average and drop months";"IRRE"
"for which no measurements were collected such a processing will have a";"CODE"
"smoothing effect on the data";"-"
"the idea in this example will be to predict the co2 concentration in function";"CODE"
"of the date we are as well interested in extrapolating for upcoming year";"CODE"
"after 2001";"-"
"as a first step we will divide the data and the target to estimate the data";"-"
"being a date we will convert it into a numeric";"CODE"
"design the proper kernel";"-"
"to design the kernel to use with our gaussian process we can make some";"-"
"assumption regarding the data at hand we observe that they have several";"-"
"characteristics we see a long term rising trend a pronounced seasonal";"CODE"
"variation and some smaller irregularities we can use different appropriate";"CODE"
"kernel that would capture these features";"TASK"
"first the long term rising trend could be fitted using a radial basis";"CODE"
"function rbf kernel with a large length scale parameter the rbf kernel";"IRRE"
"with a large length scale enforces this component to be smooth a trending";"CODE"
"increase is not enforced as to give a degree of freedom to our model the";"CODE"
"specific length scale and the amplitude are free hyperparameters";"IRRE"
"the seasonal variation is explained by the periodic exponential sine squared";"CODE"
"kernel with a fixed periodicity of 1 year the length scale of this periodic";"CODE"
"component controlling its smoothness is a free parameter in order to allow";"IRRE"
"decaying away from exact periodicity the product with an rbf kernel is";"CODE"
"taken the length scale of this rbf component controls the decay time and is";"CODE"
"a further free parameter this type of kernel is also known as locally";"IRRE"
"periodic kernel";"-"
"the small irregularities are to be explained by a rational quadratic kernel";"-"
"component whose length scale and alpha parameter which quantifies the";"IRRE"
"diffuseness of the length scales are to be determined a rational quadratic";"-"
"kernel is equivalent to an rbf kernel with several length scale and will";"-"
"better accommodate the different irregularities";"-"
"finally the noise in the dataset can be accounted with a kernel consisting";"CODE"
"of an rbf kernel contribution which shall explain the correlated noise";"META"
"components such as local weather phenomena and a white kernel contribution";"META"
"for the white noise the relative amplitudes and the rbf s length scale are";"IRRE"
"further free parameters";"IRRE"
"thus our final kernel is an addition of all previous kernel";"TASK"
"model fitting and extrapolation";"-"
"now we are ready to use a gaussian process regressor and fit the available";"IRRE"
"data to follow the example from the literature we will subtract the mean";"CODE"
"from the target we could have used normalize y true however doing so";"CODE"
"would have also scaled the target dividing y by its standard deviation";"-"
"thus the hyperparameters of the different kernel would have had different";"TASK"
"meaning since they would not have been expressed in ppm";"-"
"now we will use the gaussian process to predict on";"IRRE"
"training data to inspect the goodness of fit";"-"
"future data to see the extrapolation done by the model";"TASK"
"thus we create synthetic data from 1958 to the current month in addition";"TASK"
"we need to add the subtracted mean computed during training";"TASK"
"our fitted model is capable to fit previous data properly and extrapolate to";"CODE"
"future year with confidence";"TASK"
"interpretation of kernel hyperparameters";"IRRE"
"now we can have a look at the hyperparameters of the kernel";"IRRE"
"thus most of the target signal with the mean subtracted is explained by a";"-"
"long term rising trend for 45 ppm and a length scale of 52 years the";"CODE"
"periodic component has an amplitude of 2 6ppm a decay time of 90 years and";"-"
"a length scale of 1 5 the long decay time indicates that we have a";"IRRE"
"component very close to a seasonal periodicity the correlated noise has an";"IRRE"
"amplitude of 0 2 ppm with a length scale of 0 12 years and a white noise";"-"
"contribution of 0 04 ppm thus the overall noise level is very small";"META"
"indicating that the data can be very well explained by the model";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation";"-"
"we will work in a setting where x will contain a single feature we create a";"IRRE"
"function that will generate the target to be predicted we will add an";"TASK"
"option to add some noise to the generated target";"TASK"
"let s have a look to the target generator where we will not add any noise to";"TASK"
"observe the signal that we would like to predict";"-"
"the target is transforming the input x using a sine function now we will";"CODE"
"generate few noisy training samples to illustrate the noise level we will";"-"
"plot the true signal together with the noisy training samples";"-"
"optimisation of kernel hyperparameters in gpr";"IRRE"
"now we will create a";"IRRE"
"class sklearn gaussian process gaussianprocessregressor";"IRRE"
"using an additive kernel adding a";"TASK"
"class sklearn gaussian process kernels rbf and";"IRRE"
"class sklearn gaussian process kernels whitekernel kernels";"IRRE"
"the class sklearn gaussian process kernels whitekernel is a kernel that";"IRRE"
"will able to estimate the amount of noise present in the data while the";"CODE"
"class sklearn gaussian process kernels rbf will serve at fitting the";"IRRE"
"non linearity between the data and the target";"-"
"however we will show that the hyperparameter space contains several local";"IRRE"
"minima it will highlights the importance of initial hyperparameter values";"IRRE"
"we will create a model using a kernel with a high noise level and a large";"IRRE"
"length scale which will explain all variations in the data by noise";"CODE"
"we see that the optimum kernel found still has a high noise level and an even";"TASK"
"larger length scale the length scale reaches the maximum bound that we";"CODE"
"allowed for this parameter and we got a warning as a result";"IRRE"
"more importantly we observe that the model does not provide useful";"CODE"
"predictions the mean prediction seems to be constant it does not follow the";"CODE"
"expected noise free signal";"-"
"now we will initialize the class sklearn gaussian process kernels rbf";"IRRE"
"with a larger length scale initial value and the";"IRRE"
"class sklearn gaussian process kernels whitekernel with a smaller initial";"IRRE"
"noise level lower while keeping the parameter bounds unchanged";"IRRE"
"first we see that the model s predictions are more precise than the";"IRRE"
"previous model s this new model is able to estimate the noise free";"CODE"
"functional relationship";"CODE"
"looking at the kernel hyperparameters we see that the best combination found";"IRRE"
"has a smaller noise level and shorter length scale than the first model";"CODE"
"we can inspect the negative log marginal likelihood lml of";"-"
"class sklearn gaussian process gaussianprocessregressor";"IRRE"
"for different hyperparameters to get a sense of the local minima";"IRRE"
"we see that there are two local minima that correspond to the combination of";"-"
"hyperparameters previously found depending on the initial values for the";"IRRE"
"hyperparameters the gradient based optimization might or might not";"IRRE"
"converge to the best model it is thus important to repeat the optimization";"CODE"
"several times for different initializations this can be done by setting the";"CODE"
"n restarts optimizer parameter of the";"IRRE"
"class sklearn gaussian process gaussianprocessregressor class";"IRRE"
"let s try again to fit our model with the bad initial values but this time";"CODE"
"with 10 random restarts";"IRRE"
"as we hoped random restarts allow the optimization to find the best set";"IRRE"
"of hyperparameters despite the bad initial values";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset generation";"IRRE"
"we will start by generating a synthetic dataset the true generative process";"IRRE"
"is defined as math f x x sin x";"CODE"
"we will use this dataset in the next experiment to illustrate how gaussian";"IRRE"
"process regression is working";"-"
"example with noise free target";"-"
"in this first example we will use the true generative process without";"CODE"
"adding any noise for training the gaussian process regression we will only";"TASK"
"select few samples";"CODE"
"now we fit a gaussian process on these few training data samples we will";"-"
"use a radial basis function rbf kernel and a constant parameter to fit the";"CODE"
"amplitude";"-"
"after fitting our model we see that the hyperparameters of the kernel have";"IRRE"
"been optimized now we will use our kernel to compute the mean prediction";"-"
"of the full dataset and plot the 95 confidence interval";"IRRE"
"we see that for a prediction made on a data point close to the one from the";"CODE"
"training set the 95 confidence has a small amplitude whenever a sample";"IRRE"
"falls far from training data our model s prediction is less accurate and the";"CODE"
"model prediction is less precise higher uncertainty";"CODE"
"example with noisy targets";"-"
"we can repeat a similar experiment adding an additional noise to the target";"TASK"
"this time it will allow seeing the effect of the noise on the fitted model";"CODE"
"we add some random gaussian noise to the target with an arbitrary";"IRRE"
"standard deviation";"-"
"we create a similar gaussian process model in addition to the kernel this";"TASK"
"time we specify the parameter alpha which can be interpreted as the";"IRRE"
"variance of a gaussian noise";"CODE"
"let s plot the mean prediction and the uncertainty region as before";"CODE"
"the noise affects the predictions close to the training samples the";"IRRE"
"predictive uncertainty near to the training samples is larger because we";"META"
"explicitly model a given level target noise independent of the input";"CODE"
"variable";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"sequence similarity matrix under the kernel";"-"
"regression";"-"
"classification";"IRRE"
"whether there are a s in the sequence";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"helper function";"CODE"
"before presenting each individual kernel available for gaussian processes";"CODE"
"we will define a helper function allowing us plotting samples drawn from";"CODE"
"the gaussian process";"-"
"this function will take a";"CODE"
"class sklearn gaussian process gaussianprocessregressor model and will";"IRRE"
"drawn sample from the gaussian process if the model was not fit the samples";"CODE"
"are drawn from the prior distribution while after model fitting the samples are";"CODE"
"drawn from the posterior distribution";"META"
"label f sampled function idx 1";"CODE"
"dataset and gaussian process generation";"IRRE"
"we will create a training dataset that we will use in the different sections";"IRRE"
"kernel cookbook";"-"
"in this section we illustrate some samples drawn from the prior and posterior";"CODE"
"distributions of the gaussian process with different kernels";"META"
"radial basis function kernel";"CODE"
"plot prior";"-"
"plot posterior";"-"
"rational quadratic kernel";"-"
"plot prior";"-"
"plot posterior";"-"
"exp sine squared kernel";"-"
"plot prior";"-"
"plot posterior";"-"
"dot product kernel";"CODE"
"plot prior";"-"
"plot posterior";"-"
"mat rn kernel";"-"
"plot prior";"-"
"plot posterior";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"to use this experimental feature we need to explicitly ask for it";"TASK"
"from sklearn experimental import enable iterative imputer noqa f401";"CODE"
"2k samples is enough for the purpose of the example";"CODE"
"remove the following two lines for a slower run with different error bars";"CODE"
"we scale data before imputation and training a target estimator";"IRRE"
"because our target estimator and some of the imputers assume";"IRRE"
"that the features have similar scales";"TASK"
"estimate the score on the entire dataset with no missing values";"IRRE"
"add a single missing value to each row";"TASK"
"estimate the score after imputation mean and median strategies";"-"
"estimate the score after iterative imputation of the missing values";"IRRE"
"with different estimators";"IRRE"
"we tuned the hyperparameters of the randomforestregressor to get a good";"IRRE"
"enough predictive performance for a restricted execution time";"CODE"
"iterative imputer is sensitive to the tolerance and";"-"
"dependent on the estimator used internally";"CODE"
"we tuned the tolerance to keep this example run with limited computational";"CODE"
"resources while not changing the results too much compared to keeping the";"IRRE"
"stricter default value for the tolerance parameter";"IRRE"
"plot california housing results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"download the data and make missing values sets";"IRRE"
"first we download the two datasets diabetes dataset is shipped with";"IRRE"
"scikit learn it has 442 entries each with 10 features california housing";"TASK"
"dataset is much larger with 20640 entries and 8 features it needs to be";"TASK"
"downloaded we will only use the first 300 entries for the sake of speeding";"CODE"
"up the calculations but feel free to use the whole dataset";"IRRE"
"add missing values in 75 of the lines";"IRRE"
"impute the missing data and score";"-"
"now we will write a function which will score the results on the differently";"CODE"
"imputed data including the case of no imputation for full data";"CODE"
"we will use class sklearn ensemble randomforestregressor for the target";"CODE"
"regression";"-"
"to use the experimental iterativeimputer we need to explicitly ask for it";"TASK"
"from sklearn experimental import enable iterative imputer noqa f401";"CODE"
"estimate the score";"-"
"first we want to estimate the score on the original data";"-"
"replace missing values by 0";"IRRE"
"now we will estimate the score on the data where the missing values are";"IRRE"
"replaced by 0";"OUTD"
"impute missing values with mean";"IRRE"
"knn imputation of the missing values";"IRRE"
"class sklearn impute knnimputer imputes missing values using the weighted";"IRRE"
"or unweighted mean of the desired number of nearest neighbors if your features";"TASK"
"have vastly different scales as in the california housing dataset";"CODE"
"consider re scaling them to potentially improve performance";"CODE"
"iterative imputation of the missing values";"IRRE"
"another option is the class sklearn impute iterativeimputer this uses";"CODE"
"round robin regression modeling each feature with missing values as a";"IRRE"
"function of other features in turn we use the class s default choice";"CODE"
"of the regressor model class sklearn linear model bayesianridge";"IRRE"
"to predict missing feature values the performance of the predictor";"IRRE"
"may be negatively affected by vastly different scales of the features";"TASK"
"so we re scale the features in the california housing dataset";"CODE"
"plot the results";"IRRE"
"finally we are going to visualize the score";"CODE"
"plot diabetes results";"IRRE"
"plot california dataset results";"IRRE"
"you can also try different techniques for instance the median is a more";"CODE"
"robust estimator for data with high magnitude variables which could dominate";"CODE"
"results otherwise known as a long tail";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the dataset simulated hourly wages";"IRRE"
"the data generating process is laid out in the code below work experience in";"CODE"
"years and a measure of ability are drawn from normal distributions the";"META"
"hourly wage of one of the parents is drawn from beta distribution we then";"META"
"create an indicator of college degree which is positively impacted by ability";"IRRE"
"and parental hourly wage finally we model hourly wages as a linear function";"CODE"
"of all the previous variables and a random component note that all variables";"IRRE"
"have a positive effect on hourly wages";"-"
"description of the simulated data";"META"
"the following plot shows the distribution of each variable and pairwise";"IRRE"
"scatter plots key to our ovb story is the positive relationship between";"-"
"ability and college degree";"-"
"in the next section we train predictive models and we therefore split the";"CODE"
"target column from over features and we split the data into a training and a";"CODE"
"testing set";"IRRE"
"income prediction with fully observed variables";"IRRE"
"first we train a predictive model a";"-"
"class sklearn linear model linearregression model in this experiment";"CODE"
"we assume that all variables used by the true generative model are available";"IRRE"
"this model predicts well the hourly wages as shown by the high r2 score we";"CODE"
"plot the model coefficients to show that we exactly recover the values of";"IRRE"
"the true generative model";"-"
"income prediction with partial observations";"-"
"in practice intellectual abilities are not observed or are only estimated";"CODE"
"from proxies that inadvertently measure education as well e g by iq tests";"IRRE"
"but omitting the ability feature from a linear model inflates the estimate";"TASK"
"via a positive ovb";"-"
"the predictive power of our model is similar when we omit the ability feature";"TASK"
"in terms of r2 score we now check if the coefficient of the model are";"IRRE"
"different from the true generative model";"CODE"
"to compensate for the omitted variable the model inflates the coefficient of";"CODE"
"the college degree feature therefore interpreting this coefficient value";"CODE"
"as a causal effect of the true generative model is incorrect";"-"
"lessons learned";"-"
"machine learning models are not designed for the estimation of causal";"CODE"
"effects while we showed this with a linear model ovb can affect any type of";"CODE"
"model";"-"
"whenever interpreting a coefficient or a change in predictions brought about";"CODE"
"by a change in one of the features it is important to keep in mind";"CODE"
"potentially unobserved variables that could be correlated with both the";"IRRE"
"feature in question and the target variable such variables are called";"IRRE"
"confounding variables https en wikipedia org wiki confounding in";"CODE"
"order to still estimate causal effect in the presence of confounding";"TASK"
"researchers usually conduct experiments in which the treatment variable e g";"IRRE"
"college degree is randomized when an experiment is prohibitively expensive";"IRRE"
"or unethical researchers can sometimes use other causal inference techniques";"CODE"
"such as instrumental variables";"IRRE"
"https en wikipedia org wiki instrumental variables estimation iv";"CODE"
"estimations";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the dataset wages";"IRRE"
"we fetch the data from openml http openml org";"CODE"
"note that setting the parameter as frame to true will retrieve the data";"IRRE"
"as a pandas dataframe";"-"
"then we identify features x and target y the column wage is our";"TASK"
"target variable i e the variable which we want to predict";"IRRE"
"note that the dataset contains categorical and numerical variables";"IRRE"
"we will need to take this into account when preprocessing the dataset";"CODE"
"thereafter";"-"
"our target for prediction the wage";"CODE"
"wages are described as floating point number in dollars per hour";"CODE"
"we split the sample into a train and a test dataset";"IRRE"
"only the train dataset will be used in the following exploratory analysis";"IRRE"
"this is a way to emulate a real situation where predictions are performed on";"CODE"
"an unknown target and we don t want our analysis and decisions to be biased";"CODE"
"by our knowledge of the test data";"IRRE"
"first let s get some insights by looking at the variables distributions and";"CODE"
"at the pairwise relationships between them only numerical";"-"
"variables will be used in the following plot each dot represents a sample";"CODE"
"marginal dependencies";"CODE"
"looking closely at the wage distribution reveals that it has a";"META"
"long tail for this reason we should take its logarithm";"CODE"
"to turn it approximately into a normal distribution linear models such";"META"
"as ridge or lasso work best for a normal distribution of error";"META"
"the wage is increasing when education is increasing";"-"
"note that the dependence between wage and education";"TASK"
"represented here is a marginal dependence i e it describes the behavior";"CODE"
"of a specific variable without keeping the others fixed";"IRRE"
"also the experience and age are strongly linearly correlated";"-"
"the pipeline";"CODE"
"the machine learning pipeline";"CODE"
"to design our machine learning pipeline we first manually";"CODE"
"check the type of data that we are dealing with";"-"
"as seen previously the dataset contains columns with different data types";"IRRE"
"and we need to apply a specific preprocessing for each data types";"CODE"
"in particular categorical variables cannot be included in linear model if not";"CODE"
"coded as integers first in addition to avoid categorical features to be";"TASK"
"treated as ordered values we need to one hot encode them";"IRRE"
"our pre processor will";"-"
"one hot encode i e generate a column by category the categorical";"-"
"columns only for non binary categorical variables";"CODE"
"as a first approach we will see after how the normalisation of numerical";"-"
"values will affect our discussion keep numerical values as they are";"IRRE"
"verbose feature names out false avoid to prepend the preprocessor names";"CODE"
"we use a ridge regressor";"-"
"with a very small regularization to model the logarithm of the wage";"-"
"processing the dataset";"IRRE"
"first we fit the model";"-"
"then we check the performance of the computed model by plotting its predictions";"CODE"
"against the actual values on the test set and by computing";"IRRE"
"the median absolute error";"-"
"the model learnt is far from being a good model making accurate predictions";"CODE"
"this is obvious when looking at the plot above where good predictions";"IRRE"
"should lie on the black dashed line";"-"
"in the following section we will interpret the coefficients of the model";"CODE"
"while we do so we should keep in mind that any conclusion we draw is";"CODE"
"about the model that we build rather than about the true real world";"-"
"generative process of the data";"-"
"interpreting coefficients scale matters";"CODE"
"first of all we can take a look to the values of the coefficients of the";"IRRE"
"regressor we have fitted";"-"
"the age coefficient is expressed in dollars hour per living years while the";"CODE"
"education one is expressed in dollars hour per years of education this";"CODE"
"representation of the coefficients has the benefit of making clear the";"-"
"practical predictions of the model an increase of math 1 year in age";"-"
"means a decrease of math 0 030867 dollars hour while an increase of";"CODE"
"math 1 year in education means an increase of math 0 054699";"-"
"dollars hour on the other hand categorical variables as union or sex are";"CODE"
"adimensional numbers taking either the value 0 or 1 their coefficients";"IRRE"
"are expressed in dollars hour then we cannot compare the magnitude of";"IRRE"
"different coefficients since the features have different natural scales and";"TASK"
"hence value ranges because of their different unit of measure this is more";"IRRE"
"visible if we plot the coefficients";"-"
"indeed from the plot above the most important factor in determining wage";"CODE"
"appears to be the";"-"
"variable union even if our intuition might tell us that variables";"CODE"
"like experience should have more impact";"-"
"looking at the coefficient plot to gauge feature importance can be";"CODE"
"misleading as some of them vary on a small scale while others like age";"CODE"
"varies a lot more several decades";"CODE"
"this is visible if we compare the standard deviations of different";"IRRE"
"features";"TASK"
"multiplying the coefficients by the standard deviation of the related";"-"
"feature would reduce all the coefficients to the same unit of measure";"TASK"
"as we will see ref after scaling num this is equivalent to normalize";"CODE"
"numerical variables to their standard deviation";"IRRE"
"as math y sum coef i times x i";"-"
"sum coef i times std i times x i std i";"CODE"
"in that way we emphasize that the";"CODE"
"greater the variance of a feature the larger the weight of the corresponding";"TASK"
"coefficient on the output all else being equal";"IRRE"
"now that the coefficients have been scaled we can safely compare them";"IRRE"
"note";"TASK"
"why does the plot above suggest that an increase in age leads to a";"CODE"
"decrease in wage why is the ref initial pairplot";"IRRE"
"marginal dependencies telling the opposite";"CODE"
"this difference is the difference between marginal and conditional dependence";"CODE"
"the plot above tells us about dependencies between a specific feature and";"TASK"
"the target when all other features remain constant i e conditional";"CODE"
"dependencies an increase of the age will induce a decrease";"CODE"
"of the wage when all other features remain constant on the contrary an";"CODE"
"increase of the experience will induce an increase of the wage when all";"-"
"other features remain constant";"CODE"
"also age experience and education are the three variables that most";"IRRE"
"influence the model";"-"
"interpreting coefficients being cautious about causality";"CODE"
"linear models are a great tool for measuring statistical association but we";"META"
"should be cautious when making statements about causality after all";"-"
"correlation doesn t always imply causation this is particularly difficult in";"CODE"
"the social sciences because the variables we observe only function as proxies";"IRRE"
"for the underlying causal process";"CODE"
"in our particular case we can think of the education of an individual as a";"CODE"
"proxy for their professional aptitude the real variable we re interested in";"CODE"
"but can t observe we d certainly like to think that staying in school for";"META"
"longer would increase technical competency but it s also quite possible that";"IRRE"
"causality goes the other way too that is those who are technically";"IRRE"
"competent tend to stay in school for longer";"CODE"
"an employer is unlikely to care which case it is or if it s a mix of both";"CODE"
"as long as they remain convinced that a person with more education is better";"CODE"
"suited for the job they will be happy to pay out a higher wage";"CODE"
"this confounding of effects becomes problematic when thinking about some";"CODE"
"form of intervention e g government subsidies of university degrees or";"CODE"
"promotional material encouraging individuals to take up higher education";"-"
"the usefulness of these measures could end up being overstated especially if";"CODE"
"the degree of confounding is strong our model predicts a math 0 054699";"-"
"increase in hourly wage for each year of education the actual causal effect";"CODE"
"might be lower because of this confounding";"CODE"
"checking the variability of the coefficients";"CODE"
"we can check the coefficient variability through cross validation";"CODE"
"it is a form of data perturbation related to";"CODE"
"resampling https en wikipedia org wiki resampling statistics";"CODE"
"if coefficients vary significantly when changing the input dataset";"CODE"
"their robustness is not guaranteed and they should probably be interpreted";"CODE"
"with caution";"-"
"the problem of correlated variables";"IRRE"
"the age and experience coefficients are affected by strong variability which";"CODE"
"might be due to the collinearity between the 2 features as age and";"TASK"
"experience vary together in the data their effect is difficult to tease";"CODE"
"apart";"-"
"to verify this interpretation we plot the variability of the age and";"CODE"
"experience coefficient";"-"
"covariation";"CODE"
"two regions are populated when the experience coefficient is";"-"
"positive the age one is negative and vice versa";"-"
"to go further we remove one of the two features age and check what is the impact";"TASK"
"on the model stability";"-"
"the estimation of the experience coefficient now shows a much reduced";"-"
"variability experience remains important for all models trained during";"CODE"
"cross validation";"-"
"scaling num";"-"
"preprocessing numerical variables";"IRRE"
"as said above see ref the pipeline we could also choose to scale";"CODE"
"numerical values before training the model";"IRRE"
"this can be useful when we apply a similar amount of regularization to all of them";"CODE"
"in the ridge";"CODE"
"the preprocessor is redefined in order to subtract the mean and scale";"CODE"
"variables to unit variance";"CODE"
"the model will stay unchanged";"-"
"again we check the performance of the computed";"CODE"
"model using the median absolute error";"-"
"for the coefficient analysis scaling is not needed this time because it";"CODE"
"was performed during the preprocessing step";"CODE"
"we now inspect the coefficients across several cross validation folds";"-"
"the result is quite similar to the non normalized case";"IRRE"
"linear models with regularization";"-"
"in machine learning practice ridge regression is more often used with";"-"
"non negligible regularization";"-"
"above we limited this regularization to a very little amount regularization";"CODE"
"improves the conditioning of the problem and reduces the variance of the";"CODE"
"estimates class sklearn linear model ridgecv applies cross validation";"IRRE"
"in order to determine which value of the regularization parameter alpha";"IRRE"
"is best suited for prediction";"CODE"
"alphas np logspace 10 10 21 alpha values to be chosen from by cross validation";"IRRE"
"first we check which value of math alpha has been selected";"IRRE"
"then we check the quality of the predictions";"-"
"the ability to reproduce the data of the regularized model is similar to";"-"
"the one of the non regularized model";"-"
"the coefficients are significantly different";"-"
"age and experience coefficients are both positive but they now have less";"META"
"influence on the prediction";"-"
"the regularization reduces the influence of correlated";"-"
"variables on the model because the weight is shared between the two";"IRRE"
"predictive variables so neither alone would have strong weights";"CODE"
"on the other hand the weights obtained with regularization are more";"-"
"stable see the ref ridge regression user guide section this";"CODE"
"increased stability is visible from the plot obtained from data";"CODE"
"perturbations in a cross validation this plot can be compared with";"IRRE"
"the ref previous one covariation";"CODE"
"linear models with sparse coefficients";"IRRE"
"another possibility to take into account correlated variables in the dataset";"CODE"
"is to estimate sparse coefficients in some way we already did it manually";"IRRE"
"when we dropped the age column in a previous ridge estimation";"-"
"lasso models see the ref lasso user guide section estimates sparse";"IRRE"
"coefficients class sklearn linear model lassocv applies cross";"IRRE"
"validation in order to determine which value of the regularization parameter";"IRRE"
"alpha is best suited for the model estimation";"CODE"
"alphas np logspace 10 10 21 alpha values to be chosen from by cross validation";"IRRE"
"first we verify which value of math alpha has been selected";"IRRE"
"then we check the quality of the predictions";"-"
"for our dataset again the model is not very predictive";"CODE"
"a lasso model identifies the correlation between";"-"
"age and experience and suppresses one of them for the sake of the prediction";"CODE"
"it is important to keep in mind that the coefficients that have been";"CODE"
"dropped may still be related to the outcome by themselves the model";"TASK"
"chose to suppress them because they bring little or no additional";"IRRE"
"information on top of the other features additionally this selection";"CODE"
"is unstable for correlated features and should be interpreted with";"CODE"
"caution";"-"
"indeed we can check the variability of the coefficients across folds";"CODE"
"we observe that the age and experience coefficients are varying a lot";"CODE"
"depending of the fold";"TASK"
"wrong causal interpretation";"META"
"policy makers might want to know the effect of education on wage to assess";"-"
"whether or not a certain policy designed to entice people to pursue more";"CODE"
"education would make economic sense while machine learning models are great";"CODE"
"for measuring statistical associations they are generally unable to infer";"CODE"
"causal effects";"-"
"it might be tempting to look at the coefficient of education on wage from our";"CODE"
"last model or any model for that matter and conclude that it captures the";"IRRE"
"true effect of a change in the standardized education variable on wages";"CODE"
"unfortunately there are likely unobserved confounding variables that either";"CODE"
"inflate or deflate that coefficient a confounding variable is a variable that";"CODE"
"causes both education and wage one example of such variable is ability";"IRRE"
"presumably more able people are more likely to pursue education while at the";"CODE"
"same time being more likely to earn a higher hourly wage at any level of";"-"
"education in this case ability induces a positive omitted variable bias";"CODE"
"https en wikipedia org wiki omitted variable bias ovb on the education";"CODE"
"coefficient thereby exaggerating the effect of education on wages";"-"
"see the ref sphx glr auto examples inspection plot causal interpretation py";"CODE"
"for a simulated case of ability ovb";"CODE"
"lessons learned";"-"
"coefficients must be scaled to the same unit of measure to retrieve";"TASK"
"feature importance scaling them with the standard deviation of the";"CODE"
"feature is a useful proxy";"TASK"
"coefficients in multivariate linear models represent the dependency";"CODE"
"between a given feature and the target conditional on the other";"TASK"
"features";"TASK"
"correlated features induce instabilities in the coefficients of linear";"TASK"
"models and their effects cannot be well teased apart";"-"
"different linear models respond differently to feature correlation and";"TASK"
"coefficients could significantly vary from one another";"CODE"
"inspecting coefficients across the folds of a cross validation loop";"IRRE"
"gives an idea of their stability";"-"
"interpreting causality is difficult when there are confounding effects if";"CODE"
"the relationship between two variables is also affected by something";"IRRE"
"unobserved we should be careful when making conclusions about causality";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"bike sharing dataset preprocessing";"IRRE"
"we will use the bike sharing dataset the goal is to predict the number of bike";"IRRE"
"rentals using weather and season data as well as the datetime information";"IRRE"
"make an explicit copy to avoid settingwithcopywarning from pandas";"CODE"
"we use only a subset of the data to speed up the example";"IRRE"
"the feature weather has a particularity the category heavy rain is a rare";"TASK"
"category";"-"
"because of this rare category we collapse it into rain";"CODE"
"we now have a closer look at the year feature";"TASK"
"we see that we have data from two years we use the first year to train the";"CODE"
"model and the second year to test the model";"IRRE"
"we can check the dataset information to see that we have heterogeneous data types we";"IRRE"
"have to preprocess the different columns accordingly";"-"
"from the previous information we will consider the category columns as nominal";"CODE"
"categorical features in addition we will consider the date and time information as";"TASK"
"categorical features as well";"TASK"
"we manually define the columns containing numerical and categorical";"CODE"
"features";"TASK"
"before we go into the details regarding the preprocessing of the different machine";"CODE"
"learning pipelines we will try to get some additional intuition regarding the dataset";"CODE"
"that will be helpful to understand the model s statistical performance and results of";"IRRE"
"the partial dependence analysis";"CODE"
"we plot the average number of bike rentals by grouping the data by season and";"-"
"by year";"-"
"decorate the plot";"-"
"the first striking difference between the train and test set is that the number of";"IRRE"
"bike rentals is higher in the test set for this reason it will not be surprising to";"CODE"
"get a machine learning model that underestimates the number of bike rentals we";"IRRE"
"also observe that the number of bike rentals is lower during the spring season in";"-"
"addition we see that during working days there is a specific pattern around 6 7";"TASK"
"am and 5 6 pm with some peaks of bike rentals we can keep in mind these different";"-"
"insights and use them to understand the partial dependence plot";"IRRE"
"preprocessor for machine learning models";"CODE"
"since we later use two different models a";"IRRE"
"class sklearn neural network mlpregressor and a";"IRRE"
"class sklearn ensemble histgradientboostingregressor we create two different";"IRRE"
"preprocessors specific for each model";"CODE"
"preprocessor for the neural network model";"CODE"
"we will use a class sklearn preprocessing quantiletransformer to scale the";"CODE"
"numerical features and encode the categorical features with a";"TASK"
"class sklearn preprocessing onehotencoder";"IRRE"
"preprocessor for the gradient boosting model";"CODE"
"for the gradient boosting model we leave the numerical features as is and only";"CODE"
"encode the categorical features using a";"TASK"
"class sklearn preprocessing ordinalencoder";"IRRE"
"1 way partial dependence with different models";"CODE"
"in this section we will compute 1 way partial dependence with two different";"CODE"
"machine learning models i a multi layer perceptron and ii a";"-"
"gradient boosting model with these two models we illustrate how to compute and";"IRRE"
"interpret both partial dependence plot pdp for both numerical and categorical";"CODE"
"features and individual conditional expectation ice";"TASK"
"multi layer perceptron";"-"
"let s fit a class sklearn neural network mlpregressor and compute";"IRRE"
"single variable partial dependence plots";"CODE"
"we configured a pipeline using the preprocessor that we created specifically for the";"CODE"
"neural network and tuned the neural network size and learning rate to get a reasonable";"-"
"compromise between training time and predictive performance on a test set";"IRRE"
"importantly this tabular dataset has very different dynamic ranges for its";"CODE"
"features neural networks tend to be very sensitive to features with varying";"TASK"
"scales and forgetting to preprocess the numeric feature would lead to a very";"TASK"
"poor model";"-"
"it would be possible to get even higher predictive performance with a larger";"CODE"
"neural network but the training would also be significantly more expensive";"META"
"note that it is important to check that the model is accurate enough on a";"CODE"
"test set before plotting the partial dependence since there would be little";"IRRE"
"use in explaining the impact of a given feature on the prediction function of";"TASK"
"a model with poor predictive performance in this regard our mlp model works";"CODE"
"reasonably well";"-"
"we will plot the averaged partial dependence";"CODE"
"features of interest";"TASK"
"type of partial dependence plot";"CODE"
"information regarding categorical features";"TASK"
"gradient boosting";"-"
"let s now fit a class sklearn ensemble histgradientboostingregressor and";"IRRE"
"compute the partial dependence on the same features we also use the";"TASK"
"specific preprocessor we created for this model";"CODE"
"here we used the default hyperparameters for the gradient boosting model";"CODE"
"without any preprocessing as tree based models are naturally robust to";"-"
"monotonic transformations of numerical features";"TASK"
"note that on this tabular dataset gradient boosting machines are both";"TASK"
"significantly faster to train and more accurate than neural networks it is";"-"
"also significantly cheaper to tune their hyperparameters the defaults tend";"CODE"
"to work well while this is not often the case for neural networks";"CODE"
"we will plot the partial dependence for some of the numerical and categorical";"CODE"
"features";"TASK"
"analysis of the plots";"-"
"we will first look at the pdps for the numerical features for both models the";"CODE"
"general trend of the pdp of the temperature is that the number of bike rentals is";"CODE"
"increasing with temperature we can make a similar analysis but with the opposite";"META"
"trend for the humidity features the number of bike rentals is decreasing when the";"CODE"
"humidity increases finally we see the same trend for the wind speed feature the";"CODE"
"number of bike rentals is decreasing when the wind speed is increasing for both";"CODE"
"models we also observe that class sklearn neural network mlpregressor has much";"IRRE"
"smoother predictions than class sklearn ensemble histgradientboostingregressor";"IRRE"
"now we will look at the partial dependence plots for the categorical features";"CODE"
"we observe that the spring season is the lowest bar for the season feature with the";"TASK"
"weather feature the rain category is the lowest bar regarding the hour feature";"TASK"
"we see two peaks around the 7 am and 6 pm these findings are in line with the";"CODE"
"the observations we made earlier on the dataset";"IRRE"
"however it is worth noting that we are creating potential meaningless";"-"
"synthetic samples if features are correlated";"TASK"
"ice vs pdp";"-"
"ice vs pdp";"-"
"pdp is an average of the marginal effects of the features we are averaging the";"TASK"
"response of all samples of the provided set thus some effects could be hidden in";"IRRE"
"this regard it is possible to plot each individual response this representation is";"CODE"
"called the individual effect plot ice in the plot below we plot 50 randomly";"IRRE"
"selected ices for the temperature and humidity features";"CODE"
"we see that the ice for the temperature feature gives us some additional information";"TASK"
"some of the ice lines are flat while some others show a decrease of the dependence";"CODE"
"for temperature above 35 degrees celsius we observe a similar pattern for the";"CODE"
"humidity feature some of the ices lines show a sharp decrease when the humidity is";"TASK"
"above 80";"-"
"not all ice lines are parallel this indicates that the model finds";"IRRE"
"interactions between features we can repeat the experiment by constraining the";"CODE"
"gradient boosting model to not use any interactions between features using the";"TASK"
"parameter interaction cst";"IRRE"
"2d interaction plots";"CODE"
"pdps with two features of interest enable us to visualize interactions among them";"CODE"
"however ices cannot be plotted in an easy manner and thus interpreted we will show";"IRRE"
"the representation of available in";"-"
"meth sklearn inspection partialdependencedisplay from estimator that is a 2d";"CODE"
"heatmap";"-"
"the two way partial dependence plot shows the dependence of the number of bike rentals";"CODE"
"on joint values of temperature and humidity";"IRRE"
"we clearly see an interaction between the two features for a temperature higher than";"CODE"
"20 degrees celsius the humidity has an impact on the number of bike rentals";"-"
"that seems independent on the temperature";"CODE"
"on the other hand for temperatures lower than 20 degrees celsius both the";"CODE"
"temperature and humidity continuously impact the number of bike rentals";"-"
"furthermore the slope of the of the impact ridge of the 20 degrees celsius";"-"
"threshold is very dependent on the humidity level the ridge is steep under";"CODE"
"dry conditions but much smoother under wetter conditions above 70 of humidity";"META"
"we now contrast those results with the same plots computed for the model";"IRRE"
"constrained to learn a prediction function that does not depend on such";"CODE"
"non linear feature interactions";"TASK"
"the 1d partial dependence plots for the model constrained to not model feature";"CODE"
"interactions show local spikes for each features individually in particular for";"CODE"
"for the humidity feature those spikes might be reflecting a degraded behavior";"TASK"
"of the model that attempts to somehow compensate for the forbidden interactions";"CODE"
"by overfitting particular training points note that the predictive performance";"CODE"
"of this model as measured on the test set is significantly worse than that of";"IRRE"
"the original unconstrained model";"CODE"
"also note that the number of local spikes visible on those plots is depends on";"TASK"
"the grid resolution parameter of the pd plot itself";"IRRE"
"those local spikes result in a noisily gridded 2d pd plot it is quite";"IRRE"
"challenging to tell whether or not there are no interaction between those";"CODE"
"features because of the high frequency oscillations in the humidity feature";"TASK"
"however it can clearly be seen that the simple interaction effect observed when";"CODE"
"the temperature crosses the 20 degrees boundary is no longer visible for this";"CODE"
"model";"-"
"the partial dependence between categorical features will provide a discrete";"TASK"
"representation that can be shown as a heatmap for instance the interaction between";"CODE"
"the season the weather and the target would be as follow";"IRRE"
"3d representation";"-"
"let s make the same partial dependence plot for the 2 features interaction";"CODE"
"this time in 3 dimensions";"CODE"
"unused but required import for doing 3d projections with matplotlib 3 2";"CODE"
"import mpl toolkits mplot3d noqa f401";"CODE"
"pretty init view";"IRRE"
"plt partial dependence custom values";"IRRE"
"custom inspection points";"CODE"
"none of the examples so far specify which points are evaluated to create the";"IRRE"
"partial dependence plots by default we use percentiles defined by the input dataset";"CODE"
"in some cases it can be helpful to specify the exact points where you would like the";"CODE"
"model evaluated for instance if a user wants to test the model behavior on";"IRRE"
"out of distribution data or compare two models that were fit on slightly different";"IRRE"
"data the custom values parameter allows the user to pass in the values that they";"IRRE"
"want the model to be evaluated on this overrides the grid resolution and";"CODE"
"percentiles parameters let s return to our gradient boosting example above";"IRRE"
"but with custom values";"IRRE"
"we set custom values for temp feature";"IRRE"
"all other features are evaluated based on the data";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data loading and feature engineering";"TASK"
"let s use pandas to load a copy of the titanic dataset the following shows";"CODE"
"how to apply separate preprocessing on numerical and categorical features";"TASK"
"we further include two random variables that are not correlated in any way";"CODE"
"with the target variable survived";"IRRE"
"random num is a high cardinality numerical variable as many unique";"IRRE"
"values as records";"IRRE"
"random cat is a low cardinality categorical variable 3 possible";"IRRE"
"values";"IRRE"
"we define a predictive model based on a random forest therefore we will make";"CODE"
"the following preprocessing steps";"-"
"use class sklearn preprocessing ordinalencoder to encode the";"IRRE"
"categorical features";"TASK"
"use class sklearn impute simpleimputer to fill missing values for";"IRRE"
"numerical features using a mean strategy";"TASK"
"accuracy of the model";"-"
"before inspecting the feature importances it is important to check that";"CODE"
"the model predictive performance is high enough indeed there would be little";"CODE"
"interest in inspecting the important features of a non predictive model";"CODE"
"here one can observe that the train accuracy is very high the forest model";"CODE"
"has enough capacity to completely memorize the training set but it can still";"TASK"
"generalize well enough to the test set thanks to the built in bagging of";"IRRE"
"random forests";"IRRE"
"it might be possible to trade some accuracy on the training set for a";"CODE"
"slightly better accuracy on the test set by limiting the capacity of the";"IRRE"
"trees for instance by setting min samples leaf 5 or";"CODE"
"min samples leaf 10 so as to limit overfitting while not introducing too";"CODE"
"much underfitting";"-"
"however let us keep our high capacity random forest model for now so that we can";"CODE"
"illustrate some pitfalls about feature importance on variables with many";"CODE"
"unique values";"IRRE"
"tree s feature importance from mean decrease in impurity mdi";"CODE"
"the impurity based feature importance ranks the numerical features to be the";"TASK"
"most important features as a result the non predictive random num";"IRRE"
"variable is ranked as one of the most important features";"CODE"
"this problem stems from two limitations of impurity based feature";"CODE"
"importances";"CODE"
"impurity based importances are biased towards high cardinality features";"CODE"
"impurity based importances are computed on training set statistics and";"CODE"
"therefore do not reflect the ability of feature to be useful to make";"CODE"
"predictions that generalize to the test set when the model has enough";"IRRE"
"capacity";"-"
"the bias towards high cardinality features explains why the random num has";"IRRE"
"a really large importance in comparison with random cat while we would";"CODE"
"expect that both random features have a null importance";"CODE"
"the fact that we use training set statistics explains why both the";"IRRE"
"random num and random cat features have a non null importance";"IRRE"
"as an alternative the permutation importances of rf are computed on a";"CODE"
"held out test set this shows that the low cardinality categorical feature";"IRRE"
"sex and pclass are the most important features indeed permuting the";"CODE"
"values of these features will lead to the most decrease in accuracy score of the";"IRRE"
"model on the test set";"IRRE"
"also note that both random features have very low importances close to 0 as";"CODE"
"expected";"-"
"it is also possible to compute the permutation importances on the training";"CODE"
"set this reveals that random num and random cat get a significantly";"IRRE"
"higher importance ranking than when computed on the test set the difference";"CODE"
"between those two plots is a confirmation that the rf model has enough";"IRRE"
"capacity to use that random numerical and categorical features to overfit";"IRRE"
"we can further retry the experiment by limiting the capacity of the trees";"CODE"
"to overfit by setting min samples leaf at 20 data points";"IRRE"
"observing the accuracy score on the training and testing set we observe that";"IRRE"
"the two metrics are very similar now therefore our model is not overfitting";"CODE"
"anymore we can then check the permutation importances with this new model";"CODE"
"now we can observe that on both sets the random num and random cat";"IRRE"
"features have a lower importance compared to the overfitting random forest";"CODE"
"however the conclusions regarding the importance of the other features are";"CODE"
"still valid";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"random forest feature importance on breast cancer data";"CODE"
"first we define a function to ease the plotting";"CODE"
"labels argument in boxplot is deprecated in matplotlib 3 9 and has been";"OUTD"
"renamed to tick labels the following code handles this but as a";"META"
"scikit learn user you probably can write simpler code by using labels";"TASK"
"matplotlib 3 9 or tick labels matplotlib 3 9";"-"
"we then train a class sklearn ensemble randomforestclassifier on the";"IRRE"
"ref breast cancer dataset and evaluate its accuracy on a test set";"IRRE"
"next we plot the tree based feature importance and the permutation";"CODE"
"importance the permutation importance is calculated on the training set to";"CODE"
"show how much the model relies on each feature during training";"TASK"
"the plot on the left shows the gini importance of the model as the";"CODE"
"scikit learn implementation of";"TASK"
"class sklearn ensemble randomforestclassifier uses a random subsets of";"IRRE"
"math sqrt n text features features at each split it is able to dilute";"TASK"
"the dominance of any single correlated feature as a result the individual";"TASK"
"feature importance may be distributed more evenly among the correlated";"CODE"
"features since the features have large cardinality and the classifier is";"TASK"
"non overfitted we can relatively trust those values";"IRRE"
"the permutation importance on the right plot shows that permuting a feature";"CODE"
"drops the accuracy by at most 0 012 which would suggest that none of the";"-"
"features are important this is in contradiction with the high test accuracy";"CODE"
"computed as baseline some feature must be important";"TASK"
"similarly the change in accuracy score computed on the test set appears to be";"IRRE"
"driven by chance";"-"
"nevertheless one can still compute a meaningful permutation importance in the";"CODE"
"presence of correlated features as demonstrated in the following section";"TASK"
"handling multicollinear features";"TASK"
"when features are collinear permuting one feature has little effect on the";"TASK"
"models performance because it can get the same information from a correlated";"CODE"
"feature note that this is not the case for all predictive models and depends";"CODE"
"on their underlying implementation";"TASK"
"one way to handle multicollinear features is by performing hierarchical";"CODE"
"clustering on the spearman rank order correlations picking a threshold and";"-"
"keeping a single feature from each cluster first we plot a heatmap of the";"TASK"
"correlated features";"TASK"
"ensure the correlation matrix is symmetric";"-"
"we convert the correlation matrix to a distance matrix before performing";"CODE"
"hierarchical clustering using ward s linkage";"-"
"next we manually pick a threshold by visual inspection of the dendrogram to";"CODE"
"group our features into clusters and choose a feature from each cluster to";"TASK"
"keep select those features from our dataset and train a new random forest";"CODE"
"the test accuracy of the new random forest did not change much compared to the";"IRRE"
"random forest trained on the complete dataset";"CODE"
"we can finally explore the permutation importance of the selected subset of";"CODE"
"features";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"preparing the data";"-"
"load the covtype dataset which contains 581 012 samples";"IRRE"
"with 54 features each distributed among 6 classes the goal of this dataset";"IRRE"
"is to predict forest cover type from cartographic variables only";"CODE"
"no remotely sensed data after loading we transform it into a binary";"CODE"
"classification problem to match the version of the dataset in the";"IRRE"
"libsvm webpage 2 which was the one used in 1";"-"
"y y 2 1 we will try to separate class 2 from the other 6 classes";"CODE"
"partitioning the data";"-"
"here we select 5 000 samples for training and 10 000 for testing";"CODE"
"to actually reproduce the results in the original tensor sketch paper";"IRRE"
"select 100 000 for training";"CODE"
"feature normalization";"TASK"
"now scale features to the range 0 1 to match the format of the dataset in";"TASK"
"the libsvm webpage and then normalize to unit length as done in the";"CODE"
"original tensor sketch paper 1";"-"
"establishing a baseline model";"-"
"as a baseline train a linear svm on the original features and print the";"CODE"
"accuracy we also measure and store accuracies and training times to";"-"
"plot them later";"-"
"establishing the kernel approximation model";"-"
"then we train linear svms on the features generated by";"TASK"
"class polynomialcountsketch with different values for n components";"IRRE"
"showing that these kernel feature approximations improve the accuracy";"TASK"
"of linear classification in typical application scenarios n components";"CODE"
"should be larger than the number of features in the input representation";"CODE"
"in order to achieve an improvement with respect to linear classification";"TASK"
"as a rule of thumb the optimum of evaluation score run time cost is";"CODE"
"typically achieved at around n components 10 n features though this";"TASK"
"might depend on the specific dataset being handled note that since the";"CODE"
"original samples have 54 features the explicit feature map of the";"TASK"
"polynomial kernel of degree four would have approximately 8 5 million";"-"
"features precisely 54 4 thanks to class polynomialcountsketch we can";"TASK"
"condense most of the discriminative information of that feature space into a";"CODE"
"much more compact representation while we run the experiment only a single time";"CODE"
"n runs 1 in this example in practice one should repeat the experiment several";"CODE"
"times to compensate for the stochastic nature of class polynomialcountsketch";"CODE"
"establishing the kernelized svm model";"-"
"train a kernelized svm to see how well class polynomialcountsketch";"IRRE"
"is approximating the performance of the kernel this of course may take";"CODE"
"some time as the svc class has a relatively poor scalability this is the";"CODE"
"reason why kernel approximators are so useful";"-"
"comparing the results";"IRRE"
"finally plot the results of the different methods against their training";"CODE"
"times as we can see the kernelized svm achieves a higher accuracy";"-"
"but its training time is much larger and most importantly will grow";"CODE"
"much faster if the number of training samples increases";"-"
"references";"CODE"
"1 pham ninh and rasmus pagh fast and scalable polynomial kernels via";"-"
"explicit feature maps kdd 13 2013";"TASK"
"https doi org 10 1145 2487575 2487591";"CODE"
"2 libsvm binary datasets repository";"IRRE"
"https www csie ntu edu tw cjlin libsvmtools datasets binary html";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"models robustness to recover the ground truth weights";"-"
"generate synthetic dataset";"IRRE"
"we generate a dataset where x and y are linearly linked 10 of the";"IRRE"
"features of x will be used to generate y the other features are not";"TASK"
"useful at predicting y in addition we generate a dataset where n samples";"TASK"
"n features such a setting is challenging for an ols model and leads";"TASK"
"potentially to arbitrary large weights having a prior on the weights and a";"-"
"penalty alleviates the problem finally gaussian noise is added";"CODE"
"fit the regressors";"-"
"we now fit both bayesian models and the ols to later compare the models";"IRRE"
"coefficients";"-"
"plot the true and estimated coefficients";"-"
"now we compare the coefficients of each model with the weights of";"IRRE"
"the true generative model";"-"
"due to the added noise none of the models recover the true weights indeed";"TASK"
"all models always have more than 10 non zero coefficients compared to the ols";"IRRE"
"estimator the coefficients using a bayesian ridge regression are slightly";"-"
"shifted toward zero which stabilises them the ard regression provides a";"-"
"sparser solution some of the non informative coefficients are set exactly to";"IRRE"
"zero while shifting others closer to zero some non informative coefficients";"CODE"
"are still present and retain large values";"IRRE"
"plot the marginal log likelihood";"-"
"indeed both models minimize the log likelihood up to an arbitrary cutoff";"-"
"defined by the max iter parameter";"IRRE"
"bayesian regressions with polynomial feature expansion";"TASK"
"generate synthetic dataset";"IRRE"
"we create a target that is a non linear function of the input feature";"CODE"
"noise following a standard uniform distribution is added";"TASK"
"sort the data to make plotting easier later";"-"
"extrapolation";"-"
"fit the regressors";"-"
"here we try a degree 10 polynomial to potentially overfit though the bayesian";"CODE"
"linear models regularize the size of the polynomial coefficients as";"-"
"fit intercept true by default for";"CODE"
"class sklearn linear model ardregression and";"IRRE"
"class sklearn linear model bayesianridge then";"IRRE"
"class sklearn preprocessing polynomialfeatures should not introduce an";"CODE"
"additional bias feature by setting return std true the bayesian regressors";"TASK"
"return the standard deviation of the posterior distribution for the model";"CODE"
"parameters";"IRRE"
"plotting polynomial regressions with std errors of the scores";"CODE"
"the error bars represent one standard deviation of the predicted gaussian";"IRRE"
"distribution of the query points notice that the ard regression captures the";"CODE"
"ground truth the best when using the default parameters in both models but";"IRRE"
"further reducing the lambda init hyperparameter of the bayesian ridge can";"IRRE"
"reduce its bias see example";"-"
"ref sphx glr auto examples linear model plot bayesian ridge curvefit py";"-"
"finally due to the intrinsic limitations of a polynomial regression both";"CODE"
"models fail when extrapolating";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sinusoidal data with noise";"-"
"fit by cubic polynomial";"-"
"plot the true and predicted curves with log marginal likelihood l";"-"
"bayesian ridge regression with different initial value pairs";"IRRE"
"init 1 np var y train 1 0 default values";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"let s start by loading the dataset and creating some sample weights";"CODE"
"normalize the sample weights";"-"
"to fit the elastic net using the precompute option together with the sample";"-"
"weights we must first center the design matrix and rescale it by the";"-"
"normalized weights prior to computing the gram matrix";"-"
"we can now proceed with fitting we must passed the centered design matrix to";"-"
"fit otherwise the elastic net estimator will detect that it is uncentered";"IRRE"
"and discard the gram matrix we passed however if we pass the scaled design";"-"
"matrix the preprocessing code will incorrectly rescale it a second time";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate toy data";"-"
"add four strong outliers to the dataset";"TASK"
"fit the huber regressor over a series of epsilon values";"IRRE"
"fit a ridge regressor to compare it to huber regressor";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate synthetic dataset";"IRRE"
"we generate a dataset where the number of samples is lower than the total";"IRRE"
"number of features this leads to an underdetermined system i e the solution";"CODE"
"is not unique and thus we cannot apply an ref ordinary least squares by";"CODE"
"itself regularization introduces a penalty term to the objective function";"CODE"
"which modifies the optimization problem and can help alleviate the";"-"
"underdetermined nature of the system";"CODE"
"the target y is a linear combination with alternating signs of sinusoidal";"-"
"signals only the 10 lowest out of the 100 frequencies in x are used to";"OUTD"
"generate y while the rest of the features are not informative this results";"CODE"
"in a high dimensional sparse feature space where some degree of";"TASK"
"l1 penalization is necessary";"-"
"true coef n informative 0 sparsify coef";"CODE"
"some of the informative features have close frequencies to induce";"CODE"
"anti correlations";"-"
"a random phase is introduced using func numpy random random sample";"IRRE"
"and some gaussian noise implemented by func numpy random normal";"IRRE"
"is added to both the features and the target";"TASK"
"such sparse noisy and correlated features can be obtained for instance from";"CODE"
"sensor nodes monitoring some environmental variables as they typically register";"IRRE"
"similar values depending on their positions spatial correlations";"IRRE"
"we can visualize the target";"-"
"we split the data into train and test sets for simplicity in practice one";"IRRE"
"should use a class sklearn model selection timeseriessplit";"CODE"
"cross validation to estimate the variance of the test score here we set";"IRRE"
"shuffle false as we must not use training data that succeed the testing";"IRRE"
"data when dealing with data that have a temporal relationship";"CODE"
"in the following we compute the performance of three l1 based models in terms";"CODE"
"of the goodness of fit math r 2 score and the fitting time then we make a";"-"
"plot to compare the sparsity of the estimated coefficients with respect to the";"IRRE"
"ground truth coefficients and finally we analyze the previous results";"CODE"
"lasso";"-"
"in this example we demo a class sklearn linear model lasso with a fixed";"CODE"
"value of the regularization parameter alpha in practice the optimal";"IRRE"
"parameter alpha should be selected by passing a";"IRRE"
"class sklearn model selection timeseriessplit cross validation strategy to a";"CODE"
"class sklearn linear model lassocv to keep the example simple and fast to";"IRRE"
"execute we directly set the optimal value for alpha here";"IRRE"
"automatic relevance determination ard";"-"
"an ard regression is the bayesian version of the lasso it can produce";"META"
"interval estimates for all of the parameters including the error variance if";"CODE"
"required it is a suitable option when the signals have gaussian noise see";"CODE"
"the example ref sphx glr auto examples linear model plot ard py for a";"CODE"
"comparison of class sklearn linear model ardregression and";"IRRE"
"class sklearn linear model bayesianridge regressors";"IRRE"
"elasticnet";"-"
"class sklearn linear model elasticnet is a middle ground between";"IRRE"
"class sklearn linear model lasso and class sklearn linear model ridge";"IRRE"
"as it combines a l1 and a l2 penalty the amount of regularization is";"-"
"controlled by the two hyperparameters l1 ratio and alpha for l1 ratio";"IRRE"
"0 the penalty is pure l2 and the model is equivalent to a";"-"
"class sklearn linear model ridge similarly l1 ratio 1 is a pure l1";"IRRE"
"penalty and the model is equivalent to a class sklearn linear model lasso";"IRRE"
"for 0 l1 ratio 1 the penalty is a combination of l1 and l2";"CODE"
"as done before we train the model with fix values for alpha and l1 ratio";"CODE"
"to select their optimal value we used an";"IRRE"
"class sklearn linear model elasticnetcv not shown here to keep the";"IRRE"
"example simple";"-"
"plot and analysis of the results";"IRRE"
"in this section we use a heatmap to visualize the sparsity of the true";"CODE"
"and estimated coefficients of the respective linear models";"-"
"in the present example class sklearn linear model elasticnet yields the";"CODE"
"best score and captures the most of the predictive features yet still fails";"TASK"
"at finding all the true components notice that both";"-"
"class sklearn linear model elasticnet and";"IRRE"
"class sklearn linear model ardregression result in a less sparse model";"IRRE"
"than a class sklearn linear model lasso";"IRRE"
"conclusions";"-"
"class sklearn linear model lasso is known to recover sparse data";"IRRE"
"effectively but does not perform well with highly correlated features indeed";"CODE"
"if several correlated features contribute to the target";"TASK"
"class sklearn linear model lasso would end up selecting a single one of";"CODE"
"them in the case of sparse yet non correlated features a";"TASK"
"class sklearn linear model lasso model would be more suitable";"IRRE"
"class sklearn linear model elasticnet introduces some sparsity on the";"CODE"
"coefficients and shrinks their values to zero thus in the presence of";"IRRE"
"correlated features that contribute to the target the model is still able to";"TASK"
"reduce their weights without setting them exactly to zero this results in a";"IRRE"
"less sparse model than a pure class sklearn linear model lasso and may";"IRRE"
"capture non predictive features as well";"TASK"
"class sklearn linear model ardregression is better when handling gaussian";"IRRE"
"noise but is still unable to handle correlated features and requires a larger";"TASK"
"amount of time due to fitting a prior";"-"
"references";"CODE"
"1 doi lasso type recovery of sparse representations for";"CODE"
"high dimensional data n meinshausen b yu the annals of statistics";"-"
"2009 vol 37 no 1 246 270 10 1214 07 aos582";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"comparing the two lasso implementations on dense data";"TASK"
"we create a linear regression problem that is suitable for the lasso";"IRRE"
"that is to say with more features than samples we then store the data";"TASK"
"matrix in both dense the usual and sparse format and train a lasso on";"IRRE"
"each we compute the runtime of both and check that they learned the";"CODE"
"same model by computing the euclidean norm of the difference between the";"CODE"
"coefficients they learned because the data is dense we expect better";"IRRE"
"runtime with a dense data format";"CODE"
"create a copy of x in sparse format";"IRRE"
"compare the regression coefficients";"IRRE"
"comparing the two lasso implementations on sparse data";"TASK"
"we make the previous problem sparse by replacing all small values with 0";"IRRE"
"and run the same comparisons as above because the data is now sparse we";"IRRE"
"expect the implementation that uses the sparse data format to be faster";"TASK"
"make a copy of the previous data";"-"
"make xs sparse by replacing the values lower than 2 5 with 0s";"IRRE"
"create a copy of xs in sparse format";"IRRE"
"compute the proportion of non zero coefficient in the data matrix";"CODE"
"compare the regression coefficients";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we will use the diabetes dataset";"IRRE"
"scikit learn provides an estimator called";"IRRE"
"class sklearn linear model lassolarsic that uses either akaike s";"IRRE"
"information criterion aic or the bayesian information criterion bic to";"CODE"
"select the best model before fitting";"CODE"
"this model we will scale the dataset";"CODE"
"in the following we are going to fit two models to compare the values";"IRRE"
"reported by aic and bic";"-"
"to be in line with the definition in zht2007 we need to rescale the";"CODE"
"aic and the bic indeed zou et al are ignoring some constant terms";"CODE"
"compared to the original definition of aic derived from the maximum";"IRRE"
"log likelihood of a linear model you can refer to";"CODE"
"ref mathematical detail section for the user guide lasso lars ic";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"x x std axis 0 standardize data easier to set the l1 ratio parameter";"IRRE"
"compute paths";"-"
"eps 5e 3 the smaller it is the longer is the path";"IRRE"
"display results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset";"IRRE"
"in this example we will use the diabetes dataset";"IRRE"
"in addition we add some random features to the original data to";"TASK"
"better illustrate the feature selection performed by the lasso model";"CODE"
"show only a subset of the columns";"IRRE"
"selecting lasso via an information criterion";"CODE"
"class sklearn linear model lassolarsic provides a lasso estimator that";"IRRE"
"uses the akaike information criterion aic or the bayes information";"CODE"
"criterion bic to select the optimal value of the regularization";"IRRE"
"parameter alpha";"IRRE"
"before fitting the model we will standardize the data with a";"CODE"
"class sklearn preprocessing standardscaler in addition we will";"TASK"
"measure the time to fit and tune the hyperparameter alpha in order to";"IRRE"
"compare with the cross validation strategy";"IRRE"
"we will first fit a lasso model with the aic criterion";"-"
"we store the aic metric for each value of alpha used during fit";"CODE"
"now we perform the same analysis using the bic criterion";"CODE"
"we can check which value of alpha leads to the minimum aic and bic";"IRRE"
"finally we can plot the aic and bic values for the different alpha values";"IRRE"
"the vertical lines in the plot correspond to the alpha chosen for each";"CODE"
"criterion the selected alpha corresponds to the minimum of the aic or bic";"CODE"
"criterion";"-"
"model selection with an information criterion is very fast it relies on";"CODE"
"computing the criterion on the in sample set provided to fit both criteria";"IRRE"
"estimate the model generalization error based on the training set error and";"IRRE"
"penalize this overly optimistic error however this penalty relies on a";"CODE"
"proper estimation of the degrees of freedom and the noise variance both are";"CODE"
"derived for large samples asymptotic results and assume the model is";"IRRE"
"correct i e that the data are actually generated by this model";"META"
"these models also tend to break when the problem is badly conditioned more";"CODE"
"features than samples it is then required to provide an estimate of the";"TASK"
"noise variance";"CODE"
"selecting lasso via cross validation";"CODE"
"the lasso estimator can be implemented with different solvers coordinate";"TASK"
"descent and least angle regression they differ with regards to their";"-"
"execution speed and sources of numerical errors";"-"
"in scikit learn two different estimators are available with integrated";"IRRE"
"cross validation class sklearn linear model lassocv and";"IRRE"
"class sklearn linear model lassolarscv that respectively solve the";"IRRE"
"problem with coordinate descent and least angle regression";"-"
"in the remainder of this section we will present both approaches for both";"CODE"
"algorithms we will use a 20 fold cross validation strategy";"-"
"lasso via coordinate descent";"-"
"let s start by making the hyperparameter tuning using";"IRRE"
"class sklearn linear model lassocv";"IRRE"
"lasso via least angle regression";"-"
"let s start by making the hyperparameter tuning using";"IRRE"
"class sklearn linear model lassolarscv";"IRRE"
"summary of cross validation approach";"-"
"both algorithms give roughly the same results";"IRRE"
"lars computes a solution path only for each kink in the path as a result it";"CODE"
"is very efficient when there are only of few kinks which is the case if";"CODE"
"there are few features or samples also it is able to compute the full path";"TASK"
"without setting any hyperparameter on the opposite coordinate descent";"IRRE"
"computes the path points on a pre specified grid here we use the default";"IRRE"
"thus it is more efficient if the number of grid points is smaller than the";"CODE"
"number of kinks in the path such a strategy can be interesting if the number";"CODE"
"of features is really large and there are enough samples to be selected in";"TASK"
"each of the cross validation fold in terms of numerical errors for heavily";"CODE"
"correlated variables lars will accumulate more errors while the coordinate";"CODE"
"descent algorithm will only sample the path on a grid";"CODE"
"note how the optimal value of alpha varies for each fold this illustrates";"CODE"
"why nested cross validation is a good strategy when trying to evaluate the";"CODE"
"performance of a method for which a parameter is chosen by cross validation";"CODE"
"this choice of parameter may not be optimal for a final evaluation on";"CODE"
"unseen test set only";"IRRE"
"conclusion";"-"
"in this tutorial we presented two approaches for selecting the best";"CODE"
"hyperparameter alpha one strategy finds the optimal value of alpha";"IRRE"
"by only using the training set and some information criterion and another";"IRRE"
"strategy is based on cross validation";"CODE"
"in this example both approaches are working similarly the in sample";"CODE"
"hyperparameter selection even shows its efficacy in terms of computational";"CODE"
"performance however it can only be used when the number of samples is large";"CODE"
"enough compared to the number of features";"TASK"
"that s why hyperparameter optimization via cross validation is a safe";"IRRE"
"strategy it works in different settings";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"classify small against large digits";"IRRE"
"l1 ratio 0 5 l1 weight in the elastic net regularization";"CODE"
"set regularization parameter";"IRRE"
"increase tolerance for short training time";"IRRE"
"coef l1 lr contains zeros due to the";"-"
"l1 sparsity inducing norm";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset generation";"IRRE"
"we generate a synthetic dataset using func sklearn datasets make blobs function";"IRRE"
"the dataset consists of 1 000 samples from three different classes";"IRRE"
"centered around 5 0 0 1 5 and 5 1 after generation we apply a linear";"-"
"transformation to introduce some correlation between features and make the problem";"CODE"
"more challenging this results in a 2d dataset with three overlapping classes";"IRRE"
"suitable for demonstrating the differences between multinomial and one vs rest";"CODE"
"logistic regression";"-"
"classifier training";"IRRE"
"we train two different logistic regression classifiers multinomial and one vs rest";"CODE"
"the multinomial classifier handles all classes simultaneously while the one vs rest";"CODE"
"approach trains a binary classifier for each class against all others";"CODE"
"decision boundaries visualization";"-"
"let s visualize the decision boundaries of both models that is provided by the";"CODE"
"method predict of the classifiers";"IRRE"
"we see that the decision boundaries are different this difference stems from their";"CODE"
"approaches";"-"
"multinomial logistic regression considers all classes simultaneously during";"IRRE"
"optimization";"-"
"one vs rest logistic regression fits each class independently against all others";"CODE"
"these distinct strategies can lead to varying decision boundaries especially in";"CODE"
"complex multi class problems";"IRRE"
"hyperplanes visualization";"-"
"we also visualize the hyperplanes that correspond to the line when the probability";"CODE"
"estimate for a class is of 0 5";"CODE"
"while the hyperplanes for classes 0 and 2 are quite similar between the two methods";"CODE"
"we observe that the hyperplane for class 1 is notably different this difference stems";"CODE"
"from the fundamental approaches of one vs rest and multinomial logistic regression";"CODE"
"for one vs rest logistic regression";"CODE"
"each hyperplane is determined independently by considering one class against all";"CODE"
"others";"-"
"for class 1 the hyperplane represents the decision boundary that best separates";"CODE"
"class 1 from the combined classes 0 and 2";"CODE"
"this binary approach can lead to simpler decision boundaries but may not capture";"META"
"complex relationships between all classes simultaneously";"IRRE"
"there is no possible interpretation of the conditional class probabilities";"CODE"
"for multinomial logistic regression";"CODE"
"all hyperplanes are determined simultaneously considering the relationships between";"-"
"all classes at once";"IRRE"
"the loss minimized by the model is a proper scoring rule which means that the model";"-"
"is optimized to estimate the conditional class probabilities that are therefore";"CODE"
"meaningful";"-"
"each hyperplane represents the decision boundary where the probability of one class";"IRRE"
"becomes higher than the others based on the overall probability distribution";"META"
"this approach can capture more nuanced relationships between classes potentially";"CODE"
"leading to more accurate classification in multi class problems";"IRRE"
"the difference in hyperplanes especially for class 1 highlights how these methods";"CODE"
"can produce different decision boundaries despite similar overall accuracy";"-"
"in practice using multinomial logistic regression is recommended since it minimizes a";"CODE"
"well formulated loss function leading to better calibrated class probabilities and";"CODE"
"thus more interpretable results when it comes to decision boundaries one should";"IRRE"
"formulate a utility function to transform the class probabilities into a meaningful";"CODE"
"quantity for the problem at hand one vs rest allows for different decision boundaries";"CODE"
"but does not allow for fine grained control over the trade off between the classes as";"CODE"
"a utility function would";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load data";"TASK"
"here we remove the third class to make the problem a binary classification";"IRRE"
"compute regularization path";"-"
"create a pipeline with standardscaler and logisticregression to normalize";"CODE"
"the data before fitting a linear model in order to speed up convergence and";"CODE"
"make the coefficients comparable also as a side effect since the data is now";"-"
"centered around 0 we don t need to fit an intercept";"CODE"
"plot regularization path";"-"
"colorblind friendly palette ibm color blind safe palette";"CODE"
"colors 648fff 785ef0 dc267f fe6100";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate data";"-"
"generate some 2d coefficients with sine waves with random frequency and phase";"IRRE"
"fit models";"-"
"plot support and time series";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate some random data";"IRRE"
"threshold coefficients to render them non negative";"CODE"
"add some noise";"TASK"
"split the data in train set and test set";"IRRE"
"fit the non negative least squares";"-"
"fit an ols";"-"
"comparing the regression coefficients between ols and nnls we can observe";"-"
"they are highly correlated the dashed line is the identity relation";"-"
"but the non negative constraint shrinks some to 0";"CODE"
"the non negative least squares inherently yield sparse results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data loading and preparation";"CODE"
"load the diabetes dataset for simplicity we only keep a single feature in the data";"CODE"
"then we split the data and target into training and test sets";"IRRE"
"x x 2 use only one feature";"TASK"
"linear regression model";"-"
"we create a linear regression model and fit it on the training data note that by";"TASK"
"default an intercept is added to the model we can control this behavior by setting";"CODE"
"the fit intercept parameter";"IRRE"
"model evaluation";"-"
"we evaluate the model s performance on the test set using the mean squared error";"IRRE"
"and the coefficient of determination";"-"
"plotting the results";"IRRE"
"finally we visualize the results on the train and test data";"CODE"
"ols on this single feature subset learns a linear function that minimizes";"CODE"
"the mean squared error on the training data we can see how well or poorly";"-"
"it generalizes by looking at the r 2 score and mean squared error on the";"-"
"test set in higher dimensions pure ols often overfits especially if the";"IRRE"
"data is noisy regularization techniques like ridge or lasso can help";"-"
"reduce that";"-"
"ordinary least squares and ridge regression variance";"CODE"
"next we illustrate the problem of high variance more clearly by using";"CODE"
"a tiny synthetic dataset we sample only two data points then repeatedly";"IRRE"
"add small gaussian noise to them and refit both ols and ridge we plot";"TASK"
"each new line to see how much ols can jump around whereas ridge remains";"CODE"
"more stable thanks to its penalty term";"CODE"
"conclusion";"-"
"in the first example we applied ols to a real dataset showing";"CODE"
"how a plain linear model can fit the data by minimizing the squared error";"-"
"on the training set";"IRRE"
"in the second example ols lines varied drastically each time noise";"CODE"
"was added reflecting its high variance when data is sparse or noisy by";"CODE"
"contrast ridge regression introduces a regularization term that shrinks";"CODE"
"the coefficients stabilizing predictions";"-"
"techniques like class sklearn linear model ridge or";"IRRE"
"class sklearn linear model lasso which applies an l1 penalty are both";"IRRE"
"common ways to improve generalization and reduce overfitting a well tuned";"-"
"ridge or lasso often outperforms pure ols when features are correlated data";"TASK"
"is noisy or sample size is small";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate the data";"-"
"y xw";"-"
"x 0 n nonzero coefs";"-"
"distort the clean signal";"TASK"
"plot the sparse signal";"IRRE"
"plot the noise free reconstruction";"CODE"
"plot the noisy reconstruction";"CODE"
"plot the noisy reconstruction with number of non zeros set by cv";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the french motor third party liability claims dataset";"IRRE"
"let s load the motor claim dataset from openml";"CODE"
"https www openml org d 41214";"CODE"
"the number of claims claimnb is a positive integer that can be modeled";"CODE"
"as a poisson distribution it is then assumed to be the number of discrete";"META"
"events occurring with a constant rate in a given time interval exposure";"CODE"
"in units of years";"-"
"here we want to model the frequency y claimnb exposure conditionally";"-"
"on x via a scaled poisson distribution and use exposure as";"META"
"sample weight";"-"
"the remaining columns can be used to predict the frequency of claim events";"OUTD"
"those columns are very heterogeneous with a mix of categorical and numeric";"-"
"variables with different scales possibly very unevenly distributed";"IRRE"
"in order to fit linear models with those predictors it is therefore";"CODE"
"necessary to perform standard feature transformations as follows";"CODE"
"a constant prediction baseline";"CODE"
"it is worth noting that more than 93 of policyholders have zero claims if";"OUTD"
"we were to convert this problem into a binary classification task it would";"CODE"
"be significantly imbalanced and even a simplistic model that would only";"CODE"
"predict mean can achieve an accuracy of 93";"-"
"to evaluate the pertinence of the used metrics we will consider as a";"-"
"baseline a dummy estimator that constantly predicts the mean frequency of";"CODE"
"the training sample";"-"
"let s compute the performance of this constant prediction baseline with 3";"CODE"
"different regression metrics";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we start by defining a function that we intend to approximate and prepare";"CODE"
"plotting it";"-"
"extend the test data into the future";"CODE"
"we again plot the underlying splines";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset generation";"IRRE"
"to illustrate the behaviour of quantile regression we will generate two";"-"
"synthetic datasets the true generative random processes for both datasets";"IRRE"
"will be composed by the same expected value with a linear relationship with a";"IRRE"
"single feature x";"TASK"
"we will create two subsequent problems by changing the distribution of the";"IRRE"
"target y while keeping the same expected value";"IRRE"
"in the first case a heteroscedastic normal noise is added";"CODE"
"in the second case an asymmetric pareto noise is added";"CODE"
"let s first visualize the datasets as well as the distribution of the";"IRRE"
"residuals y mean y";"-"
"with the heteroscedastic normal distributed target we observe that the";"META"
"variance of the noise is increasing when the value of the feature x is";"TASK"
"increasing";"-"
"with the asymmetric pareto distributed target we observe that the positive";"META"
"residuals are bounded";"-"
"these types of noisy targets make the estimation via";"IRRE"
"class sklearn linear model linearregression less efficient i e we need";"IRRE"
"more data to get stable results and in addition large outliers can have a";"TASK"
"huge impact on the fitted coefficients stated otherwise in a setting with";"IRRE"
"constant variance ordinary least squares estimators converge much faster to";"CODE"
"the true coefficients with increasing sample size";"-"
"in this asymmetric setting the median or different quantiles give additional";"CODE"
"insights on top of that median estimation is much more robust to outliers";"-"
"and heavy tailed distributions but note that extreme quantiles are estimated";"META"
"by very few data points 95 quantile are more or less estimated by the 5";"CODE"
"largest values and thus also a bit sensitive outliers";"IRRE"
"in the remainder of this tutorial we will show how";"CODE"
"class sklearn linear model quantileregressor can be used in practice and";"IRRE"
"give the intuition into the properties of the fitted models finally";"CODE"
"we will compare the both class sklearn linear model quantileregressor";"IRRE"
"and class sklearn linear model linearregression";"IRRE"
"fitting a quantileregressor";"-"
"in this section we want to estimate the conditional median as well as";"CODE"
"a low and high quantile fixed at 5 and 95 respectively thus we will get";"-"
"three linear models one for each quantile";"CODE"
"we will use the quantiles at 5 and 95 to find the outliers in the training";"IRRE"
"sample beyond the central 90 interval";"CODE"
"now we can plot the three linear models and the distinguished samples that";"-"
"are within the central 90 interval from samples that are outside this";"CODE"
"interval";"CODE"
"since the noise is still normally distributed in particular is symmetric";"TASK"
"the true conditional mean and the true conditional median coincide indeed";"-"
"we see that the estimated median almost hits the true mean we observe the";"-"
"effect of having an increasing noise variance on the 5 and 95 quantiles";"CODE"
"the slopes of those quantiles are very different and the interval between";"CODE"
"them becomes wider with increasing x";"-"
"to get an additional intuition regarding the meaning of the 5 and 95";"TASK"
"quantiles estimators one can count the number of samples above and below the";"-"
"predicted quantiles represented by a cross on the above plot considering";"-"
"that we have a total of 100 samples";"-"
"we can repeat the same experiment using the asymmetric pareto distributed";"META"
"target";"-"
"due to the asymmetry of the distribution of the noise we observe that the";"META"
"true mean and estimated conditional median are different we also observe";"-"
"that each quantile model has different parameters to better fit the desired";"IRRE"
"quantile note that ideally all quantiles would be parallel in this case";"CODE"
"which would become more visible with more data points or less extreme";"CODE"
"quantiles e g 10 and 90";"-"
"comparing quantileregressor and linearregression";"IRRE"
"in this section we will linger on the difference regarding the loss functions that";"CODE"
"class sklearn linear model quantileregressor and";"IRRE"
"class sklearn linear model linearregression are minimizing";"IRRE"
"indeed class sklearn linear model linearregression is a least squares";"IRRE"
"approach minimizing the mean squared error mse between the training and";"-"
"predicted targets in contrast";"-"
"class sklearn linear model quantileregressor with quantile 0 5";"IRRE"
"minimizes the mean absolute error mae instead";"-"
"why does it matter the loss functions specify what exactly the model is aiming";"CODE"
"to predict see";"-"
"ref user guide on the choice of scoring function which scoring function";"CODE"
"in short a model minimizing mse predicts the mean expectation and a model";"-"
"minimizing mae predicts the median";"-"
"let s compute the training errors of such models in terms of mean";"CODE"
"squared error and mean absolute error we will use the asymmetric pareto";"IRRE"
"distributed target to make it more interesting as mean and median are not";"META"
"equal";"-"
"on the training set we see that mae is lower for";"IRRE"
"class sklearn linear model quantileregressor than";"IRRE"
"class sklearn linear model linearregression in contrast to that mse is";"IRRE"
"lower for class sklearn linear model linearregression than";"CODE"
"class sklearn linear model quantileregressor these results confirms that";"IRRE"
"mae is the loss minimized by class sklearn linear model quantileregressor";"IRRE"
"while mse is the loss minimized";"CODE"
"class sklearn linear model linearregression";"IRRE"
"we can make a similar evaluation by looking at the test error obtained by";"IRRE"
"cross validation";"-"
"we reach similar conclusions on the out of sample evaluation";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"add outlier data";"TASK"
"fit line using all data";"-"
"robustly fit linear model with ransac algorithm";"-"
"predict data of estimated models";"-"
"compare estimated coefficients";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"purpose of this example";"CODE"
"for the purpose of showing how ridge regularization works we will create a";"IRRE"
"non noisy data set then we will train a regularized model on a range of";"IRRE"
"regularization strengths math alpha and plot how the trained";"-"
"coefficients and the mean squared error between those and the original values";"IRRE"
"behave as functions of the regularization strength";"CODE"
"creating a non noisy data set";"IRRE"
"we make a toy data set with 100 samples and 10 features that s suitable to";"TASK"
"detect regression out of the 10 features 8 are informative and contribute to";"TASK"
"the regression while the remaining 2 features do not have any effect on the";"CODE"
"target variable their true coefficients are 0 please note that in this";"CODE"
"example the data is non noisy hence we can expect our regression model to";"CODE"
"recover exactly the true coefficients w";"-"
"obtain the true coefficients";"CODE"
"training the ridge regressor";"-"
"we use class sklearn linear model ridge a linear model with l2";"IRRE"
"regularization we train several models each with a different value for the";"IRRE"
"model parameter alpha which is a positive constant that multiplies the";"IRRE"
"penalty term controlling the regularization strength for each trained model";"CODE"
"we then compute the error between the true coefficients w and the";"-"
"coefficients found by the model clf we store the identified coefficients";"-"
"and the calculated errors for the corresponding coefficients in lists which";"CODE"
"makes it convenient for us to plot them";"CODE"
"generate values for alpha that are evenly distributed on a logarithmic scale";"IRRE"
"train the model with different regularisation strengths";"CODE"
"plotting trained coefficients and mean squared errors";"-"
"we now plot the 10 different regularized coefficients as a function of the";"CODE"
"regularization parameter alpha where each color represents a different";"IRRE"
"coefficient";"-"
"on the right hand side we plot how the errors of the coefficients from the";"CODE"
"estimator change as a function of regularization";"CODE"
"interpreting the plots";"CODE"
"the plot on the left hand side shows how the regularization strength alpha";"-"
"affects the ridge regression coefficients smaller values of alpha weak";"IRRE"
"regularization allow the coefficients to closely resemble the true";"CODE"
"coefficients w used to generate the data set this is because no";"IRRE"
"additional noise was added to our artificial data set as alpha increases";"TASK"
"the coefficients shrink towards zero gradually reducing the impact of the";"CODE"
"features that were formerly more significant";"TASK"
"the right hand side plot shows the mean squared error mse between the";"-"
"coefficients found by the model and the true coefficients w it provides a";"-"
"measure that relates to how exact our ridge model is in comparison to the true";"IRRE"
"generative model a low error means that it found coefficients closer to the";"CODE"
"ones of the true generative model in this case since our toy data set was";"CODE"
"non noisy we can see that the least regularized model retrieves coefficients";"-"
"closest to the true coefficients w error is close to 0";"CODE"
"when alpha is small the model captures the intricate details of the";"CODE"
"training data whether those were caused by noise or by actual information as";"CODE"
"alpha increases the highest coefficients shrink more rapidly rendering";"CODE"
"their corresponding features less influential in the training process this";"CODE"
"can enhance a model s ability to generalize to unseen data if there was a lot";"CODE"
"of noise to capture but it also poses the risk of losing performance if the";"IRRE"
"regularization becomes too strong compared to the amount of noise the data";"IRRE"
"contained as in this example";"CODE"
"in real world scenarios where data typically includes noise selecting an";"CODE"
"appropriate alpha value becomes crucial in striking a balance between an";"IRRE"
"overfitting and an underfitting model";"-"
"here we saw that class sklearn linear model ridge adds a penalty to the";"TASK"
"coefficients to fight overfitting another problem that occurs is linked to";"-"
"the presence of outliers in the training dataset an outlier is a data point";"CODE"
"that differs significantly from other observations concretely these outliers";"CODE"
"impact the left hand side term of the loss function that we showed earlier";"CODE"
"some other linear models are formulated to be robust to outliers such as the";"CODE"
"class sklearn linear model huberregressor you can learn more about it in";"IRRE"
"the ref sphx glr auto examples linear model plot huber vs ridge py example";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"x is the 10x10 hilbert matrix";"-"
"compute paths";"-"
"display results";"IRRE"
"ax set xlim ax get xlim 1 reverse axis";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"make sure that it x is 2d";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"define the estimators to compare";"IRRE"
"load the dataset";"IRRE"
"transform the results in a pandas dataframe for easy plotting";"CODE"
"define what to plot";"CODE"
"first plot train and test scores";"IRRE"
"second plot n iter and fit time";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"import some data to play with";"CODE"
"we only take the first two features we could";"TASK"
"avoid this ugly slicing by using a two dim dataset";"CODE"
"shuffle";"-"
"standardize";"-"
"plot also the training points";"CODE"
"plot the three one against all classifiers";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we create 50 separable points";"IRRE"
"fit the model";"-"
"plot the line the points and the nearest vectors to the plane";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we create 20 points";"IRRE"
"and assign a bigger weight to the last 10 samples";"IRRE"
"plot the weighted data points";"CODE"
"fit the unweighted model";"-"
"fit the weighted model";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate train data";"-"
"generate some regular novel observations";"-"
"generate some abnormal novel observations";"-"
"ocsvm hyperparameters";"IRRE"
"fit the one class svm";"IRRE"
"fit the one class svm using a kernel approximation and sgd";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we use saga solver";"-"
"turn down for faster run time";"CODE"
"add initial chance level values for plotting purpose";"IRRE"
"small number of epochs for fast runtime";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"turn down for faster convergence";"CODE"
"load data from https www openml org d 554";"CODE"
"turn up tolerance for faster convergence";"CODE"
"print best c 4f clf c";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"outliers only in the y direction";"CODE"
"linear model y 3 x n 2 0 1 2";"-"
"10 outliers";"-"
"outliers in the x direction";"CODE"
"linear model y 3 x n 2 0 1 2";"-"
"10 outliers";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"fremtpl2freq dataset from https www openml org d 41214";"CODE"
"fremtpl2sev dataset from https www openml org d 41215";"CODE"
"sum claimamount over identical ids";"-"
"unquote string fields";"IRRE"
"aggregate observed and predicted variables by feature level";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset preparation";"IRRE"
"we start by generating the s curve dataset";"IRRE"
"unused but required import for doing 3d projections with matplotlib 3 2";"CODE"
"import mpl toolkits mplot3d noqa f401";"CODE"
"let s look at the original data also define some helping";"CODE"
"functions which we will use further on";"CODE"
"define algorithms for the manifold learning";"CODE"
"manifold learning is an approach to non linear dimensionality reduction";"-"
"algorithms for this task are based on the idea that the dimensionality of";"CODE"
"many data sets is only artificially high";"IRRE"
"read more in the ref user guide manifold";"CODE"
"n neighbors 12 neighborhood which is used to recover the locally linear structure";"IRRE"
"n components 2 number of coordinates for the manifold";"CODE"
"locally linear embeddings";"IRRE"
"locally linear embedding lle can be thought of as a series of local";"IRRE"
"principal component analyses which are globally compared to find the";"IRRE"
"best non linear embedding";"-"
"read more in the ref user guide locally linear embedding";"CODE"
"isomap embedding";"-"
"non linear dimensionality reduction through isometric mapping";"-"
"isomap seeks a lower dimensional embedding which maintains geodesic";"CODE"
"distances between all points read more in the ref user guide isomap";"CODE"
"multidimensional scaling";"-"
"multidimensional scaling mds seeks a low dimensional representation";"-"
"of the data in which the distances respect well the distances in the";"CODE"
"original high dimensional space";"-"
"read more in the ref user guide multidimensional scaling";"CODE"
"spectral embedding for non linear dimensionality reduction";"CODE"
"this implementation uses laplacian eigenmaps which finds a low dimensional";"TASK"
"representation of the data using a spectral decomposition of the graph laplacian";"-"
"read more in the ref user guide spectral embedding";"CODE"
"t distributed stochastic neighbor embedding";"META"
"it converts similarities between data points to joint probabilities and";"CODE"
"tries to minimize the kullback leibler divergence between the joint probabilities";"CODE"
"of the low dimensional embedding and the high dimensional data t sne has a cost";"-"
"function that is not convex i e with different initializations we can get";"IRRE"
"different results read more in the ref user guide t sne";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load digits dataset";"TASK"
"we will load the digits dataset and only use six first of the ten available classes";"CODE"
"we can plot the first hundred digits from this data set";"CODE"
"helper function to plot embedding";"CODE"
"below we will use different techniques to embed the digits dataset we will plot";"IRRE"
"the projection of the original data onto each embedding it will allow us to";"-"
"check whether or digits are grouped together in the embedding space or";"CODE"
"scattered across it";"-"
"hown images np array 1 0 1 0 just something big";"-"
"plot every digit on the embedding";"-"
"show an annotation box for a group of digits";"CODE"
"don t show points that are too close";"CODE"
"embedding techniques comparison";"-"
"below we compare different techniques however there are a couple of things";"IRRE"
"to note";"TASK"
"the class sklearn ensemble randomtreesembedding is not";"IRRE"
"technically a manifold embedding method as it learn a high dimensional";"IRRE"
"representation on which we apply a dimensionality reduction method";"-"
"however it is often useful to cast a dataset into a representation in";"IRRE"
"which the classes are linearly separable";"IRRE"
"the class sklearn discriminant analysis lineardiscriminantanalysis and";"IRRE"
"the class sklearn neighbors neighborhoodcomponentsanalysis are supervised";"CODE"
"dimensionality reduction method i e they make use of the provided labels";"-"
"contrary to other methods";"-"
"the class sklearn manifold tsne is initialized with the embedding that is";"IRRE"
"generated by pca in this example it ensures global stability of the embedding";"CODE"
"i e the embedding does not depend on random initialization";"CODE"
"once we declared all the methods of interest we can run and perform the projection";"CODE"
"of the original data we will store the projected data as well as the computational";"-"
"time needed to perform each projection";"CODE"
"data flat x shape 1 1 0 01 make x invertible";"-"
"finally we can plot the resulting projection given by each method";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"unused but required import for doing 3d projections with matplotlib 3 2";"CODE"
"import mpl toolkits mplot3d noqa f401";"CODE"
"variables for manifold learning";"CODE"
"create our sphere";"IRRE"
"sever the poles from the sphere";"CODE"
"plot our dataset";"IRRE"
"perform locally linear embedding manifold learning";"IRRE"
"perform isomap manifold learning";"CODE"
"perform multi dimensional scaling";"CODE"
"perform spectral embedding";"CODE"
"perform t distributed stochastic neighbor embedding";"META"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset preparation";"IRRE"
"we start by uniformly generating 20 points in a 2d space";"CODE"
"generate the data";"-"
"center the data";"-"
"now we compute pairwise distances between all points and add";"TASK"
"a small amount of noise to the distance matrix we make sure";"IRRE"
"to keep the noisy distance matrix symmetric";"-"
"compute pairwise euclidean distances";"CODE"
"add noise to the distances";"TASK"
"here we compute metric non metric and classical mds of the noisy distance matrix";"IRRE"
"rescaling the non metric mds solution to match the spread of the original data";"CODE"
"to make the visual comparisons easier we rotate the original data and all mds";"-"
"solutions to their pca axes and flip horizontal and vertical mds axes if needed";"-"
"to match the original data orientation";"-"
"rotate the data cmds does not need to be rotated it is inherently pca aligned";"TASK"
"align the sign of pcs";"-"
"finally we plot the original data and all mds reconstructions";"CODE"
"plot the edges";"-"
"a sequence of line0 line1 line2 where";"CODE"
"linen x0 y0 x1 y1 xm ym";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"swiss roll";"-"
"we start by generating the swiss roll dataset";"IRRE"
"now let s take a look at our data";"CODE"
"computing the lle and t sne embeddings we find that lle seems to unroll the";"-"
"swiss roll pretty effectively t sne on the other hand is able";"-"
"to preserve the general structure of the data but poorly represents the";"META"
"continuous nature of our original data instead it seems to unnecessarily";"-"
"clump sections of points together";"CODE"
"note";"TASK"
"lle seems to be stretching the points from the center purple";"CODE"
"of the swiss roll however we observe that this is simply a byproduct";"CODE"
"of how the data was generated there is a higher density of points near the";"CODE"
"center of the roll which ultimately affects how lle reconstructs the";"CODE"
"data in a lower dimension";"-"
"swiss hole";"-"
"now let s take a look at how both algorithms deal with us adding a hole to";"CODE"
"the data first we generate the swiss hole dataset and plot it";"IRRE"
"computing the lle and t sne embeddings we obtain similar results to the";"IRRE"
"swiss roll lle very capably unrolls the data and even preserves";"-"
"the hole t sne again seems to clump sections of points together but we";"CODE"
"note that it preserves the general topology of the original data";"TASK"
"concluding remarks";"-"
"we note that t sne benefits from testing more combinations of parameters";"IRRE"
"better results could probably have been obtained by better tuning these";"IRRE"
"parameters";"IRRE"
"we observe that as seen in the manifold learning on";"CODE"
"handwritten digits example t sne generally performs better than lle";"CODE"
"on real world data";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"another example using s curve";"-"
"another example using a 2d uniform grid";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"example settings";"IRRE"
"define outlier anomaly detection methods to be compared";"IRRE"
"the sgdoneclasssvm must be used in a pipeline with a kernel approximation";"CODE"
"to give similar results to the oneclasssvm";"IRRE"
"define datasets";"IRRE"
"compare given classifiers under given settings";"IRRE"
"add outliers";"TASK"
"fit the data and tag outliers";"-"
"plot the levels lines and the points";"CODE"
"if name local outlier factor lof does not implement predict";"TASK"
"colors np array 377eb8 ff7f00";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load data and train model";"TASK"
"for this example we load a blood transfusion service center data set from";"CODE"
"openml https www openml org d 1464 this is a binary classification";"CODE"
"problem where the target is whether an individual donated blood then the";"CODE"
"data is split into a train and test dataset and a logistic regression is";"IRRE"
"fitted with the train dataset";"IRRE"
"create class confusionmatrixdisplay";"IRRE"
"with the fitted model we compute the predictions of the model on the test";"IRRE"
"dataset these predictions are used to compute the confusion matrix which";"IRRE"
"is plotted with the class confusionmatrixdisplay";"IRRE"
"create class roccurvedisplay";"IRRE"
"the roc curve requires either the probabilities or the non thresholded";"CODE"
"decision values from the estimator since the logistic regression provides";"IRRE"
"a decision function we will use it to plot the roc curve";"CODE"
"create class precisionrecalldisplay";"IRRE"
"similarly the precision recall curve can be plotted using y score from";"IRRE"
"the prevision sections";"META"
"combining the display objects into a single plot";"CODE"
"the display objects store the computed values that were passed as arguments";"IRRE"
"this allows for the visualizations to be easliy combined using matplotlib s";"CODE"
"api in the following example we place the displays next to each other in a";"CODE"
"row";"-"
"sphinx gallery thumbnail number 4";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"compact text representation";"-"
"estimators will only show the parameters that have been set to non default";"IRRE"
"values when displayed as a string this reduces the visual noise and makes it";"CODE"
"easier to spot what the differences are when comparing instances";"-"
"rich html representation";"-"
"in notebooks estimators and pipelines will use a rich html representation";"TASK"
"this is particularly useful to summarise the";"IRRE"
"structure of pipelines and other composite estimators with interactivity to";"CODE"
"provide detail click on the example image below to expand pipeline";"CODE"
"elements see ref visualizing composite estimators for how you can use";"CODE"
"this feature";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"fit isotonicregression and linearregression models";"-"
"lr fit x np newaxis y x needs to be 2d for linearregression";"CODE"
"plot results";"IRRE"
"note that we explicitly passed out of bounds clip to the constructor of";"CODE"
"isotonicregression to control the way the model extrapolates outside of the";"-"
"range of data observed in the training set this clipping extrapolation can";"CODE"
"be seen on the plot of the decision function on the right hand";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"theoretical bounds";"-"
"the distortion introduced by a random projection p is asserted by";"CODE"
"the fact that p is defining an eps embedding with good probability";"CODE"
"as defined by";"CODE"
"math";"-"
"1 eps u v 2 p u p v 2 1 eps u v 2";"-"
"where u and v are any rows taken from a dataset of shape n samples";"CODE"
"n features and p is a projection by a random gaussian n 0 1 matrix";"IRRE"
"of shape n components n features or a sparse achlioptas matrix";"TASK"
"the minimum number of components to guarantees the eps embedding is";"-"
"given by";"-"
"math";"-"
"n components geq 4 log n samples eps 2 2 eps 3 3";"-"
"the first plot shows that with an increasing number of samples n samples";"-"
"the minimal number of dimensions n components increased logarithmically";"IRRE"
"in order to guarantee an eps embedding";"-"
"range of admissible distortions";"-"
"range of number of samples observation to embed";"-"
"the second plot shows that an increase of the admissible";"-"
"distortion eps allows to reduce drastically the minimal number of";"IRRE"
"dimensions n components for a given number of samples n samples";"CODE"
"range of admissible distortions";"-"
"range of number of samples observation to embed";"-"
"empirical validation";"-"
"we validate the above bounds on the 20 newsgroups text document";"CODE"
"tf idf word frequencies dataset or on the digits dataset";"IRRE"
"for the 20 newsgroups dataset some 300 documents with 100k";"CODE"
"features in total are projected using a sparse random matrix to smaller";"IRRE"
"euclidean spaces with various values for the target number of dimensions";"CODE"
"n components";"-"
"for the digits dataset some 8x8 gray level pixels data for 300";"CODE"
"handwritten digits pictures are randomly projected to spaces for various";"CODE"
"larger number of dimensions n components";"-"
"the default dataset is the 20 newsgroups dataset to run the example on the";"CODE"
"digits dataset pass the use digits dataset command line argument to";"IRRE"
"this script";"CODE"
"for each value of n components we plot";"CODE"
"2d distribution of sample pairs with pairwise distances in original";"META"
"and projected spaces as x and y axis respectively";"-"
"1d histogram of the ratio of those distances projected original";"CODE"
"select only non identical samples pairs";"CODE"
"todo compute the expected value of eps and add them to the previous plot";"TASK"
"as vertical lines region";"IRRE"
"we can see that for low values of n components the distribution is wide";"IRRE"
"with many distorted pairs and a skewed distribution due to the hard";"META"
"limit of zero ratio on the left as distances are always positives";"-"
"while for larger values of n components the distortion is controlled";"IRRE"
"and the distances are well preserved by the random projection";"IRRE"
"remarks";"-"
"according to the jl lemma projecting 300 samples without too much distortion";"-"
"will require at least several thousands dimensions irrespective of the";"CODE"
"number of features of the original dataset";"TASK"
"hence using random projections on the digits dataset which only has 64";"IRRE"
"features in the input space does not make sense it does not allow";"CODE"
"for dimensionality reduction in this case";"CODE"
"on the twenty newsgroups on the other hand the dimensionality can be";"CODE"
"decreased from 56 436 down to 10 000 while reasonably preserving";"CODE"
"pairwise distances";"-"
"python package and dataset imports load dataset";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"standard scientific python imports";"CODE"
"import datasets classifiers and performance metrics";"CODE"
"the digits dataset";"IRRE"
"timing and accuracy plots";"-"
"to apply a classifier on this data we need to flatten the image to";"CODE"
"turn the data in a samples feature matrix";"TASK"
"we learn the digits on the first half of the digits";"-"
"now predict the value of the digit on the second half";"IRRE"
"data test scaler transform data test";"IRRE"
"create a classifier a support vector classifier";"IRRE"
"create pipeline from kernel approximation";"CODE"
"and linear svm";"-"
"fit and predict using linear and kernel svm";"IRRE"
"plot the results";"IRRE"
"second y axis for timings";"CODE"
"horizontal lines for exact rbf and linear kernels";"CODE"
"vertical line for dataset dimensionality 64";"IRRE"
"legends and labels";"CODE"
"decision surfaces of rbf kernel svm and linear svm";"-"
"the second plot visualized the decision surfaces of the rbf kernel svm and";"-"
"the linear svm with approximate kernel maps";"-"
"the plot shows decision surfaces of the classifiers projected onto";"CODE"
"the first two principal components of the data this visualization should";"CODE"
"be taken with a grain of salt since it is just an interesting slice through";"CODE"
"the decision surface in 64 dimensions in particular note that";"TASK"
"a datapoint represented as a dot does not necessarily be classified";"CODE"
"into the region it is lying in since it will not lie on the plane";"CODE"
"that the first two principal components span";"-"
"the usage of class rbfsampler and class nystroem is described in detail";"IRRE"
"in ref kernel approximation";"-"
"visualize the decision surface projected down to the first";"CODE"
"two principal components of the dataset";"IRRE"
"generate grid along first two principal components";"-"
"steps along first component";"-"
"steps along second component";"-"
"combine";"-"
"title for the plots";"CODE"
"predict and plot";"-"
"plot the decision boundary for that we will assign a color to each";"IRRE"
"point in the mesh x min x max x y min y max";"CODE"
"put the result into a color plot";"IRRE"
"lv eps 0 01 adjust a mapping from calculated contour levels to color";"CODE"
"plot also the training points";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"add noise to targets";"TASK"
"construct the kernel based regression models";"CODE"
"compare times of svr and kernel ridge regression";"IRRE"
"look at the results";"IRRE"
"the previous figure compares the learned model of krr and svr when both";"IRRE"
"complexity regularization and bandwidth of the rbf kernel are optimized using";"META"
"grid search the learned functions are very similar however fitting krr is";"CODE"
"approximately 3 4 times faster than fitting svr both with grid search";"-"
"prediction of 100000 target values could be in theory approximately three";"IRRE"
"times faster with svr since it has learned a sparse model using only";"IRRE"
"approximately 1 3 of the training datapoints as support vectors however in";"CODE"
"practice this is not necessarily the case because of implementation details";"TASK"
"in the way the kernel function is computed for each model that can make the";"CODE"
"krr model as fast or even faster despite computing more arithmetic";"-"
"operations";"-"
"visualize training and prediction times";"-"
"this figure compares the time for fitting and prediction of krr and svr for";"CODE"
"different sizes of the training set fitting krr is faster than svr for";"IRRE"
"medium sized training sets less than a few thousand samples however for";"IRRE"
"larger training sets svr scales better with regard to prediction time svr";"IRRE"
"should be faster than krr for all sizes of the training set because of the";"IRRE"
"learned sparse solution however this is not necessarily the case in practice";"CODE"
"because of implementation details note that the degree of sparsity and thus";"TASK"
"the prediction time depends on the parameters epsilon and c of the svr";"IRRE"
"visualize the learning curves";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"metadata routing is only available if explicitly enabled";"-"
"this utility function is a dummy to check if a metadata is passed";"CODE"
"a utility function to nicely print the routing information of an object";"CODE"
"consuming estimator";"-"
"here we demonstrate how an estimator can expose the required api to support";"CODE"
"metadata routing as a consumer imagine a simple classifier accepting";"IRRE"
"sample weight as a metadata on its fit and groups in its";"IRRE"
"predict method";"-"
"all classifiers need to expose a classes attribute once they re fit";"IRRE"
"return a constant value of 1 not a very smart classifier";"IRRE"
"the above estimator now has all it needs to consume metadata this is";"TASK"
"accomplished by some magic done in class base baseestimator there are";"CODE"
"now three methods exposed by the above class set fit request";"IRRE"
"set predict request and get metadata routing there is also a";"IRRE"
"set score request for sample weight which is present since";"CODE"
"class base classifiermixin implements a score method accepting";"IRRE"
"sample weight the same applies to regressors which inherit from";"META"
"class base regressormixin";"IRRE"
"by default no metadata is requested which we can see as";"CODE"
"the above output means that sample weight and groups are not";"IRRE"
"requested by exampleclassifier and if a router is given those metadata it";"IRRE"
"should raise an error since the user has not explicitly set whether they are";"TASK"
"required or not the same is true for sample weight in the score";"CODE"
"method which is inherited from class base classifiermixin in order to";"CODE"
"explicitly set request values for those metadata we can use these methods";"IRRE"
"note";"TASK"
"please note that as long as the above estimator is not used in a";"TASK"
"meta estimator the user does not need to set any requests for the";"CODE"
"metadata and the set values are ignored since a consumer does not";"IRRE"
"validate or route given metadata a simple usage of the above estimator";"-"
"would work as expected";"-"
"routing meta estimator";"-"
"now we show how to design a meta estimator to be a router as a simplified";"-"
"example here is a meta estimator which doesn t do much other than routing";"CODE"
"the metadata";"-"
"this method defines the routing for this meta estimator";"CODE"
"in order to do so a metadatarouter instance is created and the";"TASK"
"routing is added to it more explanations follow below";"TASK"
"get routing for object returns a copy of the metadatarouter";"CODE"
"constructed by the above get metadata routing method that is";"CODE"
"internally called";"IRRE"
"meta estimators are responsible for validating the given metadata";"CODE"
"method refers to the parent s method i e fit in this example";"CODE"
"metadatarouter route params maps the given metadata to the metadata";"-"
"required by the underlying estimator based on the routing information";"CODE"
"defined by the metadatarouter the output of type bunch has a key";"IRRE"
"for each consuming object and those hold keys for their consuming";"CODE"
"methods which then contain key for the metadata which should be";"CODE"
"routed to them";"-"
"a sub estimator is fitted and its classes are attributed to the";"IRRE"
"meta estimator";"-"
"as in fit we get a copy of the object s metadatarouter";"IRRE"
"then we validate the given metadata";"-"
"and then prepare the input to the underlying predict method";"CODE"
"let s break down different parts of the above code";"CODE"
"first the meth utils metadata routing get routing for object takes our";"CODE"
"meta estimator self and returns a";"CODE"
"class utils metadata routing metadatarouter or a";"IRRE"
"class utils metadata routing metadatarequest if the object is a consumer";"CODE"
"based on the output of the estimator s get metadata routing method";"IRRE"
"then in each method we use the route params method to construct a";"CODE"
"dictionary of the form object name method name metadata";"CODE"
"value to pass to the underlying estimator s method the object name";"IRRE"
"estimator in the above routed params estimator fit example is the";"CODE"
"same as the one added in the get metadata routing validate metadata";"TASK"
"makes sure all given metadata are requested to avoid silent bugs";"CODE"
"next we illustrate the different behaviors and notably the type of errors";"-"
"raised";"CODE"
"note that the above example is calling our utility function";"TASK"
"check metadata via the exampleclassifier it checks that";"IRRE"
"sample weight is correctly passed to it if it is not like in the";"CODE"
"following example it would print that sample weight is none";"CODE"
"if we pass an unknown metadata an error is raised";"CODE"
"and if we pass a metadata which is not explicitly requested";"TASK"
"also if we explicitly set it as not requested but it is provided";"IRRE"
"another concept to introduce is aliased metadata this is when an";"CODE"
"estimator requests a metadata with a different variable name than the default";"CODE"
"variable name for instance in a setting where there are two estimators in a";"IRRE"
"pipeline one could request sample weight1 and the other";"CODE"
"sample weight2 note that this doesn t change what the estimator expects";"CODE"
"it only tells the meta estimator how to map the provided metadata to what is";"-"
"required here s an example where we pass aliased sample weight to the";"CODE"
"meta estimator but the meta estimator understands that";"META"
"aliased sample weight is an alias for sample weight and passes it as";"CODE"
"sample weight to the underlying estimator";"-"
"passing sample weight here will fail since it is requested with an";"CODE"
"alias and sample weight with that name is not requested";"CODE"
"this leads us to the get metadata routing the way routing works in";"CODE"
"scikit learn is that consumers request what they need and routers pass that";"CODE"
"along additionally a router exposes what it requires itself so that it can";"CODE"
"be used inside another router e g a pipeline inside a grid search object";"CODE"
"the output of the get metadata routing which is a dictionary";"IRRE"
"representation of a class utils metadata routing metadatarouter includes";"CODE"
"the complete tree of requested metadata by all nested objects and their";"CODE"
"corresponding method routings i e which method of a sub estimator is used";"CODE"
"in which method of a meta estimator";"CODE"
"as you can see the only metadata requested for method fit is";"CODE"
"sample weight with aliased sample weight as the alias the";"-"
"utils metadata routing metadatarouter class enables us to easily create";"IRRE"
"the routing object which would create the output we need for our";"IRRE"
"get metadata routing";"-"
"in order to understand how aliases work in meta estimators imagine our";"-"
"meta estimator inside another one";"-"
"in the above example this is how the fit method of meta meta est";"CODE"
"will call their sub estimator s fit methods";"IRRE"
"user feeds my weights as aliased sample weight into meta meta est";"CODE"
"meta meta est fit x y aliased sample weight my weights";"-"
"the first sub estimator meta est expects aliased sample weight";"-"
"self estimator fit x y aliased sample weight aliased sample weight";"CODE"
"the second sub estimator est expects sample weight";"-"
"self estimator fit x y sample weight aliased sample weight";"CODE"
"consuming and routing meta estimator";"-"
"for a slightly more complex example consider a meta estimator that routes";"META"
"metadata to an underlying estimator as before but it also uses some metadata";"META"
"in its own methods this meta estimator is a consumer and a router at the";"IRRE"
"same time implementing one is very similar to what we had before but with a";"TASK"
"few tweaks";"-"
"defining metadata routing request values for usage in the meta estimator";"CODE"
"defining metadata routing request values for usage in the sub estimator";"CODE"
"since sample weight is used and consumed here it should be defined as";"CODE"
"an explicit argument in the method s signature all other metadata which";"CODE"
"are only routed will be passed as fit params";"-"
"we add sample weight to the fit params dictionary";"TASK"
"as in fit we get a copy of the object s metadatarouter";"IRRE"
"we validate the given metadata";"-"
"and then prepare the input to the underlying predict method";"CODE"
"the key parts where the above meta estimator differs from our previous";"CODE"
"meta estimator is accepting sample weight explicitly in fit and";"-"
"including it in fit params since sample weight is an explicit";"-"
"argument we can be sure that set fit request sample weight is";"IRRE"
"present for this method the meta estimator is both a consumer as well as a";"CODE"
"router of sample weight";"-"
"in get metadata routing we add self to the routing using";"TASK"
"add self request to indicate this estimator is consuming";"CODE"
"sample weight as well as being a router which also adds a";"TASK"
"self request key to the routing info as illustrated below now let s";"CODE"
"look at some examples";"-"
"no metadata requested";"CODE"
"sample weight requested by sub estimator";"CODE"
"sample weight requested by meta estimator";"CODE"
"note the difference in the requested metadata representations above";"CODE"
"we can also alias the metadata to pass different values to the fit methods";"IRRE"
"of the meta and the sub estimator";"-"
"however fit of the meta estimator only needs the alias for the";"TASK"
"sub estimator and addresses their own sample weight as sample weight since";"TASK"
"it doesn t validate and route its own required metadata";"CODE"
"alias only on the sub estimator";"-"
"this is useful when we don t want the meta estimator to use the metadata but";"CODE"
"the sub estimator should";"-"
"the meta estimator cannot use aliased sample weight because it expects";"-"
"it passed as sample weight this would apply even if";"CODE"
"set fit request sample weight true was set on it";"IRRE"
"simple pipeline";"CODE"
"a slightly more complicated use case is a meta estimator resembling a";"CODE"
"class pipeline pipeline here is a meta estimator which accepts a";"CODE"
"transformer and a classifier when calling its fit method it applies the";"IRRE"
"transformer s fit and transform before running the classifier on the";"CODE"
"transformed data upon predict it applies the transformer s transform";"CODE"
"before predicting with the classifier s predict method on the transformed";"CODE"
"new data";"CODE"
"we add the routing for the transformer";"CODE"
"the metadata is routed such that it retraces how";"-"
"simplepipeline internally calls the transformer s fit and";"CODE"
"transform methods in its own methods fit and predict";"IRRE"
"we add the routing for the classifier";"CODE"
"note the usage of class utils metadata routing methodmapping to";"TASK"
"declare which methods of the child estimator callee are used in which";"IRRE"
"methods of the meta estimator caller as you can see simplepipeline uses";"IRRE"
"the transformer s transform and fit methods in fit and its";"CODE"
"transform method in predict and that s what you see implemented in";"TASK"
"the routing structure of the pipeline class";"CODE"
"another difference in the above example with the previous ones is the usage";"CODE"
"of func utils metadata routing process routing which processes the input";"CODE"
"parameters does the required validation and returns the routed params";"CODE"
"which we had created in previous examples this reduces the boilerplate code";"IRRE"
"a developer needs to write in each meta estimator s method developers are";"TASK"
"strongly recommended to use this function unless there is a good reason";"CODE"
"against it";"-"
"in order to test the above pipeline let s add an example transformer";"CODE"
"note that in the above example we have implemented fit transform which";"TASK"
"calls fit and transform with the appropriate metadata this is only";"CODE"
"required if transform accepts metadata since the default fit transform";"CODE"
"implementation in class base transformermixin doesn t pass metadata to";"CODE"
"transform";"CODE"
"now we can test our pipeline and see if metadata is correctly passed around";"IRRE"
"this example uses our simplepipeline our exampletransformer and our";"CODE"
"routerconsumerclassifier which uses our exampleclassifier";"IRRE"
"we set transformer s fit to receive sample weight";"IRRE"
"we set transformer s transform to receive groups";"CODE"
"we want this sub estimator to receive sample weight in fit";"CODE"
"but not groups in predict";"META"
"and we want the meta estimator to receive sample weight as well";"-"
"deprecation default value change";"IRRE"
"in this section we show how one should handle the case where a router becomes";"CODE"
"also a consumer especially when it consumes the same metadata as its";"-"
"sub estimator or a consumer starts consuming a metadata which it wasn t in";"-"
"an older release in this case a warning should be raised for a while to";"CODE"
"let users know the behavior is changed from previous versions";"CODE"
"as explained above this is a valid usage if my weights aren t supposed";"CODE"
"to be passed as sample weight to metaregressor";"-"
"now imagine we further develop metaregressor and it now also consumes";"IRRE"
"sample weight";"-"
"show warning to remind user to explicitly set the value with";"IRRE"
"set method request sample weight boolean";"CODE"
"the above implementation is almost the same as metaregressor and";"TASK"
"because of the default request value defined in metadata request fit";"CODE"
"there is a warning raised when fitted";"CODE"
"when an estimator consumes a metadata which it didn t consume before the";"CODE"
"following pattern can be used to warn the users about it";"OUTD"
"at the end we disable the configuration flag for metadata routing";"CODE"
"third party development and scikit learn dependency";"CODE"
"as seen above information is communicated between classes using";"CODE"
"class utils metadata routing metadatarequest and";"CODE"
"class utils metadata routing metadatarouter it is strongly not advised";"IRRE"
"but possible to vendor the tools related to metadata routing if you strictly";"CODE"
"want to have a scikit learn compatible estimator without depending on the";"TASK"
"scikit learn package if all of the following conditions are met you do not";"CODE"
"need to modify your code at all";"TASK"
"your estimator inherits from class base baseestimator";"CODE"
"the parameters consumed by your estimator s methods e g fit are";"IRRE"
"explicitly defined in the method s signature as opposed to being";"CODE"
"args or kwargs";"IRRE"
"your estimator does not route any metadata to the underlying objects i e";"CODE"
"it s not a router";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"get the separating hyperplane";"-"
"xx np linspace min x 5 max x 5 make sure the line is long enough";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the faces datasets";"IRRE"
"test data targets 30 test on independent people";"IRRE"
"test on a subset of people";"IRRE"
"upper half of the faces";"-"
"lower half of the faces";"-"
"fit estimators";"IRRE"
"plot the completed faces";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset preprocessing and model training";"IRRE"
"different outlier detection models require different preprocessing in the";"CODE"
"presence of categorical variables";"IRRE"
"class sklearn preprocessing ordinalencoder is often a good strategy for";"CODE"
"tree based models such as class sklearn ensemble isolationforest whereas";"CODE"
"neighbors based models such as class sklearn neighbors localoutlierfactor";"IRRE"
"would be impacted by the ordering induced by ordinal encoding to avoid";"CODE"
"inducing an ordering on should rather use";"CODE"
"class sklearn preprocessing onehotencoder";"IRRE"
"neighbors based models may also require scaling of the numerical features see";"TASK"
"for instance ref neighbors scaling in the presence of outliers a good";"CODE"
"option is to use a class sklearn preprocessing robustscaler";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"train models on the diabetes dataset";"IRRE"
"first we train a decision tree and a multi layer perceptron on the diabetes";"-"
"dataset";"IRRE"
"plotting partial dependence for two features";"CODE"
"we plot partial dependence curves for features age and bmi body mass";"CODE"
"index for the decision tree with two features";"TASK"
"func sklearn inspection partialdependencedisplay from estimator expects to plot";"CODE"
"two curves here the plot function place a grid of two plots using the space";"CODE"
"defined by ax";"CODE"
"the partial dependence curves can be plotted for the multi layer perceptron";"CODE"
"in this case line kw is passed to";"CODE"
"func sklearn inspection partialdependencedisplay from estimator to change the";"CODE"
"color of the curve";"-"
"plotting partial dependence of the two models together";"CODE"
"the tree disp and mlp disp";"-"
"class sklearn inspection partialdependencedisplay objects contain all the";"CODE"
"computed information needed to recreate the partial dependence curves this";"CODE"
"means we can easily create additional plots without needing to recompute the";"TASK"
"curves";"-"
"one way to plot the curves is to place them in the same figure with the";"CODE"
"curves of each model on each row first we create a figure with two axes";"IRRE"
"within two rows and one column the two axes are passed to the";"CODE"
"func sklearn inspection partialdependencedisplay plot functions of";"CODE"
"tree disp and mlp disp the given axes will be used by the plotting";"-"
"function to draw the partial dependence the resulting plot places the";"CODE"
"decision tree partial dependence curves in the first row of the";"CODE"
"multi layer perceptron in the second row";"CODE"
"another way to compare the curves is to plot them on top of each other here";"IRRE"
"we create a figure with one row and two columns the axes are passed into the";"IRRE"
"func sklearn inspection partialdependencedisplay plot function as a list";"CODE"
"which will plot the partial dependence curves of each model on the same axes";"CODE"
"the length of the axes list must be equal to the number of plots drawn";"TASK"
"sphinx gallery thumbnail number 4";"-"
"tree disp axes is a numpy array container the axes used to draw the";"OUTD"
"partial dependence plots this can be passed to mlp disp to have the same";"CODE"
"affect of drawing the plots on top of each other furthermore the";"-"
"mlp disp figure stores the figure which allows for resizing the figure";"CODE"
"after calling plot in this case tree disp axes has two dimensions thus";"CODE"
"plot will only show the y label and y ticks on the left most plot";"-"
"plotting partial dependence for one feature";"CODE"
"here we plot the partial dependence curves for a single feature age on";"CODE"
"the same axes in this case tree disp axes is passed into the second";"CODE"
"plot function";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"displaying a pipeline with a preprocessing step and classifier";"CODE"
"this section constructs a class sklearn pipeline pipeline with a preprocessing";"CODE"
"step class sklearn preprocessing standardscaler and classifier";"IRRE"
"class sklearn linear model logisticregression and displays its visual";"IRRE"
"representation";"-"
"to visualize the diagram the default is display diagram";"CODE"
"pipe click on the diagram below to see the details of each step";"CODE"
"to view the text pipeline change to display text";"CODE"
"put back the default display";"CODE"
"displaying a pipeline chaining multiple preprocessing steps classifier";"CODE"
"this section constructs a class sklearn pipeline pipeline with multiple";"CODE"
"preprocessing steps class sklearn preprocessing polynomialfeatures and";"TASK"
"class sklearn preprocessing standardscaler and a classifier step";"IRRE"
"class sklearn linear model logisticregression and displays its visual";"IRRE"
"representation";"-"
"pipe click on the diagram below to see the details of each step";"CODE"
"displaying a pipeline and dimensionality reduction and classifier";"CODE"
"this section constructs a class sklearn pipeline pipeline with a";"CODE"
"dimensionality reduction step class sklearn decomposition pca";"IRRE"
"a classifier class sklearn svm svc and displays its visual";"IRRE"
"representation";"-"
"pipe click on the diagram below to see the details of each step";"CODE"
"displaying a complex pipeline chaining a column transformer";"CODE"
"this section constructs a complex class sklearn pipeline pipeline with a";"CODE"
"class sklearn compose columntransformer and a classifier";"IRRE"
"class sklearn linear model logisticregression and displays its visual";"IRRE"
"representation";"-"
"pipe click on the diagram below to see the details of each step";"CODE"
"displaying a grid search over a pipeline with a classifier";"CODE"
"this section constructs a class sklearn model selection gridsearchcv";"CODE"
"over a class sklearn pipeline pipeline with";"CODE"
"class sklearn ensemble randomforestclassifier and displays its visual";"IRRE"
"representation";"-"
"grid search click on the diagram below to see the details of each step";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load data and train a svc";"TASK"
"first we load the wine dataset and convert it to a binary classification";"IRRE"
"problem then we train a support vector classifier on a training dataset";"IRRE"
"plotting the roc curve";"-"
"next we plot the roc curve with a single call to";"IRRE"
"func sklearn metrics roccurvedisplay from estimator the returned";"CODE"
"svc disp object allows us to continue using the already computed roc curve";"CODE"
"for the svc in future plots";"CODE"
"training a random forest and plotting the roc curve";"IRRE"
"we train a random forest classifier and create a plot comparing it to the svc";"IRRE"
"roc curve notice how svc disp uses";"-"
"func sklearn metrics roccurvedisplay plot to plot the svc roc curve";"-"
"without recomputing the values of the roc curve itself furthermore we";"IRRE"
"pass alpha 0 8 to the plot functions to adjust the alpha values of the";"IRRE"
"curves";"-"
"first we load the iris dataset as a dataframe to demonstrate the set output api";"IRRE"
"to configure an estimator such as class preprocessing standardscaler to return";"IRRE"
"dataframes call set output this feature requires pandas to be installed";"IRRE"
"set output can be called after fit to configure transform after the fact";"IRRE"
"in a class pipeline pipeline set output configures all steps to output";"IRRE"
"dataframes";"-"
"each transformer in the pipeline is configured to return dataframes this";"CODE"
"means that the final logistic regression step contains the feature names of the input";"CODE"
"note if one uses the method set params the transformer will be";"TASK"
"replaced by a new one with the default output format";"CODE"
"to keep the intended behavior use set output on the new transformer";"CODE"
"beforehand";"CODE"
"next we load the titanic dataset to demonstrate set output with";"IRRE"
"class compose columntransformer and heterogeneous data";"IRRE"
"the set output api can be configured globally by using func set config and";"IRRE"
"setting transform output to pandas";"IRRE"
"with the global configuration all transformers output dataframes this allows us to";"CODE"
"easily plot the logistic regression coefficients with the corresponding feature names";"TASK"
"in order to demonstrate the func config context functionality below let";"CODE"
"us first reset transform output to its default value";"IRRE"
"when configuring the output type with func config context the";"IRRE"
"configuration at the time when transform or fit transform are";"CODE"
"called is what counts setting these only when you construct or fit";"IRRE"
"the transformer has no effect";"CODE"
"the output of transform will be a pandas dataframe";"IRRE"
"outside of the context manager the output will be a numpy array";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"ellipse needs degrees";"TASK"
"eigenvector normalization";"-"
"ell set facecolor 56b4e9";"IRRE"
"color 56b4e9";"-"
"parameters of the dataset";"IRRE"
"colors np array 0072b2 f0e442 d55e00";"-"
"mean precision prior 0 8 to minimize the influence of the prior";"-"
"generate data";"-"
"plot results in two different figures";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"as the dp will not use every component it has access to";"-"
"unless it needs it we shouldn t plot the redundant";"TASK"
"components";"-"
"plot an ellipse to show the gaussian component";"IRRE"
"angle 180 0 angle np pi convert to degrees";"-"
"number of samples per component";"-"
"generate random sample two components";"IRRE"
"fit a gaussian mixture with em using five components";"-"
"fit a dirichlet process gaussian mixture using five components";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"angle 180 angle np pi convert to degrees";"-"
"break up the dataset into non overlapping training 75 and testing";"IRRE"
"25 sets";"IRRE"
"only take the first fold";"-"
"try gmms using different types of covariances";"CODE"
"since we have class labels for the training data we can";"CODE"
"initialize the gmm parameters in a supervised manner";"IRRE"
"train the other parameters using the em algorithm";"IRRE"
"plot the test data with crosses";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate some data";"-"
"run a gaussianmixture with max iter 0 to output the initialization means";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate random sample two components";"IRRE"
"generate spherical data centered on 20 20";"CODE"
"generate zero centered stretched gaussian data";"-"
"concatenate the two datasets into the final training set";"IRRE"
"fit a gaussian mixture model with two components";"-"
"display predicted scores by the model as a contour plot";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation";"-"
"we generate two components each one containing n samples by randomly";"IRRE"
"sampling the standard normal distribution as returned by numpy random randn";"IRRE"
"one component is kept spherical yet shifted and re scaled the other one is";"TASK"
"deformed to have a more general covariance matrix";"CODE"
"component 1 np dot np random randn n samples 2 c general";"IRRE"
"component 2 0 7 np random randn n samples 2 np array 4 1 spherical";"IRRE"
"we can visualize the different components";"-"
"model training and selection";"CODE"
"we vary the number of components from 1 to 6 and the type of covariance";"CODE"
"parameters to use";"IRRE"
"full each component has its own general covariance matrix";"CODE"
"tied all components share the same general covariance matrix";"CODE"
"diag each component has its own diagonal covariance matrix";"CODE"
"spherical each component has its own single variance";"CODE"
"we score the different models and keep the best model the lowest bic this";"CODE"
"is done by using class sklearn model selection gridsearchcv and a";"CODE"
"user defined score function which returns the negative bic score as";"CODE"
"class sklearn model selection gridsearchcv is designed to maximize a";"CODE"
"score maximizing the negative bic is equivalent to minimizing the bic";"-"
"the best set of parameters and estimator are stored in best parameters and";"IRRE"
"best estimator respectively";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"as the dp will not use every component it has access to";"-"
"unless it needs it we shouldn t plot the redundant";"TASK"
"components";"-"
"plot an ellipse to show the gaussian component";"IRRE"
"angle 180 0 angle np pi convert to degrees";"-"
"as the dp will not use every component it has access to";"-"
"unless it needs it we shouldn t plot the redundant";"TASK"
"components";"-"
"parameters";"IRRE"
"generate random sample following a sine curve";"IRRE"
"fit a gaussian mixture with em using ten components";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"import some data to play with";"CODE"
"split the data into a training set and a test set";"IRRE"
"run classifier using a model that is too regularized c too low to see";"CODE"
"the impact on the results";"IRRE"
"plot non normalized confusion matrix";"-"
"binary classification";"IRRE"
"for binary problems func sklearn metrics confusion matrix has the ravel method";"CODE"
"we can use get counts of true negatives false positives false negatives and";"-"
"true positives";"-"
"to obtain true negatives false positives false negatives and true";"CODE"
"positives counts at different thresholds one can use";"-"
"func sklearn metrics confusion matrix at thresholds";"-"
"this is fundamental for binary classification";"CODE"
"metrics like func sklearn metrics roc auc score and";"-"
"func sklearn metrics det curve";"-"
"plot tns fps fns and tps vs thresholds";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"cost sensitive learning with constant gains and costs";"CODE"
"in this first section we illustrate the use of the";"CODE"
"class sklearn model selection tunedthresholdclassifiercv in a setting of";"IRRE"
"cost sensitive learning when the gains and costs associated to each entry of the";"CODE"
"confusion matrix are constant we use the problematic presented in 2 using the";"IRRE"
"statlog german credit dataset 1";"IRRE"
"statlog german credit dataset";"IRRE"
"we fetch the german credit dataset from openml";"CODE"
"we check the feature types available in x";"TASK"
"many features are categorical and usually string encoded we need to encode";"TASK"
"these categories when we develop our predictive model let s check the targets";"CODE"
"another observation is that the dataset is imbalanced we would need to be careful";"TASK"
"when evaluating our predictive model and use a family of metrics that are adapted";"CODE"
"to this setting";"IRRE"
"in addition we observe that the target is string encoded some metrics";"TASK"
"e g precision and recall require to provide the label of interest also called";"IRRE"
"the positive label here we define that our goal is to predict whether or not";"CODE"
"a sample is a bad credit";"-"
"to carry our analysis we split our dataset using a single stratified split";"IRRE"
"we are ready to design our predictive model and the associated evaluation strategy";"CODE"
"evaluation metrics";"-"
"in this section we define a set of metrics that we use later to see";"CODE"
"the effect of tuning the cut off point we evaluate the predictive model using";"CODE"
"the receiver operating characteristic roc curve and the precision recall curve";"IRRE"
"the values reported on these plots are therefore the true positive rate tpr";"IRRE"
"also known as the recall or the sensitivity and the false positive rate fpr";"IRRE"
"also known as the specificity for the roc curve and the precision and recall for";"CODE"
"the precision recall curve";"IRRE"
"from these four metrics scikit learn does not provide a scorer for the fpr we";"CODE"
"therefore need to define a small custom function to compute it";"CODE"
"as previously stated the positive label is not defined as the value 1 and calling";"IRRE"
"some of the metrics with this non standard value raise an error we need to";"CODE"
"provide the indication of the positive label to the metrics";"-"
"we therefore need to define a scikit learn scorer using";"CODE"
"func sklearn metrics make scorer where the information is passed we store all";"CODE"
"the custom scorers in a dictionary to use them we need to pass the fitted model";"TASK"
"the data and the target on which we want to evaluate the predictive model";"-"
"tpr score recall score tpr and recall are the same metric";"IRRE"
"in addition the original research 1 defines a custom business metric we";"CODE"
"call a business metric any metric function that aims at quantifying how the";"IRRE"
"predictions correct or wrong might impact the business value of deploying a";"IRRE"
"given machine learning model in a specific application context for our";"CODE"
"credit prediction task the authors provide a custom cost matrix which";"TASK"
"encodes that classifying a bad credit as good is 5 times more costly on";"IRRE"
"average than the opposite it is less costly for the financing institution to";"CODE"
"not grant a credit to a potential customer that will not default and";"CODE"
"therefore miss a good customer that would have otherwise both reimbursed the";"CODE"
"credit and paid interests than to grant a credit to a customer that will";"CODE"
"default";"CODE"
"we define a python function that weighs the confusion matrix and returns the";"CODE"
"overall cost";"-"
"the rows of the confusion matrix hold the counts of observed classes";"IRRE"
"while the columns hold counts of predicted classes recall that here we";"CODE"
"consider bad as the positive class second row and column";"IRRE"
"scikit learn model selection tools expect that we follow a convention";"CODE"
"that higher means better hence the following gain matrix assigns";"IRRE"
"negative gains costs to the two kinds of prediction errors";"-"
"a gain of 1 for each false positive good credit labeled as bad";"CODE"
"a gain of 5 for each false negative bad credit labeled as good";"CODE"
"a 0 gain for true positives and true negatives";"CODE"
"note that theoretically given that our model is calibrated and our data";"TASK"
"set representative and large enough we do not need to tune the";"TASK"
"threshold but can safely set it to 1 5 of the cost ratio as stated by";"IRRE"
"eq 2 in elkan s paper 2";"-"
"0 1 1 gain for false positives";"CODE"
"5 0 5 gain for false negatives";"CODE"
"vanilla predictive model";"-"
"we use class sklearn ensemble histgradientboostingclassifier as a predictive model";"IRRE"
"that natively handles categorical features and missing values";"IRRE"
"we evaluate the performance of our predictive model using the roc and precision recall";"IRRE"
"curves";"-"
"we recall that these curves give insights on the statistical performance of the";"IRRE"
"predictive model for different cut off points for the precision recall curve the";"CODE"
"reported metrics are the precision and recall and for the roc curve the reported";"IRRE"
"metrics are the tpr same as recall and fpr";"IRRE"
"here the different cut off points correspond to different levels of posterior";"CODE"
"probability estimates ranging between 0 and 1 by default model predict uses a";"CODE"
"cut off point at a probability estimate of 0 5 the metrics for such a cut off point";"CODE"
"are reported with the blue dot on the curves it corresponds to the statistical";"CODE"
"performance of the model when using model predict";"CODE"
"however we recall that the original aim was to minimize the cost or maximize the";"IRRE"
"gain as defined by the business metric we can compute the value of the business";"IRRE"
"metric";"-"
"at this stage we don t know if any other cut off can lead to a greater gain to find";"CODE"
"the optimal one we need to compute the cost gain using the business metric for all";"CODE"
"possible cut off points and choose the best this strategy can be quite tedious to";"CODE"
"implement by hand but the";"TASK"
"class sklearn model selection tunedthresholdclassifiercv class is here to help us";"CODE"
"it automatically computes the cost gain for all possible cut off points and optimizes";"CODE"
"for the scoring";"CODE"
"cost sensitive learning example";"-"
"tuning the cut off point";"CODE"
"we use class sklearn model selection tunedthresholdclassifiercv to tune the";"CODE"
"cut off point we need to provide the business metric to optimize as well as the";"TASK"
"positive label internally the optimum cut off point is chosen such that it maximizes";"CODE"
"the business metric via cross validation by default a 5 fold stratified";"CODE"
"cross validation is used";"-"
"tore cv results true necessary to inspect all results";"IRRE"
"we plot the roc and precision recall curves for the vanilla model and the tuned model";"IRRE"
"also we plot the cut off points that would be used by each model because we are";"CODE"
"reusing the same code later we define a function that generates the plots";"CODE"
"the first remark is that both classifiers have exactly the same roc and";"IRRE"
"precision recall curves it is expected because by default the classifier is fitted";"IRRE"
"on the same training data in a later section we discuss more in detail the";"-"
"available options regarding model refitting and cross validation";"-"
"the second remark is that the cut off points of the vanilla and tuned model are";"CODE"
"different to understand why the tuned model has chosen this cut off point we can";"CODE"
"look at the right hand side plot that plots the objective score that is our exactly";"IRRE"
"the same as our business metric we see that the optimum threshold corresponds to the";"-"
"maximum of the objective score this maximum is reached for a decision threshold";"CODE"
"much lower than 0 5 the tuned model enjoys a much higher recall at the cost of";"IRRE"
"of significantly lower precision the tuned model is much more eager to";"-"
"predict the bad class label to larger fraction of individuals";"IRRE"
"we can now check if choosing this cut off point leads to a better score on the testing";"IRRE"
"set";"IRRE"
"we observe that tuning the decision threshold almost improves our business gains";"-"
"by factor of 2";"-"
"tunedthresholdclassifiercv no cv";"IRRE"
"consideration regarding model refitting and cross validation";"-"
"in the above experiment we used the default setting of the";"CODE"
"class sklearn model selection tunedthresholdclassifiercv in particular the";"CODE"
"cut off point is tuned using a 5 fold stratified cross validation also the";"CODE"
"underlying predictive model is refitted on the entire training data once the cut off";"CODE"
"point is chosen";"CODE"
"these two strategies can be changed by providing the refit and cv parameters";"IRRE"
"for instance one could provide a fitted estimator and set cv prefit in which";"IRRE"
"case the cut off point is found on the entire dataset provided at fitting time";"CODE"
"also the underlying classifier is not be refitted by setting refit false here we";"IRRE"
"can try to do such experiment";"CODE"
"then we evaluate our model with the same approach as before";"CODE"
"we observe the that the optimum cut off point is different from the one found";"CODE"
"in the previous experiment if we look at the right hand side plot we";"CODE"
"observe that the business gain has large plateau of near optimal 0 gain for a";"CODE"
"large span of decision thresholds this behavior is symptomatic of an";"CODE"
"overfitting because we disable cross validation we tuned the cut off point";"CODE"
"on the same set as the model was trained on and this is the reason for the";"CODE"
"observed overfitting";"CODE"
"this option should therefore be used with caution one needs to make sure that the";"CODE"
"data provided at fitting time to the";"-"
"class sklearn model selection tunedthresholdclassifiercv is not the same as the";"CODE"
"data used to train the underlying classifier this could happen sometimes when the";"CODE"
"idea is just to tune the predictive model on a completely new validation set without a";"CODE"
"costly complete refit";"CODE"
"when cross validation is too costly a potential alternative is to use a";"-"
"single train test split by providing a floating number in range 0 1 to the cv";"CODE"
"parameter it splits the data into a training and testing set let s explore this";"IRRE"
"option";"-"
"regarding the cut off point we observe that the optimum is similar to the multiple";"CODE"
"repeated cross validation case however be aware that a single split does not account";"CODE"
"for the variability of the fit predict process and thus we are unable to know if there";"CODE"
"is any variance in the cut off point the repeated cross validation averages out";"CODE"
"this effect";"CODE"
"another observation concerns the roc and precision recall curves of the tuned model";"IRRE"
"as expected these curves differ from those of the vanilla model given that we";"CODE"
"trained the underlying classifier on a subset of the data provided during fitting and";"IRRE"
"reserved a validation set for tuning the cut off point";"CODE"
"cost sensitive learning when gains and costs are not constant";"CODE"
"as stated in 2 gains and costs are generally not constant in real world problems";"CODE"
"in this section we use a similar example as in 2 for the problem of";"CODE"
"detecting fraud in credit card transaction records";"-"
"the credit card dataset";"IRRE"
"the dataset contains information about credit card records from which some are";"CODE"
"fraudulent and others are legitimate the goal is therefore to predict whether or";"CODE"
"not a credit card record is fraudulent";"-"
"first we check the class distribution of the datasets";"IRRE"
"the dataset is highly imbalanced with fraudulent transaction representing only 0 17";"IRRE"
"of the data since we are interested in training a machine learning model we should";"CODE"
"also make sure that we have enough samples in the minority class to train the model";"CODE"
"we observe that we have around 500 samples that is on the low end of the number of";"CODE"
"samples required to train a machine learning model in addition of the target";"TASK"
"distribution we check the distribution of the amount of the";"META"
"fraudulent transactions";"-"
"addressing the problem with a business metric";"TASK"
"now we create the business metric that depends on the amount of each transaction we";"IRRE"
"define the cost matrix similarly to 2 accepting a legitimate transaction provides";"CODE"
"a gain of 2 of the amount of the transaction however accepting a fraudulent";"-"
"transaction result in a loss of the amount of the transaction as stated in 2 the";"IRRE"
"gain and loss related to refusals of fraudulent and legitimate transactions are not";"-"
"trivial to define here we define that a refusal of a legitimate transaction";"CODE"
"is estimated to a loss of 5 while the refusal of a fraudulent transaction is";"CODE"
"estimated to a gain of 50 therefore we define the following function to";"CODE"
"compute the total benefit of a given decision";"CODE"
"from this business metric we create a scikit learn scorer that given a fitted";"CODE"
"classifier and a test set compute the business metric in this regard we use";"IRRE"
"the func sklearn metrics make scorer factory the variable amount is an";"IRRE"
"additional metadata to be passed to the scorer and we need to use";"TASK"
"ref metadata routing metadata routing to take into account this information";"CODE"
"so at this stage we observe that the amount of the transaction is used twice once";"CODE"
"as a feature to train our predictive model and once as a metadata to compute the";"TASK"
"the business metric and thus the statistical performance of our model when used as a";"CODE"
"feature we are only required to have a column in data that contains the amount of";"TASK"
"each transaction to use this information as metadata we need to have an external";"CODE"
"variable that we can pass to the scorer or the model that internally routes this";"CODE"
"metadata to the scorer so let s create this variable";"CODE"
"we first evaluate some baseline policies to serve as reference recall that";"IRRE"
"class 0 is the legitimate class and class 1 is the fraudulent class";"IRRE"
"a policy that considers all transactions as legitimate would create a profit of";"IRRE"
"around 220 000 we make the same evaluation for a classifier that predicts all";"CODE"
"transactions as fraudulent";"-"
"such a policy would entail a catastrophic loss around 670 000 this is";"CODE"
"expected since the vast majority of the transactions are legitimate and the";"-"
"policy would refuse them at a non trivial cost";"IRRE"
"a predictive model that adapts the accept reject decisions on a per";"-"
"transaction basis should ideally allow us to make a profit larger than the";"-"
"220 000 of the best of our constant baseline policies";"CODE"
"we start with a logistic regression model with the default decision threshold";"CODE"
"at 0 5 here we tune the hyperparameter c of the logistic regression with a";"IRRE"
"proper scoring rule the log loss to ensure that the model s probabilistic";"CODE"
"predictions returned by its predict proba method are as accurate as";"IRRE"
"possible irrespectively of the choice of the value of the decision";"IRRE"
"threshold";"-"
"the business metric shows that our predictive model with a default decision";"CODE"
"threshold is already winning over the baseline in terms of profit and it would be";"CODE"
"already beneficial to use it to accept or reject transactions instead of";"CODE"
"accepting all transactions";"-"
"tuning the decision threshold";"-"
"now the question is is our model optimum for the type of decision that we want to do";"CODE"
"up to now we did not optimize the decision threshold we use the";"IRRE"
"class sklearn model selection tunedthresholdclassifiercv to optimize the decision";"CODE"
"given our business scorer to avoid a nested cross validation we will use the";"IRRE"
"best estimator found during the previous grid search";"IRRE"
"since our business scorer requires the amount of each transaction we need to pass";"TASK"
"this information in the fit method the";"CODE"
"class sklearn model selection tunedthresholdclassifiercv is in charge of";"CODE"
"automatically dispatching this metadata to the underlying scorer";"IRRE"
"we observe that the tuned decision threshold is far away from the default 0 5";"CODE"
"we observe that tuning the decision threshold increases the expected profit";"-"
"when deploying our model as indicated by the business metric it is therefore";"CODE"
"valuable whenever possible to optimize the decision threshold with respect";"CODE"
"to the business metric";"-"
"manually setting the decision threshold instead of tuning it";"IRRE"
"in the previous example we used the";"CODE"
"class sklearn model selection tunedthresholdclassifiercv to find the optimal";"CODE"
"decision threshold however in some cases we might have some prior knowledge about";"CODE"
"the problem at hand and we might be happy to set the decision threshold manually";"IRRE"
"the class class sklearn model selection fixedthresholdclassifier allows us to";"CODE"
"manually set the decision threshold at prediction time it behave as the previous";"IRRE"
"tuned model but no search is performed during the fitting process note that here";"TASK"
"we use class sklearn frozen frozenestimator to wrap the predictive model to";"IRRE"
"avoid any refitting";"CODE"
"here we will reuse the decision threshold found in the previous section to create a";"IRRE"
"new model and check that it gives the same results";"IRRE"
"we observe that we obtained the exact same results but the fitting process";"IRRE"
"was much faster since we did not perform any hyper parameter search";"IRRE"
"finally the estimate of the average business metric itself can be unreliable in";"CODE"
"particular when the number of data points in the minority class is very small";"CODE"
"any business impact estimated by cross validation of a business metric on";"IRRE"
"historical data offline evaluation should ideally be confirmed by a b testing";"IRRE"
"on live data online evaluation note however that a b testing models is";"TASK"
"beyond the scope of the scikit learn library itself";"CODE"
"at the end we disable the configuration flag for metadata routing";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"visualize our data";"-"
"first we must understand the structure of our data it has 100 randomly";"IRRE"
"generated input datapoints 3 classes split unevenly across datapoints";"CODE"
"and 10 groups split evenly across datapoints";"CODE"
"as we ll see some cross validation objects do specific things with";"CODE"
"labeled data others behave differently with grouped data and others";"CODE"
"do not use this information";"CODE"
"to begin we ll visualize our data";"-"
"generate the class group data";"IRRE"
"generate uneven groups";"-"
"visualize dataset groups";"IRRE"
"define a function to visualize cross validation behavior";"CODE"
"we ll define a function that lets us visualize the behavior of each";"CODE"
"cross validation object we ll perform 4 splits of the data on each";"CODE"
"split we ll visualize the indices chosen for the training set";"IRRE"
"in blue and the test set in red";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we will load the diabetes dataset and create an instance of a linear";"IRRE"
"regression model";"-"
"func sklearn model selection cross val predict returns an array of the";"CODE"
"same size of y where each entry is a prediction obtained by cross";"CODE"
"validation";"-"
"since cv 10 it means that we trained 10 models and each model was";"-"
"used to predict on one of the 10 folds we can now use the";"IRRE"
"class sklearn metrics predictionerrordisplay to visualize the";"IRRE"
"prediction errors";"-"
"on the left axis we plot the observed values math y vs the predicted";"IRRE"
"values math hat y given by the models on the right axis we plot the";"IRRE"
"residuals i e the difference between the observed values and the predicted";"IRRE"
"values vs the predicted values";"IRRE"
"it is important to note that we used";"CODE"
"func sklearn model selection cross val predict for visualization";"CODE"
"purpose only in this example";"CODE"
"it would be problematic to";"-"
"quantitatively assess the model performance by computing a single";"CODE"
"performance metric from the concatenated predictions returned by";"CODE"
"func sklearn model selection cross val predict";"CODE"
"when the different cv folds vary by size and distributions";"META"
"it is recommended to compute per fold performance metrics using";"CODE"
"func sklearn model selection cross val score or";"CODE"
"func sklearn model selection cross validate instead";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate synthetic data";"-"
"define the classifiers";"CODE"
"here we define two different classifiers the goal is to visually compare their";"IRRE"
"statistical performance across thresholds using the roc and det curves";"CODE"
"compare roc and det curves";"IRRE"
"det curves are commonly plotted in normal deviate scale to achieve this the";"CODE"
"det display transforms the error rates as returned by the";"CODE"
"func sklearn metrics det curve and the axis scale using";"-"
"scipy stats norm";"-"
"notice that it is easier to visually assess the overall performance of";"CODE"
"different classification algorithms using det curves than using roc curves as";"IRRE"
"roc curves are plot in a linear scale different classifiers usually appear";"IRRE"
"similar for a large part of the plot and differ the most in the top left";"CODE"
"corner of the graph on the other hand because det curves represent straight";"-"
"lines in normal deviate scale they tend to be distinguishable as a whole and";"CODE"
"the area of interest spans a large part of the plot";"CODE"
"det curves give direct feedback of the detection error tradeoff to aid in";"-"
"operating point analysis the user can then decide the fnr they are willing to";"CODE"
"accept at the expense of the fpr or vice versa";"-"
"non informative classifier baseline for the roc and det curves";"CODE"
"the diagonal black dotted lines in the plots above correspond to a";"CODE"
"class sklearn dummy dummyclassifier using the default prior strategy to";"CODE"
"serve as baseline for comparison with other classifiers this classifier makes";"CODE"
"constant predictions independent of the input features in x making it a";"CODE"
"non informative classifier";"CODE"
"to further understand the non informative baseline of the roc and det curves";"CODE"
"we recall the following mathematical definitions";"IRRE"
"math text fpr frac text fp text fp text tn";"-"
"math text fnr frac text fn text tp text fn";"-"
"math text tpr frac text tp text tp text fn";"-"
"a classifier that always predict the positive class would have no true";"IRRE"
"negatives nor false negatives giving math text fpr text tpr 1 and";"IRRE"
"math text fnr 0 i e";"-"
"a single point in the upper right corner of the roc plane";"CODE"
"a single point in the lower right corner of the det plane";"CODE"
"similarly a classifier that always predict the negative class would have no";"IRRE"
"true positives nor false positives thus math text fpr text tpr 0";"-"
"and math text fnr 1 i e";"-"
"a single point in the lower left corner of the roc plane";"CODE"
"a single point in the upper left corner of the det plane";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the dataset";"IRRE"
"we will work with the digits dataset the goal is to classify handwritten";"IRRE"
"digits images";"-"
"we transform the problem into a binary classification for easier";"CODE"
"understanding the goal is to identify whether a digit is 8 or not";"-"
"in order to train a classifier on images we need to flatten them into vectors";"CODE"
"each image of 8 by 8 pixels needs to be transformed to a vector of 64 pixels";"TASK"
"thus we will get a final data array of shape n images n pixels";"CODE"
"as presented in the introduction the data will be split into a training";"CODE"
"and a testing set of equal size";"IRRE"
"define our grid search strategy";"CODE"
"we will select a classifier by searching the best hyper parameters on folds";"IRRE"
"of the training set to do this we need to define";"CODE"
"the scores to select the best candidate";"CODE"
"we can also define a function to be passed to the refit parameter of the";"CODE"
"class sklearn model selection gridsearchcv instance it will implement the";"CODE"
"custom strategy to select the best candidate from the cv results attribute";"CODE"
"of the class sklearn model selection gridsearchcv once the candidate is";"CODE"
"selected it is automatically refitted by the";"IRRE"
"class sklearn model selection gridsearchcv instance";"CODE"
"here the strategy is to short list the models which are the best in terms of";"CODE"
"precision and recall from the selected models we finally select the fastest";"CODE"
"model at predicting notice that these custom choices are completely";"CODE"
"arbitrary";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"introduction";"CODE"
"when tuning hyperparameters we often want to balance model complexity and";"IRRE"
"performance the one standard error rule is a common approach select the simplest";"CODE"
"model whose performance is within one standard error of the best model s performance";"CODE"
"this helps to avoid overfitting by preferring simpler models when their performance is";"CODE"
"statistically comparable to more complex ones";"IRRE"
"helper functions";"CODE"
"we define two helper functions";"CODE"
"1 lower bound calculates the threshold for acceptable performance";"CODE"
"best score 1 std";"CODE"
"2 best low complexity selects the model with the fewest pca components that";"META"
"exceeds this threshold";"CODE"
"set up the pipeline and parameter grid";"IRRE"
"we create a pipeline with two steps";"CODE"
"1 dimensionality reduction using pca";"-"
"2 classification using logisticregression";"IRRE"
"we ll search over different numbers of pca components to find the optimal complexity";"META"
"perform the search with gridsearchcv";"CODE"
"we use gridsearchcv with our custom best low complexity function as the refit";"META"
"parameter this function will select the model with the fewest pca components that";"CODE"
"still performs within one standard deviation of the best model";"TASK"
"use a non stratified cv strategy to make sure that the inter fold";"CODE"
"standard deviation of the test scores is informative";"IRRE"
"n jobs 1 increase this on your machine to use more physical cores";"IRRE"
"load the digits dataset and fit the model";"IRRE"
"visualize the results";"IRRE"
"we ll create a bar chart showing the test scores for different numbers of pca";"IRRE"
"components along with horizontal lines indicating the best score and the";"-"
"one standard deviation threshold";"-"
"create a polars dataframe for better data manipulation and visualization";"IRRE"
"sort by number of components";"-"
"calculate the lower bound threshold";"-"
"get the best model information";"CODE"
"add a column to mark the selected model";"TASK"
"get the number of cv splits from the results";"IRRE"
"extract individual scores for each split";"CODE"
"calculate mean and std of test scores";"CODE"
"find best score and threshold";"-"
"create a single figure for visualization";"IRRE"
"plot individual points";"CODE"
"plot individual test points";"IRRE"
"plot individual train points";"CODE"
"plot mean lines with error bands";"-"
"add threshold lines";"TASK"
"color 9b59b6 purple";"-"
"color e67e22 orange";"-"
"highlight selected model";"CODE"
"color 9b59b6 purple";"-"
"set titles and labels";"IRRE"
"set axis properties";"IRRE"
"adjust layout";"-"
"print the results";"IRRE"
"we print information about the selected model including its complexity and";"CODE"
"performance we also show a summary table of all models using polars";"CODE"
"create a summary table with polars";"IRRE"
"add a column to mark the selected model";"TASK"
"conclusion";"-"
"the one standard error rule helps us select a simpler model fewer pca components";"CODE"
"while maintaining performance statistically comparable to the best model";"CODE"
"this approach can help prevent overfitting and improve model interpretability";"CODE"
"and efficiency";"CODE"
"in this example we ve seen how to implement this rule using a custom refit";"CODE"
"callable with class sklearn model selection gridsearchcv";"IRRE"
"key takeaways";"-"
"1 the one standard error rule provides a good rule of thumb to select simpler models";"CODE"
"2 custom refit callables in class sklearn model selection gridsearchcv allow for";"CODE"
"flexible model selection strategies";"CODE"
"3 visualizing both train and test scores helps identify potential overfitting";"IRRE"
"this approach can be applied to other model selection scenarios where balancing";"CODE"
"complexity and performance is important or in cases where a use case specific";"CODE"
"selection of the best model is desired";"CODE"
"display the figure";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we will start by simulating moon shaped data where the ideal separation";"-"
"between classes is non linear adding to it a moderate degree of noise";"TASK"
"datapoints will belong to one of two possible classes to be predicted by two";"CODE"
"features we will simulate 50 samples for each class";"CODE"
"we will compare the performance of class sklearn svm svc estimators that";"IRRE"
"vary on their kernel parameter to decide which choice of this";"CODE"
"hyper parameter predicts our simulated data best";"IRRE"
"we will evaluate the performance of the models using";"CODE"
"class sklearn model selection repeatedstratifiedkfold repeating 10 times";"CODE"
"a 10 fold stratified cross validation using a different randomization of the";"IRRE"
"data in each repetition the performance will be evaluated using";"CODE"
"class sklearn metrics roc auc score";"IRRE"
"we can now inspect the results of our search sorted by their";"IRRE"
"mean test score";"IRRE"
"we can see that the estimator using the rbf kernel performed best";"CODE"
"closely followed by linear both estimators with a poly kernel";"CODE"
"performed worse with the one using a two degree polynomial achieving a much";"CODE"
"lower performance than all other models";"CODE"
"usually the analysis just ends here but half the story is missing the";"META"
"output of class sklearn model selection gridsearchcv does not provide";"CODE"
"information on the certainty of the differences between the models";"CODE"
"we don t know if these are statistically significant";"IRRE"
"to evaluate this we need to conduct a statistical test";"TASK"
"specifically to contrast the performance of two models we should";"IRRE"
"statistically compare their auc scores there are 100 samples auc";"IRRE"
"scores for each model as we repreated 10 times a 10 fold cross validation";"CODE"
"however the scores of the models are not independent all models are";"CODE"
"evaluated on the same 100 partitions increasing the correlation";"CODE"
"between the performance of the models";"CODE"
"since some partitions of the data can make the distinction of the classes";"IRRE"
"particularly easy or hard to find for all models the models scores will";"META"
"co vary";"CODE"
"let s inspect this partition effect by plotting the performance of all models";"CODE"
"in each fold and calculating the correlation between models across folds";"-"
"create df of model scores ordered by performance";"IRRE"
"plot 30 examples of dependency between cv fold and auc scores";"CODE"
"print correlation of auc scores across folds";"CODE"
"we can observe that the performance of the models highly depends on the fold";"CODE"
"as a consequence if we assume independence between samples we will be";"CODE"
"underestimating the variance computed in our statistical tests increasing";"IRRE"
"the number of false positive errors i e detecting a significant difference";"-"
"between models when such does not exist 1";"TASK"
"several variance corrected statistical tests have been developed for these";"CODE"
"cases in this example we will show how to implement one of them the so";"CODE"
"called nadeau and bengio s corrected t test under two different statistical";"IRRE"
"frameworks frequentist and bayesian";"-"
"comparing two models frequentist approach";"-"
"we can start by asking is the first model significantly better than the";"-"
"second model when ranked by mean test score";"IRRE"
"to answer this question using a frequentist approach we could";"CODE"
"run a paired t test and compute the p value this is also known as";"IRRE"
"diebold mariano test in the forecast literature 5";"CODE"
"many variants of such a t test have been developed to account for the";"CODE"
"non independence of samples problem";"CODE"
"described in the previous section we will use the one proven to obtain the";"CODE"
"highest replicability scores which rate how similar the performance of a";"CODE"
"model is when evaluating it on different random partitions of the same";"IRRE"
"dataset while maintaining a low rate of false positives and false negatives";"CODE"
"the nadeau and bengio s corrected t test 2 that uses a 10 times repeated";"IRRE"
"10 fold cross validation 3";"-"
"this corrected paired t test is computed as";"IRRE"
"math";"-"
"t frac frac 1 k cdot r sum i 1 k sum j 1 r x ij";"CODE"
"sqrt frac 1 k cdot r frac n test n train hat sigma 2";"IRRE"
"where math k is the number of folds";"-"
"math r the number of repetitions in the cross validation";"CODE"
"math x is the difference in performance of the models";"CODE"
"math n test is the number of samples used for testing";"IRRE"
"math n train is the number of samples used for training";"CODE"
"and math hat sigma 2 represents the variance of the observed";"CODE"
"differences";"-"
"let s implement a corrected right tailed paired t test to evaluate if the";"TASK"
"performance of the first model is significantly better than that of the";"CODE"
"second model our null hypothesis is that the second model performs at least";"CODE"
"as good as the first model";"-"
"kr k times r r times repeated k fold crossvalidation";"-"
"kr equals the number of times the model was evaluated";"-"
"p val t sf np abs t stat df right tailed t test";"IRRE"
"model 1 scores model scores iloc 0 values scores of the best model";"IRRE"
"model 2 scores model scores iloc 1 values scores of the second best model";"IRRE"
"n differences shape 0 number of test sets";"IRRE"
"we can compare the corrected t and p values with the uncorrected ones";"IRRE"
"using the conventional significance alpha level at p 0 05 we observe that";"-"
"the uncorrected t test concludes that the first model is significantly better";"IRRE"
"than the second";"-"
"with the corrected approach in contrast we fail to detect this difference";"CODE"
"in the latter case however the frequentist approach does not let us";"CODE"
"conclude that the first and second model have an equivalent performance if";"CODE"
"we wanted to make this assertion we need to use a bayesian approach";"CODE"
"comparing two models bayesian approach";"-"
"we can use bayesian estimation to calculate the probability that the first";"-"
"model is better than the second bayesian estimation will output a";"IRRE"
"distribution followed by the mean math mu of the differences in the";"META"
"performance of two models";"CODE"
"to obtain the posterior distribution we need to define a prior that models";"CODE"
"our beliefs of how the mean is distributed before looking at the data";"META"
"and multiply it by a likelihood function that computes how likely our";"CODE"
"observed differences are given the values that the mean of differences";"IRRE"
"could take";"-"
"bayesian estimation can be carried out in many forms to answer our question";"CODE"
"but in this example we will implement the approach suggested by benavoli and";"CODE"
"colleagues 4";"-"
"one way of defining our posterior using a closed form expression is to select";"CODE"
"a prior conjugate to the likelihood function benavoli and colleagues 4";"CODE"
"show that when comparing the performance of two classifiers we can model the";"CODE"
"prior as a normal gamma distribution with both mean and variance unknown";"META"
"conjugate to a normal likelihood to thus express the posterior as a normal";"-"
"distribution";"META"
"marginalizing out the variance from this normal posterior we can define the";"CODE"
"posterior of the mean parameter as a student s t distribution specifically";"IRRE"
"math";"-"
"st mu n 1 overline x frac 1 n frac n test n train";"IRRE"
"hat sigma 2";"-"
"where math n is the total number of samples";"-"
"math overline x represents the mean difference in the scores";"CODE"
"math n test is the number of samples used for testing";"IRRE"
"math n train is the number of samples used for training";"CODE"
"and math hat sigma 2 represents the variance of the observed";"CODE"
"differences";"-"
"notice that we are using nadeau and bengio s corrected variance in our";"CODE"
"bayesian approach as well";"-"
"let s compute and plot the posterior";"CODE"
"initialize random variable";"IRRE"
"let s plot the posterior distribution";"META"
"we can calculate the probability that the first model is better than the";"-"
"second by computing the area under the curve of the posterior distribution";"META"
"from zero to infinity and also the reverse we can calculate the probability";"IRRE"
"that the second model is better than the first by computing the area under";"-"
"the curve from minus infinity to zero";"IRRE"
"in contrast with the frequentist approach we can compute the probability";"-"
"that one model is better than the other";"-"
"note that we obtained similar results as those in the frequentist approach";"TASK"
"given our choice of priors we are essentially performing the same";"CODE"
"computations but we are allowed to make different assertions";"META"
"region of practical equivalence";"-"
"sometimes we are interested in determining the probabilities that our models";"CODE"
"have an equivalent performance where equivalent is defined in a practical";"CODE"
"way a naive approach 4 would be to define estimators as practically";"IRRE"
"equivalent when they differ by less than 1 in their accuracy but we could";"META"
"also define this practical equivalence taking into account the problem we are";"CODE"
"trying to solve for example a difference of 5 in accuracy would mean an";"CODE"
"increase of 1000 in sales and we consider any quantity above that as";"-"
"relevant for our business";"CODE"
"in this example we are going to define the";"CODE"
"region of practical equivalence rope to be math 0 01 0 01 that is";"-"
"we will consider two models as practically equivalent if they differ by less";"IRRE"
"than 1 in their performance";"CODE"
"to compute the probabilities of the classifiers being practically equivalent";"IRRE"
"we calculate the area under the curve of the posterior over the rope";"-"
"interval";"CODE"
"we can plot how the posterior is distributed over the rope interval";"CODE"
"as suggested in 4 we can further interpret these probabilities using the";"CODE"
"same criteria as the frequentist approach is the probability of falling";"-"
"inside the rope bigger than 95 alpha value of 5 in that case we can";"CODE"
"conclude that both models are practically equivalent";"IRRE"
"the bayesian estimation approach also allows us to compute how uncertain we";"META"
"are about our estimation of the difference this can be calculated using";"CODE"
"credible intervals for a given probability they show the range of values";"IRRE"
"that the estimated quantity in our case the mean difference in";"IRRE"
"performance can take";"CODE"
"for example a 50 credible interval x y tells us that there is a 50";"CODE"
"probability that the true mean difference of performance between models is";"CODE"
"between x and y";"-"
"let s determine the credible intervals of our data using 50 75 and 95";"CODE"
"as shown in the table there is a 50 probability that the true mean";"CODE"
"difference between models will be between 0 000977 and 0 019023 70";"-"
"probability that it will be between 0 005422 and 0 025422 and 95";"-"
"probability that it will be between 0 016445 and 0 036445";"-"
"pairwise comparison of all models frequentist approach";"-"
"we could also be interested in comparing the performance of all our models";"CODE"
"evaluated with class sklearn model selection gridsearchcv in this case";"CODE"
"we would be running our statistical test multiple times which leads us to";"CODE"
"the multiple comparisons problem";"-"
"https en wikipedia org wiki multiple comparisons problem";"CODE"
"there are many possible ways to tackle this problem but a standard approach";"CODE"
"is to apply a bonferroni correction";"-"
"https en wikipedia org wiki bonferroni correction bonferroni can be";"CODE"
"computed by multiplying the p value by the number of comparisons we are";"IRRE"
"testing";"IRRE"
"let s compare the performance of the models using the corrected t test";"IRRE"
"p val n comparisons implement bonferroni correction";"TASK"
"bonferroni can output p values higher than 1";"IRRE"
"we observe that after correcting for multiple comparisons the only model";"CODE"
"that significantly differs from the others is 2 poly";"CODE"
"rbf the model ranked first by";"-"
"class sklearn model selection gridsearchcv does not significantly";"CODE"
"differ from linear or 3 poly";"CODE"
"pairwise comparison of all models bayesian approach";"-"
"when using bayesian estimation to compare multiple models we don t need to";"TASK"
"correct for multiple comparisons for reasons why see 4";"CODE"
"we can carry out our pairwise comparisons the same way as in the first";"CODE"
"section";"-"
"using the bayesian approach we can compute the probability that a model";"-"
"performs better worse or practically equivalent to another";"IRRE"
"results show that the model ranked first by";"IRRE"
"class sklearn model selection gridsearchcv rbf has approximately a";"CODE"
"6 8 chance of being worse than linear and a 1 8 chance of being worse";"IRRE"
"than 3 poly";"-"
"rbf and linear have a 43 probability of being practically";"IRRE"
"equivalent while rbf and 3 poly have a 10 chance of being so";"CODE"
"similarly to the conclusions obtained using the frequentist approach all";"-"
"models have a 100 probability of being better than 2 poly and none have";"-"
"a practically equivalent performance with the latter";"IRRE"
"take home messages";"-"
"small differences in performance measures might easily turn out to be";"CODE"
"merely by chance but not because one model predicts systematically better";"IRRE"
"than the other as shown in this example statistics can tell you how";"CODE"
"likely that is";"-"
"when statistically comparing the performance of two models evaluated in";"IRRE"
"gridsearchcv it is necessary to correct the calculated variance which";"CODE"
"could be underestimated since the scores of the models are not independent";"CODE"
"from each other";"CODE"
"a frequentist approach that uses a variance corrected paired t test can";"IRRE"
"tell us if the performance of one model is better than another with a";"CODE"
"degree of certainty above chance";"CODE"
"a bayesian approach can provide the probabilities of one model being";"-"
"better worse or practically equivalent than another it can also tell us";"IRRE"
"how confident we are of knowing that the true differences of our models";"-"
"fall under a certain range of values";"IRRE"
"if multiple models are statistically compared a multiple comparisons";"IRRE"
"correction is needed when using the frequentist approach";"-"
"rubric references";"CODE"
"1 dietterich t g 1998 approximate statistical tests for";"IRRE"
"comparing supervised classification learning algorithms";"CODE"
"http web cs iastate edu jtian cs573 papers dietterich 98 pdf";"CODE"
"neural computation 10 7";"-"
"2 nadeau c bengio y 2000 inference for the generalization";"CODE"
"error";"-"
"https papers nips cc paper 1661 inference for the generalization error pdf";"CODE"
"in advances in neural information processing systems";"CODE"
"3 bouckaert r r frank e 2004 evaluating the replicability";"-"
"of significance tests for comparing learning algorithms";"IRRE"
"https www cms waikato ac nz ml publications 2004 bouckaert frank pdf";"CODE"
"in pacific asia conference on knowledge discovery and data mining";"-"
"4 benavoli a corani g dem ar j zaffalon m 2017 time";"-"
"for a change a tutorial for comparing multiple classifiers through";"CODE"
"bayesian analysis";"-"
"http www jmlr org papers volume18 16 305 16 305 pdf";"CODE"
"the journal of machine learning research 18 1 see the python";"CODE"
"library that accompanies this paper here";"CODE"
"https github com janezd baycomp";"CODE"
"5 diebold f x mariano r s 1995 comparing predictive accuracy";"-"
"http www est uc3m es esp nueva docencia comp col get lade tecnicas prediccion practicas0708 comparing 20predictive 20accuracy 20 dielbold pdf";"CODE"
"journal of business economic statistics 20 1 134 144";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data loading";"CODE"
"we load two categories from the training set you can adjust the number of";"CODE"
"categories by adding their names to the list or setting categories none when";"TASK"
"calling the dataset loader func sklearn datasets fetch 20newsgroups to get";"IRRE"
"the 20 of them";"-"
"pipeline with hyperparameter tuning";"CODE"
"we define a pipeline combining a text feature vectorizer with a simple";"CODE"
"classifier yet effective for text classification";"CODE"
"we define a grid of hyperparameters to be explored by the";"CODE"
"class sklearn model selection randomizedsearchcv using a";"IRRE"
"class sklearn model selection gridsearchcv instead would explore all the";"CODE"
"possible combinations on the grid which can be costly to compute whereas the";"-"
"parameter n iter of the class sklearn model selection randomizedsearchcv";"IRRE"
"controls the number of different random combination that are evaluated notice";"IRRE"
"that setting n iter larger than the number of possible combinations in a";"IRRE"
"grid would lead to repeating already explored combinations we search for the";"CODE"
"best parameter combination for both the feature extraction vect and the";"TASK"
"classifier clf";"IRRE"
"vect ngram range 1 1 1 2 unigrams or bigrams";"-"
"in this case n iter 40 is not an exhaustive search of the hyperparameters";"CODE"
"grid in practice it would be interesting to increase the parameter n iter";"IRRE"
"to get a more informative analysis as a consequence the computional time";"CODE"
"increases we can reduce it by taking advantage of the parallelisation over";"-"
"the parameter combinations evaluation by increasing the number of cpus used";"IRRE"
"via the parameter n jobs";"IRRE"
"the prefixes vect and clf are required to avoid possible ambiguities in";"CODE"
"the pipeline but are not necessary for visualizing the results because of";"CODE"
"this we define a function that will rename the tuned hyperparameters and";"CODE"
"improve the readability";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"learning curve";"-"
"learning curves show the effect of adding more samples during the training";"TASK"
"process the effect is depicted by checking the statistical performance of";"CODE"
"the model in terms of training score and testing score";"IRRE"
"here we compute the learning curve of a naive bayes classifier and a svm";"IRRE"
"classifier with a rbf kernel using the digits dataset";"IRRE"
"the meth sklearn model selection learningcurvedisplay from estimator";"CODE"
"displays the learning curve given the dataset and the predictive model to";"IRRE"
"analyze to get an estimate of the scores uncertainty this method uses";"CODE"
"a cross validation procedure";"-"
"we first analyze the learning curve of the naive bayes classifier its shape";"IRRE"
"can be found in more complex datasets very often the training score is very";"IRRE"
"high when using few samples for training and decreases when increasing the";"CODE"
"number of samples whereas the test score is very low at the beginning and";"IRRE"
"then increases when adding samples the training and test scores become more";"TASK"
"realistic when all the samples are used for training";"CODE"
"we see another typical learning curve for the svm classifier with rbf kernel";"IRRE"
"the training score remains high regardless of the size of the training set";"IRRE"
"on the other hand the test score increases with the size of the training";"IRRE"
"dataset indeed it increases up to a point where it reaches a plateau";"IRRE"
"observing such a plateau is an indication that it might not be useful to";"-"
"acquire new data to train the model since the generalization performance of";"CODE"
"the model will not increase anymore";"OUTD"
"complexity analysis";"META"
"in addition to these learning curves it is also possible to look at the";"TASK"
"scalability of the predictive models in terms of training and scoring times";"CODE"
"the class sklearn model selection learningcurvedisplay class does not";"CODE"
"provide such information we need to resort to the";"TASK"
"func sklearn model selection learning curve function instead and make";"CODE"
"the plot manually";"-"
"scalability regarding the fit time";"-"
"scalability regarding the score time";"-"
"we see that the scalability of the svm and naive bayes classifiers is very";"IRRE"
"different the svm classifier complexity at fit and score time increases";"IRRE"
"rapidly with the number of samples indeed it is known that the fit time";"CODE"
"complexity of this classifier is more than quadratic with the number of";"CODE"
"samples which makes it hard to scale to dataset with more than a few";"IRRE"
"10 000 samples in contrast the naive bayes classifier scales much better";"IRRE"
"with a lower complexity at fit and score time";"META"
"subsequently we can check the trade off between increased training time and";"-"
"the cross validation score";"-"
"in these plots we can look for the inflection point for which the";"CODE"
"cross validation score does not increase anymore and only the training time";"CODE"
"increases";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"pre test vs post test analysis";"IRRE"
"suppose we have a population of subjects with physiological measurements x";"-"
"that can hopefully serve as indirect bio markers of the disease and actual";"-"
"disease indicators y ground truth most of the people in the population do";"CODE"
"not carry the disease but a minority in this case around 10 does";"CODE"
"a machine learning model is built to diagnose if a person with some given";"-"
"physiological measurements is likely to carry the disease of interest to";"CODE"
"evaluate the model we need to assess its performance on a held out test set";"IRRE"
"then we can fit our diagnosis model and compute the positive likelihood";"-"
"ratio to evaluate the usefulness of this classifier as a disease diagnosis";"CODE"
"tool";"-"
"since the positive class likelihood ratio is much larger than 1 0 it means";"IRRE"
"that the machine learning based diagnosis tool is useful the post test odds";"IRRE"
"that the condition is truly present given a positive test result are more than";"IRRE"
"12 times larger than the pre test odds";"IRRE"
"cross validation of likelihood ratios";"-"
"we assess the variability of the measurements for the class likelihood ratios";"CODE"
"in some particular cases";"CODE"
"we first validate the class sklearn linear model logisticregression model";"IRRE"
"with default hyperparameters as used in the previous section";"CODE"
"we confirm that the model is useful the post test odds are between 12 and 20";"IRRE"
"times larger than the pre test odds";"IRRE"
"on the contrary let s consider a dummy model that will output random";"IRRE"
"predictions with similar odds as the average disease prevalence in the";"CODE"
"training set";"IRRE"
"here both class likelihood ratios are compatible with 1 0 which makes this";"CODE"
"classifier useless as a diagnostic tool to improve disease detection";"IRRE"
"another option for the dummy model is to always predict the most frequent";"CODE"
"class which in this case is no disease";"CODE"
"the absence of positive predictions means there will be no true positives nor";"-"
"false positives leading to an undefined lr that by no means should be";"CODE"
"interpreted as an infinite lr the classifier perfectly identifying";"IRRE"
"positive cases in such situation the";"CODE"
"func sklearn metrics class likelihood ratios function returns nan and";"CODE"
"raises a warning by default indeed the value of lr helps us discard this";"CODE"
"model";"-"
"a similar scenario may arise when cross validating highly imbalanced data with";"-"
"few samples some folds will have no samples with the disease and therefore";"CODE"
"they will output no true positives nor false negatives when used for testing";"IRRE"
"mathematically this leads to an infinite lr which should also not be";"IRRE"
"interpreted as the model perfectly identifying positive cases such event";"CODE"
"leads to a higher variance of the estimated likelihood ratios but can still";"TASK"
"be interpreted as an increment of the post test odds of having the condition";"IRRE"
"invariance with respect to prevalence";"CODE"
"the likelihood ratios are independent of the disease prevalence and can be";"CODE"
"extrapolated between populations regardless of any possible class imbalance";"IRRE"
"as long as the same model is applied to all of them notice that in the";"CODE"
"plots below the decision boundary is constant see";"CODE"
"ref sphx glr auto examples svm plot separating hyperplane unbalanced py for";"CODE"
"a study of the boundary decision for unbalanced classes";"CODE"
"here we train a class sklearn linear model logisticregression base model";"IRRE"
"on a case control study with a prevalence of 50 it is then evaluated over";"CODE"
"populations with varying prevalence we use the";"IRRE"
"func sklearn datasets make classification function to ensure the";"IRRE"
"data generating process is always the same as shown in the plots below the";"CODE"
"label 1 corresponds to the positive class disease whereas the label 0";"IRRE"
"stands for no disease";"CODE"
"fit and evaluate base model on balanced classes";"IRRE"
"we will now show the decision boundary for each level of prevalence note that";"CODE"
"we only plot a subset of the original data to better assess the linear model";"IRRE"
"decision boundary";"-"
"down sample for plotting";"CODE"
"plot fixed decision boundary of base model with varying prevalence";"CODE"
"we define a function for bootstrapping";"CODE"
"we score the base model for each prevalence using bootstrapping";"CODE"
"in the plots below we observe that the class likelihood ratios re computed";"CODE"
"with different prevalences are indeed constant within one standard deviation";"CODE"
"of those computed with on balanced classes";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"running gridsearchcv using multiple evaluation metrics";"CODE"
"the scorers can be either one of the predefined metric strings or a scorer";"CODE"
"callable like the one returned by make scorer";"IRRE"
"setting refit auc refits an estimator on the whole dataset with the";"IRRE"
"parameter setting that has the best cross validated auc score";"IRRE"
"that estimator is made available at gs best estimator along with";"IRRE"
"parameters like gs best score gs best params and";"IRRE"
"gs best index";"-"
"plotting the result";"IRRE"
"get the regular numpy array from the maskedarray";"CODE"
"plot a dotted vertical line at the best score for that scorer marked by x";"CODE"
"annotate the best score for that scorer";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"number of random trials";"IRRE"
"load the dataset";"IRRE"
"set up possible values of parameters to optimize over";"IRRE"
"we will use a support vector classifier with rbf kernel";"IRRE"
"arrays to store scores";"-"
"loop for each trial";"CODE"
"choose cross validation techniques for the inner and outer loops";"IRRE"
"independently of the dataset";"IRRE"
"e g groupkfold leaveoneout leaveonegroupout etc";"-"
"non nested parameter search and scoring";"IRRE"
"nested cv with parameter optimization";"IRRE"
"plot scores on each trial for nested and non nested cv";"CODE"
"plot bar chart of the difference";"IRRE"
"plt xlabel individual trial";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataset";"IRRE"
"we will use the ref iris dataset which consists of measurements taken";"IRRE"
"from 3 iris species our model will use the measurements to predict";"IRRE"
"the iris species";"-"
"for comparison we also generate some random feature data i e 20 features";"TASK"
"uncorrelated with the class labels in the iris dataset";"IRRE"
"use same number of samples as in iris and 20 features";"TASK"
"permutation test score";"IRRE"
"next we calculate the";"-"
"func sklearn model selection permutation test score for both the original";"CODE"
"iris dataset where there s a strong relationship between features and labels and";"TASK"
"the randomly generated features with iris labels where no dependency between features";"TASK"
"and labels is expected we use the";"IRRE"
"class sklearn svm svc classifier and ref accuracy score to evaluate";"IRRE"
"the model at each round";"-"
"func sklearn model selection permutation test score generates a null";"IRRE"
"distribution by calculating the accuracy of the classifier";"IRRE"
"on 1000 different permutations of the dataset where features";"TASK"
"remain the same but labels undergo different random permutations this is the";"CODE"
"distribution for the null hypothesis which states there is no dependency";"CODE"
"between the features and labels an empirical p value is then calculated as";"TASK"
"the proportion of permutations for which the score obtained by the model trained on";"CODE"
"the permutation is greater than or equal to the score obtained using the original";"-"
"data";"-"
"original data";"-"
"below we plot a histogram of the permutation scores the null";"-"
"distribution the red line indicates the score obtained by the classifier";"IRRE"
"on the original data without permuted labels the score is much better than those";"-"
"obtained by using permuted data and the p value is thus very low this indicates that";"IRRE"
"there is a low likelihood that this good score would be obtained by chance";"CODE"
"alone it provides evidence that the iris dataset contains real dependency";"IRRE"
"between features and labels and the classifier was able to utilize this";"CODE"
"to obtain good results the low p value can lead us to reject the null hypothesis";"IRRE"
"random data";"IRRE"
"below we plot the null distribution for the randomized data the permutation";"IRRE"
"scores are similar to those obtained using the original iris dataset";"IRRE"
"because the permutation always destroys any feature label dependency present";"TASK"
"the score obtained on the randomized data in this case";"CODE"
"though is very poor this results in a large p value confirming that there was no";"IRRE"
"feature label dependency in the randomized data";"CODE"
"another possible reason for obtaining a high p value could be that the classifier";"IRRE"
"was not able to use the structure in the data in this case the p value";"CODE"
"would only be low for classifiers that are able to utilize the dependency";"CODE"
"present in our case above where the data is random all classifiers would";"IRRE"
"have a high p value as there is no structure present in the data we might or might";"CODE"
"not fail to reject the null hypothesis depending on whether the p value is high on a";"TASK"
"more appropriate estimator as well";"-"
"finally note that this test has been shown to produce low p values even";"IRRE"
"if there is only weak structure in the data 1";"CODE"
"rubric references";"CODE"
"1 ojala and garriga permutation tests for studying classifier";"IRRE"
"performance";"CODE"
"http www jmlr org papers volume11 ojala10a ojala10a pdf the";"CODE"
"journal of machine learning research 2010 vol 11";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"in binary classification settings";"IRRE"
"dataset and model";"IRRE"
"we will use a linear svc classifier to differentiate two types of irises";"IRRE"
"add noisy features";"TASK"
"limit to the two first classes and split into training and test";"IRRE"
"linear svc will expect each feature to have a similar range of values thus";"IRRE"
"we will first scale the data using a";"CODE"
"class sklearn preprocessing standardscaler";"IRRE"
"plot the precision recall curve";"IRRE"
"to plot the precision recall curve you should use";"IRRE"
"class sklearn metrics precisionrecalldisplay indeed there is two";"IRRE"
"methods available depending if you already computed the predictions of the";"CODE"
"classifier or not";"IRRE"
"let s first plot the precision recall curve without the classifier";"IRRE"
"predictions we use";"-"
"func sklearn metrics precisionrecalldisplay from estimator that";"IRRE"
"computes the predictions for us before plotting the curve";"CODE"
"if we already got the estimated probabilities or scores for";"CODE"
"our model then we can use";"-"
"func sklearn metrics precisionrecalldisplay from predictions";"IRRE"
"in multi label settings";"IRRE"
"the precision recall curve does not support the multilabel setting however";"IRRE"
"one can decide how to handle this case we show such an example below";"CODE"
"create multi label data fit and predict";"IRRE"
"we create a multi label dataset to illustrate the precision recall in";"IRRE"
"multi label settings";"IRRE"
"use label binarize to be multi label like settings";"IRRE"
"split into training and test";"IRRE"
"we use class sklearn multiclass onevsrestclassifier for multi label";"CODE"
"prediction";"-"
"the average precision score in multi label settings";"IRRE"
"for each class";"CODE"
"a micro average quantifying score on all classes jointly";"CODE"
"plot the micro averaged precision recall curve";"IRRE"
"plot precision recall curve for each class and iso f1 curves";"CODE"
"setup plot details";"IRRE"
"add the legend for the iso f1 curves";"CODE"
"set the legend and the axes";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"get some data";"-"
"build a classifier";"IRRE"
"utility function to report best scores";"CODE"
"specify parameters and distributions to sample from";"IRRE"
"run randomized search";"IRRE"
"use a full grid over all parameters";"IRRE"
"run grid search";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load and prepare data";"CODE"
"we import the ref iris dataset which contains 3 classes each one";"CODE"
"corresponding to a type of iris plant one class is linearly separable from";"CODE"
"the other 2 the latter are not linearly separable from each other";"CODE"
"here we binarize the output and add noisy features to make the problem harder";"TASK"
"we train a class sklearn linear model logisticregression model which can";"IRRE"
"naturally handle multiclass problems thanks to the use of the multinomial";"IRRE"
"formulation";"CODE"
"one vs rest multiclass roc";"IRRE"
"the one vs the rest ovr multiclass strategy also known as one vs all";"IRRE"
"consists in computing a roc curve per each of the n classes in each step a";"IRRE"
"given class is regarded as the positive class and the remaining classes are";"CODE"
"regarded as the negative class as a bulk";"IRRE"
"note one should not confuse the ovr strategy used for the evaluation";"TASK"
"of multiclass classifiers with the ovr strategy used to train a";"IRRE"
"multiclass classifier by fitting a set of binary classifiers for instance";"IRRE"
"via the class sklearn multiclass onevsrestclassifier meta estimator";"IRRE"
"the ovr roc evaluation can be used to scrutinize any kind of classification";"CODE"
"models irrespectively of how they were trained see ref multiclass";"IRRE"
"in this section we use a class sklearn preprocessing labelbinarizer to";"CODE"
"binarize the target by one hot encoding in a ovr fashion this means that the";"CODE"
"target of shape n samples is mapped to a target of shape n samples";"-"
"n classes";"IRRE"
"y onehot test shape n samples n classes";"IRRE"
"we can as well easily check the encoding of a specific class";"IRRE"
"roc curve showing a specific class";"IRRE"
"in the following plot we show the resulting roc curve when regarding the iris";"IRRE"
"flowers as either virginica class id 2 or non virginica the rest";"CODE"
"roc curve using micro averaged ovr";"CODE"
"micro averaging aggregates the contributions from all the classes using";"IRRE"
"func numpy ravel to compute the average metrics as follows";"-"
"math tpr frac sum c tp c sum c tp c fn c";"-"
"math fpr frac sum c fp c sum c fp c tn c";"-"
"we can briefly demo the effect of func numpy ravel";"-"
"in a multi class classification setup with highly imbalanced classes";"IRRE"
"micro averaging is preferable over macro averaging in such cases one can";"CODE"
"alternatively use a weighted macro averaging not demonstrated here";"CODE"
"in the case where the main interest is not the plot but the roc auc score";"CODE"
"itself we can reproduce the value shown in the plot using";"IRRE"
"class sklearn metrics roc auc score";"IRRE"
"this is equivalent to computing the roc curve with";"CODE"
"class sklearn metrics roc curve and then the area under the curve with";"IRRE"
"class sklearn metrics auc for the raveled true and predicted classes";"CODE"
"store the fpr tpr and roc auc for all averaging strategies";"IRRE"
"compute micro average roc curve and roc area";"-"
"note by default the computation of the roc curve adds a single point at";"TASK"
"the maximal false positive rate by using linear interpolation and the";"CODE"
"mcclish correction doi analyzing a portion of the roc curve med decis";"CODE"
"making 1989 jul sep 9 3 190 5 10 1177 0272989x8900900307";"-"
"roc curve using the ovr macro average";"CODE"
"obtaining the macro average requires computing the metric independently for";"CODE"
"each class and then taking the average over them hence treating all classes";"IRRE"
"equally a priori we first aggregate the true false positive rates per class";"IRRE"
"math tpr frac 1 c sum c frac tp c tp c fn c";"-"
"math fpr frac 1 c sum c frac fp c fp c tn c";"-"
"where c is the total number of classes";"IRRE"
"interpolate all roc curves at these points";"CODE"
"mean tpr np interp fpr grid fpr i tpr i linear interpolation";"CODE"
"average it and compute auc";"-"
"this computation is equivalent to simply calling";"IRRE"
"plot all ovr roc curves together";"-"
"one vs one multiclass roc";"IRRE"
"the one vs one ovo multiclass strategy consists in fitting one classifier";"IRRE"
"per class pair since it requires to train n classes n classes 1 2";"CODE"
"classifiers this method is usually slower than one vs rest due to its";"CODE"
"o n classes 2 complexity";"IRRE"
"in this section we demonstrate the macro averaged auc using the ovo scheme";"CODE"
"for the 3 possible combinations in the ref iris dataset setosa vs";"IRRE"
"versicolor versicolor vs virginica and virginica vs setosa notice";"IRRE"
"that micro averaging is not defined for the ovo scheme";"CODE"
"roc curve using the ovo macro average";"CODE"
"in the ovo scheme the first step is to identify all possible unique";"CODE"
"combinations of pairs the computation of scores is done by treating one of";"CODE"
"the elements in a given pair as the positive class and the other element as";"IRRE"
"the negative class then re computing the score by inversing the roles and";"IRRE"
"taking the mean of both scores";"-"
"one can also assert that the macro average we computed by hand is equivalent";"CODE"
"to the implemented average macro option of the";"TASK"
"class sklearn metrics roc auc score function";"CODE"
"plot all ovo roc curves together";"-"
"we confirm that the classes versicolor and virginica are not well";"IRRE"
"identified by a linear classifier notice that the virginica vs the rest";"IRRE"
"roc auc score 0 77 is between the ovo roc auc scores for versicolor vs";"CODE"
"virginica 0 64 and setosa vs virginica 0 90 indeed the ovo";"IRRE"
"strategy gives additional information on the confusion between a pair of";"TASK"
"classes at the expense of computational cost when the number of classes";"IRRE"
"is large";"-"
"the ovo strategy is recommended if the user is mainly interested in correctly";"CODE"
"identifying a particular class or subset of classes whereas evaluating the";"IRRE"
"global performance of a classifier can still be summarized via a given";"CODE"
"averaging strategy";"-"
"when dealing with imbalanced datasets choosing the appropriate metric based on";"CODE"
"the business context or problem you are addressing is crucial";"TASK"
"it is also essential to select an appropriate averaging method micro vs macro";"CODE"
"depending on the desired outcome";"CODE"
"micro averaging aggregates metrics across all instances treating each";"-"
"individual instance equally regardless of its class this approach is useful";"CODE"
"when evaluating overall performance but note that it can be dominated by";"CODE"
"the majority class in imbalanced datasets";"IRRE"
"macro averaging calculates metrics for each class independently and then";"CODE"
"averages them giving equal weight to each class this is particularly useful";"CODE"
"when you want under represented classes to be considered as important as highly";"CODE"
"populated classes";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load and prepare data";"CODE"
"we import the ref iris dataset which contains 3 classes each one";"CODE"
"corresponding to a type of iris plant one class is linearly separable from";"CODE"
"the other 2 the latter are not linearly separable from each other";"CODE"
"in the following we binarize the dataset by dropping the virginica class";"IRRE"
"class id 2 this means that the versicolor class class id 1 is";"CODE"
"regarded as the positive class and setosa as the negative class";"IRRE"
"class id 0";"IRRE"
"we also add noisy features to make the problem harder";"TASK"
"classification and roc analysis";"IRRE"
"here we run func sklearn model selection cross validate on a";"CODE"
"class sklearn svm svc classifier then use the computed cross validation results";"IRRE"
"to plot the roc curves fold wise notice that the baseline to define the chance";"CODE"
"level dashed roc curve is a classifier that would always predict the most";"IRRE"
"frequent class";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"from sklearn experimental import enable halving search cv noqa f401";"CODE"
"we first define the parameter space for an class sklearn svm svc";"CODE"
"estimator and compute the time required to train a";"IRRE"
"class sklearn model selection halvinggridsearchcv instance as well as a";"CODE"
"class sklearn model selection gridsearchcv instance";"CODE"
"we now plot heatmaps for both search estimators";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"from sklearn experimental import enable halving search cv noqa f401";"CODE"
"we first define the parameter space and train a";"CODE"
"class sklearn model selection halvingrandomsearchcv instance";"IRRE"
"we can now use the cv results attribute of the search estimator to inspect";"IRRE"
"and plot the evolution of the search";"-"
"number of candidates and amount of resource at each iteration";"-"
"at the first iteration a small amount of resources is used the resource";"-"
"here is the number of samples that the estimators are trained on all";"CODE"
"candidates are evaluated";"-"
"at the second iteration only the best half of the candidates is evaluated";"-"
"the number of allocated resources is doubled candidates are evaluated on";"CODE"
"twice as many samples";"-"
"this process is repeated until the last iteration where only 2 candidates";"CODE"
"are left the best candidate is the candidate that has the best score at the";"-"
"last iteration";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"we generate a regression dataset that contains many features relative to the";"TASK"
"number of samples however only 10 of the features are informative in this context";"CODE"
"linear models exposing l1 penalization are commonly used to recover a sparse";"IRRE"
"set of coefficients";"IRRE"
"model definition";"IRRE"
"here we do not use a model that only exposes an l1 penalty instead we use";"CODE"
"an class sklearn linear model elasticnet model that exposes both l1 and l2";"IRRE"
"penalties";"-"
"we fix the l1 ratio parameter such that the solution found by the model is still";"TASK"
"sparse therefore this type of model tries to find a sparse solution but at the same";"IRRE"
"time also tries to shrink all coefficients towards zero";"-"
"in addition we force the coefficients of the model to be positive since we know that";"TASK"
"make regression generates a response with a positive signal so we use this";"CODE"
"pre knowledge to get a better model";"-"
"evaluate the impact of the regularization parameter";"IRRE"
"to evaluate the impact of the regularization parameter we use a validation";"IRRE"
"curve this curve shows the training and test scores of the model for different";"CODE"
"values of the regularization parameter";"IRRE"
"the regularization alpha is a parameter applied to the coefficients of the model";"IRRE"
"when it tends to zero no regularization is applied and the model tries to fit the";"CODE"
"training data with the least amount of error however it leads to overfitting when";"-"
"features are noisy when alpha increases the model coefficients are constrained";"TASK"
"and thus the model cannot fit the training data as closely avoiding overfitting";"CODE"
"however if too much regularization is applied the model underfits the data and";"-"
"is not able to properly capture the signal";"CODE"
"the validation curve helps in finding a good trade off between both extremes the";"-"
"model is not regularized and thus flexible enough to fit the signal but not too";"META"
"flexible to overfit the class sklearn model selection validationcurvedisplay";"CODE"
"allows us to display the training and validation scores across a range of alpha";"-"
"values";"IRRE"
"to find the optimal regularization parameter we can select the value of alpha";"IRRE"
"that maximizes the validation score";"-"
"coefficients comparison";"-"
"now that we have identified the optimal regularization parameter we can compare the";"IRRE"
"true coefficients and the estimated coefficients";"-"
"first let s set the regularization parameter to the optimal value and fit the";"IRRE"
"model on the training data in addition we ll show the test score for this model";"CODE"
"now we plot the true coefficients and the estimated coefficients";"-"
"while the original coefficients are sparse the estimated coefficients are not";"IRRE"
"as sparse the reason is that we fixed the l1 ratio parameter to 0 9 we could";"IRRE"
"force the model to get a sparser solution by increasing the l1 ratio parameter";"IRRE"
"however we observed that for the estimated coefficients that are close to zero in";"CODE"
"the true generative model our model shrinks them towards zero so we don t recover";"CODE"
"the true coefficients but we get a sensible outcome in line with the performance";"CODE"
"obtained on the test set";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the diabetes dataset";"IRRE"
"to illustrate the tuning of the decision threshold we will use the diabetes dataset";"IRRE"
"this dataset is available on openml https www openml org d 37 we use the";"CODE"
"func sklearn datasets fetch openml function to fetch this dataset";"CODE"
"we look at the target to understand the type of problem we are dealing with";"-"
"we can see that we are dealing with a binary classification problem since the";"IRRE"
"labels are not encoded as 0 and 1 we make it explicit that we consider the class";"TASK"
"labeled tested negative as the negative class which is also the most frequent";"IRRE"
"and the class labeled tested positive the positive as the positive class";"IRRE"
"we can also observe that this binary problem is slightly imbalanced where we have";"CODE"
"around twice more samples from the negative class than from the positive class when";"CODE"
"it comes to evaluation we should consider this aspect to interpret the results";"CODE"
"our vanilla classifier";"IRRE"
"we define a basic predictive model composed of a scaler followed by a logistic";"CODE"
"regression classifier";"IRRE"
"we evaluate our model using cross validation we use the accuracy and the balanced";"IRRE"
"accuracy to report the performance of our model the balanced accuracy is a metric";"CODE"
"that is less sensitive to class imbalance and will allow us to put the accuracy";"IRRE"
"score in perspective";"-"
"cross validation allows us to study the variance of the decision threshold across";"CODE"
"different splits of the data however the dataset is rather small and it would be";"IRRE"
"detrimental to use more than 5 folds to evaluate the dispersion therefore we use";"CODE"
"a class sklearn model selection repeatedstratifiedkfold where we apply several";"CODE"
"repetitions of 5 fold cross validation";"-"
"our predictive model succeeds to grasp the relationship between the data and the";"-"
"target the training and testing scores are close to each other meaning that our";"IRRE"
"predictive model is not overfitting we can also observe that the balanced accuracy is";"-"
"lower than the accuracy due to the class imbalance previously mentioned";"IRRE"
"for this classifier we let the decision threshold used convert the probability of";"CODE"
"the positive class into a class prediction to its default value 0 5 however this";"CODE"
"threshold might not be optimal if our interest is to maximize the balanced accuracy";"CODE"
"we should select another threshold that would maximize this metric";"CODE"
"the class sklearn model selection tunedthresholdclassifiercv meta estimator allows";"CODE"
"to tune the decision threshold of a classifier given a metric of interest";"CODE"
"tuning the decision threshold";"-"
"we create a class sklearn model selection tunedthresholdclassifiercv and";"IRRE"
"configure it to maximize the balanced accuracy we evaluate the model using the same";"-"
"cross validation strategy as previously";"-"
"in comparison with the vanilla model we observe that the balanced accuracy score";"-"
"increased of course it comes at the cost of a lower accuracy score it means that";"CODE"
"our model is now more sensitive to the positive class but makes more mistakes on the";"IRRE"
"negative class";"IRRE"
"however it is important to note that this tuned predictive model is internally the";"CODE"
"same model as the vanilla model they have the same fitted coefficients";"-"
"only the decision threshold of each model was changed during the cross validation";"CODE"
"in average a decision threshold around 0 32 maximizes the balanced accuracy which is";"-"
"different from the default decision threshold of 0 5 thus tuning the decision";"CODE"
"threshold is particularly important when the output of the predictive model";"CODE"
"is used to make decisions besides the metric used to tune the decision threshold";"OUTD"
"should be chosen carefully here we used the balanced accuracy but it might not be";"META"
"the most appropriate metric for the problem at hand the choice of the right metric";"CODE"
"is usually problem dependent and might require some domain knowledge refer to the";"CODE"
"example entitled";"-"
"ref sphx glr auto examples model selection plot cost sensitive learning py";"CODE"
"for more details";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"evaluate the models using crossvalidation";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the yeast uci dataset";"IRRE"
"in this example we use a uci dataset 1 generally referred as the yeast";"CODE"
"dataset we use the func sklearn datasets fetch openml function to load";"IRRE"
"the dataset from openml";"CODE"
"to know the type of data science problem we are dealing with we can check";"-"
"the target for which we want to build a predictive model";"CODE"
"we see that the target is discrete and composed of 10 classes we therefore";"CODE"
"deal with a multiclass classification problem";"IRRE"
"strategies comparison";"-"
"in the following experiment we use a";"CODE"
"class sklearn tree decisiontreeclassifier and a";"IRRE"
"class sklearn model selection repeatedstratifiedkfold cross validation";"CODE"
"with 3 splits and 5 repetitions";"-"
"we compare the following strategies";"IRRE"
"class sklearn tree decisiontreeclassifier can handle multiclass";"IRRE"
"classification without needing any special adjustments it works by breaking";"CODE"
"down the training data into smaller subsets and focusing on the most common";"CODE"
"class in each subset by repeating this process the model can accurately";"IRRE"
"classify input data into multiple different classes";"CODE"
"class sklearn multiclass onevsoneclassifier trains a set of binary";"IRRE"
"classifiers where each classifier is trained to distinguish between";"IRRE"
"two classes";"IRRE"
"class sklearn multiclass onevsrestclassifier trains a set of binary";"IRRE"
"classifiers where each classifier is trained to distinguish between";"IRRE"
"one class and the rest of the classes";"IRRE"
"class sklearn multiclass outputcodeclassifier trains a set of binary";"IRRE"
"classifiers where each classifier is trained to distinguish between";"IRRE"
"a set of classes from the rest of the classes the set of classes is";"IRRE"
"defined by a codebook which is randomly generated in scikit learn this";"CODE"
"method exposes a parameter code size to control the size of the codebook";"IRRE"
"we set it above one since we are not interested in compressing the class";"IRRE"
"representation";"-"
"we can now compare the statistical performance of the different strategies";"IRRE"
"we plot the score distribution of the different strategies";"META"
"at a first glance we can see that the built in strategy of the decision";"-"
"tree classifier is working quite well one vs one and the error correcting";"IRRE"
"output code strategies are working even better however the";"IRRE"
"one vs rest strategy is not working as well as the other strategies";"-"
"indeed these results reproduce something reported in the literature";"IRRE"
"as in 2 however the story is not as simple as it seems";"-"
"the importance of hyperparameters search";"CODE"
"it was later shown in 3 that the multiclass strategies would show similar";"CODE"
"scores if the hyperparameters of the base classifiers are first optimized";"IRRE"
"here we try to reproduce such result by at least optimizing the depth of the";"IRRE"
"base decision tree";"-"
"we can see that once the hyperparameters are optimized all multiclass";"IRRE"
"strategies have similar performance as discussed in 3";"CODE"
"conclusion";"-"
"we can get some intuition behind those results";"IRRE"
"first the reason for which one vs one and error correcting output code are";"IRRE"
"outperforming the tree when the hyperparameters are not optimized relies on";"IRRE"
"fact that they ensemble a larger number of classifiers the ensembling";"IRRE"
"improves the generalization performance this is a bit similar why a bagging";"CODE"
"classifier generally performs better than a single decision tree if no care";"CODE"
"is taken to optimize the hyperparameters";"IRRE"
"then we see the importance of optimizing the hyperparameters indeed it";"CODE"
"should be regularly explored when developing predictive models even if";"CODE"
"techniques such as ensembling help at reducing this impact";"CODE"
"finally it is important to recall that the estimators in scikit learn";"CODE"
"are developed with a specific strategy to handle multiclass classification";"IRRE"
"out of the box so for these estimators it means that there is no need to";"TASK"
"use different strategies these strategies are mainly useful for third party";"CODE"
"estimators supporting only binary classification in all cases we also show";"CODE"
"that the hyperparameters should be optimized";"IRRE"
"references";"CODE"
"1 https archive ics uci edu ml datasets yeast";"IRRE"
"2 reducing multiclass to binary a unifying approach for margin classifiers";"CODE"
"allwein erin l robert e schapire and yoram singer";"CODE"
"journal of machine learning research 1 dec 2000 113 141";"-"
"https www jmlr org papers volume1 allwein00a allwein00a pdf";"CODE"
"3 in defense of one vs all classification";"CODE"
"journal of machine learning research 5 jan 2004 101 141";"-"
"https www jmlr org papers volume5 rifkin04a rifkin04a pdf";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"loading a dataset";"IRRE"
"for this example we use the yeast";"CODE"
"https www openml org d 40597 dataset which contains";"CODE"
"2 417 datapoints each with 103 features and 14 possible labels each";"TASK"
"data point has at least one label as a baseline we first train a logistic";"CODE"
"regression classifier for each of the 14 labels to evaluate the performance of";"CODE"
"these classifiers we predict on a held out test set and calculate the";"IRRE"
"jaccard similarity for each sample";"CODE"
"load a multi label dataset from https www openml org d 40597";"CODE"
"fit models";"-"
"we fit class sklearn linear model logisticregression wrapped by";"IRRE"
"class sklearn multiclass onevsrestclassifier and ensemble of multiple";"IRRE"
"class sklearn multioutput classifierchain";"IRRE"
"logisticregression wrapped by onevsrestclassifier";"IRRE"
"since by default class sklearn linear model logisticregression can t";"CODE"
"handle data with multiple targets we need to use";"TASK"
"class sklearn multiclass onevsrestclassifier";"IRRE"
"after fitting the model we calculate jaccard similarity";"-"
"chain of binary classifiers";"IRRE"
"because the models in each chain are arranged randomly there is significant";"IRRE"
"variation in performance among the chains presumably there is an optimal";"CODE"
"ordering of the classes in a chain that will yield the best performance";"CODE"
"however we do not know that ordering a priori instead we can build a";"CODE"
"voting ensemble of classifier chains by averaging the binary predictions of";"IRRE"
"the chains and apply a threshold of 0 5 the jaccard similarity score of the";"CODE"
"ensemble is greater than that of the independent models and tends to exceed";"CODE"
"the score of each chain in the ensemble although this is not guaranteed";"CODE"
"with randomly ordered chains";"IRRE"
"plot results";"IRRE"
"plot the jaccard similarity scores for the independent model each of the";"CODE"
"chains and the ensemble note that the vertical axis on this plot does";"CODE"
"not begin at 0";"-"
"results interpretation";"IRRE"
"there are three main takeaways from this plot";"CODE"
"independent model wrapped by class sklearn multiclass onevsrestclassifier";"CODE"
"performs worse than the ensemble of classifier chains and some of individual chains";"IRRE"
"this is caused by the fact that the logistic regression doesn t model relationship";"CODE"
"between the labels";"-"
"class sklearn multioutput classifierchain takes advantage of correlation";"IRRE"
"among labels but due to random nature of labels ordering it could yield worse";"IRRE"
"result than an independent model";"IRRE"
"an ensemble of chains performs better because it not only captures relationship";"CODE"
"between labels but also does not make strong assumptions about their correct order";"META"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"first we try to import the packages and warn the user in case they are";"CODE"
"missing";"-"
"we define a wrapper class for implementing the scikit learn api to the";"CODE"
"nmslib as well as a loading function";"CODE"
"we benchmark the different exact approximate nearest neighbors transformers";"CODE"
"tsne requires a certain number of neighbors which depends on the";"CODE"
"perplexity parameter";"IRRE"
"add one since we include each sample as its own neighbor";"CODE"
"init random pca cannot be used with precomputed distances";"IRRE"
"sample output";"IRRE"
"benchmarking on mnist 10000";"-"
"kneighborstransformer 0 007 sec fit";"CODE"
"kneighborstransformer 1 139 sec transform";"CODE"
"nmslibtransformer 0 208 sec fit";"CODE"
"nmslibtransformer 0 315 sec transform";"CODE"
"pynndescenttransformer 4 823 sec fit";"CODE"
"pynndescenttransformer 4 884 sec transform";"CODE"
"pynndescenttransformer 0 744 sec transform";"CODE"
"benchmarking on mnist 20000";"-"
"kneighborstransformer 0 011 sec fit";"CODE"
"kneighborstransformer 5 769 sec transform";"CODE"
"nmslibtransformer 0 733 sec fit";"CODE"
"nmslibtransformer 1 077 sec transform";"CODE"
"pynndescenttransformer 14 448 sec fit";"CODE"
"pynndescenttransformer 7 103 sec transform";"CODE"
"pynndescenttransformer 1 759 sec transform";"CODE"
"notice that the pynndescenttransformer takes more time during the first";"CODE"
"fit and the first transform due to the overhead of the numba just in time";"CODE"
"compiler but after the first call the compiled python code is kept in a";"CODE"
"cache by numba and subsequent calls do not suffer from this initial overhead";"CODE"
"both class sklearn neighbors kneighborstransformer and nmslibtransformer";"CODE"
"are only run once here as they would show more stable fit and transform";"CODE"
"times they don t have the cold start problem of pynndescenttransformer";"CODE"
"init the plot";"IRRE"
"plot tsne embedding which should be very similar across methods";"-"
"sample output";"IRRE"
"benchmarking on mnist 10000";"-"
"tsne with internal nearestneighbors 24 828 sec fit transform";"CODE"
"tsne with kneighborstransformer 20 111 sec fit transform";"CODE"
"tsne with nmslibtransformer 21 757 sec fit transform";"CODE"
"benchmarking on mnist 20000";"-"
"tsne with internal nearestneighbors 51 955 sec fit transform";"CODE"
"tsne with kneighborstransformer 50 994 sec fit transform";"CODE"
"tsne with nmslibtransformer 43 536 sec fit transform";"CODE"
"we can observe that the default class sklearn manifold tsne estimator with";"CODE"
"its internal class sklearn neighbors nearestneighbors implementation is";"TASK"
"roughly equivalent to the pipeline with class sklearn manifold tsne and";"CODE"
"class sklearn neighbors kneighborstransformer in terms of performance";"CODE"
"this is expected because both pipelines rely internally on the same";"CODE"
"class sklearn neighbors nearestneighbors implementation that performs";"TASK"
"exacts neighbors search the approximate nmslibtransformer is already";"CODE"
"slightly faster than the exact search on the smallest dataset but this speed";"CODE"
"difference is expected to become more significant on datasets with a larger";"IRRE"
"number of samples";"-"
"notice however that not all approximate search methods are guaranteed to";"-"
"improve the speed of the default exact search method indeed the exact search";"CODE"
"implementation significantly improved since scikit learn 1 1 furthermore the";"TASK"
"brute force exact search method does not require building an index at fit";"CODE"
"time so to get an overall performance improvement in the context of the";"CODE"
"class sklearn manifold tsne pipeline the gains of the approximate search";"CODE"
"at transform need to be larger than the extra time spent to build the";"TASK"
"approximate search index at fit time";"-"
"finally the tsne algorithm itself is also computationally intensive";"CODE"
"irrespective of the nearest neighbors search so speeding up the nearest";"-"
"neighbors search step by a factor of 5 would not result in a speed up by a";"IRRE"
"factor of 5 for the overall pipeline";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the transformer computes the nearest neighbors graph using the maximum number";"IRRE"
"of neighbors necessary in the grid search the classifier model filters the";"CODE"
"nearest neighbors graph as required by its own n neighbors parameter";"IRRE"
"note that we give memory a directory to cache the graph computation";"TASK"
"that will be used several times when tuning the hyperparameters of the";"IRRE"
"classifier";"IRRE"
"plot the results of the grid search";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the data";"CODE"
"in this example we use the iris dataset we split the data into a train and test";"IRRE"
"dataset";"IRRE"
"k nearest neighbors classifier";"IRRE"
"we want to use a k nearest neighbors classifier considering a neighborhood of 11 data";"CODE"
"points since our k nearest neighbors model uses euclidean distance to find the";"CODE"
"nearest neighbors it is therefore important to scale the data beforehand refer to";"CODE"
"the example entitled";"-"
"ref sphx glr auto examples preprocessing plot scaling importance py for more";"CODE"
"detailed information";"CODE"
"thus we use a class sklearn pipeline pipeline to chain a scaler before to use";"CODE"
"our classifier";"IRRE"
"decision boundary";"-"
"now we fit two classifiers with different values of the parameter";"IRRE"
"weights we plot the decision boundary of each classifier as well as the original";"IRRE"
"dataset to observe the difference";"IRRE"
"conclusion";"-"
"we observe that the parameter weights has an impact on the decision boundary when";"IRRE"
"weights unifom all nearest neighbors will have the same impact on the decision";"-"
"whereas when weights distance the weight given to each neighbor is proportional";"-"
"to the inverse of the distance from that neighbor to the query point";"CODE"
"in some cases taking the distance into account might improve the model";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the data";"CODE"
"project the 64 dimensional data to a lower dimension";"-"
"use grid search cross validation to optimize the bandwidth";"-"
"use the best estimator to compute the kernel density estimate";"IRRE"
"sample 44 new points from the data";"CODE"
"turn data into a 4x11 grid";"CODE"
"plot real digits and resampled digits";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"plot the progression of histograms to kernels";"-"
"histogram 1";"-"
"ax 0 0 hist x 0 bins bins fc aaaaff density true";"-"
"histogram 2";"-"
"ax 0 1 hist x 0 bins bins 0 75 fc aaaaff density true";"-"
"tophat kde";"-"
"ax 1 0 fill x plot 0 np exp log dens fc aaaaff";"-"
"gaussian kde";"-"
"ax 1 1 fill x plot 0 np exp log dens fc aaaaff";"-"
"plot all available kernels";"-"
"axi fill x plot 0 np exp log dens k fc aaaaff";"-"
"plot a 1d density example";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate normal not abnormal training observations";"-"
"generate new normal not abnormal observations";"CODE"
"generate some abnormal novel observations";"-"
"fit the model for novelty detection novelty true";"CODE"
"do not use predict decision function and score samples on x train as this";"CODE"
"would give wrong results but only on new unseen data not used in x train";"META"
"e g x test x outliers or the meshgrid";"IRRE"
"plot the learned frontier the points and the nearest vectors to the plane";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate data with outliers";"-"
"fit the model for outlier detection default";"CODE"
"use fit predict to compute the predicted labels of the training samples";"-"
"when lof is used for outlier detection the estimator has no predict";"CODE"
"decision function and score samples methods";"CODE"
"plot results";"IRRE"
"plot circles with radius proportional to the outlier scores";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we only take two features we could avoid this ugly";"CODE"
"slicing by using a two dim dataset";"IRRE"
"h 0 05 step size in the mesh";"CODE"
"create color maps";"IRRE"
"cmap light listedcolormap ffaaaa aaffaa aaaaff";"-"
"cmap bold listedcolormap ff0000 00ff00 0000ff";"-"
"plot also the training and testing points";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load digits dataset";"TASK"
"split into train test";"CODE"
"reduce dimension to 2 with pca";"-"
"reduce dimension to 2 with lineardiscriminantanalysis";"-"
"reduce dimension to 2 with neighborhoodcomponentanalysis";"-"
"use a nearest neighbor classifier to evaluate the methods";"IRRE"
"make a list of the methods to be compared";"IRRE"
"plt figure";"-"
"plt subplot 1 3 i 1 aspect 1";"-"
"fit the method s model";"-"
"fit a nearest neighbor classifier on the embedded training set";"IRRE"
"compute the nearest neighbor accuracy on the embedded test set";"IRRE"
"embed the data set in 2 dimensions using the fitted model";"IRRE"
"plot the projected points and show the evaluation score";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"original points";"CODE"
"first we create a data set of 9 samples from 3 classes and plot the points";"IRRE"
"in the original space for this example we focus on the classification of";"CODE"
"point no 3 the thickness of a link between point no 3 and another point";"CODE"
"is proportional to their distance";"-"
"ax axis equal so that boundaries are displayed correctly as circles";"-"
"compute exponentiated distances use the log sum exp trick to";"IRRE"
"avoid numerical instabilities";"CODE"
"learning an embedding";"-"
"we use class sklearn neighbors neighborhoodcomponentsanalysis to learn an";"IRRE"
"embedding and plot the points after the transformation we then take the";"CODE"
"embedding and find the nearest neighbors";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"import some data to play with";"CODE"
"we only take the first two features we could avoid this ugly";"CODE"
"slicing by using a two dim dataset";"IRRE"
"create color maps";"IRRE"
"we create an instance of nearest centroid classifier and fit the data";"IRRE"
"plot also the training points";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"here we generate a few data points to use to train the model we also generate";"CODE"
"data in the whole range of the training data to visualize how the model would";"CODE"
"react in that whole region";"CODE"
"add noise to targets";"TASK"
"fit regression model";"-"
"here we train a model and visualize how uniform and distance";"CODE"
"weights in prediction effect predicted values";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"if basemap is available we ll use it";"CODE"
"otherwise we ll improvise later";"-"
"x y coordinates for corner cells";"CODE"
"x coordinates of the grid cells";"-"
"y coordinates of the grid cells";"-"
"get matrices arrays of species ids and locations";"-"
"xtrain np pi 180 0 convert lat long to radians";"-"
"set up the data grid for the contour plot";"IRRE"
"plot map of south america with distributions of each species";"META"
"construct a kernel density estimate of the distribution";"CODE"
"evaluate only on the land 9999 indicates ocean";"-"
"plot contours of the density";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"h 0 02 step size in the mesh";"CODE"
"iterate over datasets";"IRRE"
"split into training and test part";"IRRE"
"just plot the dataset first";"IRRE"
"cm bright listedcolormap ff0000 0000ff";"-"
"plot the training points";"CODE"
"and testing points";"IRRE"
"iterate over classifiers";"IRRE"
"plot the decision boundary for that we will assign a color to each";"IRRE"
"point in the mesh x min x max x y min y max";"CODE"
"put the result into a color plot";"IRRE"
"plot also the training points";"CODE"
"and testing points";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"different learning rate schedules and momentum parameters";"IRRE"
"for each dataset plot learning for each learning strategy";"CODE"
"digits is larger but converges fairly quickly";"META"
"some parameter combinations will not converge as can be seen on the";"IRRE"
"plots so they are ignored here";"-"
"load generate some toy datasets";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load data from https www openml org d 554";"CODE"
"split data into train partition and test partition";"IRRE"
"this example won t converge because of resource usage constraints on";"CODE"
"our continuous integration infrastructure so we catch the warning and";"CODE"
"ignore it here";"-"
"use global min max to ensure all weights are shown on the same scale";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate data";"-"
"in order to learn good latent representations from a small dataset we";"IRRE"
"artificially generate more labeled data by perturbing the training data with";"-"
"linear shifts of 1 pixel in each direction";"-"
"x minmax scale x feature range 0 1 0 1 scaling";"TASK"
"models definition";"IRRE"
"we build a classification pipeline with a bernoullirbm feature extractor and";"CODE"
"a class logisticregression sklearn linear model logisticregression";"IRRE"
"classifier";"IRRE"
"training";"-"
"the hyperparameters of the entire model learning rate hidden layer size";"IRRE"
"regularization were optimized by grid search but the search is not";"META"
"reproduced here because of runtime constraints";"CODE"
"hyper parameters these were set by cross validation";"IRRE"
"using a gridsearchcv here we are not performing cross validation to";"CODE"
"save time";"CODE"
"more components tend to give better prediction performance but larger";"CODE"
"fitting time";"-"
"training rbm logistic pipeline";"CODE"
"training the logistic regression classifier directly on the pixel";"IRRE"
"evaluation";"-"
"the features extracted by the bernoullirbm help improve the classification";"TASK"
"accuracy with respect to the logistic regression on raw pixels";"-"
"plotting";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"take only 2 features to make visualization easier";"TASK"
"feature medinc has a long tail distribution";"TASK"
"feature aveoccup has a few but very large outliers";"TASK"
"scale the output between 0 and 1 for the colorbar";"IRRE"
"plasma does not exist in matplotlib 1 5";"TASK"
"define the axis for the first plot";"CODE"
"define the axis for the zoomed in plot";"CODE"
"define the axis for the colorbar";"CODE"
"the scatter plot";"-"
"removing the top and the right spine for aesthetics";"CODE"
"make nice axis layout";"-"
"histogram for axis x1 feature 5";"TASK"
"histogram for axis x0 feature 0";"TASK"
"two plots will be shown for each scaler normalizer transformer the left";"CODE"
"figure will show a scatter plot of the full data set while the right figure";"CODE"
"will exclude the extreme values considering only 99 of the data set";"IRRE"
"excluding marginal outliers in addition the marginal distributions for each";"CODE"
"feature will be shown on the sides of the scatter plot";"TASK"
"zoom in";"-"
"results";"IRRE"
"original data";"-"
"each transformation is plotted showing two transformed features with the";"CODE"
"left plot showing the entire dataset and the right zoomed in to show the";"IRRE"
"dataset without the marginal outliers a large majority of the samples are";"IRRE"
"compacted to a specific range 0 10 for the median income and 0 6 for";"CODE"
"the average house occupancy note that there are some marginal outliers some";"TASK"
"blocks have average occupancy of more than 1200 therefore a specific";"CODE"
"pre processing can be very beneficial depending of the application in the";"CODE"
"following we present some insights and behaviors of those pre processing";"-"
"methods in the presence of marginal outliers";"CODE"
"plot all scaling standard scaler section";"-"
"standardscaler";"-"
"class sklearn preprocessing standardscaler removes the mean and scales";"IRRE"
"the data to unit variance the scaling shrinks the range of the feature";"TASK"
"values as shown in the left figure below";"IRRE"
"however the outliers have an influence when computing the empirical mean and";"-"
"standard deviation note in particular that because the outliers on each";"TASK"
"feature have different magnitudes the spread of the transformed data on";"CODE"
"each feature is very different most of the data lie in the 2 4 range for";"CODE"
"the transformed median income feature while the same data is squeezed in the";"CODE"
"smaller 0 2 0 2 range for the transformed average house occupancy";"CODE"
"class sklearn preprocessing standardscaler therefore cannot guarantee";"CODE"
"balanced feature scales in the";"TASK"
"presence of outliers";"-"
"plot all scaling minmax scaler section";"-"
"minmaxscaler";"-"
"class sklearn preprocessing minmaxscaler rescales the data set such that";"IRRE"
"all feature values are in";"IRRE"
"the range 0 1 as shown in the right panel below however this scaling";"CODE"
"compresses all inliers into the narrow range 0 0 005 for the transformed";"CODE"
"average house occupancy";"-"
"both class sklearn preprocessing standardscaler and";"IRRE"
"class sklearn preprocessing minmaxscaler are very sensitive to the";"IRRE"
"presence of outliers";"-"
"plot all scaling max abs scaler section";"-"
"maxabsscaler";"-"
"class sklearn preprocessing maxabsscaler is similar to";"IRRE"
"class sklearn preprocessing minmaxscaler except that the";"CODE"
"values are mapped across several ranges depending on whether negative";"IRRE"
"or positive values are present if only positive values are present the";"IRRE"
"range is 0 1 if only negative values are present the range is 1 0";"IRRE"
"if both negative and positive values are present the range is 1 1";"IRRE"
"on positive only data both class sklearn preprocessing minmaxscaler";"IRRE"
"and class sklearn preprocessing maxabsscaler behave similarly";"IRRE"
"class sklearn preprocessing maxabsscaler therefore also suffers from";"CODE"
"the presence of large outliers";"-"
"plot all scaling robust scaler section";"-"
"robustscaler";"-"
"unlike the previous scalers the centering and scaling statistics of";"-"
"class sklearn preprocessing robustscaler";"IRRE"
"are based on percentiles and are therefore not influenced by a small";"CODE"
"number of very large marginal outliers consequently the resulting range of";"IRRE"
"the transformed feature values is larger than for the previous scalers and";"IRRE"
"more importantly are approximately similar for both features most of the";"CODE"
"transformed values lie in a 2 3 range as seen in the zoomed in figure";"IRRE"
"note that the outliers themselves are still present in the transformed data";"TASK"
"if a separate outlier clipping is desirable a non linear transformation is";"CODE"
"required see below";"CODE"
"plot all scaling power transformer section";"CODE"
"powertransformer";"CODE"
"class sklearn preprocessing powertransformer applies a power";"CODE"
"transformation to each feature to make the data more gaussian like in order";"TASK"
"to stabilize variance and minimize skewness currently the yeo johnson";"CODE"
"and box cox transforms are supported and the optimal";"CODE"
"scaling factor is determined via maximum likelihood estimation in both";"-"
"methods by default class sklearn preprocessing powertransformer applies";"CODE"
"zero mean unit variance normalization note that";"TASK"
"box cox can only be applied to strictly positive data income and average";"-"
"house occupancy happen to be strictly positive but if negative values are";"IRRE"
"present the yeo johnson transformed is preferred";"CODE"
"plot all scaling quantile transformer section";"CODE"
"quantiletransformer uniform output";"CODE"
"class sklearn preprocessing quantiletransformer applies a non linear";"CODE"
"transformation such that the";"CODE"
"probability density function of each feature will be mapped to a uniform";"CODE"
"or gaussian distribution in this case all the data including outliers";"CODE"
"will be mapped to a uniform distribution with the range 0 1 making";"META"
"outliers indistinguishable from inliers";"CODE"
"class sklearn preprocessing robustscaler and";"IRRE"
"class sklearn preprocessing quantiletransformer are robust to outliers in";"CODE"
"the sense that adding or removing outliers in the training set will yield";"IRRE"
"approximately the same transformation but contrary to";"META"
"class sklearn preprocessing robustscaler";"IRRE"
"class sklearn preprocessing quantiletransformer will also automatically";"CODE"
"collapse any outlier by setting them to the a priori defined range boundaries";"IRRE"
"0 and 1 this can result in saturation artifacts for extreme values";"IRRE"
"quantiletransformer gaussian output";"CODE"
"to map to a gaussian distribution set the parameter";"IRRE"
"output distribution normal";"IRRE"
"plot all scaling normalizer section";"-"
"normalizer";"-"
"the class sklearn preprocessing normalizer rescales the vector for each";"CODE"
"sample to have unit norm";"CODE"
"independently of the distribution of the samples it can be seen on both";"META"
"figures below where all samples are mapped onto the unit circle in our";"CODE"
"example the two selected features have only positive values therefore the";"CODE"
"transformed data only lie in the positive quadrant this would not be the";"CODE"
"case if some original features had a mix of positive and negative values";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"construct the dataset";"CODE"
"transform the dataset with kbinsdiscretizer";"IRRE"
"predict with original dataset";"IRRE"
"predict with transformed dataset";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"h 0 02 step size in the mesh";"CODE"
"list of estimator param grid where param grid is used in gridsearchcv";"-"
"the parameter spaces in this example are limited to a narrow band to reduce";"CODE"
"its runtime in a real use case a broader search space for the algorithms";"CODE"
"should be used";"-"
"cm bright listedcolormap b30065 178000";"-"
"iterate over datasets";"IRRE"
"split into training and test part";"IRRE"
"create the grid for background colors";"IRRE"
"plot the dataset first";"IRRE"
"plot the training points";"CODE"
"and testing points";"IRRE"
"iterate over classifiers";"IRRE"
"plot the decision boundary for that we will assign a color to each";"IRRE"
"point in the mesh x min x max y min y max";"CODE"
"put the result into a color plot";"IRRE"
"plot the training points";"CODE"
"and testing points";"IRRE"
"add suptitles above the figure";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"construct the datasets";"CODE"
"transform the dataset with kbinsdiscretizer";"IRRE"
"horizontal stripes";"-"
"vertical stripes";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"n quantiles is set to the training set size rather than the default value";"IRRE"
"to avoid a warning being raised by this example";"CODE"
"lognormal distribution";"META"
"chi squared distribution";"META"
"weibull distribution";"META"
"gaussian distribution";"META"
"uniform distribution";"META"
"bimodal distribution";"META"
"create plots";"IRRE"
"colors d81b60 0188ff ffc107 b7a2ff 000000 2ec5ac";"-"
"perform power transforms and quantile transform";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load and prepare data";"CODE"
"the dataset used is the ref wine dataset available at uci this dataset has";"IRRE"
"continuous features that are heterogeneous in scale due to differing";"TASK"
"properties that they measure e g alcohol content and malic acid";"-"
"neighbors scaling";"-"
"effect of rescaling on a k neighbors models";"-"
"for the sake of visualizing the decision boundary of a";"CODE"
"class sklearn neighbors kneighborsclassifier in this section we select a";"CODE"
"subset of 2 features that have values with different orders of magnitude";"IRRE"
"keep in mind that using a subset of the features to train the model may likely";"TASK"
"leave out feature with high predictive impact resulting in a decision";"TASK"
"boundary that is much worse in comparison to a model trained on the full set";"IRRE"
"of features";"TASK"
"here the decision boundary shows that fitting scaled or non scaled data lead";"CODE"
"to completely different models the reason is that the variable proline has";"CODE"
"values which vary between 0 and 1 000 whereas the variable hue varies";"IRRE"
"between 1 and 10 because of this distances between samples are mostly";"CODE"
"impacted by differences in values of proline while values of the hue will";"IRRE"
"be comparatively ignored if one uses";"-"
"class sklearn preprocessing standardscaler to normalize this database";"CODE"
"both scaled values lay approximately between 3 and 3 and the neighbors";"IRRE"
"structure will be impacted more or less equivalently by both variables";"CODE"
"effect of rescaling on a pca dimensional reduction";"-"
"dimensional reduction using class sklearn decomposition pca consists of";"IRRE"
"finding the features that maximize the variance if one feature varies more";"TASK"
"than the others only because of their respective scales";"-"
"class sklearn decomposition pca would determine that such feature";"TASK"
"dominates the direction of the principal components";"IRRE"
"we can inspect the first principal components using all the original features";"TASK"
"indeed we find that the proline feature dominates the direction of the first";"TASK"
"principal component without scaling being about two orders of magnitude above";"-"
"the other features this is contrasted when observing the first principal";"TASK"
"component for the scaled version of the data where the orders of magnitude";"META"
"are roughly the same across all the features";"TASK"
"we can visualize the distribution of the principal components in both cases";"META"
"from the plot above we observe that scaling the features before reducing the";"CODE"
"dimensionality results in components with the same order of magnitude in this";"CODE"
"case it also improves the separability of the classes indeed in the next";"CODE"
"section we confirm that a better separability has a good repercussion on the";"-"
"overall model s performance";"CODE"
"effect of rescaling on model s performance";"CODE"
"first we show how the optimal regularization of a";"-"
"class sklearn linear model logisticregressioncv depends on the scaling or";"CODE"
"non scaling of the data";"-"
"the need for regularization is higher lower values of c for the data that";"IRRE"
"was not scaled before applying pca we now evaluate the effect of scaling on";"CODE"
"the accuracy and the mean log loss of the optimal models";"-"
"a clear difference in prediction accuracies is observed when the data is";"-"
"scaled before class sklearn decomposition pca as it vastly outperforms";"CODE"
"the unscaled version this corresponds to the intuition obtained from the plot";"CODE"
"in the previous section where the components become linearly separable when";"CODE"
"scaling before using class sklearn decomposition pca";"CODE"
"notice that in this case the models with scaled features perform better than";"CODE"
"the models with non scaled features because all the variables are expected to";"TASK"
"be predictive and we rather avoid some of them being comparatively ignored";"CODE"
"if the variables in lower scales were not predictive one may experience a";"IRRE"
"decrease of the performance after scaling the features noisy features would";"TASK"
"contribute more to the prediction after scaling and therefore scaling would";"META"
"increase overfitting";"-"
"last but not least we observe that one achieves a lower log loss by means of";"META"
"the scaling step";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"loading data from openml";"CODE"
"first we load the wine reviews dataset where the target is the points given";"CODE"
"be a reviewer";"-"
"for this example we use the following subset of numerical and categorical";"IRRE"
"features in the data the target are continuous values from 80 to 100";"IRRE"
"training and evaluating pipelines with different encoders";"CODE"
"in this section we will evaluate pipelines with";"CODE"
"class sklearn ensemble histgradientboostingregressor with different encoding";"IRRE"
"strategies first we list out the encoders we will be using to preprocess";"-"
"the categorical features";"TASK"
"next we evaluate the models using cross validation and record the results";"IRRE"
"native categorical feature support";"TASK"
"in this section we build and evaluate a pipeline that uses native categorical";"CODE"
"feature support in class sklearn ensemble histgradientboostingregressor";"TASK"
"which only supports up to 255 unique categories in our dataset the most of";"IRRE"
"the categorical features have more than 255 unique categories";"TASK"
"to workaround the limitation above we group the categorical features into";"TASK"
"low cardinality and high cardinality features the high cardinality features";"TASK"
"will be target encoded and the low cardinality features will use the native";"TASK"
"categorical feature in gradient boosting";"TASK"
"the output of the of the preprocessor must be set to pandas so the";"IRRE"
"gradient boosting model can detect the low cardinality features";"TASK"
"finally we evaluate the pipeline using cross validation and record the results";"CODE"
"plotting the results";"IRRE"
"in this section we display the results by plotting the test and train scores";"IRRE"
"when evaluating the predictive performance on the test set dropping the";"IRRE"
"categories perform the worst and the target encoders performs the best this";"CODE"
"can be explained as follows";"-"
"dropping the categorical features makes the pipeline less expressive and";"TASK"
"underfitting as a result";"IRRE"
"due to the high cardinality and to reduce the training time the one hot";"-"
"encoding scheme uses max categories 20 which prevents the features from";"TASK"
"expanding too much which can result in underfitting";"IRRE"
"if we had not set max categories 20 the one hot encoding scheme would have";"IRRE"
"likely made the pipeline overfitting as the number of features explodes with rare";"TASK"
"category occurrences that are correlated with the target by chance on the training";"-"
"set only";"IRRE"
"the ordinal encoding imposes an arbitrary order to the features which are then";"TASK"
"treated as numerical values by the";"IRRE"
"class sklearn ensemble histgradientboostingregressor since this";"CODE"
"model groups numerical features in 256 bins per feature many unrelated categories";"TASK"
"can be grouped together and as a result overall pipeline can underfit";"IRRE"
"when using the target encoder the same binning happens but since the encoded";"META"
"values are statistically ordered by marginal association with the target variable";"IRRE"
"the binning use by the class sklearn ensemble histgradientboostingregressor";"IRRE"
"makes sense and leads to good results the combination of smoothed target";"IRRE"
"encoding and binning works as a good regularizing strategy against";"-"
"overfitting while not limiting the expressiveness of the pipeline too much";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"create synthetic dataset";"IRRE"
"for this example we build a dataset with three categorical features";"CODE"
"an informative feature with medium cardinality informative";"CODE"
"an uninformative feature with medium cardinality shuffled";"TASK"
"an uninformative feature with high cardinality near unique";"CODE"
"first we generate the informative feature";"TASK"
"remove the linear relationship between y and the bin index by permuting the";"-"
"values of x informative";"IRRE"
"the uninformative feature with medium cardinality is generated by permuting the";"TASK"
"informative feature and removing the relationship with the target";"TASK"
"the uninformative feature with high cardinality is generated so that it is";"TASK"
"independent of the target variable we will show that target encoding without";"CODE"
"term cross fitting will cause catastrophic overfitting for the downstream";"CODE"
"regressor these high cardinality features are basically unique identifiers";"TASK"
"for samples which should generally be removed from machine learning datasets";"CODE"
"in this example we generate them to show how class targetencoder s default";"CODE"
"term cross fitting behavior mitigates the overfitting issue automatically";"IRRE"
"finally we assemble the dataset and perform a train test split";"CODE"
"training a ridge regressor";"-"
"in this section we train a ridge regressor on the dataset with and without";"CODE"
"encoding and explore the influence of target encoder with and without the";"-"
"internal term cross fitting first we see the ridge model trained on the";"CODE"
"raw features will have low performance this is because we permuted the order";"CODE"
"of the informative feature meaning x informative is not informative when";"CODE"
"raw";"-"
"configure transformers to always output dataframes";"IRRE"
"next we create a pipeline with the target encoder and ridge model the pipeline";"CODE"
"uses meth targetencoder fit transform which uses term cross fitting we";"CODE"
"see that the model fits the data well and generalizes to the test set";"IRRE"
"the coefficients of the linear model shows that most of the weight is on the";"-"
"feature at column index 0 which is the informative feature";"TASK"
"while meth targetencoder fit transform uses an internal";"CODE"
"term cross fitting scheme to learn encodings for the training set";"IRRE"
"meth targetencoder fit followed by meth targetencoder transform does not";"CODE"
"it uses the complete training set to learn encodings and to transform the";"CODE"
"categorical features thus we can use meth targetencoder fit followed by";"TASK"
"meth targetencoder transform to disable the term cross fitting this";"CODE"
"encoding is then passed to the ridge model";"-"
"we evaluate the model that did not use term cross fitting when encoding and";"IRRE"
"see that it overfits";"-"
"the ridge model overfits because it assigns much more weight to the";"IRRE"
"uninformative extremely high cardinality near unique and medium";"CODE"
"cardinality shuffled features than when the model used";"TASK"
"term cross fitting to encode the features";"TASK"
"conclusion";"-"
"this example demonstrates the importance of class targetencoder s internal";"CODE"
"term cross fitting it is important to use";"CODE"
"meth targetencoder fit transform to encode training data before passing it";"CODE"
"to a machine learning model when a class targetencoder is a part of a";"IRRE"
"class sklearn pipeline pipeline and the pipeline is fitted the pipeline";"CODE"
"will correctly call meth targetencoder fit transform and use";"IRRE"
"term cross fitting when encoding the training data";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"new plotting api";"CODE"
"a new plotting api is available for creating visualizations this new api";"CODE"
"allows for quickly adjusting the visuals of a plot without involving any";"CODE"
"recomputation it is also possible to add different plots to the same";"TASK"
"figure the following example illustrates plot roc curve";"-"
"but other plots utilities are supported like";"META"
"plot partial dependence";"CODE"
"plot precision recall curve and";"IRRE"
"plot confusion matrix read more about this new api in the";"CODE"
"ref user guide visualizations";"-"
"from sklearn metrics import plot roc curve";"CODE"
"plot roc curve has been removed in version 1 2 from 1 2 use roccurvedisplay instead";"OUTD"
"svc disp plot roc curve svc x test y test";"IRRE"
"rfc disp plot roc curve rfc x test y test ax svc disp ax";"IRRE"
"stacking classifier and regressor";"IRRE"
"class ensemble stackingclassifier and";"IRRE"
"class ensemble stackingregressor";"IRRE"
"allow you to have a stack of estimators with a final classifier or";"CODE"
"a regressor";"-"
"stacked generalization consists in stacking the output of individual";"IRRE"
"estimators and use a classifier to compute the final prediction stacking";"CODE"
"allows to use the strength of each individual estimator by using their output";"IRRE"
"as input of a final estimator";"CODE"
"base estimators are fitted on the full x while";"CODE"
"the final estimator is trained using cross validated predictions of the";"CODE"
"base estimators using cross val predict";"-"
"read more in the ref user guide stacking";"CODE"
"permutation based feature importance";"CODE"
"the func inspection permutation importance can be used to get an";"CODE"
"estimate of the importance of each feature for any fitted estimator";"CODE"
"labels argument in boxplot is deprecated in matplotlib 3 9 and has been";"OUTD"
"renamed to tick labels the following code handles this but as a";"META"
"scikit learn user you probably can write simpler code by using labels";"TASK"
"matplotlib 3 9 or tick labels matplotlib 3 9";"-"
"native support for missing values for gradient boosting";"IRRE"
"the class ensemble histgradientboostingclassifier";"IRRE"
"and class ensemble histgradientboostingregressor now have native";"IRRE"
"support for missing values nans this means that there is no need for";"CODE"
"imputing data when training or predicting";"-"
"precomputed sparse nearest neighbors graph";"IRRE"
"most estimators based on nearest neighbors graphs now accept precomputed";"IRRE"
"sparse graphs as input to reuse the same graph for multiple estimator fits";"IRRE"
"to use this feature in a pipeline one can use the memory parameter along";"IRRE"
"with one of the two new transformers";"CODE"
"class neighbors kneighborstransformer and";"IRRE"
"class neighbors radiusneighborstransformer the precomputation";"CODE"
"can also be performed by custom estimators to use alternative";"CODE"
"implementations such as approximate nearest neighbors methods";"TASK"
"see more details in the ref user guide neighbors transformer";"CODE"
"we can decrease the number of neighbors and the graph will not be";"IRRE"
"recomputed";"-"
"knn based imputation";"-"
"we now support imputation for completing missing values using k nearest";"IRRE"
"neighbors";"-"
"each sample s missing values are imputed using the mean value from";"IRRE"
"n neighbors nearest neighbors found in the training set two samples are";"IRRE"
"close if the features that neither is missing are close";"CODE"
"by default a euclidean distance metric";"CODE"
"that supports missing values";"IRRE"
"func sklearn metrics pairwise nan euclidean distances is used to find the nearest";"OUTD"
"neighbors";"-"
"read more in the ref user guide knnimpute";"CODE"
"tree pruning";"CODE"
"it is now possible to prune most tree based estimators once the trees are";"CODE"
"built the pruning is based on minimal cost complexity read more in the";"CODE"
"ref user guide minimal cost complexity pruning for details";"CODE"
"retrieve dataframes from openml";"CODE"
"func datasets fetch openml can now return pandas dataframe and thus";"IRRE"
"properly handle datasets with heterogeneous data";"IRRE"
"checking scikit learn compatibility of an estimator";"-"
"developers can check the compatibility of their scikit learn compatible";"-"
"estimators using func utils estimator checks check estimator for";"CODE"
"instance the check estimator linearsvc passes";"-"
"we now provide a pytest specific decorator which allows pytest";"IRRE"
"to run all checks independently and report the checks that are failing";"CODE"
"note";"TASK"
"this entry was slightly updated in version 0 24 where passing classes";"CODE"
"isn t supported anymore pass instances instead";"OUTD"
"roc auc now supports multiclass classification";"IRRE"
"the func sklearn metrics roc auc score function can also be used in multi class";"CODE"
"classification two averaging strategies are currently supported the";"IRRE"
"one vs one algorithm computes the average of the pairwise roc auc scores and";"IRRE"
"the one vs rest algorithm computes the average of the roc auc scores for each";"CODE"
"class against all other classes in both cases the multiclass roc auc scores";"CODE"
"are computed from the probability estimates that a sample belongs to a";"IRRE"
"particular class according to the model the ovo and ovr algorithms support";"CODE"
"weighting uniformly average macro and weighting by the prevalence";"CODE"
"average weighted";"-"
"read more in the ref user guide roc metrics";"CODE"
"ruff noqa cpy001";"-"
"generalized linear models and poisson loss for gradient boosting";"CODE"
"long awaited generalized linear models with non normal loss functions are now";"CODE"
"available in particular three new regressors were implemented";"TASK"
"class sklearn linear model poissonregressor";"IRRE"
"class sklearn linear model gammaregressor and";"IRRE"
"class sklearn linear model tweedieregressor the poisson regressor can be";"IRRE"
"used to model positive integer counts or relative frequencies read more in";"CODE"
"the ref user guide generalized linear regression additionally";"TASK"
"class sklearn ensemble histgradientboostingregressor supports a new";"CODE"
"poisson loss as well";"-"
"positive integer target correlated with x 5 with many zeros";"CODE"
"rich visual representation of estimators";"-"
"estimators can now be visualized in notebooks by enabling the";"TASK"
"display diagram option this is particularly useful to summarise the";"IRRE"
"structure of pipelines and other composite estimators with interactivity to";"CODE"
"provide detail click on the example image below to expand pipeline";"CODE"
"elements see ref visualizing composite estimators for how you can use";"CODE"
"this feature";"TASK"
"scalability and stability improvements to kmeans";"TASK"
"the class sklearn cluster kmeans estimator was entirely re worked and it";"IRRE"
"is now significantly faster and more stable in addition the elkan algorithm";"TASK"
"is now compatible with sparse matrices the estimator uses openmp based";"IRRE"
"parallelism instead of relying on joblib so the n jobs parameter has no";"IRRE"
"effect anymore for more details on how to control the number of threads";"CODE"
"please refer to our ref parallelism notes";"TASK"
"improvements to the histogram based gradient boosting estimators";"TASK"
"various improvements were made to";"TASK"
"class sklearn ensemble histgradientboostingclassifier and";"IRRE"
"class sklearn ensemble histgradientboostingregressor on top of the";"IRRE"
"poisson loss mentioned above these estimators now support ref sample";"-"
"weights sw hgbdt also an automatic early stopping criterion was added";"TASK"
"early stopping is enabled by default when the number of samples exceeds 10k";"CODE"
"finally users can now define ref monotonic constraints";"CODE"
"monotonic cst gbdt to constrain the predictions based on the variations of";"CODE"
"specific features in the following example we construct a target that is";"CODE"
"generally positively correlated with the first feature with some noise";"TASK"
"applying monotoinc constraints allows the prediction to capture the global";"CODE"
"effect of the first feature instead of fitting the noise for a usecase";"CODE"
"example see ref sphx glr auto examples ensemble plot hgbt regression py";"-"
"from sklearn inspection import plot partial dependence";"CODE"
"plot partial dependence has been removed in version 1 2 from 1 2 use";"CODE"
"partialdependencedisplay instead";"CODE"
"disp plot partial dependence";"CODE"
"plot partial dependence";"CODE"
"sample weight support for lasso and elasticnet";"CODE"
"the two linear regressors class sklearn linear model lasso and";"IRRE"
"class sklearn linear model elasticnet now support sample weights";"IRRE"
"ruff noqa cpy001 e501";"-"
"successive halving estimators for tuning hyper parameters";"IRRE"
"successive halving a state of the art method is now available to";"CODE"
"explore the space of the parameters and identify their best combination";"IRRE"
"class sklearn model selection halvinggridsearchcv and";"CODE"
"class sklearn model selection halvingrandomsearchcv can be";"IRRE"
"used as drop in replacement for";"CODE"
"class sklearn model selection gridsearchcv and";"CODE"
"class sklearn model selection randomizedsearchcv";"IRRE"
"successive halving is an iterative selection process illustrated in the";"CODE"
"figure below the first iteration is run with a small amount of resources";"CODE"
"where the resource typically corresponds to the number of training samples";"IRRE"
"but can also be an arbitrary integer parameter such as n estimators in a";"IRRE"
"random forest only a subset of the parameter candidates are selected for the";"IRRE"
"next iteration which will be run with an increasing amount of allocated";"CODE"
"resources only a subset of candidates will last until the end of the";"CODE"
"iteration process and the best parameter candidate is the one that has the";"IRRE"
"highest score on the last iteration";"-"
"read more in the ref user guide successive halving user guide note";"CODE"
"the successive halving estimators are still term experimental";"TASK"
"experimental";"-"
"figure model selection images sphx glr plot successive halving iterations 001 png";"CODE"
"target model selection plot successive halving iterations html";"CODE"
"align center";"-"
"from sklearn experimental import enable halving search cv noqa f401";"CODE"
"native support for categorical features in histgradientboosting estimators";"TASK"
"class sklearn ensemble histgradientboostingclassifier and";"IRRE"
"class sklearn ensemble histgradientboostingregressor now have native";"IRRE"
"support for categorical features they can consider splits on non ordered";"TASK"
"categorical data read more in the ref user guide";"CODE"
"categorical support gbdt";"-"
"figure ensemble images sphx glr plot gradient boosting categorical 001 png";"-"
"target ensemble plot gradient boosting categorical html";"-"
"align center";"-"
"the plot shows that the new native support for categorical features leads to";"CODE"
"fitting times that are comparable to models where the categories are treated";"CODE"
"as ordered quantities i e simply ordinal encoded native support is also";"-"
"more expressive than both one hot encoding and ordinal encoding however to";"CODE"
"use the new categorical features parameter it is still required to";"TASK"
"preprocess the data within a pipeline as demonstrated in this ref example";"CODE"
"sphx glr auto examples ensemble plot gradient boosting categorical py";"-"
"improved performances of histgradientboosting estimators";"CODE"
"the memory footprint of class ensemble histgradientboostingregressor and";"IRRE"
"class ensemble histgradientboostingclassifier has been significantly";"IRRE"
"improved during calls to fit in addition histogram initialization is now";"IRRE"
"done in parallel which results in slight speed improvements";"TASK"
"see more in the benchmark page";"CODE"
"https scikit learn org scikit learn benchmarks";"CODE"
"new self training meta estimator";"CODE"
"a new self training implementation based on yarowski s algorithm";"CODE"
"https doi org 10 3115 981658 981684 can now be used with any";"CODE"
"classifier that implements term predict proba the sub classifier";"IRRE"
"will behave as a";"-"
"semi supervised classifier allowing it to learn from unlabeled data";"CODE"
"read more in the ref user guide self training";"CODE"
"new sequentialfeatureselector transformer";"CODE"
"a new iterative transformer to select features is available";"CODE"
"class sklearn feature selection sequentialfeatureselector";"CODE"
"sequential feature selection can add features one at a time forward";"TASK"
"selection or remove features from the list of the available features";"TASK"
"backward selection based on a cross validated score maximization";"CODE"
"see the ref user guide sequential feature selection";"TASK"
"new polynomialcountsketch kernel approximation function";"CODE"
"the new class sklearn kernel approximation polynomialcountsketch";"CODE"
"approximates a polynomial expansion of a feature space when used with linear";"TASK"
"models but uses much less memory than";"META"
"class sklearn preprocessing polynomialfeatures";"TASK"
"for comparison here is the score of a linear baseline for the same data";"CODE"
"individual conditional expectation plots";"-"
"a new kind of partial dependence plot is available the individual";"CODE"
"conditional expectation ice plot ice plots visualize the dependence of the";"CODE"
"prediction on a feature for each sample separately with one line per sample";"CODE"
"see the ref user guide individual conditional";"-"
"from sklearn inspection import plot partial dependence";"CODE"
"plot partial dependence has been removed in version 1 2 from 1 2 use";"CODE"
"partialdependencedisplay instead";"CODE"
"display plot partial dependence";"CODE"
"new poisson splitting criterion for decisiontreeregressor";"CODE"
"the integration of poisson regression estimation continues from version 0 23";"CODE"
"class sklearn tree decisiontreeregressor now supports a new poisson";"CODE"
"splitting criterion setting criterion poisson might be a good choice";"IRRE"
"if your target is a count or a frequency";"-"
"positive integer target correlated with x 5 with many zeros";"CODE"
"new documentation improvements";"CODE"
"new examples and documentation pages have been added in a continuous effort";"CODE"
"to improve the understanding of machine learning practices";"-"
"a new section about ref common pitfalls and recommended";"CODE"
"practices common pitfalls";"-"
"an example illustrating how to ref statistically compare the performance of";"IRRE"
"models sphx glr auto examples model selection plot grid search stats py";"CODE"
"evaluated using class sklearn model selection gridsearchcv";"CODE"
"an example on how to ref interpret coefficients of linear models";"CODE"
"sphx glr auto examples inspection plot linear model coefficient interpretation py";"CODE"
"an ref example";"-"
"sphx glr auto examples cross decomposition plot pcr vs pls py";"-"
"comparing principal component regression and partial least squares";"-"
"ruff noqa cpy001";"-"
"keyword and positional arguments";"-"
"the scikit learn api exposes many functions and methods which have many input";"CODE"
"parameters for example before this release one could instantiate a";"CODE"
"class ensemble histgradientboostingregressor as";"IRRE"
"histgradientboostingregressor squared error 0 1 100 31 none";"-"
"20 0 0 255 none none false auto loss 0 1 10 1e 7";"-"
"0 none";"-"
"understanding the above code requires the reader to go to the api";"CODE"
"documentation and to check each and every parameter for its position and";"CODE"
"its meaning to improve the readability of code written based on scikit learn";"CODE"
"now users have to provide most parameters with their names as keyword";"IRRE"
"arguments instead of positional arguments for example the above code would";"CODE"
"histgradientboostingregressor";"-"
"loss squared error";"-"
"learning rate 0 1";"-"
"max iter 100";"-"
"max leaf nodes 31";"-"
"max depth none";"-"
"min samples leaf 20";"-"
"l2 regularization 0 0";"-"
"max bins 255";"-"
"categorical features none";"TASK"
"monotonic cst none";"-"
"warm start false";"-"
"early stopping auto";"-"
"scoring loss";"-"
"validation fraction 0 1";"-"
"n iter no change 10";"-"
"tol 1e 7";"-"
"verbose 0";"IRRE"
"random state none";"IRRE"
"which is much more readable positional arguments have been deprecated since";"CODE"
"version 0 23 and will now raise a typeerror a limited number of";"META"
"positional arguments are still allowed in some cases for example in";"CODE"
"class decomposition pca where pca 10 is still allowed but pca 10";"TASK"
"false is not allowed";"-"
"spline transformers";"CODE"
"one way to add nonlinear terms to a dataset s feature set is to generate";"TASK"
"spline basis functions for continuous numerical features with the new";"CODE"
"class preprocessing splinetransformer splines are piecewise polynomials";"CODE"
"parametrized by their polynomial degree and the positions of the knots the";"-"
"class preprocessing splinetransformer implements a b spline basis";"CODE"
"figure linear model images sphx glr plot polynomial interpolation 001 png";"CODE"
"target linear model plot polynomial interpolation html";"CODE"
"align center";"-"
"the following code shows splines in action for more information please";"CODE"
"refer to the ref user guide spline transformer";"CODE"
"quantile regressor";"-"
"quantile regression estimates the median or other quantiles of math y";"IRRE"
"conditional on math x while ordinary least squares ols estimates the";"IRRE"
"conditional mean";"-"
"as a linear model the new class linear model quantileregressor gives";"CODE"
"linear predictions math hat y w x xw for the math q th quantile";"CODE"
"math q in 0 1 the weights or coefficients math w are then found by";"CODE"
"the following minimization problem";"-"
"math";"-"
"min w frac 1 n text samples";"-"
"sum i pb q y i x i w alpha w 1";"-"
"this consists of the pinball loss also known as linear loss";"CODE"
"see also class sklearn metrics mean pinball loss";"IRRE"
"math";"-"
"pb q t q max t 0 1 q max t 0";"-"
"begin cases";"CODE"
"q t t 0";"-"
"0 t 0";"-"
"1 q t t 0";"-"
"end cases";"CODE"
"and the l1 penalty controlled by parameter alpha similar to";"IRRE"
"class linear model lasso";"IRRE"
"please check the following example to see how it works and the ref user";"CODE"
"guide quantile regression for more details";"CODE"
"figure linear model images sphx glr plot quantile regression 002 png";"-"
"target linear model plot quantile regression html";"-"
"align center";"-"
"scale 50";"-"
"feature names support";"TASK"
"when an estimator is passed a pandas dataframe";"-"
"https pandas pydata org docs user guide dsintro html dataframe during";"CODE"
"term fit the estimator will set a feature names in attribute";"TASK"
"containing the feature names this is a part of";"TASK"
"slep007 https scikit learn enhancement proposals readthedocs io en latest slep007 proposal html";"CODE"
"note that feature names support is only enabled";"TASK"
"when the column names in the dataframe are all strings feature names in";"CODE"
"is used to check that the column names of the dataframe passed in";"OUTD"
"non term fit such as term predict are consistent with features in";"TASK"
"term fit";"-"
"the support of term get feature names out is available for transformers";"CODE"
"that already had get feature names and transformers with a one to one";"CODE"
"correspondence between input and output such as";"CODE"
"class preprocessing standardscaler term get feature names out support";"TASK"
"will be added to all other transformers in future releases additionally";"TASK"
"meth compose columntransformer get feature names out is available to";"CODE"
"combine feature names of its transformers";"TASK"
"when this preprocessor is used with a pipeline the feature names used";"CODE"
"by the classifier are obtained by slicing and calling";"IRRE"
"term get feature names out";"TASK"
"a more flexible plotting api";"CODE"
"class metrics confusionmatrixdisplay";"IRRE"
"class metrics precisionrecalldisplay class metrics detcurvedisplay";"IRRE"
"and class inspection partialdependencedisplay now expose two class";"IRRE"
"methods from estimator and from predictions which allow users to create";"IRRE"
"a plot given the predictions or an estimator this means the corresponding";"CODE"
"plot functions are deprecated please check ref example one";"OUTD"
"sphx glr auto examples model selection plot confusion matrix py and";"CODE"
"ref example two";"CODE"
"sphx glr auto examples classification plot digits classification py for";"CODE"
"how to use the new plotting functionalities";"CODE"
"online one class svm";"IRRE"
"the new class class linear model sgdoneclasssvm implements an online";"CODE"
"linear version of the one class svm using a stochastic gradient descent";"IRRE"
"combined with kernel approximation techniques";"-"
"class linear model sgdoneclasssvm can be used to approximate the solution";"CODE"
"of a kernelized one class svm implemented in class svm oneclasssvm with";"CODE"
"a fit time complexity linear in the number of samples note that the";"TASK"
"complexity of a kernelized one class svm is at best quadratic in the number";"CODE"
"of samples class linear model sgdoneclasssvm is thus well suited for";"CODE"
"datasets with a large number of training samples 10 000 for which the sgd";"IRRE"
"variant can be several orders of magnitude faster please check this";"CODE"
"ref example";"-"
"sphx glr auto examples miscellaneous plot anomaly comparison py to see how";"-"
"it s used and the ref user guide sgd online one class svm for more";"CODE"
"details";"-"
"figure miscellaneous images sphx glr plot anomaly comparison 001 png";"-"
"target miscellaneous plot anomaly comparison html";"-"
"align center";"-"
"histogram based gradient boosting models are now stable";"-"
"class sklearn ensemble histgradientboostingregressor and";"IRRE"
"class ensemble histgradientboostingclassifier are no longer experimental";"IRRE"
"and can simply be imported and used as";"CODE"
"from sklearn ensemble import histgradientboostingclassifier";"CODE"
"new documentation improvements";"CODE"
"this release includes many documentation improvements out of over 2100";"CODE"
"merged pull requests about 800 of them are improvements to our";"TASK"
"documentation";"CODE"
"ruff noqa cpy001";"-"
"quantile support hgbdt";"-"
"quantile loss in class ensemble histgradientboostingregressor";"IRRE"
"class ensemble histgradientboostingregressor can model quantiles with";"IRRE"
"loss quantile and the new parameter quantile";"IRRE"
"simple regression function for x cos x";"CODE"
"for a usecase example see";"CODE"
"ref sphx glr auto examples ensemble plot hgbt regression py";"-"
"get feature names out available in all transformers";"TASK"
"term get feature names out is now available in all transformers thereby";"TASK"
"concluding the implementation of";"TASK"
"slep007 https scikit learn enhancement proposals readthedocs io en latest slep007 proposal html";"CODE"
"this enables class pipeline pipeline to construct the output feature names for";"CODE"
"more complex pipelines";"META"
"here we slice the pipeline to include all the steps but the last one the output";"CODE"
"feature names of this pipeline slice are the features put into logistic";"CODE"
"regression these names correspond directly to the coefficients in the logistic";"CODE"
"regression";"-"
"grouping infrequent categories in class preprocessing onehotencoder";"IRRE"
"class preprocessing onehotencoder supports aggregating infrequent";"IRRE"
"categories into a single output for each feature the parameters to enable";"CODE"
"the gathering of infrequent categories are min frequency and";"-"
"max categories see the ref user guide encoder infrequent categories";"-"
"for more details";"CODE"
"since dog and snake are infrequent categories they are grouped together when";"CODE"
"transformed";"CODE"
"performance improvements";"TASK"
"reductions on pairwise distances for dense float64 datasets has been refactored";"CODE"
"to better take advantage of non blocking thread parallelism for example";"CODE"
"meth neighbors nearestneighbors kneighbors and";"-"
"meth neighbors nearestneighbors radius neighbors can respectively be up to 20 and";"-"
"5 faster than previously in summary the following functions and estimators";"CODE"
"now benefit from improved performance";"CODE"
"func metrics pairwise distances argmin";"-"
"func metrics pairwise distances argmin min";"-"
"class cluster affinitypropagation";"IRRE"
"class cluster birch";"IRRE"
"class cluster meanshift";"IRRE"
"class cluster optics";"IRRE"
"class cluster spectralclustering";"IRRE"
"func feature selection mutual info regression";"CODE"
"class neighbors kneighborsclassifier";"IRRE"
"class neighbors kneighborsregressor";"IRRE"
"class neighbors radiusneighborsclassifier";"IRRE"
"class neighbors radiusneighborsregressor";"IRRE"
"class neighbors localoutlierfactor";"IRRE"
"class neighbors nearestneighbors";"IRRE"
"class manifold isomap";"IRRE"
"class manifold locallylinearembedding";"IRRE"
"class manifold tsne";"IRRE"
"func manifold trustworthiness";"-"
"class semi supervised labelpropagation";"CODE"
"class semi supervised labelspreading";"CODE"
"to know more about the technical details of this work you can read";"CODE"
"this suite of blog posts https blog scikit learn org technical performances";"CODE"
"moreover the computation of loss functions has been refactored using";"OUTD"
"cython resulting in performance improvements for the following estimators";"CODE"
"class linear model logisticregression";"IRRE"
"class linear model gammaregressor";"IRRE"
"class linear model poissonregressor";"IRRE"
"class linear model tweedieregressor";"IRRE"
"class decomposition minibatchnmf an online version of nmf";"IRRE"
"the new class class decomposition minibatchnmf implements a faster but";"CODE"
"less accurate version of non negative matrix factorization";"META"
"class decomposition nmf class decomposition minibatchnmf divides the";"IRRE"
"data into mini batches and optimizes the nmf model in an online manner by";"CODE"
"cycling over the mini batches making it better suited for large datasets in";"CODE"
"particular it implements partial fit which can be used for online";"TASK"
"learning when the data is not readily available from the start or when the";"CODE"
"data does not fit into memory";"CODE"
"class cluster bisectingkmeans divide and cluster";"IRRE"
"the new class class cluster bisectingkmeans is a variant of";"CODE"
"class cluster kmeans using divisive hierarchical clustering instead of";"CODE"
"creating all centroids at once centroids are picked progressively based on a";"CODE"
"previous clustering a cluster is split into two new clusters repeatedly";"CODE"
"until the target number of clusters is reached giving a hierarchical";"-"
"structure to the clustering";"CODE"
"ruff noqa cpy001 e501";"-"
"pandas output with set output api";"IRRE"
"scikit learn s transformers now support pandas output with the set output api";"IRRE"
"to learn more about the set output api see the example";"IRRE"
"ref sphx glr auto examples miscellaneous plot set output py and";"IRRE"
"this video pandas dataframe output for scikit learn transformers";"CODE"
"some examples https youtu be 5bcg8vfx2x8";"CODE"
"interaction constraints in histogram based gradient boosting trees";"CODE"
"class ensemble histgradientboostingregressor and";"IRRE"
"class ensemble histgradientboostingclassifier now supports interaction constraints";"CODE"
"with the interaction cst parameter for details see the";"CODE"
"ref user guide interaction cst hgbt in the following example features are not";"CODE"
"allowed to interact";"CODE"
"new and enhanced displays";"CODE"
"class metrics predictionerrordisplay provides a way to analyze regression";"IRRE"
"models in a qualitative manner";"-"
"class model selection learningcurvedisplay is now available to plot";"CODE"
"results from func model selection learning curve";"CODE"
"class inspection partialdependencedisplay exposes a new parameter";"CODE"
"categorical features to display partial dependence for categorical features";"TASK"
"using bar plots and heatmaps";"IRRE"
"faster parser in func datasets fetch openml";"IRRE"
"func datasets fetch openml now supports a new pandas parser that is";"IRRE"
"more memory and cpu efficient in v1 4 the default will change to";"CODE"
"parser auto which will automatically use the pandas parser for dense";"IRRE"
"data and liac arff for sparse data";"IRRE"
"experimental array api support in class discriminant analysis lineardiscriminantanalysis";"CODE"
"experimental support for the array api https data apis org array api latest";"CODE"
"specification was added to class discriminant analysis lineardiscriminantanalysis";"TASK"
"the estimator can now run on any array api compliant libraries such as";"CODE"
"cupy https docs cupy dev en stable overview html a gpu accelerated array";"CODE"
"library for details see the ref user guide array api";"CODE"
"improved efficiency of many estimators";"CODE"
"in version 1 1 the efficiency of many estimators relying on the computation of";"META"
"pairwise distances essentially estimators related to clustering manifold";"-"
"learning and neighbors search algorithms was greatly improved for float64";"CODE"
"dense input efficiency improvement especially were a reduced memory footprint";"CODE"
"and a much better scalability on multi core machines";"-"
"in version 1 2 the efficiency of these estimators was further improved for all";"META"
"combinations of dense and sparse inputs on float32 and float64 datasets except";"CODE"
"the sparse dense and dense sparse combinations for the euclidean and squared";"IRRE"
"euclidean distance metrics";"CODE"
"a detailed list of the impacted estimators can be found in the";"CODE"
"ref changelog release notes 1 2";"TASK"
"ruff noqa cpy001";"-"
"metadata routing";"-"
"we are in the process of introducing a new way to route metadata such as";"CODE"
"sample weight throughout the codebase which would affect how";"-"
"meta estimators such as class pipeline pipeline and";"CODE"
"class model selection gridsearchcv route metadata while the";"CODE"
"infrastructure for this feature is already included in this release the work";"CODE"
"is ongoing and not all meta estimators support this new feature you can read";"CODE"
"more about this feature in the ref metadata routing user guide";"CODE"
"metadata routing note that this feature is still under development and";"TASK"
"not implemented for most meta estimators";"TASK"
"third party developers can already start incorporating this into their";"CODE"
"meta estimators for more details see";"CODE"
"ref metadata routing developer guide";"-"
"sphx glr auto examples miscellaneous plot metadata routing py";"-"
"hdbscan hierarchical density based clustering";"-"
"originally hosted in the scikit learn contrib repository class cluster hdbscan";"CODE"
"has been adpoted into scikit learn it s missing a few features from the original";"CODE"
"implementation which will be added in future releases";"TASK"
"by performing a modified version of class cluster dbscan over multiple epsilon";"CODE"
"values simultaneously class cluster hdbscan finds clusters of varying densities";"IRRE"
"making it more robust to parameter selection than class cluster dbscan";"IRRE"
"more details in the ref user guide hdbscan";"CODE"
"targetencoder a new category encoding strategy";"CODE"
"well suited for categorical features with high cardinality";"TASK"
"class preprocessing targetencoder encodes the categories based on a shrunk";"CODE"
"estimate of the average target values for observations belonging to that category";"IRRE"
"more details in the ref user guide target encoder";"CODE"
"missing values support in decision trees";"IRRE"
"the classes class tree decisiontreeclassifier and";"IRRE"
"class tree decisiontreeregressor now support missing values for each potential";"IRRE"
"threshold on the non missing data the splitter will evaluate the split with all the";"CODE"
"missing values going to the left node or the right node";"IRRE"
"see more details in the ref user guide tree missing value support or see";"IRRE"
"ref sphx glr auto examples ensemble plot hgbt regression py for a usecase";"CODE"
"example of this feature in class ensemble histgradientboostingregressor";"CODE"
"new display class model selection validationcurvedisplay";"CODE"
"class model selection validationcurvedisplay is now available to plot results";"CODE"
"from func model selection validation curve";"CODE"
"gamma loss for gradient boosting";"CODE"
"the class class ensemble histgradientboostingregressor supports the";"IRRE"
"gamma deviance loss function via loss gamma this loss function is useful for";"CODE"
"modeling strictly positive targets with a right skewed distribution";"META"
"grouping infrequent categories in class preprocessing ordinalencoder";"IRRE"
"similarly to class preprocessing onehotencoder the class";"IRRE"
"class preprocessing ordinalencoder now supports aggregating infrequent categories";"IRRE"
"into a single output for each feature the parameters to enable the gathering of";"CODE"
"infrequent categories are min frequency and max categories";"-"
"see the ref user guide encoder infrequent categories for more details";"CODE"
"ruff noqa cpy001";"-"
"histgradientboosting natively supports categorical dtypes in dataframes";"-"
"class ensemble histgradientboostingclassifier and";"IRRE"
"class ensemble histgradientboostingregressor now directly supports dataframes with";"IRRE"
"categorical features here we have a dataset with a mixture of";"TASK"
"categorical and numerical features";"TASK"
"remove redundant and non feature columns";"TASK"
"by setting categorical features from dtype the gradient boosting classifier";"IRRE"
"treats the columns with categorical dtypes as categorical features in the";"TASK"
"algorithm";"-"
"polars output in set output";"IRRE"
"scikit learn s transformers now support polars output with the set output api";"IRRE"
"missing value support for random forest";"IRRE"
"the classes class ensemble randomforestclassifier and";"IRRE"
"class ensemble randomforestregressor now support missing values when training";"IRRE"
"every individual tree the splitter evaluates each potential threshold with the";"-"
"missing values going to the left and right nodes more details in the";"IRRE"
"ref user guide tree missing value support";"IRRE"
"add support for monotonic constraints in tree based models";"CODE"
"while we added support for monotonic constraints in histogram based gradient boosting";"CODE"
"in scikit learn 0 23 we now support this feature for all other tree based models as";"CODE"
"trees random forests extra trees and exact gradient boosting here we show this";"CODE"
"feature for random forest on a regression problem";"CODE"
"enriched estimator displays";"-"
"estimators displays have been enriched if we look at forest defined above";"CODE"
"one can access the documentation of the estimator by clicking on the icon on";"CODE"
"the top right corner of the diagram";"-"
"in addition the display changes color from orange to blue when the estimator is";"TASK"
"fitted you can also get this information by hovering on the icon i";"CODE"
"clone forest the clone is not fitted";"CODE"
"metadata routing support";"-"
"many meta estimators and cross validation routines now support metadata";"-"
"routing which are listed in the ref user guide";"CODE"
"metadata routing models for instance this is how you can do a nested";"CODE"
"cross validation with sample weights and class model selection groupkfold";"CODE"
"for now by default metadata routing is disabled and need to be explicitly";"CODE"
"enabled";"-"
"setting the flag to the default false to avoid interference with other";"CODE"
"scripts";"CODE"
"improved memory and runtime efficiency for pca on sparse data";"CODE"
"pca is now able to handle sparse matrices natively for the arpack";"CODE"
"solver by levaraging scipy sparse linalg linearoperator to avoid";"CODE"
"materializing large sparse matrices when performing the";"IRRE"
"eigenvalue decomposition of the data set covariance matrix";"IRRE"
"ruff noqa cpy001";"-"
"fixedthresholdclassifier setting the decision threshold of a binary classifier";"IRRE"
"all binary classifiers of scikit learn use a fixed decision threshold of 0 5";"CODE"
"to convert probability estimates i e output of predict proba into class";"IRRE"
"predictions however 0 5 is almost never the desired threshold for a given";"CODE"
"problem class model selection fixedthresholdclassifier allows wrapping any";"CODE"
"binary classifier and setting a custom decision threshold";"IRRE"
"lowering the threshold i e allowing more samples to be classified as the positive";"IRRE"
"class increases the number of true positives at the cost of more false positives";"IRRE"
"as is well known from the concavity of the roc curve";"CODE"
"tunedthresholdclassifiercv tuning the decision threshold of a binary classifier";"CODE"
"the decision threshold of a binary classifier can be tuned to optimize a";"CODE"
"given metric using class model selection tunedthresholdclassifiercv";"CODE"
"it is particularly useful to find the best decision threshold when the model";"CODE"
"is meant to be deployed in a specific application context where we can assign";"IRRE"
"different gains or costs for true positives true negatives false positives";"CODE"
"and false negatives";"-"
"let s illustrate this by considering an arbitrary case where";"CODE"
"each true positive gains 1 unit of profit e g euro year of life in good";"-"
"health etc";"-"
"true negatives gain or cost nothing";"-"
"each false negative costs 2";"-"
"each false positive costs 0 1";"-"
"our metric quantifies the average profit per sample which is defined by the";"CODE"
"following python function";"CODE"
"it is interesting to observe that the average gain per prediction is negative";"CODE"
"which means that this decision system is making a loss on average";"CODE"
"tuning the threshold to optimize this custom metric gives a smaller threshold";"CODE"
"that allows more samples to be classified as the positive class as a result";"IRRE"
"the average gain per prediction improves";"-"
"we observe that tuning the decision threshold can turn a machine";"-"
"learning based system that makes a loss on average into a beneficial one";"CODE"
"in practice defining a meaningful application specific metric might involve";"CODE"
"making those costs for bad predictions and gains for good predictions depend on";"CODE"
"auxiliary metadata specific to each individual data point such as the amount";"CODE"
"of a transaction in a fraud detection system";"CODE"
"to achieve this class model selection tunedthresholdclassifiercv";"CODE"
"leverages metadata routing support ref metadata routing user";"-"
"guide metadata routing allowing to optimize complex business metrics as";"META"
"detailed in ref post tuning the decision threshold for cost sensitive";"CODE"
"learning";"-"
"sphx glr auto examples model selection plot cost sensitive learning py";"CODE"
"performance improvements in pca";"TASK"
"class decomposition pca has a new solver covariance eigh which is";"CODE"
"up to an order of magnitude faster and more memory efficient than the other";"IRRE"
"solvers for datasets with many data points and few features";"CODE"
"the new solver also accepts sparse input data";"CODE"
"the full solver has also been improved to use less memory and allows";"-"
"faster transformation the default svd solver auto option takes";"CODE"
"advantage of the new solver and is now able to select an appropriate solver";"CODE"
"for sparse datasets";"IRRE"
"similarly to most other pca solvers the new covariance eigh solver can leverage";"CODE"
"gpu computation if the input data is passed as a pytorch or cupy array by";"CODE"
"enabling the experimental support for ref array api array api";"CODE"
"columntransformer is subscriptable";"CODE"
"the transformers of a class compose columntransformer can now be directly";"CODE"
"accessed using indexing by name";"-"
"custom imputation strategies for the simpleimputer";"CODE"
"class impute simpleimputer now supports custom strategies for imputation";"CODE"
"using a callable that computes a scalar value from the non missing values of";"IRRE"
"a column vector";"-"
"ruff noqa cpy001 e501";"-"
"frozenestimator freezing an estimator";"-"
"this meta estimator allows you to take an estimator and freeze its fit method meaning";"IRRE"
"that calling fit does not perform any operations also fit predict and";"CODE"
"fit transform call predict and transform respectively without calling fit the";"IRRE"
"original estimator s other methods and properties are left unchanged an interesting";"CODE"
"use case for this is to use a pre fitted model as a transformer step in a pipeline";"CODE"
"or to pass a pre fitted model to some of the meta estimators here s a short example";"-"
"fitting the threshold classifier skipped fitting the inner sgdclassifier for more";"CODE"
"details refer to the example ref sphx glr auto examples frozen plot frozen examples py";"-"
"transforming data other than x in a pipeline";"CODE"
"the class pipeline pipeline now supports transforming passed data other than x";"CODE"
"if necessary this can be done by setting the new transform input parameter this";"CODE"
"is particularly useful when passing a validation set through the pipeline";"IRRE"
"as an example imagine estimatorwithvalidationset is an estimator which accepts";"IRRE"
"a validation set we can now have a pipeline which will transform the validation set";"CODE"
"and pass it to the estimator";"-"
"with sklearn config context enable metadata routing true";"-"
"est gs gridsearchcv";"-"
"pipeline";"CODE"
"standardscaler";"-"
"estimatorwithvalidationset set fit request x val true y val true";"IRRE"
"telling pipeline to transform these inputs up to the step which is";"CODE"
"requesting them";"CODE"
"transform input x val";"CODE"
"param grid estimatorwithvalidationset param to optimize list range 5";"IRRE"
"cv 5";"-"
"fit x y x val x val y val y val";"-"
"in the above code the key parts are the call to set fit request to specify that";"IRRE"
"x val and y val are required by the estimatorwithvalidationset fit method and";"IRRE"
"the transform input parameter to tell the pipeline to transform x val before";"CODE"
"passing it to estimatorwithvalidationset fit";"IRRE"
"note that at this time scikit learn estimators have not yet been extended to accept";"TASK"
"user specified validation sets this feature is released early to collect feedback";"TASK"
"from third party libraries who might benefit from it";"CODE"
"multiclass support for logisticregression solver newton cholesky";"CODE"
"the newton cholesky solver originally introduced in scikit learn version";"CODE"
"1 2 was previously limited to binary";"-"
"class linear model logisticregression and some other generalized linear";"IRRE"
"regression estimators namely class linear model poissonregressor";"IRRE"
"class linear model gammaregressor and";"IRRE"
"class linear model tweedieregressor";"IRRE"
"this new release includes support for multiclass multinomial";"CODE"
"class linear model logisticregression";"IRRE"
"this solver is particularly useful when the number of features is small to";"TASK"
"medium it has been empirically shown to converge more reliably and faster";"IRRE"
"than other solvers on some medium sized datasets with one hot encoded";"IRRE"
"categorical features as can be seen in the benchmark results of the";"TASK"
"pull request";"CODE"
"https github com scikit learn scikit learn pull 28840 issuecomment 2065368727";"CODE"
"missing value support for extra trees";"IRRE"
"the classes class ensemble extratreesclassifier and";"IRRE"
"class ensemble extratreesregressor now support missing values more details in the";"IRRE"
"ref user guide tree missing value support";"IRRE"
"download any dataset from the web";"CODE"
"the function func datasets fetch file allows downloading a file from any given url";"CODE"
"this convenience function provides built in local disk caching sha256 digest";"CODE"
"integrity check and an automated retry mechanism on network error";"CODE"
"the goal is to provide the same convenience and reliability as dataset fetchers while";"IRRE"
"giving the flexibility to work with data from arbitrary online sources and file";"CODE"
"formats";"CODE"
"the downloaded file can then be loaded with generic or domain specific functions such";"CODE"
"as pandas read csv pandas read parquet etc";"CODE"
"array api support";"CODE"
"many more estimators and functions have been updated to support array api compatible";"CODE"
"inputs since version 1 5 in particular the meta estimators for hyperparameter tuning";"CODE"
"from the mod sklearn model selection module and the metrics from the";"CODE"
"mod sklearn metrics module";"CODE"
"please refer to the ref array api support array api page for instructions to use";"CODE"
"scikit learn with array api compatible libraries such as pytorch or cupy";"CODE"
"almost complete metadata routing support";"CODE"
"support for routing metadata has been added to all remaining estimators and";"CODE"
"functions except adaboost see ref metadata routing user guide metadata routing";"CODE"
"for more details";"CODE"
"free threaded cpython 3 13 support";"CODE"
"scikit learn has preliminary support for free threaded cpython in particular";"CODE"
"free threaded wheels are available for all of our supported platforms";"CODE"
"free threaded also known as nogil cpython 3 13 is an experimental version of";"CODE"
"cpython 3 13 which aims at enabling efficient multi threaded use cases by";"CODE"
"removing the global interpreter lock gil";"CODE"
"for more details about free threaded cpython see py free threading doc https py free threading github io";"CODE"
"in particular how to install a free threaded cpython https py free threading github io installing cpython";"CODE"
"and ecosystem compatibility tracking https py free threading github io tracking";"CODE"
"feel free to try free threaded cpython on your use case and report any issues";"CODE"
"improvements to the developer api for third party libraries";"CODE"
"we have been working on improving the developer api for third party libraries";"CODE"
"this is still a work in progress but a fair amount of work has been done in this";"CODE"
"release this release includes";"CODE"
"func sklearn utils validation validate data is introduced and replaces the";"CODE"
"previously private baseestimator validate data method this function extends";"CODE"
"func sklearn utils validation check array and adds support for remembering";"TASK"
"input feature counts and names";"TASK"
"estimator tags are now revamped and a part of the public api via";"CODE"
"class sklearn utils tags estimators should now override the";"CODE"
"meth baseestimator sklearn tags method instead of implementing a more tags";"TASK"
"method if you d like to support multiple scikit learn versions you can implement";"TASK"
"both methods in your class";"IRRE"
"as a consequence of developing a public tag api we ve removed the xfail checks";"CODE"
"tag and tests which are expected to fail are directly passed to";"IRRE"
"func sklearn utils estimator checks check estimator and";"IRRE"
"func sklearn utils estimator checks parametrize with checks see their";"-"
"corresponding api docs for more details";"CODE"
"many tests in the common test suite are updated and raise more helpful error";"CODE"
"messages we ve also added some new tests which should help you more easily fix";"TASK"
"potential issues with your estimators";"-"
"an updated version of our ref develop is also available which we recommend you";"CODE"
"check out";"-"
"ruff noqa cpy001";"-"
"improved estimator s html representation";"-"
"the html representation of estimators now includes a section containing the list of";"CODE"
"parameters and their values non default parameters are highlighted in orange a copy";"IRRE"
"button is also available to copy the fully qualified parameter name without the";"IRRE"
"need to call the get params method it is particularly useful when defining a";"CODE"
"parameter grid for a grid search or a randomized search with a complex pipeline";"IRRE"
"see the example below and click on the different estimator s blocks to see the";"IRRE"
"improved html representation";"-"
"custom validation set for histogram based gradient boosting estimators";"IRRE"
"the class ensemble histgradientboostingclassifier and";"IRRE"
"class ensemble histgradientboostingregressor now support directly passing a custom";"IRRE"
"validation set for early stopping to the fit method using the x val y val and";"IRRE"
"sample weight val parameters";"IRRE"
"in a class pipeline pipeline the validation set x val can be transformed along";"CODE"
"with x using the transform input parameter";"CODE"
"plotting roc curves from cross validation results";"IRRE"
"the class class metrics roccurvedisplay has a new class method from cv results";"CODE"
"that allows to easily plot multiple roc curves from the results of";"IRRE"
"func model selection cross validate";"CODE"
"array api support";"CODE"
"several functions have been updated to support array api compatible inputs since";"CODE"
"version 1 6 especially metrics from the mod sklearn metrics module";"CODE"
"in addition it is no longer required to install the array api compat package to use";"CODE"
"the experimental array api support in scikit learn";"CODE"
"please refer to the ref array api support array api page for instructions to use";"CODE"
"scikit learn with array api compatible libraries such as pytorch or cupy";"CODE"
"improved api consistency of multi layer perceptron";"CODE"
"the class neural network mlpregressor has a new parameter loss and now supports";"IRRE"
"the poisson loss in addition to the default squared error loss";"TASK"
"moreover the class neural network mlpclassifier and";"IRRE"
"class neural network mlpregressor estimators now support sample weights";"IRRE"
"these improvements have been made to improve the consistency of these estimators";"TASK"
"with regard to the other estimators in scikit learn";"-"
"migration toward sparse arrays";"IRRE"
"in order to prepare scipy migration from sparse matrices to sparse arrays https docs scipy org doc scipy reference sparse migration to sparray html";"CODE"
"all scikit learn estimators that accept sparse matrices as input now also accept";"IRRE"
"sparse arrays";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation";"-"
"we use the digits dataset we only use a subset of randomly selected samples";"IRRE"
"we selected 340 samples of which only 40 will be associated with a known label";"CODE"
"therefore we store the indices of the 300 other samples for which we are not";"CODE"
"supposed to know their labels";"-"
"shuffle everything around";"-"
"semi supervised learning";"CODE"
"we fit a class sklearn semi supervised labelspreading and use it to predict";"CODE"
"the unknown labels";"-"
"classification report";"IRRE"
"confusion matrix";"-"
"plot the most uncertain predictions";"META"
"here we will pick and show the 10 most uncertain predictions";"META"
"pick the top 10 most uncertain labels";"META"
"plot";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"compute the entropies of transduced label distributions";"META"
"select up to 5 digit examples that the classifier is most uncertain about";"CODE"
"keep track of indices that we get labels for";"CODE"
"for more than 5 iterations visualize the gain only on the first 5";"CODE"
"for more than 5 iterations visualize the gain only on the first 5";"CODE"
"labeling 5 points remote from labeled set";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we generate a dataset with two concentric circles in addition a label";"TASK"
"is associated with each sample of the dataset that is 0 belonging to";"IRRE"
"the outer circle 1 belonging to the inner circle and 1 unknown";"TASK"
"here all labels but two are tagged as unknown";"META"
"plot raw data";"-"
"the aim of class sklearn semi supervised labelspreading is to associate";"CODE"
"a label to sample where the label is initially unknown";"IRRE"
"now we can check which labels have been associated with each sample";"-"
"when the label was unknown";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we need manual cross validation so that we don t treat 1 as a separate";"CODE"
"class when computing accuracy";"IRRE"
"the amount of labeled samples that at the end of fitting";"CODE"
"the last iteration the classifier labeled a sample in";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"loading dataset containing first five categories";"IRRE"
"parameters";"IRRE"
"supervised pipeline";"CODE"
"selftraining pipeline";"CODE"
"labelspreading pipeline";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"y 10 y rand 0 1 1 set random samples to be unlabeled";"IRRE"
"we observe that the decision boundaries are already quite similar to those";"CODE"
"using the full labeled data available for training even when using a very";"CODE"
"small subset of the labels";"IRRE"
"interpretation of predict proba";"CODE"
"predict proba in labelspreading";"CODE"
"class sklearn semi supervised labelspreading constructs a similarity graph";"CODE"
"from the data by default using an rbf kernel this means each sample is";"CODE"
"connected to every other with a weight that decays with their squared";"-"
"euclidean distance scaled by a parameter gamma";"IRRE"
"once we have that weighted graph labels are propagated along the graph";"-"
"edges each sample gradually takes on a soft label distribution that reflects";"META"
"a weighted average of the labels of its neighbors until the process converges";"-"
"these per sample distributions are stored in label distributions";"META"
"predict proba computes the class probabilities for a new point by taking a";"CODE"
"weighted average of the rows in label distributions where the weights come";"META"
"from the rbf kernel similarities between the new point and the training";"CODE"
"samples the averaged values are then renormalized so that they sum to one";"IRRE"
"just keep in mind that these probabilities are graph based scores not";"-"
"calibrated posteriors don t over interpret their absolute values";"IRRE"
"ls ls100 0 fitted labelspreading instance";"CODE"
"x query np array 3 5 1 5 point in the soft blue region";"CODE"
"step 1 similarities between query and all training samples";"CODE"
"w rbf kernel x query x gamma ls gamma gamma 20 by default";"CODE"
"step 2 weighted average of label distributions";"META"
"step 3 normalize to sum to 1";"-"
"predict proba in selftrainingclassifier";"CODE"
"class sklearn semi supervised selftrainingclassifier works by repeatedly";"CODE"
"fitting its base estimator on the currently labeled data then adding";"TASK"
"pseudo labels for unlabeled points whose predicted probabilities exceed a";"CODE"
"confidence threshold this process continues until no new points can be";"CODE"
"labeled at which point the classifier has a final fitted base estimator";"CODE"
"stored in the attribute estimator";"META"
"when you call predict proba on the selftrainingclassifier it simply";"IRRE"
"delegates to this final estimator";"CODE"
"in both methods semi supervised learning can be understood as constructing a";"CODE"
"categorical distribution over classes for each sample";"CODE"
"class sklearn semi supervised labelspreading keeps these distributions soft and";"CODE"
"updates them through graph based propagation";"IRRE"
"predictions including predict proba remain tied to the training set which";"IRRE"
"must be stored for inference";"TASK"
"class sklearn semi supervised selftrainingclassifier instead uses these";"CODE"
"distributions internally to decide which unlabeled points to assign pseudo labels";"CODE"
"during training but at prediction time the returned probabilities come directly from";"CODE"
"the final fitted estimator and therefore the decision rule does not require storing";"CODE"
"the training data";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"import some data to play with";"CODE"
"x iris data 2 we only take the first two features we could";"TASK"
"avoid this ugly slicing by using a two dim dataset";"CODE"
"h 0 02 step size in the mesh";"CODE"
"we create an instance of svm and fit out data";"IRRE"
"plot also the training points";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"import some data to play with";"CODE"
"take the first two features we could avoid this by using a two dim dataset";"CODE"
"we create an instance of svm and fit out data we do not scale our";"IRRE"
"data since we want to plot the support vectors";"-"
"c 1 0 svm regularization parameter";"IRRE"
"title for the plots";"CODE"
"set up 2x2 grid for plotting";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"hinge is the standard svm loss";"-"
"obtain the support vectors through the decision function";"CODE"
"we can also calculate the decision function manually";"CODE"
"decision function np dot x clf coef 0 clf intercept 0";"CODE"
"the support vectors are the samples that lie within the margin";"CODE"
"boundaries whose size is conventionally constrained to 1";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate train data";"-"
"generate some regular novel observations";"-"
"generate some abnormal novel observations";"-"
"fit the model";"-"
"generate grid for the boundary display";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"utility class to move the midpoint of a colormap to be around";"CODE"
"the values of interest";"IRRE"
"load and prepare data set";"IRRE"
"dataset for grid search";"IRRE"
"dataset for decision function visualization we only keep the first two";"CODE"
"features in x and sub sample the dataset to keep only 2 classes and";"IRRE"
"make it a binary classification problem";"IRRE"
"it is usually a good idea to scale the data for svm training";"CODE"
"we are cheating a bit in this example in scaling all of the data";"CODE"
"instead of fitting the transformation on the training set and";"CODE"
"just applying it on the test set";"IRRE"
"train classifiers";"IRRE"
"for an initial search a logarithmic grid with basis";"IRRE"
"10 is often helpful using a basis of 2 a finer";"-"
"tuning can be achieved but at a much higher cost";"META"
"now we need to fit a classifier for all parameters in the 2d version";"CODE"
"we use a smaller set of parameters here because it takes a while to train";"IRRE"
"visualization";"-"
"draw visualization of parameter effects";"IRRE"
"evaluate decision function in a grid";"CODE"
"visualize decision function for these parameters";"CODE"
"visualize parameter s effect on decision function";"IRRE"
"draw heatmap of the validation accuracy as a function of gamma and c";"CODE"
"the score are encoded as colors with the hot colormap which varies from dark";"CODE"
"red to bright yellow as the most interesting scores are all located in the";"CODE"
"0 92 to 0 97 range we use a custom normalizer to set the mid point to 0 92 so";"IRRE"
"as to make it easier to visualize the small variations of score values in the";"IRRE"
"interesting range while not brutally collapsing all the low score values to";"IRRE"
"the same color";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we create 40 separable points";"IRRE"
"fit the model don t regularize for illustration purposes";"CODE"
"plot the decision function";"CODE"
"plot support vectors";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we create two clusters of random points";"IRRE"
"fit the model and get the separating hyperplane";"-"
"fit the model and get the separating hyperplane using weighted classes";"IRRE"
"plot the samples";"-"
"plot the decision functions for both classifiers";"CODE"
"plot decision boundary and margins for weighted classes";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load some data to play with";"CODE"
"add non informative features";"TASK"
"create the pipeline";"IRRE"
"create a feature selection transform a scaler and an instance of svm that we";"IRRE"
"combine together to have a full blown estimator";"-"
"plot the cross validation score as a function of percentile of features";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"creating a dataset";"IRRE"
"we create a two dimensional classification dataset with 16 samples and two classes we";"IRRE"
"plot the samples with the colors matching their respective targets";"-"
"plotting settings";"IRRE"
"plot samples by color and add legend";"TASK"
"we can see that the samples are not clearly separable by a straight line";"META"
"training svc model and plotting decision boundaries";"-"
"we define a function that fits a class sklearn svm svc classifier";"CODE"
"allowing the kernel parameter as an input and then plots the decision";"IRRE"
"boundaries learned by the model using";"-"
"class sklearn inspection decisionboundarydisplay";"IRRE"
"notice that for the sake of simplicity the c parameter is set to its";"IRRE"
"default value c 1 in this example and the gamma parameter is set to";"CODE"
"gamma 2 across all kernels although it is automatically ignored for the";"IRRE"
"linear kernel in a real classification task where performance matters";"CODE"
"parameter tuning by using class sklearn model selection gridsearchcv for";"CODE"
"instance is highly recommended to capture different structures within the";"CODE"
"data";"-"
"setting response method predict in";"IRRE"
"class sklearn inspection decisionboundarydisplay colors the areas based";"IRRE"
"on their predicted class using response method decision function allows";"CODE"
"us to also plot the decision boundary and the margins to both sides of it";"-"
"finally the support vectors used during training which always lay on the";"CODE"
"margins are identified by means of the support vectors attribute of";"META"
"the trained svcs and plotted as well";"-"
"train the svc";"CODE"
"settings for plotting";"IRRE"
"plot decision boundary and margins";"-"
"plot bigger circles around samples that serve as support vectors";"-"
"plot samples by color and add legend";"TASK"
"linear kernel";"-"
"linear kernel is the dot product of the input samples";"CODE"
"math k mathbf x 1 mathbf x 2 mathbf x 1 top mathbf x 2";"-"
"it is then applied to any combination of two data points samples in the";"CODE"
"dataset the dot product of the two points determines the";"CODE"
"func sklearn metrics pairwise cosine similarity between both points the";"CODE"
"higher the value the more similar the points are";"IRRE"
"training a class sklearn svm svc on a linear kernel results in an";"IRRE"
"untransformed feature space where the hyperplane and the margins are";"TASK"
"straight lines due to the lack of expressivity of the linear kernel the";"-"
"trained classes do not perfectly capture the training data";"CODE"
"polynomial kernel";"-"
"the polynomial kernel changes the notion of similarity the kernel function";"CODE"
"is defined as";"CODE"
"math";"-"
"k mathbf x 1 mathbf x 2 gamma cdot";"CODE"
"mathbf x 1 top mathbf x 2 r d";"-"
"where math d is the degree degree of the polynomial math gamma";"-"
"gamma controls the influence of each individual training sample on the";"-"
"decision boundary and math r is the bias term coef0 that shifts the";"-"
"data up or down here we use the default value for the degree of the";"CODE"
"polynomial in the kernel function degree 3 when coef0 0 the default";"CODE"
"the data is only transformed but no additional dimension is added using a";"TASK"
"polynomial kernel is equivalent to creating";"-"
"class sklearn preprocessing polynomialfeatures and then fitting a";"TASK"
"class sklearn svm svc with a linear kernel on the transformed data";"CODE"
"although this alternative approach would be computationally expensive for most";"CODE"
"datasets";"IRRE"
"the polynomial kernel with gamma 2 adapts well to the training data";"-"
"causing the margins on both sides of the hyperplane to bend accordingly";"CODE"
"rbf kernel";"-"
"the radial basis function rbf kernel also known as the gaussian kernel is";"CODE"
"the default kernel for support vector machines in scikit learn it measures";"CODE"
"similarity between two data points in infinite dimensions and then approaches";"IRRE"
"classification by majority vote the kernel function is defined as";"CODE"
"math";"-"
"k mathbf x 1 mathbf x 2 exp left gamma cdot";"CODE"
"mathbf x 1 mathbf x 2 2 right";"-"
"where math gamma gamma controls the influence of each individual";"-"
"training sample on the decision boundary";"-"
"the larger the euclidean distance between two points";"CODE"
"math mathbf x 1 mathbf x 2 2";"-"
"the closer the kernel function is to zero this means that two points far away";"CODE"
"are more likely to be dissimilar";"-"
"in the plot we can see how the decision boundaries tend to contract around";"CODE"
"data points that are close to each other";"CODE"
"sigmoid kernel";"-"
"the sigmoid kernel function is defined as";"CODE"
"math";"-"
"k mathbf x 1 mathbf x 2 tanh gamma cdot";"CODE"
"mathbf x 1 top mathbf x 2 r";"-"
"where the kernel coefficient math gamma gamma controls the influence";"-"
"of each individual training sample on the decision boundary and math r is";"-"
"the bias term coef0 that shifts the data up or down";"CODE"
"in the sigmoid kernel the similarity between two data points is computed";"CODE"
"using the hyperbolic tangent function math tanh the kernel function";"CODE"
"scales and possibly shifts the dot product of the two points";"CODE"
"math mathbf x 1 and math mathbf x 2";"-"
"we can see that the decision boundaries obtained with the sigmoid kernel";"-"
"appear curved and irregular the decision boundary tries to separate the";"-"
"classes by fitting a sigmoid shaped curve resulting in a complex boundary";"IRRE"
"that may not generalize well to unseen data from this example it becomes";"CODE"
"obvious that the sigmoid kernel has very specific use cases when dealing";"CODE"
"with data that exhibits a sigmoidal shape in this example careful fine";"CODE"
"tuning might find more generalizable decision boundaries because of its";"-"
"specificity the sigmoid kernel is less commonly used in practice compared to";"IRRE"
"other kernels";"-"
"conclusion";"-"
"in this example we have visualized the decision boundaries trained with the";"CODE"
"provided dataset the plots serve as an intuitive demonstration of how";"IRRE"
"different kernels utilize the training data to determine the classification";"IRRE"
"boundaries";"-"
"the hyperplanes and margins although computed indirectly can be imagined as";"-"
"planes in the transformed feature space however in the plots they are";"CODE"
"represented relative to the original feature space resulting in curved";"TASK"
"decision boundaries for the polynomial rbf and sigmoid kernels";"CODE"
"please note that the plots do not evaluate the individual kernel s accuracy or";"TASK"
"quality they are intended to provide a visual understanding of how the";"CODE"
"different kernels use the training data";"IRRE"
"for a comprehensive evaluation fine tuning of class sklearn svm svc";"CODE"
"parameters using techniques such as";"IRRE"
"class sklearn model selection gridsearchcv is recommended to capture the";"CODE"
"underlying structures within the data";"CODE"
"xor dataset";"IRRE"
"a classical example of a dataset which is not linearly separable is the xor";"IRRE"
"pattern here we demonstrate how different kernels work on such a dataset";"IRRE"
"as you can see from the plots above only the rbf kernel can find a";"CODE"
"reasonable decision boundary for the above dataset";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we create 40 separable points";"IRRE"
"figure number";"-"
"fit the model";"-"
"get the separating hyperplane";"-"
"plot the parallels to the separating hyperplane that pass through the";"-"
"support vectors margin away from hyperplane in direction";"CODE"
"perpendicular to hyperplane this is sqrt 1 a 2 away vertically in";"CODE"
"2 d";"-"
"plot the line the points and the nearest vectors to the plane";"CODE"
"put the result into a contour plot";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate sample data";"-"
"add noise to targets";"TASK"
"fit regression model";"-"
"look at the results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"data generation";"-"
"in this example we investigate the effect of reparametrizing the regularization";"CODE"
"parameter c to account for the number of samples when using either l1 or l2";"IRRE"
"penalty for such purpose we create a synthetic dataset with a large number of";"IRRE"
"features out of which only a few are informative we therefore expect the";"CODE"
"regularization to shrink the coefficients towards zero l2 penalty or exactly";"-"
"zero l1 penalty";"-"
"l1 penalty case";"CODE"
"in the l1 case theory says that provided a strong regularization the";"CODE"
"estimator cannot predict as well as a model knowing the true distribution";"META"
"even in the limit where the sample size grows to infinity as it may set some";"IRRE"
"weights of otherwise predictive features to zero which induces a bias it does";"TASK"
"say however that it is possible to find the right set of non zero parameters";"IRRE"
"as well as their signs by tuning c";"-"
"we define a linear svc with the l1 penalty";"CODE"
"we compute the mean test score for different values of c via";"IRRE"
"cross validation";"-"
"plot results without scaling c";"IRRE"
"plot results by scaling c";"IRRE"
"in the region of small c strong regularization all the coefficients";"CODE"
"learned by the models are zero leading to severe underfitting indeed the";"-"
"accuracy in this region is at the chance level";"CODE"
"using the default scale results in a somewhat stable optimal value of c";"IRRE"
"whereas the transition out of the underfitting region depends on the number of";"CODE"
"training samples the reparametrization leads to even more stable results";"IRRE"
"see e g theorem 3 of arxiv on the prediction performance of the lasso";"CODE"
"1402 1700 or arxiv simultaneous analysis of lasso and dantzig selector";"CODE"
"0801 1095 where the regularization parameter is always assumed to be";"IRRE"
"proportional to 1 sqrt n samples";"-"
"l2 penalty case";"CODE"
"we can do a similar experiment with the l2 penalty in this case the";"CODE"
"theory says that in order to achieve prediction consistency the penalty";"-"
"parameter should be kept constant as the number of samples grow";"IRRE"
"plot results without scaling c";"IRRE"
"plot results by scaling c";"IRRE"
"for the l2 penalty case the reparametrization seems to have a smaller impact";"CODE"
"on the stability of the optimal value for the regularization the transition";"IRRE"
"out of the overfitting region occurs in a more spread range and the accuracy";"CODE"
"does not seem to be degraded up to chance level";"CODE"
"try increasing the value to n splits 1 000 for better results in the l2";"CODE"
"case which is not shown here due to the limitations on the documentation";"CODE"
"builder";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"down sample for plotting";"CODE"
"we define constant weights as expected by the plotting function";"CODE"
"assign random weights to all points";"IRRE"
"assign bigger weights to the positive class";"IRRE"
"this model does not include sample weights";"CODE"
"this other model includes sample weights";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"loading and vectorizing the 20 newsgroups text dataset";"CODE"
"we define a function to load data from ref 20newsgroups dataset which";"CODE"
"comprises around 18 000 newsgroups posts on 20 topics split in two subsets";"CODE"
"one for training or development and the other one for testing or for";"CODE"
"performance evaluation note that by default the text samples contain some";"CODE"
"message metadata such as headers footers signatures and quotes";"IRRE"
"to other posts the fetch 20newsgroups function therefore accepts a";"CODE"
"parameter named remove to attempt stripping such information that can make";"IRRE"
"the classification problem too easy this is achieved using simple";"CODE"
"heuristics that are neither perfect nor standard hence disabled by default";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"loading text data";"CODE"
"we load data from ref 20newsgroups dataset which comprises around 18 000";"CODE"
"newsgroups posts on 20 topics for illustrative purposes and to reduce the";"CODE"
"computational cost we select a subset of 4 topics only accounting for around";"CODE"
"3 400 documents see the example";"CODE"
"ref sphx glr auto examples text plot document classification 20newsgroups py";"CODE"
"to gain intuition on the overlap of such topics";"CODE"
"notice that by default the text samples contain some message metadata such";"CODE"
"as headers footers signatures and quotes to other posts we use";"IRRE"
"the remove parameter from func sklearn datasets fetch 20newsgroups to";"IRRE"
"strip those features and have a more sensible clustering problem";"TASK"
"quantifying the quality of clustering results";"IRRE"
"in this section we define a function to score different clustering pipelines";"CODE"
"using several metrics";"-"
"clustering algorithms are fundamentally unsupervised learning methods";"CODE"
"however since we happen to have class labels for this specific dataset it is";"CODE"
"possible to use evaluation metrics that leverage this supervised ground";"CODE"
"truth information to quantify the quality of the resulting clusters examples";"IRRE"
"of such metrics are the following";"-"
"homogeneity which quantifies how much clusters contain only members of a";"-"
"single class";"IRRE"
"completeness which quantifies how much members of a given class are";"CODE"
"assigned to the same clusters";"IRRE"
"v measure the harmonic mean of completeness and homogeneity";"CODE"
"rand index which measures how frequently pairs of data points are grouped";"IRRE"
"consistently according to the result of the clustering algorithm and the";"IRRE"
"ground truth class assignment";"IRRE"
"adjusted rand index a chance adjusted rand index such that random cluster";"IRRE"
"assignment have an ari of 0 0 in expectation";"IRRE"
"if the ground truth labels are not known evaluation can only be performed";"CODE"
"using the model results itself in that case the silhouette coefficient comes in";"CODE"
"handy see ref sphx glr auto examples cluster plot kmeans silhouette analysis py";"-"
"for an example on how to do it";"CODE"
"for more reference see ref clustering evaluation";"CODE"
"k means clustering on text features";"TASK"
"two feature extraction methods are used in this example";"CODE"
"class sklearn feature extraction text tfidfvectorizer uses an in memory";"TASK"
"vocabulary a python dict to map the most frequent words to features";"TASK"
"indices and hence compute a word occurrence frequency sparse matrix the";"IRRE"
"word frequencies are then reweighted using the inverse document frequency";"CODE"
"idf vector collected feature wise over the corpus";"TASK"
"class sklearn feature extraction text hashingvectorizer hashes word";"TASK"
"occurrences to a fixed dimensional space possibly with collisions the word";"-"
"count vectors are then normalized to each have l2 norm equal to one";"-"
"projected to the euclidean unit sphere which seems to be important for";"CODE"
"k means to work in high dimensional space";"-"
"furthermore it is possible to post process those extracted features using";"TASK"
"dimensionality reduction we will explore the impact of those choices on the";"CODE"
"clustering quality in the following";"CODE"
"feature extraction using tfidfvectorizer";"TASK"
"we first benchmark the estimators using a dictionary vectorizer along with an";"-"
"idf normalization as provided by";"-"
"class sklearn feature extraction text tfidfvectorizer";"TASK"
"after ignoring terms that appear in more than 50 of the documents as set by";"IRRE"
"max df 0 5 and terms that are not present in at least 5 documents set by";"CODE"
"min df 5 the resulting number of unique terms n features is around";"TASK"
"8 000 we can additionally quantify the sparsity of the x tfidf matrix as";"TASK"
"the fraction of non zero entries divided by the total number of elements";"-"
"we find that around 0 7 of the entries of the x tfidf matrix are non zero";"CODE"
"kmeans sparse high dim";"IRRE"
"clustering sparse data with k means";"IRRE"
"as both class sklearn cluster kmeans and";"IRRE"
"class sklearn cluster minibatchkmeans optimize a non convex objective";"IRRE"
"function their clustering is not guaranteed to be optimal for a given random";"CODE"
"init even further on sparse high dimensional data such as text vectorized";"IRRE"
"using the bag of words approach k means can initialize centroids on extremely";"IRRE"
"isolated data points those data points can stay their own centroids all";"CODE"
"along";"-"
"the following code illustrates how the previous phenomenon can sometimes lead";"-"
"to highly imbalanced clusters depending on the random initialization";"IRRE"
"to avoid this problem one possibility is to increase the number of runs with";"CODE"
"independent random initiations n init in such case the clustering with the";"IRRE"
"best inertia objective function of k means is chosen";"CODE"
"all those clustering evaluation metrics have a maximum value of 1 0 for a";"IRRE"
"perfect clustering result higher values are better values of the adjusted";"IRRE"
"rand index close to 0 0 correspond to a random labeling notice from the";"IRRE"
"scores above that the cluster assignment is indeed well above chance level";"IRRE"
"but the overall quality can certainly improve";"META"
"keep in mind that the class labels may not reflect accurately the document";"CODE"
"topics and therefore metrics that use labels are not necessarily the best to";"CODE"
"evaluate the quality of our clustering pipeline";"CODE"
"performing dimensionality reduction using lsa";"CODE"
"a n init 1 can still be used as long as the dimension of the vectorized";"TASK"
"space is reduced first to make k means more stable for such purpose we use";"CODE"
"class sklearn decomposition truncatedsvd which works on term count tf idf";"CODE"
"matrices since svd results are not normalized we redo the normalization to";"IRRE"
"improve the class sklearn cluster kmeans result using svd to reduce the";"IRRE"
"dimensionality of tf idf document vectors is often known as latent semantic";"CODE"
"analysis https en wikipedia org wiki latent semantic analysis lsa in";"CODE"
"the information retrieval and text mining literature";"CODE"
"using a single initialization means the processing time will be reduced for";"IRRE"
"both class sklearn cluster kmeans and";"IRRE"
"class sklearn cluster minibatchkmeans";"IRRE"
"we can observe that clustering on the lsa representation of the document is";"CODE"
"significantly faster both because of n init 1 and because the";"IRRE"
"dimensionality of the lsa feature space is much smaller furthermore all the";"TASK"
"clustering evaluation metrics have improved we repeat the experiment with";"-"
"class sklearn cluster minibatchkmeans";"IRRE"
"top terms per cluster";"-"
"since class sklearn feature extraction text tfidfvectorizer can be";"TASK"
"inverted we can identify the cluster centers which provide an intuition of";"CODE"
"the most influential words for each cluster see the example script";"CODE"
"ref sphx glr auto examples text plot document classification 20newsgroups py";"CODE"
"for a comparison with the most predictive words for each target class";"CODE"
"hashingvectorizer";"-"
"an alternative vectorization can be done using a";"CODE"
"class sklearn feature extraction text hashingvectorizer instance which";"TASK"
"does not provide idf weighting as this is a stateless model the fit method";"CODE"
"does nothing when idf weighting is needed it can be added by pipelining the";"CODE"
"class sklearn feature extraction text hashingvectorizer output to a";"IRRE"
"class sklearn feature extraction text tfidftransformer instance in this";"CODE"
"case we also add lsa to the pipeline to reduce the dimension and sparcity of";"CODE"
"the hashed vector space";"-"
"one can observe that the lsa step takes a relatively long time to fit";"-"
"especially with hashed vectors the reason is that a hashed space is typically";"IRRE"
"large set to n features 50 000 in this example one can try lowering the";"CODE"
"number of features at the expense of having a larger fraction of features with";"TASK"
"hash collisions as shown in the example notebook";"TASK"
"ref sphx glr auto examples text plot hashing vs dict vectorizer py";"-"
"we now fit and evaluate the kmeans and minibatch kmeans instances on this";"CODE"
"hashed lsa reduced data";"-"
"both methods lead to good results that are similar to running the same models";"IRRE"
"on the traditional lsa vectors without hashing";"CODE"
"clustering evaluation summary";"-"
"class sklearn cluster kmeans and class sklearn cluster minibatchkmeans";"IRRE"
"suffer from the phenomenon called the curse of dimensionality";"IRRE"
"https en wikipedia org wiki curse of dimensionality for high dimensional";"CODE"
"datasets such as text data that is the reason why the overall scores improve";"IRRE"
"when using lsa using lsa reduced data also improves the stability and";"-"
"requires lower clustering time though keep in mind that the lsa step itself";"CODE"
"takes a long time especially with hashed vectors";"-"
"the silhouette coefficient is defined between 0 and 1 in all cases we obtain";"CODE"
"values close to 0 even if they improve a bit after using lsa because its";"IRRE"
"definition requires measuring distances in contrast with other evaluation";"IRRE"
"metrics such as the v measure and the adjusted rand index which are only based";"IRRE"
"on cluster assignments rather than distances notice that strictly speaking";"IRRE"
"one should not compare the silhouette coefficient between spaces of different";"IRRE"
"dimension due to the different notions of distance they imply";"-"
"the homogeneity completeness and hence v measure metrics do not yield a";"CODE"
"baseline with regards to random labeling this means that depending on the";"CODE"
"number of samples clusters and ground truth classes a completely random";"IRRE"
"labeling will not always yield the same values in particular random labeling";"IRRE"
"won t yield zero scores especially when the number of clusters is large this";"CODE"
"problem can safely be ignored when the number of samples is more than a";"-"
"thousand and the number of clusters is less than 10 which is the case of the";"CODE"
"present example for smaller sample sizes or larger number of clusters it is";"CODE"
"safer to use an adjusted index such as the adjusted rand index ari see the";"IRRE"
"example";"-"
"ref sphx glr auto examples cluster plot adjusted for chance measures py for";"CODE"
"a demo on the effect of random labeling";"IRRE"
"the size of the error bars show that class sklearn cluster minibatchkmeans";"IRRE"
"is less stable than class sklearn cluster kmeans for this relatively small";"CODE"
"dataset it is more interesting to use when the number of samples is much";"IRRE"
"bigger but it can come at the expense of a small degradation in clustering";"META"
"quality compared to the traditional k means algorithm";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load data";"TASK"
"we load data from ref 20newsgroups dataset which comprises around";"CODE"
"18000 newsgroups posts on 20 topics split in two subsets one for training and";"CODE"
"one for testing for the sake of simplicity and reducing the computational";"CODE"
"cost we select a subset of 7 topics and use the training set only";"IRRE"
"define preprocessing functions";"CODE"
"a token may be a word part of a word or anything comprised between spaces or";"CODE"
"symbols in a string here we define a function that extracts the tokens using";"CODE"
"a simple regular expression regex that matches unicode word characters this";"CODE"
"includes most characters that can be part of a word in any language as well";"CODE"
"as numbers and the underscore";"-"
"we define an additional function that counts the frequency of occurrence of";"CODE"
"each token in a given document it returns a frequency dictionary to be used";"CODE"
"by the vectorizers";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"total impurity of leaves vs effective alphas of pruned tree";"CODE"
"minimal cost complexity pruning recursively finds the node with the weakest";"META"
"link the weakest link is characterized by an effective alpha where the";"CODE"
"nodes with the smallest effective alpha are pruned first to get an idea of";"CODE"
"what values of ccp alpha could be appropriate scikit learn provides";"IRRE"
"func decisiontreeclassifier cost complexity pruning path that returns the";"CODE"
"effective alphas and the corresponding total leaf impurities at each step of";"-"
"the pruning process as alpha increases more of the tree is pruned which";"CODE"
"increases the total impurity of its leaves";"-"
"in the following plot the maximum effective alpha value is removed because";"IRRE"
"it is the trivial tree with only one node";"IRRE"
"next we train a decision tree using the effective alphas the last value";"IRRE"
"in ccp alphas is the alpha value that prunes the whole tree";"CODE"
"leaving the tree clfs 1 with one node";"-"
"for the remainder of this example we remove the last element in";"CODE"
"clfs and ccp alphas because it is the trivial tree with only one";"IRRE"
"node here we show that the number of nodes and tree depth decreases as alpha";"-"
"increases";"-"
"accuracy vs alpha for training and testing sets";"IRRE"
"when ccp alpha is set to zero and keeping the other default parameters";"IRRE"
"of class decisiontreeclassifier the tree overfits leading to";"IRRE"
"a 100 training accuracy and 88 testing accuracy as alpha increases more";"IRRE"
"of the tree is pruned thus creating a decision tree that generalizes better";"CODE"
"in this example setting ccp alpha 0 015 maximizes the testing accuracy";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"first load the copy of the iris dataset shipped with scikit learn";"IRRE"
"display the decision functions of trees trained on all pairs of features";"CODE"
"parameters";"IRRE"
"we only take the two corresponding features";"TASK"
"train";"-"
"plot the decision boundary";"-"
"plot the training points";"CODE"
"display the structure of a single decision tree trained on all the features";"CODE"
"together";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"decision tree on a 1d regression task";"TASK"
"here we fit a tree on a 1d regression task";"TASK"
"the ref decision trees tree is";"-"
"used to fit a sine curve with addition noisy observation as a result it";"TASK"
"learns local linear regressions approximating the sine curve";"IRRE"
"we can see that if the maximum depth of the tree controlled by the";"-"
"max depth parameter is set too high the decision trees learn too fine";"IRRE"
"details of the training data and learn from the noise i e they overfit";"CODE"
"create a random 1d dataset";"IRRE"
"fit regression model";"-"
"here we fit two models with different maximum depths";"-"
"predict";"-"
"get predictions on the test set";"IRRE"
"plot the results";"IRRE"
"as you can see the model with a depth of 5 yellow learns the details of the";"-"
"training data to the point that it overfits to the noise on the other hand";"CODE"
"the model with a depth of 2 blue learns the major tendencies in the data well";"CODE"
"and does not overfit in real use cases you need to make sure that the tree";"CODE"
"is not overfitting the training data which can be done using cross validation";"CODE"
"decision tree regression with multi output targets";"IRRE"
"here the ref decision trees tree";"-"
"is used to predict simultaneously the noisy x and y observations of a circle";"OUTD"
"given a single underlying feature as a result it learns local linear";"IRRE"
"regressions approximating the circle";"-"
"we can see that if the maximum depth of the tree controlled by the";"-"
"max depth parameter is set too high the decision trees learn too fine";"IRRE"
"details of the training data and learn from the noise i e they overfit";"CODE"
"create a random dataset";"IRRE"
"fit regression model";"-"
"predict";"-"
"get predictions on the test set";"IRRE"
"plot the results";"IRRE"
"as you can see the higher the value of max depth the more details of the data";"IRRE"
"are caught by the model however the model also overfits to the data and is";"-"
"influenced by the noise";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"train tree classifier";"CODE"
"first we fit a class sklearn tree decisiontreeclassifier using the";"IRRE"
"func sklearn datasets load iris dataset";"IRRE"
"tree structure";"CODE"
"the decision classifier has an attribute called tree which allows access";"IRRE"
"to low level attributes such as node count the total number of nodes";"META"
"and max depth the maximal depth of the tree the";"-"
"tree compute node depths method computes the depth of each node in the";"IRRE"
"tree tree also stores the entire binary tree structure represented as a";"CODE"
"number of parallel arrays the i th element of each array holds information";"CODE"
"about the node i node 0 is the tree s root some of the arrays only";"-"
"apply to either leaves or split nodes in this case the values of the nodes";"IRRE"
"of the other type is arbitrary for example the arrays feature and";"CODE"
"threshold only apply to split nodes the values for leaf nodes in these";"CODE"
"arrays are therefore arbitrary";"CODE"
"among these arrays we have";"-"
"children left i id of the left child of node i or 1 if leaf node";"CODE"
"children right i id of the right child of node i or 1 if leaf node";"CODE"
"feature i feature used for splitting node i";"TASK"
"threshold i threshold value at node i";"IRRE"
"n node samples i the number of training samples reaching node i";"-"
"impurity i the impurity at node i";"-"
"weighted n node samples i the weighted number of training samples";"-"
"reaching node i";"-"
"value i j k the summary of the training samples that reached node i for";"IRRE"
"output j and class k for regression tree class is set to 1 see below";"IRRE"
"for more information about value";"CODE"
"using the arrays we can traverse the tree structure to compute various";"CODE"
"properties below we will compute the depth of each node and whether or not";"-"
"it is a leaf";"-"
"tack 0 0 start with the root node id 0 and its depth 0";"-"
"pop ensures each node is only visited once";"CODE"
"if the left and right child of a node is not the same we have a split";"CODE"
"node";"-"
"if a split node append left and right children and depth to stack";"CODE"
"so we can loop through them";"IRRE"
"what is the values array used here";"IRRE"
"the tree value array is a 3d array of shape";"IRRE"
"n nodes n classes n outputs which provides the proportion of samples";"IRRE"
"reaching a node for each class and for each output";"CODE"
"each node has a value array which is the proportion of weighted samples reaching";"IRRE"
"this node for each output and class with respect to the parent node";"CODE"
"one could convert this to the absolute weighted number of samples reaching a node";"CODE"
"by multiplying this number by tree weighted n node samples node idx for the";"CODE"
"given node note sample weights are not used in this example so the weighted";"CODE"
"number of samples is the number of samples reaching the node because each sample";"-"
"has a weight of 1 by default";"CODE"
"for example in the above tree built on the iris dataset the root node has";"CODE"
"value 0 33 0 304 0 366 indicating there are 33 of class 0 samples";"IRRE"
"30 4 of class 1 samples and 36 6 of class 2 samples at the root node one can";"CODE"
"convert this to the absolute number of samples by multiplying by the number of";"CODE"
"samples reaching the root node which is tree weighted n node samples 0";"-"
"then the root node has value 37 34 41 indicating there are 37 samples";"IRRE"
"of class 0 34 samples of class 1 and 41 samples of class 2 at the root node";"IRRE"
"traversing the tree the samples are split and as a result the value array";"IRRE"
"reaching each node changes the left child of the root node has value 1 0 0";"IRRE"
"or value 37 0 0 when converted to the absolute number of samples";"IRRE"
"because all 37 samples in the left child node are from class 0";"CODE"
"note in this example n outputs 1 but the tree classifier can also handle";"CODE"
"multi output problems the value array at each node would just be a 2d";"IRRE"
"array instead";"-"
"we can compare the above output to the plot of the decision tree";"IRRE"
"here we show the proportions of samples of each class that reach each";"IRRE"
"node corresponding to the actual elements of tree value array";"IRRE"
"decision path";"-"
"we can also retrieve the decision path of samples of interest the";"CODE"
"decision path method outputs an indicator matrix that allows us to";"IRRE"
"retrieve the nodes the samples of interest traverse through a non zero";"IRRE"
"element in the indicator matrix at position i j indicates that";"IRRE"
"the sample i goes through the node j or for one sample i the";"CODE"
"positions of the non zero elements in row i of the indicator matrix";"-"
"designate the ids of the nodes that sample goes through";"-"
"the leaf ids reached by samples of interest can be obtained with the";"CODE"
"apply method this returns an array of the node ids of the leaves";"CODE"
"reached by each sample of interest using the leaf ids and the";"CODE"
"decision path we can obtain the splitting conditions that were used to";"OUTD"
"predict a sample or a group of samples first let s do it for one sample";"CODE"
"note that node index is a sparse matrix";"TASK"
"obtain ids of the nodes sample id goes through i e row sample id";"-"
"continue to the next node if it is a leaf node";"CODE"
"check if value of the split feature for sample 0 is below threshold";"IRRE"
"for a group of samples we can determine the common nodes the samples go";"CODE"
"through";"-"
"boolean array indicating the nodes both samples go through";"CODE"
"obtain node ids using position in array";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"raise a comprehensible error and list the contents of the";"IRRE"
"directory to help debugging on the mailing list";"-"
"picking up the local install this will work only if the";"CODE"
"install is an inplace build";"-"
"from sklearn check build check build import check build noqa f401";"CODE"
"check if a random seed exists in the environment if not create one";"IRRE"
"usr bin env python3";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"xxx if this import ever fails does it really vendor either";"CODE"
"cython tempita or numpy npy tempita";"-"
"usr bin env python3";"CODE"
"set config display diagram doctest skip";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"geometric mean as reference category";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"goals";"-"
"provide a common private module for loss functions classes";"CODE"
"to be used in";"-"
"logisticregression";"-"
"poissonregressor gammaregressor tweedieregressor";"-"
"histgradientboostingregressor histgradientboostingclassifier";"IRRE"
"gradientboostingregressor gradientboostingclassifier";"IRRE"
"sgdregressor sgdclassifier";"IRRE"
"replace link module of glms";"CODE"
"note the shape of raw prediction for multiclass classifications are";"CODE"
"gradientboostingclassifier n samples n classes";"IRRE"
"histgradientboostingclassifier n classes n samples";"IRRE"
"note instead of inheritance like";"TASK"
"class baseloss baselink cylossfunction";"CODE"
"note naturally we would inherit in the following order";"TASK"
"class halfsquarederror identitylink cyhalfsquarederror baseloss";"IRRE"
"but because of https github com cython cython issues 4350 we set baseloss as";"IRRE"
"the last one this of course changes the mro";"CODE"
"class halfsquarederror identitylink cyhalfsquarederror baseloss";"IRRE"
"we use composition this way we improve maintainability by avoiding the above";"CODE"
"mentioned cython edge case and have easier to understand code which method calls";"IRRE"
"which code";"-"
"for gradient boosted decision trees";"CODE"
"this variable indicates whether the loss requires the leaves values to";"IRRE"
"be updated once the tree has been trained the trees are trained to";"CODE"
"predict a newton raphson step see grower finalize leaf but for";"CODE"
"some losses e g least absolute deviation we need to adjust the tree";"TASK"
"values to account for the line search of the gradient descent";"IRRE"
"procedure see the original paper greedy function approximation a";"CODE"
"gradient boosting machine by friedman";"-"
"https statweb stanford edu jhf ftp trebst pdf for the theory";"CODE"
"be graceful to shape n samples 1 n samples";"-"
"be graceful to shape n samples 1 n samples";"-"
"be graceful to shape n samples 1 n samples";"-"
"be graceful to shape n samples 1 n samples";"-"
"as default take weighted average of the target over the samples";"CODE"
"axis 0 and then transform into link scale raw prediction";"CODE"
"if the hessians are constant we consider them equal to 1";"CODE"
"this is correct for halfsquarederror";"CODE"
"for absoluteerror hessians are actually 0 but they are";"META"
"always ignored anyway";"-"
"note naturally we would inherit in the following order";"TASK"
"class halfsquarederror identitylink cyhalfsquarederror baseloss";"IRRE"
"but because of https github com cython cython issues 4350 we";"META"
"set baseloss as the last one this of course changes the mro";"IRRE"
"elf quantile quantile this is better stored outside of cython";"CODE"
"see formula before algo 4 in friedman 2001 but we apply it to y true";"CODE"
"not to the residual y true raw prediction an estimator like";"-"
"histgradientboostingregressor might then call it on the residual e g";"IRRE"
"fit intercept only y true raw prediction";"CODE"
"this is non zero only if y true is neither 0 nor 1";"CODE"
"be graceful to shape n samples 1 n samples";"-"
"this is non zero only if y true is neither 0 nor 1";"CODE"
"be graceful to shape n samples 1 n samples";"-"
"halftweedieloss power 1 5 is already there as default";"CODE"
"for numerical derivatives see";"CODE"
"https en wikipedia org wiki numerical differentiation";"CODE"
"https en wikipedia org wiki finite difference coefficient";"IRRE"
"we use central finite differences of accuracy 4";"IRRE"
"y common params y pred params type ignore operator";"-"
"generate a y true and raw prediction in valid range";"OUTD"
"use small values such that exp value is not nan";"IRRE"
"if link is identity we must respect the interval of y pred";"CODE"
"halfmultinomialloss";"-"
"raw prediction with entries exp 10 but exp 10 on the diagonal";"META"
"this is close enough to np inf which would produce nan";"CODE"
"comparing loss value constant term to zero would result in large";"IRRE"
"round off errors";"CODE"
"todo what could we test if loss approx hessian";"TASK"
"for multiclass loss we should only change the predictions of the";"CODE"
"class for which the derivative is taken for e g offset k eps";"CODE"
"for class k";"CODE"
"as a softmax is computed offsetting the whole array by a constant";"CODE"
"would have no effect on the probabilities and thus on the loss";"-"
"todo what could we test if loss approx hessian";"TASK"
"the argmin of binomial loss for y true 0 and y true 1 is resp";"CODE"
"inf and inf due to logit cf complete separation therefore we";"CODE"
"use 0 y true 1";"-"
"need to ravel arrays because assert allclose requires matching";"CODE"
"dimensions";"-"
"y true 5 0 exceedance of class 0";"IRRE"
"find minimum by optimization";"-"
"assert a shape tuple scalar";"CODE"
"the constraint corresponds to sum raw prediction 0 without it we would";"CODE"
"need to apply loss symmetrize raw prediction to opt x before comparing";"TASK"
"make sure baseline prediction is the expected functional func e g mean";"CODE"
"or median";"-"
"test baseline at boundary";"IRRE"
"alternative gradient formula according to esl";"CODE"
"quick sanity check of the parameters of the clone";"IRRE"
"sklearn output config is used by set output to configure the output";"IRRE"
"container of an estimator";"-"
"filters conditional methods that should be hidden based";"-"
"on the available if decorator";"CODE"
"happens if k is part of a kwargs";"IRRE"
"k has no default value";"IRRE"
"avoid calling repr on nested estimators";"IRRE"
"reorder the parameters from self get params using the init";"IRRE"
"signature";"-"
"simple optimization to gain speed inspect is slow";"-"
"nested params defaultdict dict grouped by prefix";"CODE"
"n char max is the approximate maximum number of non blank";"CODE"
"characters to render we pass it as an optional parameter to ease";"CODE"
"the tests";"IRRE"
"n max elements to show 30 number of elements to show in sequences";"-"
"use ellipsis for sequences with a lot of elements";"CODE"
"use bruteforce ellipsis when there are a lot of non blank characters";"CODE"
"lim n char max 2 apprx number of chars to keep on both ends";"CODE"
"the regex s s d n";"CODE"
"matches from the start of the string until the nth non blank";"CODE"
"character";"CODE"
"matches the start of string";"CODE"
"pattern n matches n repetitions of pattern";"-"
"s s matches a non blank char following zero or more blanks";"CODE"
"the left side and right side aren t on the same line";"-"
"to avoid weird cuts e g";"CODE"
"categoric ore";"-"
"we need to start the right side with an appropriate newline";"TASK"
"character so that it renders properly as";"CODE"
"categoric";"-"
"handle unknown ignore";"-"
"so we add n n which matches until the next n";"TASK"
"only add ellipsis if it results in a shorter repr";"TASK"
"for python 3 11 empty instance no slots";"CODE"
"and dict will return a state equal to none";"IRRE"
"python 3 11";"CODE"
"non optimized default implementation override when a better";"TASK"
"method is possible for a given clustering algorithm";"CODE"
"non optimized default implementation override when a better";"TASK"
"method is possible for a given clustering algorithm";"CODE"
"we do not route parameters here since consumers don t route but";"CODE"
"since it s possible for a transform method to also consume";"CODE"
"metadata we check if that s the case and we raise a warning telling";"CODE"
"users that they should implement a custom fit transform method";"TASK"
"to forward metadata to transform as well";"CODE"
"for that we calculate routing and check if anything would be routed";"IRRE"
"to transform if we were to route them";"CODE"
"fit method of arity 1 unsupervised transformation";"CODE"
"fit method of arity 2 supervised transformation";"CODE"
"note that passing attributes n features in forces check is fitted";"TASK"
"to check if the attribute is present otherwise it will pass on";"IRRE"
"stateless estimators requires fit false";"CODE"
"we do not route parameters here since consumers don t route but";"CODE"
"since it s possible for a predict method to also consume";"CODE"
"metadata we check if that s the case and we raise a warning telling";"CODE"
"users that they should implement a custom fit predict method";"TASK"
"to forward metadata to predict as well";"CODE"
"for that we calculate routing and check if anything would be routed";"IRRE"
"to predict if we were to route them";"-"
"override for transductive outlier detectors like localoulierfactor";"CODE"
"we don t want to validate again for each call to partial fit";"CODE"
"we want all classifiers that don t expose a random state";"IRRE"
"to be deterministic and we don t want to expose this one";"CODE"
"calibratedclassifiercv estimator is not validated yet";"TASK"
"set classes using all y";"IRRE"
"for temperature scaling if y contains strings then encode it";"CODE"
"right here to avoid fitting labelencoder again within the";"CODE"
"fit calibrator function";"CODE"
"sample weight checks";"-"
"routed params splitter bunch split no routing for splitter";"CODE"
"check that each cross validation fold can have at least one";"-"
"example per class";"IRRE"
"ensure shape n samples 1 in the binary case";"CODE"
"select the probability column of the positive class";"CODE"
"check that the sample weight dtype is consistent with the";"-"
"predictions to avoid unintentional upcasts";"CODE"
"note here we don t pass on fit params because the supported";"TASK"
"calibrators don t support fit params anyway";"CODE"
"compute the arithmetic mean of the predictions of the calibrated";"-"
"classifiers";"IRRE"
"reshape binary output from n samples to n samples 1";"IRRE"
"check that the sample weight dtype is consistent with the predictions";"-"
"to avoid unintentional upcasts";"CODE"
"else sigmoid";"-"
"reshape binary output from n samples to n samples 1";"IRRE"
"when binary predictions consists only of predictions for";"CODE"
"clf classes 1 but pos class indices 0";"IRRE"
"normalize the probabilities";"-"
"in the edge case where for each class calibrator returns a zero";"CODE"
"probability for a given sample use the uniform distribution";"CODE"
"instead";"-"
"deal with cases where the predicted probability minimally exceeds 1 0";"CODE"
"the max abs prediction threshold was approximated using";"-"
"logit np finfo np float64 eps which is about 36";"CODE"
"f predictions f follows platt s notations";"-"
"if the predictions have large values we scale them in order to bring";"IRRE"
"them within a suitable range this has no effect on the final";"CODE"
"prediction result because linear models like logisitic regression";"IRRE"
"without a penalty are invariant to multiplying the features by a";"TASK"
"constant";"CODE"
"we rescale the features in a copy inplace rescaling could confuse";"TASK"
"the caller and make the code harder to reason about";"IRRE"
"bayesian priors see platt end of section 2 2";"CODE"
"it corresponds to the number of samples taking into account the";"CODE"
"sample weight";"-"
"astype below is needed to ensure y true and raw prediction have the";"-"
"same dtype with result np float64 0 np array 1 2 dtype np float32";"CODE"
"in numpy 2 result dtype is float64";"IRRE"
"in numpy 2 result dtype is float32";"IRRE"
"todo remove casting to np float64 when minimum supported scipy is 1 11 2";"CODE"
"with scipy 1 11 2 the lbfgs implementation will cast to float64";"TASK"
"https github com scipy scipy pull 18825";"CODE"
"here we cast to float64 to support scipy 1 11 2";"CODE"
"the tuned multiplicative parameter is converted back to the original";"IRRE"
"input feature scale the offset parameter does not need rescaling since";"CODE"
"we did not rescale the outcome variable";"CODE"
"check if it is the output of predict proba";"IRRE"
"todo simplify once upstream issue is addressed";"TASK"
"https github com data apis array api extra issues 478";"CODE"
"logits convert to logits x guarantees xp float64 or xp float32";"CODE"
"todo numpy 2 0";"TASK"
"ensure raw prediction has the same dtype as labels using astype";"-"
"without this dtype promotion rules differ across numpy versions";"META"
"beta np float64 0";"CODE"
"logits np array 1 2 dtype np float32";"CODE"
"result beta logits";"IRRE"
"numpy 2 result dtype is float32";"IRRE"
"numpy 2 result dtype is float64";"IRRE"
"this can cause dtype mismatch errors downstream e g buffer dtype";"CODE"
"if not log beta minimizer success pragma no cover";"-"
"if strategy quantile determine bin edges by distribution of data";"META"
"we always have to show the legend for at least the reference line";"CODE"
"it makes no sense to run the algorithm in this case so return 1 or";"CODE"
"n samples clusters depending on preferences";"CODE"
"place preference on the diagonal of s";"CODE"
"r np zeros n samples n samples initialize messages";"IRRE"
"intermediate results";"IRRE"
"remove degeneracies";"OUTD"
"execute parallel affinity propagation updates";"CODE"
"tmp a s compute responsibilities";"-"
"y tmp ind i np max a s axis 1";"CODE"
"tmp rnew";"CODE"
"damping";"-"
"tmp rp compute availabilities";"-"
"tmp anew";"CODE"
"damping";"-"
"check for convergence";"CODE"
"k i size identify exemplars";"-"
"c i np arange k identify clusters";"CODE"
"refine the final set of exemplars and clusters and return results";"IRRE"
"reduce labels to a sorted gapless list";"-"
"public api";"CODE"
"else self affinity euclidean";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"mypy error module sklearn cluster has no attribute hierarchical fast";"META"
"from sklearn cluster import type ignore attr defined";"CODE"
"for non fully connected graphs";"CODE"
"make the connectivity matrix symmetric";"-"
"convert connectivity matrix to lil";"-"
"connectivity is a sparse matrix at this point";"CODE"
"compute the number of nodes";"-"
"xxx can we do without completing the matrix";"CODE"
"explicitly cast connectivity to ensure safety";"-"
"ensure zero distances aren t ignored by setting them to epsilon";"IRRE"
"use scipy sparse csgraph to generate a minimum spanning tree";"IRRE"
"convert the graph to scipy cluster hierarchy array format";"CODE"
"undo the epsilon values";"IRRE"
"sort edges of the min spanning tree by weight";"-"
"convert edge list into standard hierarchical clustering format";"CODE"
"compute parents";"-"
"hierarchical tree building functions";"CODE"
"from scipy cluster import hierarchy imports pil";"CODE"
"create inertia matrix";"IRRE"
"we keep only the upper triangular for the moments";"CODE"
"generator expressions are faster than arrays on the following";"-"
"build moments as a list";"-"
"prepare the main fields";"CODE"
"recursive merge loop";"IRRE"
"identify the merge";"-"
"if return distance store inertia value";"IRRE"
"update the moments";"CODE"
"update the structure matrix a and the inertia matrix";"CODE"
"list comprehension is faster than a for loop";"IRRE"
"list comprehension is faster than a for loop";"IRRE"
"separate leaves in children empty lists up to now";"-"
"sort children to get consistent output with unstructured version";"IRRE"
"children np array children return numpy array for efficient caching";"CODE"
"2 is scaling factor to compare w unstructured version";"IRRE"
"single average and complete linkage";"CODE"
"single linkage is handled differently";"-"
"from scipy cluster import hierarchy imports pil";"CODE"
"for the linkage function of hierarchy to work on precomputed";"CODE"
"data provide as first argument an ndarray of the shape returned";"IRRE"
"by sklearn metrics pairwise distances";"-"
"translate to something understood by scipy";"-"
"we need the fast cythonized metric from neighbors";"CODE"
"the cython routines used require contiguous arrays";"CODE"
"sort edges of the min spanning tree by weight";"-"
"convert edge list into standard hierarchical clustering format";"CODE"
"put the diagonal to zero";"-"
"fixme we compute all the distances while we could have only computed";"CODE"
"the interesting distances";"CODE"
"create inertia heap and connection matrix";"IRRE"
"lil seems to the best format to access the rows quickly";"CODE"
"without the numpy overhead of slicing csr indices and data";"CODE"
"we are storing the graph in a list of intfloatdict";"CODE"
"we keep only the upper triangular for the heap";"CODE"
"generator expressions are faster than arrays on the following";"-"
"prepare the main fields";"CODE"
"recursive merge loop";"IRRE"
"identify the merge";"-"
"store distances";"-"
"keep track of the number of elements per cluster";"-"
"update the structure matrix a and the inertia matrix";"CODE"
"a clever min or max operation between a i and a j";"-"
"here we use the information from coord col containing the";"CODE"
"distances to update the heap";"CODE"
"clear a i and a j to save memory";"CODE"
"separate leaves in children empty lists up to now";"-"
"return numpy array for efficient caching";"CODE"
"matching names to tree building strategies";"-"
"functions for cutting hierarchical clustering tree";"CODE"
"in this function we store nodes as a heap to avoid recomputing";"CODE"
"the max of the nodes the first element is always the smallest";"-"
"we use negated indices as heaps work on smallest elements and we";"-"
"are interested in largest elements";"CODE"
"children 1 is the root of the tree";"-"
"as we have a heap nodes 0 is the smallest element";"-"
"insert the 2 children and remove the largest node";"CODE"
"early stopping is likely to give a speed up only for";"CODE"
"a large number of clusters the actual threshold";"-"
"implemented here is heuristic";"TASK"
"construct the tree";"CODE"
"if self distance threshold is not none distance threshold is used";"CODE"
"else n clusters is used";"-"
"cut the tree";"-"
"copy to avoid holding a reference on the original array";"CODE"
"reassign cluster numbers";"IRRE"
"some eigenvalues of a a t are negative causing";"IRRE"
"sqrt to be np nan this causes some vectors in vt";"CODE"
"to be np nan";"-"
"initialize with 1 1 as in arpack";"IRRE"
"initialize with 1 1 as in arpack";"IRRE"
"else tuple";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"make sure node1 is closest to itself even if all distances are equal";"CODE"
"this can only happen when all node centroids are duplicates leading to all";"CODE"
"distances between centroids being zero";"-"
"the list of subclusters centroids and squared norms";"-"
"to manipulate throughout";"-"
"keep centroids and squared norm as views in this way";"CODE"
"if we change init centroids and init sq norm it is";"IRRE"
"sufficient";"-"
"because of numerical issues this could become negative";"CODE"
"if partial fit is called for the first time or fit is called we";"IRRE"
"start a new tree";"CODE"
"the first root is the leaf manipulate this object throughout";"CODE"
"to enable getting back subclusters";"-"
"cannot vectorize enough to convince to use cython";"-"
"perform just the final global clustering step";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dbscan metric is not validated yet";"TASK"
"calculate neighborhood for all samples this leaves the original";"CODE"
"point in which needs to be considered later i e point i is in the";"CODE"
"neighborhood of point i while true its useless information";"CODE"
"set the diagonal to explicit values as a point is its own";"IRRE"
"neighbor";"-"
"x x copy copy to avoid in place modification";"CODE"
"this has worst case o n 2 memory complexity";"CODE"
"initially all samples are noise";"IRRE"
"a list of all core samples found";"-"
"fix for scipy sparse indexing issue";"IRRE"
"no core samples";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"mixin class for feature agglomeration";"CODE"
"a fast way to compute the mean of grouped features";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"redistribution and use in source and binary forms with or without";"META"
"modification are permitted provided that the following conditions are met";"-"
"1 redistributions of source code must retain the above copyright notice";"META"
"this list of conditions and the following disclaimer";"CODE"
"2 redistributions in binary form must reproduce the above copyright notice";"META"
"this list of conditions and the following disclaimer in the documentation";"CODE"
"and or other materials provided with the distribution";"META"
"3 neither the name of the copyright holder nor the names of its contributors";"OUTD"
"may be used to endorse or promote products derived from this software without";"CODE"
"specific prior written permission";"-"
"this software is provided by the copyright holders and contributors as is";"OUTD"
"and any express or implied warranties including but not limited to the";"META"
"implied warranties of merchantability and fitness for a particular purpose";"CODE"
"are disclaimed in no event shall the copyright holder or contributors be";"OUTD"
"liable for any direct indirect incidental special exemplary or";"CODE"
"consequential damages including but not limited to procurement of";"META"
"substitute goods or services loss of use data or profits or business";"-"
"interruption however caused and on any theory of liability whether in";"CODE"
"contract strict liability or tort including negligence or otherwise";"-"
"arising in any way out of the use of this software even if advised of the";"CODE"
"possibility of such damage";"-"
"encodings are arbitrary but must be strictly negative";"TASK"
"the current encodings are chosen as extensions to the 1 noise label";"CODE"
"avoided enums so that the end user only deals with simple labels";"CODE"
"the probability could also be 1 since infinite points are certainly";"IRRE"
"infinite outliers however 0 is convention from the hdbscan library";"CODE"
"implementation";"TASK"
"a nan probability is chosen to emphasize the fact that the";"-"
"corresponding data was not considered in the clustering problem";"CODE"
"check if the mutual reachability matrix has any rows which have";"IRRE"
"less than min samples non zero elements";"-"
"check connected component on mutual reachability";"-"
"if more than one connected component is present";"-"
"it means that the graph is disconnected";"-"
"compute the minimum spanning tree for the sparse graph";"IRRE"
"sort edges of the min spanning tree by weight";"-"
"convert edge list into standard hierarchical clustering format";"CODE"
"we need csr format to avoid a conversion in brute mst when calling";"CODE"
"csgraph connected components";"-"
"note that distance matrix is manipulated in place however we do not";"TASK"
"need it for anything else past this point hence the operation is safe";"CODE"
"warn if the mst couldn t be constructed around the missing distances";"CODE"
"the cython routines used require contiguous arrays";"CODE"
"get distance to kth nearest neighbour";"-"
"mutual reachability distance is implicit in mst from data matrix";"CODE"
"hdbscan metric is not validated yet";"TASK"
"todo 1 10 remove warn option";"TASK"
"and leave copy to its default value where applicable in examples and doctests";"IRRE"
"non precomputed matrices may contain non finite values";"IRRE"
"pass only the purely finite indices into hdbscan";"IRRE"
"we will later assign all non finite points their";"IRRE"
"corresponding labels as specified in outlier encoding";"-"
"reduce x to make the checks for missing outlier samples more";"CODE"
"convenient";"-"
"samples with missing data are denoted by the presence of";"TASK"
"np nan";"-"
"outlier samples are denoted by the presence of np inf";"TASK"
"continue with only finite samples";"IRRE"
"handle sparse precomputed distance matrices separately";"IRRE"
"only non sparse precomputed distance matrices are handled here";"IRRE"
"and thereby allowed to contain numpy inf for missing distances";"CODE"
"perform data validation after removing infinite values numpy inf";"IRRE"
"from the given distance matrix";"CODE"
"todo support np nan in cython implementation for precomputed";"TASK"
"dense hdbscan";"-"
"we can t do much with sparse matrices";"IRRE"
"todo benchmark kd vs ball tree efficiency";"TASK"
"metric is a valid balltree metric";"-"
"remap indices to align with original data in the case of";"CODE"
"non finite entries samples with np inf are mapped to 1 and";"IRRE"
"those with np nan are mapped to 2";"-"
"there may be overlap for points w both np inf and np nan";"CODE"
"infinite outliers have probability 0 by convention though this";"IRRE"
"is arbitrary";"-"
"number of non noise clusters";"-"
"need to handle iteratively seen each cluster may have a different";"TASK"
"number of samples hence we can t create a homogeneous 3d array";"IRRE"
"todo implement weighted argmin pwd backend";"TASK"
"infer indices from labels generated during fit";"CODE"
"overwrite infinite missing outlier samples otherwise simple noise";"TASK"
"buffers to avoid new allocations at each iteration";"CODE"
"compute new pairwise distances between centers and closest other";"CODE"
"center of each center for next iterations";"CODE"
"first check the labels for strict convergence";"CODE"
"no strict convergence check for tol based convergence";"CODE"
"rerun e step so that predicted labels match cluster centers";"CODE"
"threadpoolctl context to limit the number of threads in second level of";"CODE"
"nested parallelism i e blas to avoid oversubscription";"CODE"
"buffers to avoid new allocations at each iteration";"CODE"
"first check the labels for strict convergence";"CODE"
"no strict convergence check for tol based convergence";"CODE"
"rerun e step so that predicted labels match cluster centers";"CODE"
"same as labels inertia but in a threadpool limits context";"META"
"the blas call inside a prange in lloyd iter chunked dense is known to";"IRRE"
"cause a small memory leak when there are less chunks than the number";"-"
"of available threads it only happens when the openmp library is";"CODE"
"vcomp microsoft openmp and the blas library is mkl see 18653";"CODE"
"manually fit on batches";"-"
"fit on the whole data";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"if n neighbors 1 cannot fit nearestneighbors with n neighbors 0";"-"
"separate function for each seed s iterative loop";"CODE"
"for each seed climb gradient until convergence or max iter";"CODE"
"top thresh 1e 3 bandwidth when mean has converged";"-"
"find mean of points within bandwidth";"CODE"
"break depending on seeding strategy this condition may occur";"CODE"
"my old mean my mean save the old mean";"CODE"
"if converged or at max iter adds the cluster";"TASK"
"bin points";"CODE"
"select only those bins as seeds which have enough members";"CODE"
"we use n jobs 1 because this will be used in nested calls under";"IRRE"
"parallel calls to mean shift single seed so there is no need for";"IRRE"
"for further parallelism";"CODE"
"execute iterations on all seeds in parallel";"CODE"
"copy results in a dictionary";"IRRE"
"if all res i 1 i e len points within 0";"CODE"
"nothing near seeds";"-"
"post processing remove near duplicate points";"CODE"
"if the distance between two kernels is less than the bandwidth";"-"
"then we have to remove one because it is a duplicate remove the";"-"
"one with fewer points";"CODE"
"unique i 1 leave the current point as unique";"CODE"
"assign labels a point belongs to the cluster that it is closest to";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"optics metric is not validated yet";"TASK"
"x x copy copy to avoid in place modification";"CODE"
"set each diagonal to an explicit value so each point is its";"IRRE"
"own neighbor";"-"
"extract clusters from the calculated orders and reachability";"CODE"
"optics helper functions";"CODE"
"prefer skip nested validation false metric is not validated yet";"TASK"
"start all points as unprocessed";"CODE"
"here we first do a knn query for each point this differs from";"CODE"
"the original optics that only used epsilon range queries";"-"
"todo handle working memory somehow";"TASK"
"optics puts an upper limit on these use inf for undefined";"CODE"
"main optics loop not parallelizable the order that entries are";"CODE"
"written to the ordering list is important";"CODE"
"note that this implementation is o n 2 theoretically but";"TASK"
"supposedly with very low constant factors";"CODE"
"choose next based on smallest reachability distance";"CODE"
"and prefer smaller ids on ties possibly np inf";"-"
"assume that radius neighbors is faster without distances";"-"
"and we don t need all distances nevertheless this means";"CODE"
"we may be doing some work twice";"CODE"
"getting indices of neighbors that have not been processed";"-"
"neighbors of current point are already processed";"CODE"
"only compute distances to unprocessed neighbors";"-"
"the same logic as neighbors p is ignored if explicitly set";"IRRE"
"in the dict params";"CODE"
"find a maximal area";"-"
"it s not a steep point but still goes up";"TASK"
"region should include no more than min samples consecutive";"CODE"
"non steep xward points";"CODE"
"our implementation adds an inf to the end of reachability plot";"TASK"
"this helps to find potential clusters at the end of the";"CODE"
"reachability plot even if there s no upward region at the end of it";"CODE"
"das steep down areas introduced in section 4 3 2 of the paper";"CODE"
"mib 0 0 maximum in between section 4 3 2";"-"
"our implementation corrects a mistake in the original";"TASK"
"paper i e in definition 9 steep downward point";"CODE"
"r p 1 x1 r p 1 should be";"-"
"r p 1 x1 r p 1";"-"
"the following loop is almost exactly as figure 19 of the paper";"IRRE"
"it jumps over the areas which are not either steep down or up areas";"CODE"
"just continue if steep index has been a part of a discovered xward";"CODE"
"area";"-"
"steep downward areas";"CODE"
"steep upward areas";"-"
"line sc2";"-"
"definition 11 criterion 4";"IRRE"
"find the first index from the left side which is almost";"CODE"
"at the same level as the end of the detected cluster";"CODE"
"find the first index from the right side which is almost";"CODE"
"at the same level as the beginning of the detected";"-"
"cluster";"-"
"our implementation corrects a mistake in the original";"TASK"
"paper i e in definition 11 4c r x r sd should be";"IRRE"
"r x r sd";"-"
"predecessor correction";"-"
"definition 11 criterion 3 a";"IRRE"
"definition 11 criterion 1";"IRRE"
"definition 11 criterion 2";"IRRE"
"add smaller clusters first";"TASK"
"generate sample data";"-"
"the data is voluntary shifted away from zero to check clustering";"CODE"
"algorithm robustness with regards to non centered data";"-"
"todo affinitypropagation must preserve dtype for its fitted attributes";"CODE"
"and test must be created accordingly to this new behavior";"IRRE"
"for more details see https github com scikit learn scikit learn issues 11000";"CODE"
"with copy true s should not be modified";"-"
"with copy false s will be modified inplace";"-"
"test that copy true and copy false lead to the same result";"IRRE"
"sanity check for the number of samples in leaves and roots";"CODE"
"test that fit is equivalent to calling partial fit multiple times";"IRRE"
"test that same global labels are obtained after calling partial fit";"IRRE"
"with none";"-"
"test the predict method predicts the nearest centroid";"IRRE"
"n samples n samples per cluster";"-"
"birch must preserve inputs dtype";"CODE"
"test that n clusters param works properly";"IRRE"
"test that n clusters agglomerative clustering gives";"IRRE"
"the same results";"IRRE"
"test that a small number of clusters raises a warning";"IRRE"
"test that sparse and dense data give same results";"IRRE"
"birch must preserve inputs dtype";"CODE"
"second partial fit calls will error when n features is not consistent";"TASK"
"with the first call";"IRRE"
"test that nodes have at max branching factor number of subclusters";"IRRE"
"purposefully set a low threshold to maximize the subclusters";"IRRE"
"no error";"-"
"check if results is the same for dense and sparse data";"IRRE"
"all labels from fit or predict should be equal 0";"CODE"
"tests the dbscan algorithm with a similarity array";"IRRE"
"parameters chosen specifically for this task";"IRRE"
"compute similarities";"-"
"compute dbscan";"-"
"number of clusters ignoring noise if present";"-"
"tests the dbscan algorithm with a feature vector array";"TASK"
"parameters chosen specifically for this task";"IRRE"
"different eps to other test because distance is not normalised";"IRRE"
"compute dbscan";"-"
"parameters chosen for task";"TASK"
"number of clusters ignoring noise if present";"-"
"ensure it is sparse not merely on diagonals";"IRRE"
"test that precomputed neighbors graph is filtered if computed with";"IRRE"
"a radius larger than dbscan s eps";"-"
"test that the input is not modified by dbscan";"IRRE"
"add zeros on the diagonal that will be implicit when creating";"TASK"
"the sparse matrix if x is modified in place the zeros from";"IRRE"
"the diagonal will be made explicit";"-"
"make sure that we did not modify x in place even by creating";"-"
"explicit 0s values";"IRRE"
"tests the dbscan algorithm with a callable metric";"IRRE"
"parameters chosen specifically for this task";"IRRE"
"different eps to other test because distance is not normalised";"IRRE"
"metric is the function reference not the string key";"CODE"
"compute dbscan";"-"
"parameters chosen for task";"TASK"
"number of clusters ignoring noise if present";"-"
"tests that dbscan works with the metrics params argument";"IRRE"
"compute dbscan with metric params arg";"-"
"test that sample labels are the same as passing minkowski p directly";"IRRE"
"minkowski with p 1 should be equivalent to manhattan distance";"-"
"test that checks p is ignored in favor of metric params p val";"IRRE"
"tests the dbscan algorithm with balltree for neighbor calculation";"IRRE"
"number of clusters ignoring noise if present";"-"
"dbscan fit should accept a list of lists";"CODE"
"dbscan fit x must not raise exception";"CODE"
"ensure min samples is inclusive of core point";"CODE"
"ensure eps is inclusive of circumference";"-"
"ensure sample weight is validated";"-"
"ensure sample weight has an effect";"-"
"points within eps of each other";"CODE"
"and effect of non positive and non integer sample weight";"CODE"
"for non negative sample weight cores should be identical to repetition";"CODE"
"sample weight should work with precomputed distance matrix";"-"
"sample weight should work with estimator";"-"
"degenerate case every sample is a core sample either with its own";"CODE"
"cluster or including other close core samples";"CODE"
"with eps 1 and min samples 2 only the 3 samples from the denser area";"CODE"
"are core samples all other points are isolated and considered noise";"CODE"
"only the sample in the middle of the dense area is core its two";"CODE"
"neighbors are edge samples remaining samples are noise";"CODE"
"it s no longer possible to extract core samples with eps 1";"OUTD"
"everything is noise";"-"
"see https github com scikit learn scikit learn issues 4641 for";"CODE"
"more details";"-"
"sample matrix with initial two row all zero";"IRRE"
"x np array 0 0 1 reshape 1 3 n samples n features";"TASK"
"test transform";"IRRE"
"test inverse transform";"IRRE"
"ensure the matrix is not symmetric";"-"
"check that clustering is arbitrarily good";"-"
"this is a heuristic to guard against regression";"CODE"
"validation for brute is handled by pairwise distances";"CODE"
"we use a looser threshold due to dbscan producing a more constrained";"CODE"
"clustering representation";"-"
"compare that the sparse and dense non precomputed routines return the same labels";"IRRE"
"where the 0th observation contains the outlier";"-"
"ensure that nothing is done for noise";"CODE"
"without epsilon we should see many noise points as children of root";"CODE"
"arbitrary heuristic would prefer something more precise";"-"
"for this random seed an epsilon of 0 18 will produce exactly 2 noise";"CODE"
"points at that cut in single linkage";"CODE"
"create symmetric sparse matrix with 2 connected components";"IRRE"
"callables are not supported for either";"IRRE"
"the set of valid metrics for kdtree at the time of writing this test is a";"IRRE"
"strict subset of those supported in balltree";"IRRE"
"ensure the clusters are distinct with no overlap";"-"
"the threshold should be calculated per sample based on the largest";"CODE"
"lambda of any simbling node in this case all points are siblings";"CODE"
"and the largest value is exactly max lambda";"IRRE"
"todo 1 10 remove this test";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"misc tests on linkage";"IRRE"
"smoke test featureagglomeration";"TASK"
"test hierarchical clustering on a precomputed distances matrix";"IRRE"
"test hierarchical clustering on a precomputed distances matrix";"IRRE"
"check that we obtain the correct solution for structured linkage trees";"CODE"
"avoiding a mask with only true entries";"CODE"
"check that ward tree raises a valueerror with a connectivity matrix";"IRRE"
"of the wrong shape";"META"
"check that fitting with no samples raises an error";"CODE"
"check that we obtain the correct solution for unstructured linkage trees";"CODE"
"with specified a number of clusters just for the sake of";"CODE"
"raising a warning and testing the warning code";"IRRE"
"check that the height of the results of linkage tree is sorted";"IRRE"
"check that zero vectors in x produce an error when";"-"
"cosine affinity is used";"IRRE"
"check that when compute distances is true or distance threshold is";"-"
"given the fitted model has an attribute distances";"META"
"check that we obtain the correct number of clusters with";"CODE"
"agglomerative clustering";"-"
"test caching";"IRRE"
"turn caching off now";"-"
"check that we obtain the same solution with early stopping of the";"CODE"
"tree building";"-"
"check that we raise a typeerror on dense matrices";"CODE"
"test that using ward with another metric than euclidean raises an";"CODE"
"exception";"CODE"
"test using another metric than euclidean works with linkage complete";"CODE"
"compare our structured implementation to scipy";"TASK"
"test that using a distance matrix affinity precomputed has same";"IRRE"
"results with connectivity constraints";"CODE"
"check that we obtain the correct solution in a simplistic case";"CODE"
"check that fitting with no samples raises a valueerror";"IRRE"
"check that we get the correct result in two emblematic cases";"CODE"
"non regression test for issue 19875";"IRRE"
"greater than 1 non regression test for 16151";"IRRE"
"non regression test for 26657";"IRRE"
"non regression test for 21964";"IRRE"
"make cluster centers readonly";"CODE"
"no center should be one of the 0 sample weight point";"CODE"
"i e be at a distance 0 from it";"CODE"
"test convergence using 1d constant data";"IRRE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 28926";"CODE"
"test estimate bandwidth";"IRRE"
"test estimate bandwidth when n samples 1 and quantile 1 so that";"IRRE"
"n neighbors is set to 1";"IRRE"
"test meanshift algorithm";"IRRE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"test meanshift predict";"IRRE"
"init away from the data crash with a sensible warning";"IRRE"
"non regression before fit there should be not fitted attributes";"META"
"test the bin seeding technique which can be used in the mean shift";"IRRE"
"algorithm";"-"
"data is just 6 points in the plane";"CODE"
"with a bin coarseness of 1 0 and min bin freq of 1 3 bins should be";"-"
"found";"-"
"with a bin coarseness of 1 0 and min bin freq of 2 2 bins should be";"-"
"found";"-"
"with a bin size of 0 01 and min bin freq of 1 6 bins should be found";"-"
"we bail and use the whole data here";"IRRE"
"tight clusters around 0 0 and 1 1 only get two bins";"CODE"
"check that mean shift works when the estimated bandwidth is 0";"CODE"
"estimate bandwidth with default args returns 0 on this dataset";"IRRE"
"get bin seeds with a 0 bin size should return the dataset itself";"IRRE"
"meanshift with binning and a 0 estimated bandwidth should be equivalent";"-"
"to no binning";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"small and easy test no clusters around other clusters";"IRRE"
"but with a clear noise data";"META"
"global random seed is not used here since the expected labels";"IRRE"
"are hardcoded for these specific data";"CODE"
"check float min samples and min cluster size";"CODE"
"this may fail if the predecessor correction is not at work";"CODE"
"the first cluster should contain all point from c1 but due to how the data is";"CODE"
"generated some points from c2 may end up in it";"CODE"
"the second cluster should contain all points from c1 and c2";"CODE"
"in auto mode";"-"
"parameters chosen specifically for this task";"IRRE"
"compute optics";"-"
"number of clusters ignoring noise if present";"-"
"check attribute types and sizes";"META"
"test that we check a minimum number of samples";"IRRE"
"compute optics";"-"
"run the fit";"CODE"
"test an extraction of eps too close to original eps";"IRRE"
"compute optics";"-"
"make sure no warning is raised if metric and data are both boolean";"CODE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 18996";"CODE"
"make sure a single conversion warning is raised if metric is boolean";"CODE"
"but data isn t";"META"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 18996";"CODE"
"silence a deprecationwarning from joblib 1 5 1 in python 3 14";"CODE"
"make sure no conversion warning is raised if";"META"
"metric isn t boolean no matter what the data type is";"CODE"
"fit boolean data";"CODE"
"fit numeric data";"-"
"test extract where extraction eps is close to scaled max eps";"IRRE"
"compute optics";"-"
"cluster ordering starts at 0 max cluster label 2 is 3 clusters";"-"
"test that optics clustering labels are 5 difference of dbscan";"IRRE"
"calculate optics with dbscan extract at 0 3 epsilon";"-"
"calculate dbscan labels";"-"
"verify label mismatch is 5 labels";"-"
"try arbitrary minimum sizes";"CODE"
"redx x 2 astype global dtype copy false reduce for speed";"CODE"
"check behaviour is the same when min cluster size is a fraction";"-"
"ensure that we consider all unprocessed points";"CODE"
"not only direct neighbors when picking the next point";"CODE"
"expected values computed with future elki 0 7 5 using";"IRRE"
"java jar elki jar cli dbc in csv dbc filter fixeddbidsfilter";"CODE"
"algorithm clustering optics opticsheap optics minpts 5";"-"
"where the fixeddbidsfilter gives 0 indexed ids";"-"
"tests against known extraction array";"IRRE"
"does not work with metric euclidean because sklearn euclidean has";"CODE"
"worse numeric precision minkowski is slower but more accurate";"META"
"elki currently does not print the core distances which are not used much";"CODE"
"in literature but we can at least ensure to have this consistency";"META"
"expected values computed with future elki 0 7 5 using";"IRRE"
"testing an easy dbscan case not including clusters with different";"IRRE"
"densities";"-"
"add zeros on the diagonal that will be implicit when creating";"TASK"
"the sparse matrix if x is modified in place the zeros from";"IRRE"
"the diagonal will be made explicit";"-"
"make sure that we did not modify x in place even by creating";"-"
"explicit 0s values";"IRRE"
"non regression test for 21380";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"interleave the validated column specifiers";"CODE"
"add transformer tuple for remainder";"CODE"
"convert all columns to using their string labels";"CODE"
"selection is done with one dimension";"CODE"
"validate names";"-"
"validate estimators";"-"
"used to validate the transformers in the transformers list";"CODE"
"determine key type raises a valueerror if some transformer";"CODE"
"columns are callables";"IRRE"
"use bunch object to improve autocomplete";"CODE"
"an actual transformer";"CODE"
"list of tuples name feature names out";"TASK"
"no feature names";"TASK"
"prefix the feature names out with the transformers name";"TASK"
"verbose feature names out is false";"TASK"
"check that names are all unique without a prefix";"-"
"there are more than 5 overlapping names we only show the 5";"-"
"of the feature names";"TASK"
"transformers are fitted excludes drop cases";"CODE"
"sanity check that transformers is exhausted";"CODE"
"iter only generates transformers that have a non empty";"IRRE"
"selection here we set empty slices for transformers that";"CODE"
"generate no output which are safe for indexing";"CODE"
"else func is transform one";"CODE"
"else func is transform one";"CODE"
"we use fit transform to make sure to set sparse output for which we";"IRRE"
"need the transformed data to have consistent output type in predict";"IRRE"
"estimators in columntransformer transformers are not validated yet";"CODE"
"set n features in attribute";"TASK"
"all transformers are none";"CODE"
"determine if concatenated output will be sparse or not";"IRRE"
"if columntransformer is fit using a dataframe and now a dataframe is";"CODE"
"passed to be transformed we select columns by name instead this";"CODE"
"enables the user to pass x at transform time with extra columns which";"CODE"
"were not present in fit time and the order of the columns doesn t";"CODE"
"matter";"-"
"check that all names seen in fit are in transform unless";"CODE"
"they were dropped";"-"
"ndarray was used for fitting or transforming thus we only";"CODE"
"check that n features in is consistent";"TASK"
"all transformers are none";"CODE"
"since all columns should be numeric before stacking them";"CODE"
"in a sparse matrix check array is used for the";"IRRE"
"dtype conversion if necessary";"META"
"rename before stacking as it avoids to error on temporary duplicated";"CODE"
"columns";"-"
"add prefix for feature names out takes care about raising";"TASK"
"an error if there are duplicated columns";"-"
"check for duplicated columns and raise if any";"CODE"
"here we don t care about which columns are used for which";"CODE"
"transformers and whether or not a transformer is used at all which";"CODE"
"might happen if no columns are selected for that transformer we";"CODE"
"request all metadata requested by all transformers";"CODE"
"if transformers does not comply with our api list of tuples";"CODE"
"then it will fail in this case we assume that sparse is false";"CODE"
"but the parameter validation will raise an error during fit";"IRRE"
"pass pragma no cover";"-"
"import pandas as pd doctest skip";"CODE"
"rating 5 3 4 5 doctest skip";"IRRE"
"make column selector dtype include np number rating";"CODE"
"make column selector dtype include object string city";"CODE"
"ct fit transform x doctest skip";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we are transforming the target here and not the features so we set the";"TASK"
"output of functiontransformer to be a numpy array default and to not";"CODE"
"depend on the global configuration";"CODE"
"xxx sample weight is not currently passed to the";"-"
"transformer however if transformer starts using sample weight the";"CODE"
"code should be modified accordingly at the time to consider the";"-"
"sample prop feature it is also a good use case to be considered";"TASK"
"transformedtargetregressor regressor transformer are not validated yet";"CODE"
"store the number of dimension of the target to predict an array of";"-"
"similar shape at predict";"-"
"transformers are designed to modify x which is 2d dimensional we";"CODE"
"need to modify y accordingly";"TASK"
"transform y and convert back to 1d array if needed";"CODE"
"fixme a functiontransformer can return a 1d array even when validate";"CODE"
"is set to true therefore we need to check the number of dimension";"TASK"
"first";"-"
"1d series 2d dataframe";"-"
"1d array 2d array";"-"
"single column 1d 2d";"-"
"list like";"-"
"slice";"-"
"boolean mask";"CODE"
"callable that returns any of the allowed specifiers";"IRRE"
"test with transformer weights";"IRRE"
"string keys label based";"CODE"
"list";"-"
"slice";"-"
"int keys positional";"CODE"
"list";"-"
"slice";"-"
"boolean mask";"CODE"
"scalars are only supported for pandas dataframes";"CODE"
"scalar";"-"
"callable that returns any of the allowed specifiers";"IRRE"
"test with transformer weights";"IRRE"
"test multiple columns";"IRRE"
"ensure pandas object is passed through";"IRRE"
"dataframe protocol does not have 1d columns so we only test on pandas";"IRRE"
"dataframes";"-"
"only test on pandas because the dataframe protocol requires string column";"IRRE"
"names";"-"
"integer column spec integer column names still use positional";"CODE"
"test case that ensures that the column transformer does also work when";"CODE"
"a given transformer doesn t have any columns to work on";"CODE"
"assert len ct transformers 2 including remainder";"CODE"
"assert len ct transformers 2 including remainder";"CODE"
"checks for the output indices attribute";"IRRE"
"test with transformer weights and multiple columns";"IRRE"
"test case that ensures that the attribute does also work when";"IRRE"
"a given transformer doesn t have any columns to work on";"CODE"
"checks for the output indices attribute with data frames";"IRRE"
"no distinction between 1d and 2d";"CODE"
"this shouldn t fail since boolean can be coerced into a numeric";"CODE"
"see https github com scikit learn scikit learn issues 11912";"CODE"
"this fails since strings a and b cannot be";"CODE"
"coerced into a numeric";"CODE"
"above data has sparsity of 4 8 0 5";"-"
"apply threshold even if all sparse";"IRRE"
"mixed sparsity of 4 2 8 0 75";"-"
"if nothing is sparse no sparse";"IRRE"
"if one transformer is dropped test that name is still correct";"TASK"
"because fit is also doing transform this raises already on fit";"CODE"
"if one transformer is dropped test that name is still correct";"TASK"
"because fit is also doing transform this raises already on fit";"CODE"
"general invalid";"OUTD"
"invalid for arrays";"OUTD"
"transformed n features does not match fitted n features";"TASK"
"invalid keyword parameters should raise an error message";"IRRE"
"check it are fitted transformers";"CODE"
"raise correct error when not fitted";"CODE"
"raise correct error when no feature names are available";"TASK"
"one drop ignore";"-"
"all drop return shape 0 array";"IRRE"
"passthrough";"-"
"default drop";"CODE"
"specify passthrough";"-"
"column order is not preserved passed through added to end";"TASK"
"passthrough when all actual transformers are skipped";"CODE"
"check default for make column transformer";"CODE"
"0 false true false 2 mix types";"IRRE"
"0 1 2 ints";"CODE"
"lambda x 0 lambda x 1 2 callables";"CODE"
"a b c all strings";"IRRE"
"true false false false true false false false true all bools";"IRRE"
"if inputs are column names store remainder columns as column names";"CODE"
"todo 1 9 remove this test";"CODE"
"test different ways that columns are specified with passthrough";"IRRE"
"test different ways that columns are specified with passthrough";"IRRE"
"second and third columns are doubled when remainder doubletrans";"CODE"
"columns are doubled when remainder doubletrans";"CODE"
"sparsematrixtrans creates 3 features for each column there is";"IRRE"
"one column in transformers thus";"CODE"
"sparsematrixtrans creates 3 features for each column thus";"IRRE"
"assert that function gets the full array";"CODE"
"assert that function gets the full dataframe";"CODE"
"regression test for 14510";"IRRE"
"boolean array like does not behave as boolean array with sparse matrices";"CODE"
"make sure n features in is what is passed as input to the column";"TASK"
"transformer";"CODE"
"functional test for column transformer column selector";"CODE"
"remainder passthrough or an estimator will be shown in repr html";"CODE"
"remainder drop is not shown in repr html";"CODE"
"remainder shows the columns after fitting";"CODE"
"remainder shows the indices after fitting";"CODE"
"non regression test for 26306";"IRRE"
"fit on pandas and transform on polars";"CODE"
"fit on polars and transform on pandas";"CODE"
"non regression test for issue 28781";"IRRE"
"non regression test for issue 31546";"IRRE"
"provide a transformer and functions at the same time";"CODE"
"fit with sample weight with a regressor which does not support it";"CODE"
"one of func inverse func is given but the other one is not";"META"
"check the transformer output";"IRRE"
"check the regressor output";"IRRE"
"check the transformer output";"IRRE"
"check the regressor output";"IRRE"
"all transformer in scikit learn expect 2d data functiontransformer with";"CODE"
"validate false lift this constraint without checking that the input is a";"CODE"
"2d vector we check the consistency of the data shape using a 1d and 2d y";"-"
"array";"-"
"consistency forward transform";"CODE"
"consistency inverse transform";"IRRE"
"consistency of the regressor";"-"
"check consistency with transformer accepting only 2d array and a 1d 2d y";"TASK"
"array";"-"
"consistency forward transform";"CODE"
"if y ndim 1 create a 2d array and squeeze results";"IRRE"
"consistency inverse transform";"IRRE"
"consistency of the regressor";"-"
"if y ndim 1 create a 2d array and squeeze results";"IRRE"
"check consistency with transformer accepting only 2d array and a 2d y";"CODE"
"array";"-"
"consistency forward transform";"CODE"
"consistency inverse transform";"IRRE"
"consistency of the regressor";"-"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 18866";"CODE"
"check with a 3d target with a transformer that reshapes the target";"CODE"
"force that the function only return a 1d array";"CODE"
"check that the target y passed to the transformer will always be a";"CODE"
"numpy array similarly if x is passed as a list we check that the";"-"
"predictor receive as it is";"-"
"pass pragma no cover";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"import pytest run parallel noqa f401";"CODE"
"scipy requires network access to get data";"CODE"
"import pooch noqa f401";"CODE"
"global fixtures";"-"
"https scikit learn org dev computing parallelism html sklearn tests global random seed";"IRRE"
"strict mode to differentiate between 3 14 and np float64 3 14";"CODE"
"dt config rtol 0 01";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"avoid division truncation";"CODE"
"x test should have been called x";"IRRE"
"set covariance";"IRRE"
"set precision";"IRRE"
"compute empirical covariance of the test set";"IRRE"
"compute log likelihood";"-"
"compute the error";"-"
"compute the error norm";"-"
"optionally scale the error norm";"CODE"
"finally get either the squared norm or the norm";"CODE"
"compute mahalanobis distances";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"mypy error module sklearn linear model has no attribute cd fast";"META"
"from sklearn linear model import cd fast as cd fast type ignore attr defined";"CODE"
"helper functions to compute the objective and dual objective functions";"CODE"
"of the l1 penalized estimator";"-"
"the g lasso algorithm";"-"
"early return without regularization";"IRRE"
"as a trivial regularization tikhonov like we scale down the";"IRRE"
"off diagonal coefficients of our starting point this is needed as";"CODE"
"in the cross validation the cov init can easily be";"IRRE"
"ill conditioned and the cv loop blows beside this takes";"IRRE"
"conservative stand point on the initial conditions and it tends to";"CODE"
"make the convergence go faster";"-"
"i 0 initialize the counter to be robust to max iter 0";"IRRE"
"the different l1 regression solver have different numerical errors";"-"
"be robust to the max iter 0 edge case see";"CODE"
"https github com scikit learn scikit learn issues 4134";"CODE"
"set a sub covariance buffer";"IRRE"
"to keep the contiguous matrix sub covariance equal to";"CODE"
"covariance indices idx t indices idx";"CODE"
"we only need to update 1 column and 1 line when idx changes";"CODE"
"use coordinate descent";"-"
"todo it is not ideal that the max iter of the outer";"TASK"
"solver graphical lasso is coupled with the max iter of";"IRRE"
"the inner solver cd ideally cd has its own parameter";"IRRE"
"enet max iter like enet tol a minimum of 20 is rather";"-"
"arbitrary but not unreasonable";"META"
"else mode lars";"-"
"update the precision matrix";"CODE"
"covariance does not make sense for a single feature";"CODE"
"cross validation with graphicallasso";"IRRE"
"capture the errors and move on";"-"
"covariance does not make sense for a single feature";"CODE"
"list of alpha scores covs";"-"
"no need to see the convergence warnings on this grid";"TASK"
"they will always be points that will not converge";"CODE"
"during the cross validation";"-"
"compute the cross validated loss on the current grid";"-"
"note warm restarting graphical lasso path has been tried";"TASK"
"and this did not allow to gain anything";"CODE"
"same execution time with or without";"-"
"little danse to transform the list in what we need";"IRRE"
"find the maximum avoid using built in max function to";"CODE"
"have a fully reproducible selection of the smallest alpha";"CODE"
"in case of equality";"CODE"
"refine the grid";"-"
"we do not need to go back we have chosen";"CODE"
"the highest value of alpha for which there are";"IRRE"
"non zero coefficients";"-"
"we have non converged models on the upper bound of the";"CODE"
"grid we need to refine the grid there";"TASK"
"finally compute the score with alpha 0";"CODE"
"finally fit the model with the selected alpha";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"minimum covariance determinant";"CODE"
"implementing of an algorithm by rousseeuw van driessen described in";"TASK"
"a fast algorithm for the minimum covariance determinant estimator";"CODE"
"1999 american statistical association and the american society";"-"
"for quality technometrics";"CODE"
"xxx is this really a public function it s not listed in the docs or";"CODE"
"exported by sklearn covariance deprecate";"CODE"
"initialisation";"IRRE"
"compute initial robust estimates from a random subset";"IRRE"
"get initial robust estimates from the function parameters";"IRRE"
"run a special iteration for that case to get an initial support indices";"CODE"
"compute new estimates";"CODE"
"iterative procedure for minimum covariance determinant computation";"CODE"
"if the data already has singular covariance calculate the precision";"CODE"
"as the loop below will not be entered";"IRRE"
"save old estimates values";"IRRE"
"compute a new support indices from the full data set mahalanobis distances";"CODE"
"compute new estimates";"CODE"
"update remaining iterations for early stopping";"CODE"
"check if best fit already found det 0 logdet inf";"IRRE"
"check convergence";"-"
"c step procedure converged";"-"
"determinant has increased should not happen";"-"
"check early stopping";"-"
"convert from list of indices to boolean mask";"CODE"
"formulas as in sec 3 of pison 2002 derived from eq 4 2 in croux 1999";"CODE"
"compute n trials location and shape estimates candidates in the subset";"IRRE"
"perform n trials computations from random initial supports";"IRRE"
"perform computations from every given initial estimates";"CODE"
"find the n best best results among the n trials ones";"IRRE"
"minimum breakdown value";"CODE"
"1 dimensional case quick computation";"CODE"
"rousseeuw p j and leroy a m 2005 references in robust";"CODE"
"regression and outlier detection john wiley sons chapter 4";"CODE"
"find the sample shortest halves";"IRRE"
"take the middle points mean to get the robust location estimate";"CODE"
"get precision matrix in an optimized way";"-"
"get precision matrix in an optimized way";"-"
"starting fastmcd algorithm for p dimensional case";"CODE"
"1 find candidate supports on subsets";"IRRE"
"a split the set in subsets of size 300";"IRRE"
"b perform a total of 500 trials";"CODE"
"c select 10 best location covariance for each subset";"CODE"
"the above is too big let s try with something much small";"CODE"
"and less optimal";"-"
"2 pool the candidate supports into a merged set";"IRRE"
"possibly the full dataset";"IRRE"
"find the best couples location covariance on the merged set";"IRRE"
"3 finally get the overall best locations covariance couple";"CODE"
"directly get the best couple location covariance";"CODE"
"select the best couple on the full dataset";"IRRE"
"1 find the 10 best couples location covariance";"CODE"
"considering two iterations";"-"
"2 select the best couple on the full dataset amongst the 10";"IRRE"
"check that the empirical covariance is full rank";"CODE"
"compute and store raw estimates";"-"
"get precision matrix in an optimized way";"-"
"obtain consistency at normal models";"-"
"re weight estimator";"IRRE"
"check that the covariance of the support data is not equal to 0";"TASK"
"otherwise self dist 0 and thus correction 0";"CODE"
"parameter alpha as in croux1999 eq 4 2";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"avoid division truncation";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"tests covariance module on a simple dataset";"IRRE"
"test covariance fit from data";"CODE"
"mahalanobis distances computation test";"IRRE"
"test with n features 1";"TASK"
"test with one sample";"IRRE"
"create x with 1 sample and 5 features";"TASK"
"test integer type";"IRRE"
"test centered case";"IRRE"
"tests shrunkcovariance module on a simple dataset";"CODE"
"compare shrunk covariance obtained from data and from mle estimate";"CODE"
"same test with shrinkage not provided";"IRRE"
"same test with shrinkage 0 empirical covariance";"IRRE"
"test with n features 1";"TASK"
"test shrinkage coeff on a simple data set without saving precision";"IRRE"
"tests ledoitwolf module on a simple dataset";"IRRE"
"test shrinkage coeff on a simple data set";"IRRE"
"compare shrunk covariance obtained from data and from mle estimate";"CODE"
"compare estimates given by lw and shrunkcovariance";"CODE"
"test with n features 1";"TASK"
"test shrinkage coeff on a simple data set without saving precision";"IRRE"
"same tests without assuming centered data";"IRRE"
"test shrinkage coeff on a simple data set";"IRRE"
"compare shrunk covariance obtained from data and from mle estimate";"CODE"
"compare estimates given by lw and shrunkcovariance";"CODE"
"test with n features 1";"TASK"
"test with one sample";"IRRE"
"warning should be raised when using only 1 sample";"CODE"
"test shrinkage coeff on a simple data set without saving precision";"IRRE"
"a simple implementation of the formulas from ledoit wolf";"CODE"
"the computation below achieves the following computations of the";"-"
"o ledoit and m wolf a well conditioned estimator for";"CODE"
"large dimensional covariance matrices";"CODE"
"beta and delta are given in the beginning of section 3 2";"CODE"
"compare our blocked implementation to the naive implementation";"TASK"
"test that ledoit wolf doesn t error on data that is wider than block size";"CODE"
"use a number of features that is larger than the block size";"TASK"
"check that covariance is about diagonal random normal noise";"IRRE"
"check that the result is consistent with not splitting data into blocks";"IRRE"
"sample data from a sparse multivariate normal";"CODE"
"check that the costs always decrease doesn t hold if alpha 0";"CODE"
"use 1e 10 since the cost can be exactly 0";"-"
"check that the 2 approaches give similar results";"IRRE"
"smoke test the estimator";"IRRE"
"for a centered matrix assume centered could be chosen true or false";"CODE"
"check that this returns indeed the same result for centered data";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"tests the fastmcd algorithm implementation";"TASK"
"small data set";"IRRE"
"test without outliers random independent normal data";"IRRE"
"test with a contaminated data set medium contamination";"IRRE"
"test with a contaminated data set strong contamination";"IRRE"
"medium data set";"IRRE"
"large data set";"IRRE"
"1d data set";"IRRE"
"n samples n features";"TASK"
"add some outliers";"TASK"
"compute mcd by fitting an object";"IRRE"
"compare with the estimates learnt from the inliers";"IRRE"
"check that the code does not break with x shape 3 1";"CODE"
"i e n support n samples";"-"
"check that mcd completes when the covariance matrix is singular";"CODE"
"i e one of the rows and columns are all zeros";"-"
"think of these as the values for x and y 10 values between 5 and 5";"IRRE"
"get the cartesian product of all possible coordinate pairs from above set";"IRRE"
"add a third column that s all zeros to make our data a set of point";"TASK"
"within a plane which means that the covariance matrix will be singular";"CODE"
"the below line of code should raise an exception if the covariance matrix";"CODE"
"is singular as a further test since we have points in xyz the";"IRRE"
"principle components eigenvectors of these directly relate to the";"-"
"geometry of the points since it s a plane we should be able to test";"CODE"
"that the eigenvector that corresponds to the smallest eigenvalue is the";"IRRE"
"plane normal specifically 0 0 1 since everything is in the xy plane";"IRRE"
"as i ve set it up above to do this one would start by";"CODE"
"evals evecs np linalg eigh mcd fit covariance";"CODE"
"normal evecs np argmin evals";"-"
"after which we need to assert that our normal is equal to 0 0 1";"TASK"
"do note that there is floating point error associated with this so it s";"CODE"
"best to subtract the two and then compare some small tolerance e g";"IRRE"
"1e 12";"-"
"check that mcd returns a valueerror with informative message when the";"IRRE"
"covariance of the support data is equal to 0";"CODE"
"check that a warning is raised if we observe increasing determinants";"CODE"
"during the c step in theory the sequence of determinants should be";"CODE"
"decreasing increasing determinants are likely due to ill conditioned";"-"
"covariance matrices that result in poor precision matrices";"IRRE"
"threshold 0 985 threshold for variance underesitmation";"CODE"
"assume centered data to reduce test complexity";"IRRE"
"compute mean ratio of variances";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"used previous scipy pinv2 that was updated in";"CODE"
"https github com scipy scipy pull 10067";"CODE"
"we can not set cond or rcond for pinv2 in scipy 1 3 to keep the";"CODE"
"same behavior of pinv2 for scipy 1 3 because the condition used to";"IRRE"
"determine the rank is dependent on the output of svd";"IRRE"
"x weights old 100 init to big value for first convergence check";"IRRE"
"precompute pseudo inverse matrices";"CODE"
"basically x pinv x t x 1 x t";"IRRE"
"which requires inverting a n features n features matrix";"TASK"
"as a result and as detailed in the wegelin s review cca i e mode";"IRRE"
"b will be unstable if n features n samples or n targets";"TASK"
"n samples";"-"
"center";"-"
"scale";"-"
"basic checks for plscanonical";"CODE"
"check x tp and y uq";"-"
"need to scale first";"TASK"
"check that rotations on training data lead to scores";"-"
"check that inverse transform works";"IRRE"
"sanity check for plsregression";"CODE"
"the results were checked against the r packages plspm misomics and pls";"IRRE"
"fixme one would expect y trans pls y scores but this is not";"CODE"
"the case";"CODE"
"xref https github com scikit learn scikit learn issues 22420";"CODE"
"the r python difference in the signs should be consistent across";"CODE"
"loadings weights etc";"IRRE"
"check behavior when the first column of y is constant";"CODE"
"the results are checked against a modified version of plsreg2";"IRRE"
"from the r package plsdepot";"CODE"
"for the plsregression with default parameters y loadings y weights";"CODE"
"we ignore the first full zeros row for y";"CODE"
"sanity check for plscanonical";"CODE"
"the results were checked against the r package plspm";"IRRE"
"sanity check for plscanonical on random data";"IRRE"
"the results were checked against the r package plspm";"IRRE"
"2 latents vars";"CODE"
"make sure convergencewarning is raised if max iter is too small";"CODE"
"make sure attributes are of the correct shape depending on n components";"TASK"
"ensure 2d y with 1 column is equivalent to 1d y";"-"
"check that the copy keyword works";"-"
"copy true won t modify inplace";"-"
"copy false will modify inplace";"-"
"return plssvd does not support copy param in predict or transform";"CODE"
"make sure copy true gives same transform and predictions as predict false";"CODE"
"loadings converges to reasonable values";"IRRE"
"check that it works in votingregressor";"-"
"handcrafted data where we can predict y from x with an additional scaling factor";"TASK"
"x rng normal scale 10 size 30 5 add a std of 10";"CODE"
"we need to make sure that the dimension of the latent space is large enough to";"TASK"
"perfectly predict y from x no information loss";"CODE"
"we therefore should be able to predict y from x";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"convert to array for fancy indexing";"CODE"
"read header and data";"CODE"
"f seek 0 reset file obj";"IRRE"
"f seek 0 reset file obj";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the original data can be found at";"-"
"https www dcc fc up pt ltorgo regression cal housing tgz";"CODE"
"columns are not in the same order compared to the previous";"IRRE"
"url resource on lib stat cmu edu";"-"
"avg rooms total rooms households";"-"
"avg bed rooms total bed rooms households";"-"
"avg occupancy population households";"-"
"target in units of 100 000";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the original data can be found in";"-"
"https archive ics uci edu ml machine learning databases covtype covtype data gz";"CODE"
"column names reference";"CODE"
"https archive ics uci edu ml machine learning databases covtype covtype info";"CODE"
"creating temp dir as a direct subdirectory of the target directory";"-"
"guarantees that both reside on the same filesystem so that we can use";"CODE"
"os rename to atomically move the data files to their target location";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the original data can be found at";"-"
"https archive ics uci edu ml machine learning databases kddcup99 mld kddcup data gz";"CODE"
"the original data can be found at";"-"
"https archive ics uci edu ml machine learning databases kddcup99 mld kddcup data 10 percent gz";"CODE"
"selected abnormal samples";"CODE"
"select all samples with positive logged in attribute";"META"
"xxx bug when compress 0";"-"
"error incorrect data length while decompressing the file";"CODE"
"could be corrupted";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the original data can be found in";"-"
"http vis www cs umass edu lfw lfw tgz";"CODE"
"the original funneled data can be found in";"-"
"http vis www cs umass edu lfw lfw funneled tgz";"CODE"
"the original target data can be found in";"-"
"http vis www cs umass edu lfw pairsdevtrain txt";"CODE"
"http vis www cs umass edu lfw pairsdevtest txt";"IRRE"
"http vis www cs umass edu lfw pairs txt";"CODE"
"common private utilities for data fetching from the original lfw website";"CODE"
"local disk caching and image decoding";"-"
"compute the portion of the images to load to respect the slice parameter";"IRRE"
"given by the caller";"IRRE"
"allocate some contiguous memory to host the decoded image slices";"-"
"iterate over the collected file path to load the jpeg files as numpy";"CODE"
"arrays";"-"
"logger debug loading face 05d 05d i 1 n faces";"CODE"
"checks if jpeg reading worked refer to issue 3594 for more";"CODE"
"details";"-"
"face 255 0 scale uint8 coded colors to the 0 0 1 0 floats";"CODE"
"average the color channels to compute a gray levels";"-"
"representation";"-"
"task 1 face identification on picture with names";"TASK"
"scan the data folder content to retain people with more that";"OUTD"
"min faces per person face pictures";"-"
"shuffle the faces with a deterministic rng scheme to avoid having";"CODE"
"all faces of the same person in a row as it would break some";"CODE"
"cross validation and learning algorithms such as sgd and online";"CODE"
"k means that make an iid assumption";"-"
"wrap the loader in a memoizing function that will return memmaped data";"CODE"
"arrays for optimal memory usage";"CODE"
"load and memoize the pairs as np arrays";"CODE"
"pack the results as a bunch instance";"IRRE"
"task 2 face verification on pairs of face pictures";"TASK"
"parse the index file to find the number of pairs to be able to allocate";"IRRE"
"the right amount of memory before starting to decode the jpeg files";"CODE"
"iterating over the metadata lines for each pair to find the filename to";"CODE"
"decode and load in memory";"CODE"
"wrap the loader in a memoizing function that will return memmaped data";"CODE"
"arrays for optimal memory usage";"CODE"
"select the right metadata file according to the requested subset";"CODE"
"load and memoize the pairs as np arrays";"CODE"
"pack the results as a bunch instance";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the original data can be found at";"-"
"https cs nyu edu roweis data olivettifaces mat";"CODE"
"delete raw mat data";"CODE"
"we want floating point data but float32 is enough there is only";"CODE"
"one byte of precision in the original uint8s anyway";"CODE"
"10 images per class 400 images total each class is contiguous";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"412 is a specific openml error code";"CODE"
"avoid a resourcewarning on python 3 14 and later";"CODE"
"create a tmpdir as a subfolder of dir name where the final file will";"IRRE"
"be moved to if the download is successful this guarantees that the";"CODE"
"renaming operation to the final location is atomic to ensure the";"CODE"
"concurrence safety of the dataset caching mechanism";"IRRE"
"xxx first time decompression will not be necessary by using fsrc but";"META"
"it will happen nonetheless";"-"
"https www openml org api docs data get data list data name data name";"CODE"
"adult fetch openml adult version 2 doctest skip";"CODE"
"adult frame info doctest skip";"IRRE"
"column non null count dtype";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the original vectorized data can be found at";"-"
"http www ai mit edu projects jmlr papers volume5 lewis04a a13 vector files lyrl2004 vectors test pt0 dat gz";"IRRE"
"http www ai mit edu projects jmlr papers volume5 lewis04a a13 vector files lyrl2004 vectors test pt1 dat gz";"IRRE"
"http www ai mit edu projects jmlr papers volume5 lewis04a a13 vector files lyrl2004 vectors test pt2 dat gz";"IRRE"
"http www ai mit edu projects jmlr papers volume5 lewis04a a13 vector files lyrl2004 vectors test pt3 dat gz";"IRRE"
"http www ai mit edu projects jmlr papers volume5 lewis04a a13 vector files lyrl2004 vectors train dat gz";"CODE"
"while the original stemmed token files can be found";"CODE"
"in the readme section b 12 i";"CODE"
"http www ai mit edu projects jmlr papers volume5 lewis04a lyrl2004 rcv1v2 readme htm";"CODE"
"the original data can be found at";"-"
"http jmlr csail mit edu papers volume5 lewis04a a08 topic qrels rcv1 v2 topics qrels gz";"CODE"
"load data x and sample id";"TASK"
"training data is before testing data";"IRRE"
"delete archives";"CODE"
"load target y categories and sample id bis";"CODE"
"parse the target file";"IRRE"
"delete archive";"CODE"
"samples in x are ordered with sample id";"-"
"whereas in y they are ordered with sample id bis";"-"
"save category names in a list with same order than y";"CODE"
"reorder categories in lexicographic order";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the original data can be found at";"-"
"https biodiversityinformatics amnh org open source maxent samples zip";"CODE"
"the original data can be found at";"-"
"https biodiversityinformatics amnh org open source maxent coverages zip";"CODE"
"x y coordinates for corner cells";"CODE"
"x coordinates of the grid cells";"-"
"y coordinates of the grid cells";"-"
"define parameters for the data files these should not be changed";"CODE"
"unless the data model changes they will be saved in the npz file";"CODE"
"with the downloaded data";"CODE"
"with np load samples path as x samples zip is a valid npz";"CODE"
"with np load coverages path as x coverages zip is a valid npz";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dtype no validation delegate validation to numpy";"OUTD"
"if isinstance f int file descriptor";"CODE"
"convert from array array give data the right dtype";"CODE"
"indptr np frombuffer indptr dtype np longlong never empty";"CODE"
"data np asarray data dtype dtype no op for float 32 64";"CODE"
"dtype no validation delegate validation to numpy";"OUTD"
"disable heuristic search to avoid getting inconsistent results on";"IRRE"
"different segments of the file";"-"
"generated by dump svmlight file from scikit learn s n version";"META"
"column indices are s based n zero one one based encode";"-"
"f write b n";"TASK"
"f writelines b s n line for line in comment splitlines";"CODE"
"convert comment string to list of lines in utf 8";"CODE"
"if a byte string is passed then check whether it s ascii";"IRRE"
"if a user wants to get fancy they ll have to decode themselves";"-"
"comment decode ascii just for the exception";"CODE"
"we had some issues with csr matrices with unsorted indices e g 1501";"-"
"so sort them here but first make sure we don t modify the user s x";"META"
"todo we can do this cheaper sorted indices copies the whole matrix";"CODE"
"note query id is passed to cython functions using a fused type on query id";"CODE"
"yet as of cython 3 0 memory views can t be none otherwise the runtime";"TASK"
"would not known which concrete implementation to dispatch the python call to";"TASK"
"todo simplify interfaces and implementations in svmlight format fast pyx";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the original data can be found at";"-"
"https people csail mit edu jrennie 20newsgroups 20news bydate tar gz";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"extract a reduced dataset";"IRRE"
"check that the ordering of the target names is the same";"-"
"as the ordering in the full dataset";"IRRE"
"assert that we have only 0 and 1 as labels";"CODE"
"check that the number of filenames is consistent with data target";"-"
"check that the first entry of the reduced dataset corresponds to";"IRRE"
"the first entry of the corresponding category in the full dataset";"CODE"
"check that return x y option";"IRRE"
"extract the full dataset";"IRRE"
"test subset train";"IRRE"
"test subset test";"IRRE"
"test return x y option";"IRRE"
"test subset all";"IRRE"
"check a small subset of features";"TASK"
"mock that the dataset was cached";"IRRE"
"mock that we have an outdated pickle with only x and y returned";"IRRE"
"none of the input parameters are required to be accurate since the check";"CODE"
"of the parser will be carried out first";"IRRE"
"internal quotes are not considered to follow the arff spec in liac arff";"CODE"
"assert is china image";"CODE"
"assert is flower image";"CODE"
"this reproduces a problem when bunch pickles have been created";"IRRE"
"with scikit learn 0 16 and are read with 0 17 basically there";"IRRE"
"is a surprising behaviour because reading bunch key uses";"CODE"
"bunch dict which is non empty for 0 16 bunch objects";"CODE"
"whereas assigning into bunch key uses bunch setattr see";"IRRE"
"https github com scikit learn scikit learn issues 6196 for";"CODE"
"more details";"-"
"after loading from pickle the dict should have been ignored";"CODE"
"making sure that changing the attr does change the value";"IRRE"
"associated with getitem as well";"-"
"check that dir important for autocomplete shows attributes";"CODE"
"https example com path to data json anchor";"CODE"
"the first call should trigger a download";"CODE"
"fetching again the same file to the same folder should do nothing";"CODE"
"deleting and calling again should re download";"CODE"
"the first call should trigger a download";"CODE"
"fetching again the same file to the same folder should do nothing when";"CODE"
"the sha256 match";"-"
"corrupting the local data should yield a warning and trigger a new download";"CODE"
"calling again should do nothing";"IRRE"
"deleting the local file and calling again should redownload without warning";"CODE"
"calling without a sha256 should also work without redownloading";"CODE"
"calling with a wrong sha256 should raise an informative exception";"CODE"
"test return x y option";"IRRE"
"check that pandas is imported lazily and that an informative error";"CODE"
"message is raised when pandas is missing";"CODE"
"test return x y option";"IRRE"
"enumerated names are added correctly";"TASK"
"test the return x y option";"IRRE"
"the data frames for the input features should match up to some numerical";"CODE"
"dtype conversions e g float64 int64 due to limitations of the";"CODE"
"liac arff parser";"IRRE"
"let s also check that the frame attributes also match";"META"
"note that the frame attribute is a superset of the data attribute";"META"
"however the remaining columns typically the target s are not necessarily";"IRRE"
"dtyped similarly by both parsers due to limitations of the liac arff parser";"IRRE"
"therefore extra dtype conversions are required for those columns";"CODE"
"compare categorical features by converting categorical liac uses";"IRRE"
"strings to denote the categories we rename the categories to make";"TASK"
"them comparable to the pandas parser fixing this behavior in";"CODE"
"liac arff would allow to check the consistency in the future but";"TASK"
"we do not plan to maintain the liac arff on the long term";"CODE"
"iris dataset";"IRRE"
"anneal dataset";"IRRE"
"cpu dataset";"IRRE"
"emotions dataset";"IRRE"
"adult census dataset";"IRRE"
"miceprotein";"-"
"titanic";"-"
"test some more specific behaviour";"IRRE"
"single column test";"IRRE"
"multi column test";"IRRE"
"create a temporary modified arff file";"IRRE"
"requests are already mocked by monkey patch webbased functions";"CODE"
"we want to reuse that mock for all requests except file download";"CODE"
"hence creating a thin mock over the original mock";"-"
"validate failed checksum";"-"
"exception message should have file path";"CODE"
"avoid a resourcewarning on python 3 14 and later";"CODE"
"non regressiont tests";"IRRE"
"the dataset has 17 features including 1 ignored animal";"TASK"
"so we assert that we don t have the ignored feature in the final bunch";"CODE"
"similar behaviour should be observed when the column is not the target";"-"
"test sparsity";"IRRE"
"test shapes";"IRRE"
"test descr";"IRRE"
"test ordering of categories";"IRRE"
"test number of sample for some categories";"IRRE"
"test shuffling and subset";"IRRE"
"test return x y option";"IRRE"
"the first 23149 samples are the training samples";"-"
"test some precise values";"IRRE"
"assert sum y 0 10 unexpected number of samples in class 0";"CODE"
"assert sum y 1 25 unexpected number of samples in class 1";"CODE"
"assert sum y 2 65 unexpected number of samples in class 2";"CODE"
"test for n features 30";"TASK"
"create very separate clusters check that vertices are unique and";"IRRE"
"correspond to classes";"IRRE"
"cluster by sign viewed as strings to allow uniquing";"CODE"
"ensure on vertices of hypercube";"-"
"also test return distributions and return indicator with true";"IRRE"
"test that y np dot x c bias n 0 1 0";"IRRE"
"test with small number of features";"TASK"
"x y make regression n samples 100 n features 1 n informative 3";"TASK"
"test that y np dot x c bias n 0 1 0";"IRRE"
"do not use scipy sparse linalg eigs because it cannot find all eigenvalues";"IRRE"
"check that leading diagonal elements are 1";"-"
"testing odd and even case because in the past make circles always";"CODE"
"created an even number of samples";"IRRE"
"test x s shape";"IRRE"
"test x s non zero values";"IRRE"
"tests x s zero values";"IRRE"
"test can change x s values";"IRRE"
"test y";"IRRE"
"test loading from file descriptor";"CODE"
"gh20081 testing equality between path based and";"IRRE"
"fd based load svmlight file";"CODE"
"test loading from file descriptor";"CODE"
"test x shape";"IRRE"
"test x s non zero values";"IRRE"
"21 features in file";"TASK"
"tmp close necessary under windows";"CODE"
"because we close it manually and write to it";"CODE"
"we need to remove it manually";"TASK"
"tmp close necessary under windows";"CODE"
"because we close it manually and write to it";"CODE"
"we need to remove it manually";"TASK"
"load svmfile with qid attribute";"META"
"in python 3 integers are valid file opening arguments taken as unix";"CODE"
"file descriptors";"CODE"
"slicing a csr matrix can unsort its indices so test that we sort";"IRRE"
"those correctly";"-"
"we need to pass a comment to get the version info in";"TASK"
"libsvm doesn t grok comments so they re not put in by";"CODE"
"default anymore";"OUTD"
"make sure y s shape is n samples n labels";"-"
"when it is sparse";"IRRE"
"note with dtype np int32 we are performing unsafe casts";"CODE"
"where x astype dtype overflows the result is";"IRRE"
"then platform dependent and x dense astype dtype may be";"CODE"
"different from x sparse astype dtype asarray";"IRRE"
"allow a rounding error at the last decimal place";"CODE"
"allow a rounding error at the last decimal place";"CODE"
"make sure it dumps multilabel correctly";"-"
"loses the last decimal place";"CODE"
"make sure it s using the most concise format possible";"CODE"
"make sure it s correct too";"-"
"xxx we have to update this to support python 3 x";"CODE"
"test dumping a file with query id";"CODE"
"load svmfile with longint qid attribute";"CODE"
"put some marks that are likely to happen anywhere in a row";"-"
"load the original sparse matrix into 3 independent csr matrices";"CODE"
"load the same data in 2 parts with all the possible byte offsets to";"IRRE"
"locate the split so has to test for particular boundary cases";"CODE"
"the first and last element are explicit zeros";"-"
"y as a dense array would look like";"-"
"0 0 1";"-"
"0 0 1";"-"
"1 1 0";"-"
"convert to memmap backed which are read only";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"enter parallel code block";"-"
"1e 6 is arbitrary but consistent with the spams implementation";"TASK"
"kth atom is almost never used sample a new one from the data";"CODE"
"add small noise to avoid making the sparse coding ill conditioned";"IRRE"
"noise level 0 01 newd std or 1 avoid 0 std";"CODE"
"projection on the constraint set v k 1";"CODE"
"feature vector is split into a positive and negative side";"TASK"
"compute number of expected features in code";"TASK"
"n components";"-"
"fit algorithm";"-"
"batch size";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"some constant terms";"CODE"
"we ll modify svd outputs to return unexplained variance";"CODE"
"to allow for unified computation of loglikelihood";"CODE"
"else svd method randomized";"IRRE"
"small helps numerics";"-"
"use maximum here to avoid sqrt problems";"CODE"
"loglikelihood";"-"
"cov flat len cov 1 self noise variance modify diag inplace";"CODE"
"handle corner cases first";"CODE"
"get precision using matrix inversion lemma";"META"
"note that tol is not exposed";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"avoid sqrt of negative values because of rounding errors note that";"IRRE"
"np sqrt tiny is larger than tiny and therefore this clipping also";"CODE"
"prevents division by zero in the next step";"CODE"
"u resp s contains the eigenvectors resp square roots of";"-"
"the eigenvalues of w w t";"IRRE"
"j is the index of the extracted component";"-"
"builtin max abs are faster than numpy counter parts";"-"
"np einsum allows having the lowest memory footprint";"IRRE"
"it is faster than np diag np dot w1 w t";"CODE"
"some standard non linear functions";"CODE"
"xxx these should be optimized as they can be a bottleneck";"-"
"alpha fun args get alpha 1 0 comment it out";"IRRE"
"gx np tanh x x apply the tanh inplace";"-"
"xxx compute in chunks to avoid extra allocation";"CODE"
"for i gx i in enumerate gx please don t vectorize";"CODE"
"centering the features of x";"TASK"
"whitening and preprocessing by pca";"-"
"faster when num samples n features";"TASK"
"d degenerate idx eps for numerical issues";"CODE"
"give consistent eigenvectors for both svd solvers";"CODE"
"k u d t n components see 6 33 p 140";"-"
"see 13 6 p 267 here x1 is white and data";"-"
"in x has been projected onto a subspace by pca";"CODE"
"x must be casted to floats to avoid typing issues with numpy";"CODE"
"2 0 and the line below";"-"
"x1 as float array xt copy false copy has been taken care of";"CODE"
"either partially fit on smaller batches of data";"-"
"or let the fit function itself divide the data into batches";"CODE"
"ipca transform x doctest skip";"CODE"
"center kernel in place";"-"
"adjust n components according to user inputs";"CODE"
"n components k shape 0 use all dimensions";"-"
"compute eigenvectors";"-"
"note subset by index specifies the indices of smallest largest to return";"IRRE"
"make sure that the eigenvalues are ok and fix numerical issues";"IRRE"
"flip eigenvectors sign to enforce deterministic output";"CODE"
"sort eigenvectors in descending order";"CODE"
"remove eigenvectors with a zero eigenvalue null space if required";"IRRE"
"maintenance note on eigenvectors normalization";"CODE"
"there is a link between";"-"
"the eigenvectors of k phi x phi x and the ones of phi x phi x";"-"
"if v is an eigenvector of k";"-"
"then phi x v is an eigenvector of phi x phi x";"-"
"if u is an eigenvector of phi x phi x";"-"
"then phi x u is an eigenvector of phi x phi x";"-"
"at this stage our self eigenvectors the v have norm 1 we need to scale";"CODE"
"them so that eigenvectors in kernel feature space the u have norm 1";"TASK"
"instead";"-"
"we could scale them here";"CODE"
"self eigenvectors self eigenvectors np sqrt self eigenvalues";"CODE"
"but choose to perform that later when needed in fit and in";"IRRE"
"transform";"CODE"
"when kernel precomputed k is x but it s safe to perform in place operations";"META"
"on k because a copy was made before if requested by copy x";"CODE"
"no need to use the kernel to transform x use shortcut expression";"TASK"
"no need to use the kernel to transform x use shortcut expression";"TASK"
"compute centered gram matrix between x and training data x fit";"-"
"scale eigenvectors properly account for null space for dot product";"CODE"
"project with a scalar product between k and the scaled eigenvectors";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"in the literature this is exp e log theta";"CODE"
"diff on component only calculate it when cal diff is true";"-"
"these cython functions are called in a nested loop on usually very small arrays";"IRRE"
"length n topics in that case finding the appropriate signature of the";"CODE"
"fused typed function can be more costly than its execution hence the dispatch";"CODE"
"is done outside of the loop";"IRRE"
"the next one is a copy since the inner loop overwrites it";"TASK"
"iterate between doc topic d and norm phi until convergence";"CODE"
"the optimal phi dwk is proportional to";"-"
"exp e log theta dk exp e log beta dw";"-"
"note adds doc topic prior to doc topic d in place";"TASK"
"contribution of document d to the expected sufficient";"META"
"statistics for the m step";"CODE"
"numerator";"-"
"avoid a copy of xht which will be re computed update h true";"CODE"
"preserve the xht which is not re computed update h false";"CODE"
"denominator";"-"
"numerator";"-"
"if x is sparse compute wh only where x is non zero";"IRRE"
"copy used in the denominator";"CODE"
"to avoid taking a negative power of zero";"CODE"
"speeds up computation time";"-"
"refer to numpy numpy issues 9363";"-"
"element wise multiplication";"-"
"element wise multiplication";"-"
"here numerator dot x dot w h beta loss 2 h t";"CODE"
"denominator";"-"
"h sum np sum h axis 1 shape n components";"-"
"computation of whht dot dot w h beta loss 1 h t";"CODE"
"memory efficient computation";"-"
"compute row by row avoiding the dense matrix wh";"CODE"
"add l1 and l2 regularization";"TASK"
"gamma is in 0 1";"-"
"n components";"-"
"beta loss";"-"
"param validation is done in fit transform";"CODE"
"get scaled regularization terms done for each minibatch to take into account";"CODE"
"variable sizes of minibatches";"IRRE"
"update w";"CODE"
"necessary for stability with beta loss 1";"CODE"
"update h only at fit or fit transform";"CODE"
"necessary for stability with beta loss 1";"CODE"
"raise an error for sparse input and unsupported svd solver";"CODE"
"validate the data without ever forcing a copy as any solver that";"CODE"
"supports sparse input data and the covariance eigh solver are";"CODE"
"written in a way to avoid the need for any inplace modification of";"CODE"
"the input data contrary to the other solvers";"CODE"
"the copy will happen";"-"
"later only if needed once the solver negotiation below is done";"CODE"
"tall and skinny problems are best handled by precomputing the";"-"
"covariance matrix";"CODE"
"small problem or n components mle just call full pca";"IRRE"
"this is also the case of n components in 0 1";"CODE"
"call different fits for either full or truncated svd";"CODE"
"most values in the components are zero sparsity";"IRRE"
"flip eigenvectors sign to enforce deterministic output";"CODE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"ubsampling 3 subsampling factor";"-"
"compute a wavelet dictionary";"CODE"
"check that the underlying model fails to converge";"-"
"check that the underlying model converges w o warnings";"-"
"test error raised for wrong code size";"CODE"
"used to test lars here too but there s no guarantee the number of";"IRRE"
"nonzero atoms is right";"-"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"regression test that parallel reconstruction works with n jobs 1";"CODE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"test verbosity for better coverage";"IRRE"
"convergence monitoring verbosity";"-"
"higher verbosity level";"-"
"function api verbosity";"CODE"
"v rng randn n components n features random init";"IRRE"
"partial fit should ignore max iter 17433";"-"
"v rng randn n components n features random init";"IRRE"
"v rng randn n components n features random init";"IRRE"
"v rng randn n components n features random init";"IRRE"
"v rng randn n components n features random init";"IRRE"
"v rng randn n components n features random init";"IRRE"
"v rng randn n components n features random init";"IRRE"
"v rng normal size n components n features random init";"IRRE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 5956";"CODE"
"test that sparsecoder does not error by passing reading only";"IRRE"
"arrays to child processes";"-"
"ensure that data is 2m joblib memory maps arrays";"-"
"if they are larger than 1mb the 4 accounts for float32";"CODE"
"data type";"-"
"check the dict update in batch mode vs online mode";"CODE"
"non regression test for 4866";"IRRE"
"full batch update";"CODE"
"online update";"CODE"
"note do not check integer input because lasso lars and lars fail with";"CODE"
"valueerror in lars path solver";"IRRE"
"verify numerical consistency among np float32 and np float64";"CODE"
"note do not check integer input because lasso lars and lars fail with";"CODE"
"valueerror in lars path solver";"IRRE"
"verify preserving dtype for transform in sparse coder";"CODE"
"verify preserving dtype for fit and transform in dictionary learning class";"CODE"
"verify preserving dtype for fit and transform in minibatch dictionary learning";"CODE"
"verify output matrix dtype";"IRRE"
"verify numerically consistent among np float32 and np float64";"CODE"
"optimal solution u v is not unique";"-"
"if u v is optimal solution u v is also optimal";"-"
"and column permutated u row permutated v are also optional";"CODE"
"as long as holding uv";"-"
"so here uv u 1 1 and sum v k 2 2 are verified";"-"
"instead of comparing directly u and v";"CODE"
"verify an obtained solution is not degenerate";"-"
"verify output matrix dtype";"IRRE"
"verify numerically consistent among np float32 and np float64";"CODE"
"optimal solution u v is not unique";"-"
"if u v is optimal solution u v is also optimal";"-"
"and column permutated u row permutated v are also optional";"CODE"
"as long as holding uv";"-"
"so here uv u 1 1 and sum v k 2 are verified";"-"
"instead of comparing directly u and v";"CODE"
"verify an obtained solution is not degenerate";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"test factoranalysis ability to recover the data covariance structure";"CODE"
"some random settings for the generative model";"IRRE"
"latent variable of dim 3 20 of it";"IRRE"
"using gamma to model different noise variance";"CODE"
"per component";"-"
"generate observations";"-"
"wlog mean is 0";"-"
"sample covariance";"CODE"
"model covariance";"CODE"
"return np abs getattr x y sign will not be equal";"IRRE"
"test get covariance and get precision with n components n features";"TASK"
"with n components n features and with n components 0";"TASK"
"test rotation";"IRRE"
"test against r s psych principal with rotate varimax";"IRRE"
"i e the values below stem from rotating the components in r";"IRRE"
"r s factor analysis returns quite different values therefore we only";"IRRE"
"test the rotation itself";"IRRE"
"test gram schmidt orthonormalization";"IRRE"
"generate a random orthogonal matrix";"IRRE"
"https github com scikit learn scikit learn issues 24131 issuecomment 1208091119";"CODE"
"test the fastica algorithm on very simple data";"IRRE"
"generate two sources";"-"
"mixing angle";"-"
"function as fun arg";"CODE"
"check that the mixing model described in the docstring holds";"CODE"
"xxx exact reconstruction to standard relative tolerance is not";"CODE"
"possible this is probably expected when add noise is true but we";"CODE"
"also need a non trivial atol in float32 when add noise is false";"TASK"
"note that the 2 sources are non gaussian in this test";"CODE"
"check to see if the sources have been estimated";"-"
"in the wrong order";"META"
"check that we have estimated the original sources";"-"
"test fastica class";"IRRE"
"set atol to account for the different magnitudes of the elements in sources";"IRRE"
"from 1e 4 to 1e1";"CODE"
"test for issue 697";"IRRE"
"test the fastica algorithm on very simple data";"IRRE"
"see test non square fastica";"IRRE"
"ensure a convergencewarning raised if the tolerance is sufficiently low";"CODE"
"generate two sources";"-"
"mixing matrix";"-"
"do fastica with tolerance 0 to ensure failing convergence";"CODE"
"test the fastica algorithm on very simple data";"IRRE"
"generate two sources";"-"
"mixing matrix";"-"
"check that the mixing model described in the docstring holds";"CODE"
"check to see if the sources have been estimated";"-"
"in the wrong order";"META"
"check that we have estimated the original sources";"-"
"multivariate uniform data in 0 1";"CODE"
"make sure that numerical errors do not cause sqrt of negative";"CODE"
"values";"IRRE"
"xxx for some seeds the model does not converge";"CODE"
"however this is not what we test here";"IRRE"
"make sure that numerical errors do not cause sqrt of negative";"CODE"
"values";"IRRE"
"xxx we have to set atol for this test to pass for all seeds when";"CODE"
"fitting with float32 data is this revealing a bug";"CODE"
"atol 0 0 the default rtol is enough for float64 data";"CODE"
"test fastica inverse transform";"IRRE"
"for some dataset depending on the value of global dtype the model";"IRRE"
"can fail to converge but this should not impact the definition of";"IRRE"
"a valid inverse transform";"IRRE"
"reversibility test in non reduction case";"IRRE"
"xxx we have to set atol for this test to pass for all seeds when";"CODE"
"fitting with float32 data is this revealing a bug";"CODE"
"xxx dividing by a smaller number makes";"-"
"tests fail for some seeds";"IRRE"
"atol 0 0 the default rtol is enough for float64 data";"CODE"
"the fastica solver may not converge for some data with specific";"CODE"
"random seeds but this happens after the whiten step so this is";"CODE"
"not want we want to test here";"IRRE"
"histogram kernel implemented as a callable";"TASK"
"assert kwargs no kernel params that we didn t ask for";"CODE"
"histogram kernel produces singular matrix inside linalg solve";"-"
"xxx use a least squares approximation";"-"
"transform fit data";"CODE"
"non regression test previously gamma would be 0 by default";"IRRE"
"forcing all eigenvalues to 0 under the poly kernel";"IRRE"
"transform new data";"CODE"
"inverse transform";"IRRE"
"x fit needs to retain the old unmodified copy of x";"TASK"
"transform fit data";"CODE"
"transform new data";"CODE"
"inverse transform not available for sparse matrices";"IRRE"
"xxx should we raise another exception type here for instance";"CODE"
"notimplementederror";"TASK"
"for a linear kernel kernel pca should find the same projection as pca";"CODE"
"modulo the sign direction";"-"
"fit only the first four components fifth is near zero eigenvalue so";"IRRE"
"can be trimmed due to roundoff error";"CODE"
"n components none default remove zero eig is true";"CODE"
"assert that even with all np warnings on there is no div by zero warning";"CODE"
"there might be warnings about the kernel being badly conditioned";"-"
"but there should not be warnings about division by zero";"META"
"numpy division by zero warning can have many message variants but";"META"
"at least we know that it is a runtimewarning so let s check only this";"CODE"
"fit then transform";"CODE"
"do both at once";"CODE"
"compare";"IRRE"
"non regression test for issue 12140 pr 12145";"IRRE"
"generate random data";"IRRE"
"reference full";"CODE"
"arpack";"-"
"check that the result is still correct despite the approx";"TASK"
"randomized";"IRRE"
"check that the result is still correct despite the approximation";"TASK"
"compare the shapes corresponds to the number of non zero eigenvalues";"IRRE"
"non regression test for 26280";"IRRE"
"from sklearn decomposition import nmf as nmf for testing internals";"CODE"
"test that initialization does not return negative values";"IRRE"
"here we only check for invalid parameter values that are not already";"IRRE"
"automatically tested in the common tests";"IRRE"
"test nndsvd error";"IRRE"
"test that initialize nmf error is less than the standard deviation of";"IRRE"
"the entries in the matrix";"CODE"
"test nndsvd variants correctness";"IRRE"
"test that the variants nndsvda and nndsvdar differ from basic";"CODE"
"nndsvd only where the basic version has zeros";"META"
"ignore userwarning raised when both solver mu and init nndsvd";"IRRE"
"test that the decomposition does not contain negative values";"IRRE"
"test that the fit is not too far away";"IRRE"
"test that the fit is not too far away from an exact solution";"IRRE"
"by construction";"CODE"
"test that fit transform is equivalent to fit transform for nmf";"CODE"
"test that nmf transform returns close values";"IRRE"
"test that fit transform is equivalent to fit transform for minibatchnmf";"CODE"
"only guaranteed with fresh restarts";"-"
"smoke test that checks if nmf transform works with custom initialization";"IRRE"
"test that nmf inverse transform returns close values";"IRRE"
"test that minibatchnmf transform followed by minibatchnmf inverse transform";"IRRE"
"is close to the identity";"IRRE"
"smoke test for the case of more components than features";"CODE"
"test that sparse matrices are accepted as input";"IRRE"
"test that transform works on sparse data issue 2124";"IRRE"
"test that the function is called in the same way either directly";"IRRE"
"or through the nmf class";"IRRE"
"note that the validity of parameter types and range of possible values";"IRRE"
"for scalar numerical or str parameters is already checked in the common";"CODE"
"tests here we only check for problems that cannot be captured by simple";"IRRE"
"declarative constraints on the valid parameter values";"IRRE"
"test parameters checking in public function";"IRRE"
"compare beta divergence with the reference beta divergence dense";"IRRE"
"initialization";"IRRE"
"test the function that computes np dot w h only where x is non zero";"CODE"
"test that both results have same values in x csr nonzero elements";"IRRE"
"test that wh safe and x csr have the same sparse structure";"IRRE"
"compare sparse and dense input in multiplicative update nmf";"IRRE"
"also test continuity of the results with respect to beta loss parameter";"IRRE"
"initialization";"IRRE"
"reference with dense array x";"CODE"
"compare with sparse x";"IRRE"
"compare with almost same beta loss since some values have a specific";"IRRE"
"behavior but the results should be continuous w r t beta loss";"IRRE"
"test that an error is raised if beta loss 0 and x contains zeros";"IRRE"
"test that the output has not nan values when the input contains zeros";"IRRE"
"check verbose mode of minibatchnmf for better coverage";"IRRE"
"check that n components is correctly inferred";"-"
"from the provided custom initialization";"IRRE"
"check that n components is correctly inferred from the provided";"CODE"
"custom initialization";"IRRE"
"tests that non negative factorization does not fail when setting";"IRRE"
"n components auto also tests that the inferred n component";"IRRE"
"value is the right one";"IRRE"
"should not fail";"-"
"check that warnings are raised if user provided w and h are not used";"OUTD"
"and initialization overrides value of w or h";"IRRE"
"when update h is false w is ignored regardless of init";"IRRE"
"todo use the provided w when init custom";"IRRE"
"check that an informative error is raised when custom initialization does not";"CODE"
"have the right shape";"-"
"create 3 topics and each topic has 3 distinct words";"IRRE"
"each word only belongs to a single topic";"CODE"
"default prior parameter should be 1 topics";"IRRE"
"and verbose params should not affect result";"IRRE"
"test lda batch learning offset fit method with batch learning";"IRRE"
"find top 3 words in each lda component";"-"
"test lda online learning fit method with online learning";"IRRE"
"find top 3 words in each lda component";"-"
"test lda online learning partial fit method";"IRRE"
"same as test lda batch";"IRRE"
"test lda with dense input";"IRRE"
"find top 3 words in each lda component";"-"
"test lda transform";"IRRE"
"transform result cannot be negative and should be normalized by default";"CODE"
"test lda fit transform transform";"CODE"
"fit transform and transform result should be the same";"CODE"
"test pass dense matrix with sparse negative input";"IRRE"
"test perplexity before fit";"IRRE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"test lda batch training with multi cpu";"IRRE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"test lda online training with multi cpu";"IRRE"
"test dimension mismatch in perplexity method";"IRRE"
"invalid samples";"OUTD"
"invalid topic number";"OUTD"
"test lda perplexity for batch training";"IRRE"
"perplexity should be lower after each iteration";"-"
"test lda score for batch training";"IRRE"
"score should be higher after each iteration";"-"
"test lda perplexity for sparse and dense input";"IRRE"
"score should be the same for both dense and sparse input";"CODE"
"test the relationship between lda score and perplexity";"IRRE"
"test that the perplexity computed during fit is consistent with what is";"IRRE"
"returned by the perplexity method";"IRRE"
"perplexity computed at end of fit method";"CODE"
"result of perplexity method on the train set";"IRRE"
"pytest mark thread unsafe manually captured stdout";"CODE"
"sparse m and sparse n could be larger but be aware";"IRRE"
"scipy s generation of random sparse matrix can be costly";"IRRE"
"a sparse m sparse n dense array is allocated to compare against";"IRRE"
"sparse m sparse n 1000 300 arbitrary";"IRRE"
"check the shape of fit transform";"CODE"
"check the equivalence of fit transform and fit transform";"CODE"
"test get covariance and get precision";"IRRE"
"test if we avoid numpy warnings for computing over empty arrays";"CODE"
"n features n components 2 anything n comps triggered it in 0 16";"TASK"
"check that pca output has unit variance";"IRRE"
"some low rank data with correlated features";"TASK"
"the component wise variance of the first 50 features is 3 times the";"TASK"
"mean component wise variance of the remaining 30 features";"CODE"
"the component wise variance is thus highly varying";"CODE"
"whiten the data while projecting to the lower dim subspace";"CODE"
"x x copy make sure we keep an original across iterations";"-"
"test fit transform";"IRRE"
"in that case the output components still have varying variances";"CODE"
"we always center so no test for non centering";"TASK"
"with a non zero tail strength the data is actually full rank";"META"
"only check for a truncated result with a large number of iterations";"CODE"
"to make sure that we can recover precise results";"IRRE"
"test all components except the last one which cannot be estimated by";"CODE"
"arpack";"-"
"test all components to high precision";"IRRE"
"for some choice of n components and data distribution some components";"CODE"
"might be pure noise let s ignore them in the comparison";"CODE"
"as a result the output of fit transform should be the same";"IRRE"
"and similarly for the output of transform on new data except for the";"CODE"
"last component that can be underdetermined";"-"
"check that inverse transform reconstructions for both solvers are";"CODE"
"compatible";"-"
"in this case the models should have learned the same invertible";"CODE"
"transform they should therefore both be able to reconstruct the test";"CODE"
"data";"-"
"in the absence of noisy components both models should be able to";"CODE"
"reconstruct the same low rank approximation of the original data";"CODE"
"when n features n samples and n components is larger than the rank";"TASK"
"of the training set the output of the inverse transform function";"IRRE"
"is ill defined we can only check that we reach the same fixed point";"CODE"
"after another round of transform";"CODE"
"compare to the frobenius norm";"IRRE"
"compare to the 2 norms of the score vectors";"IRRE"
"set the singular values and see what er get back";"IRRE"
"test that the projection of data is correct";"IRRE"
"test that the projection of data is correct";"IRRE"
"test that the projection of data can be inverted";"IRRE"
"x rng randn n p spherical data";"IRRE"
"x 1 0 00001 make middle component relatively small";"-"
"x 5 4 3 make a large mean";"-"
"same check that we can find the original data from the transformed";"CODE"
"signal since the data is almost of rank n components";"-"
"ensures that solver specific extreme inputs for the n components";"CODE"
"parameter raise errors";"IRRE"
"mallest d 2 the smallest dimension";"CODE"
"additional case for arpack";"CODE"
"ensure that n components mle doesn t raise error for auto full";"CODE"
"ensure that n components mle will raise an error for unsupported";"CODE"
"solvers";"-"
"check automated dimensionality setting";"IRRE"
"todo explain what this is testing";"IRRE"
"or at least use explicit variable names";"IRRE"
"todo explain what this is testing";"IRRE"
"or at least use explicit variable names";"IRRE"
"iris data 0 95 2 row col";"-"
"iris data 0 01 1 row col";"-"
"row col";"-"
"test that probabilistic pca scoring yields a reasonable score";"IRRE"
"check that probabilistic pca selects the right model";"CODE"
"sanity check for the noise variance for more details see";"CODE"
"https github com scikit learn scikit learn issues 7568";"CODE"
"https github com scikit learn scikit learn issues 8541";"CODE"
"https github com scikit learn scikit learn issues 8544";"CODE"
"check the consistency of score between solvers";"-"
"arpack raises valueerror for n components min n samples n features";"CODE"
"ensure that noise variance is 0 in edge cases";"CODE"
"when n components min n samples n features";"TASK"
"non regression test for gh 12489";"IRRE"
"ensure no divide by zero error for n components n features n samples";"TASK"
"non regression test for gh 12489";"IRRE"
"ensure no divide by zero error for n components n samples n features";"TASK"
"case n samples 10 n features and max x shape 500 full";"TASK"
"case n samples 10 n features and n features 500 covariance eigh";"TASK"
"case n components 8 min x shape full";"CODE"
"n components 1 and n components 8 min x shape randomized";"IRRE"
"case n components in 0 1 full";"CODE"
"ensure that pca does not upscale the dtype when input is float32";"CODE"
"the atol and rtol are set such that the test passes for all random seeds";"IRRE"
"on all supported platforms on our ci and conda forge with the default";"CODE"
"random seed";"IRRE"
"ensure that all int types will be upcast to float64";"CODE"
"when n components is the second highest cumulative sum of the";"-"
"explained variance ratio then n components should equal the";"CODE"
"number of features in the dataset 15669";"TASK"
"test error when tested rank not in 1 n features 1";"IRRE"
"test rank associated with tiny eigenvalues are given a log likelihood of";"IRRE"
"inf the inferred rank will be 1";"-"
"test mle with pathological x only one relevant feature should give a";"TASK"
"rank of 1";"-"
"tests that an error is raised when the number of samples is smaller";"IRRE"
"than the number of features during an mle fit";"TASK"
"non regression test for issue";"IRRE"
"https github com scikit learn scikit learn issues 16730";"CODE"
"x 1 np mean x 1 axis 1 true x dim is ndim 1";"CODE"
"make sure assess dimension works properly on a matrix of rank 1";"-"
"x np ones n samples n features rank 1 matrix";"TASK"
"except for rank 1 all eigenvalues are 0 resp close to 0 fp";"IRRE"
"the default value of n oversamples will lead to inaccurate results";"IRRE"
"we force it to the number of features";"TASK"
"random state 0 how to use global random seed here";"IRRE"
"pca with mle cannot use check array api input and values because of";"IRRE"
"rounding errors in the noisy low variance components even checking";"CODE"
"the shape of the components is problematic because the number of";"IRRE"
"components depends on trimming threshold of the mle algorithm which";"CODE"
"can depend on device specific rounding errors";"CODE"
"simpler variant of the generic check array api input checker tailored for";"CODE"
"the specific case of pca with mle trimmed components";"CODE"
"check that the explained variance values match for the";"IRRE"
"common components";"-"
"if the number of components differ check that the explained variance of";"CODE"
"the trimmed components is very small";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"y is defined by y uv noise";"CODE"
"y 0 1 rng randn y shape 0 y shape 1 add noise";"TASK"
"sparsepca can be a bit slow to avoid having test times go up we";"IRRE"
"test different aspects of the code in the same test";"IRRE"
"test overcomplete decomposition";"TASK"
"y generate toy data 3 10 8 8 random state rng wide array";"IRRE"
"test that cd gives similar results";"IRRE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"y generate toy data 3 10 8 8 random state rng wide array";"IRRE"
"test multiple cpus";"IRRE"
"test that sparsepca won t return nan when there is 0 feature in all";"IRRE"
"samples";"-"
"y generate toy data 3 10 8 8 random state rng wide array";"IRRE"
"y generate toy data 3 65 8 8 random state rng tall array";"IRRE"
"test overcomplete decomposition";"TASK"
"verify output matrix dtype";"IRRE"
"verify numericall consistentency among np float32 and np float64";"CODE"
"vary the tolerance to force the early stopping of one of the model";"CODE"
"force the max number of no improvement to a large value to check that";"TASK"
"it does help to early stop";"CODE"
"elf store covariance store covariance used only in svd solver";"CODE"
"elf tol tol used only in svd solver";"CODE"
"sw self covariance within scatter";"CODE"
"st cov x shrinkage covariance estimator total scatter";"CODE"
"sb st sw between scatter";"-"
"evecs evecs np argsort evals 1 sort eigenvectors";"IRRE"
"1 within univariate scaling by with classes std dev";"CODE"
"avoid division by zero in normalization";"CODE"
"2 within variance scaling";"CODE"
"svd of centered within scaled data";"CODE"
"scaling of within covariance is v 1 s";"CODE"
"3 between variance scaling";"CODE"
"scale weighted centers";"-"
"centers are living in a space with n classes 1 dim maximum";"IRRE"
"use svd to find projection in the space spanned by the";"CODE"
"n classes centers";"IRRE"
"lineardiscriminantanalysis covariance estimator is not validated yet";"TASK"
"if self priors is none estimate priors from sample";"CODE"
"cnts xp unique counts y non negative ints";"CODE"
"maximum number of components no matter what n components is";"-"
"specified";"-"
"if size self classes 2 treat binary case as a special case";"CODE"
"only overrides for the docstring";"CODE"
"caling rotation linalg eigh cov scalings are eigenvalues";"IRRE"
"rotation rotation np argsort scaling 1 sort eigenvectors";"IRRE"
"caling scaling np argsort scaling 1 sort eigenvalues";"IRRE"
"xc u s v t";"-"
"caling s 2 n samples 1 scalings are squared singular values";"IRRE"
"cov v s 2 n 1 v t";"-"
"support for shrinkage could be implemented as in";"TASK"
"https github com scikit learn scikit learn issues 32590";"CODE"
"return log posterior see eq 4 12 p 110 of the esl";"IRRE"
"norm2 np array norm2 t shape len x n classes";"IRRE"
"only overrides for the docstring";"CODE"
"draw sample indices";"-"
"retrieve settings";"IRRE"
"build estimators";"-"
"draw random feature sample indices using normalized sample weight";"IRRE"
"as probabilities if provided";"-"
"note row sampling can be achieved either through setting sample weight or";"TASK"
"by indexing the former is more memory efficient therefore use this method";"CODE"
"if possible otherwise use indexing";"-"
"row sampling by setting sample weight";"IRRE"
"row sampling by indexing";"-"
"basebagging estimator is not validated yet";"TASK"
"convert data x is required to be 2d and indexable";"CODE"
"remap output";"IRRE"
"check parameters";"IRRE"
"validate max samples";"-"
"store validated integer row sampling value";"IRRE"
"validate max features";"TASK"
"store validated integer feature sampling value";"TASK"
"store sample weight needed in get estimators indices note that";"TASK"
"we intentionally do not materialize sample weight none as an array";"CODE"
"of ones to avoid unnecessarily cluttering trained estimator pickles";"CODE"
"other checks";"-"
"free allocated memory if any";"-"
"parallel loop";"IRRE"
"advance random state to state after training";"IRRE"
"the first n estimators";"-"
"reduce";"-"
"todo slep6 remove if condition for unrouted sample weight when metadata";"CODE"
"routing can t be disabled";"-"
"set parameters";"IRRE"
"don t instantiate estimators now parameters of estimator might";"IRRE"
"still change eg when grid searching with the nested object syntax";"CODE"
"self estimators needs to be filled by the derived classes in fit";"CODE"
"compute the number of jobs";"-"
"partition estimators between jobs";"-"
"defined by metaestimatormixin";"CODE"
"if estimators does not comply with our api list of tuples then it will";"CODE"
"fail in this case we assume that allow nan and sparse are false but";"CODE"
"the parameter validation will raise an error during fit";"IRRE"
"pass pragma no cover";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"validate or convert input data";"CODE"
"compute missing values in feature mask checks if x has missing values and";"IRRE"
"will raise an error if the underlying tree base estimator can t handle missing";"CODE"
"values only the criterion is required to determine if the tree supports";"IRRE"
"missing values";"IRRE"
"pre sort indices to avoid that each individual tree of the";"CODE"
"ensemble sorts the indices";"-"
"reshape is necessary to preserve the data contiguity against vs";"-"
"np newaxis that does not";"CODE"
"free allocated memory if any";"-"
"we draw from the random state to get the random state we";"IRRE"
"would have got if we hadn t used a warm start";"-"
"parallel loop we prefer the threading backend as the cython code";"CODE"
"for fitting the trees is internally releasing the python gil";"CODE"
"making threading more efficient than multiprocessing in";"CODE"
"that case however for joblib 0 12 we respect any";"CODE"
"parallel backend contexts set at a higher level";"IRRE"
"since correctness does not rely on using threads";"CODE"
"collect newly grown trees";"CODE"
"fixme we could consider to support multiclass multioutput if";"IRRE"
"we introduce or reuse a constructor parameter e g";"CODE"
"oob score allowing our user to pass a callable defining the";"IRRE"
"scoring strategy on oob sample";"-"
"decapsulate classes attributes";"IRRE"
"prediction requires x to be in csr format";"CODE"
"n classes is a ndarray at this stage";"CODE"
"all the supported type of target will have the same number of";"-"
"classes in all outputs";"IRRE"
"for regression n classes does not exist and we create an empty";"CODE"
"axis to be consistent with the classification case and make";"CODE"
"the array operations compatible with the 2 settings";"IRRE"
"default implementation";"TASK"
"get drawn indices along both sample and feature axes";"TASK"
"tree random state is actually an immutable integer seed rather";"IRRE"
"than a mutable randomstate instance so it s safe to use it";"IRRE"
"repeatedly when calling this property";"IRRE"
"operations accessing random state must be performed identically";"IRRE"
"to those in parallel build trees";"-"
"only the criterion is required to determine if the tree supports";"CODE"
"missing values";"IRRE"
"binary and multiclass";"IRRE"
"roll the first n outputs axis to the last axis we will reshape";"IRRE"
"from a shape of n outputs n samples n classes to a shape of";"IRRE"
"n samples n classes n outputs";"IRRE"
"drop the n outputs axis if there is a single output";"IRRE"
"all dtypes should be the same so just take the first";"-"
"check data";"-"
"assign chunk of trees to jobs";"IRRE"
"avoid storing the output of every estimator by summing them here";"IRRE"
"check data";"-"
"assign chunk of trees to jobs";"IRRE"
"avoid storing the output of every estimator by summing them here";"IRRE"
"parallel loop";"IRRE"
"single output regression";"IRRE"
"multioutput regression";"IRRE"
"drop the n outputs axis if there is a single output";"IRRE"
"note we don t sum in parallel because the gil isn t released in";"TASK"
"the fast method";"-"
"average over the forest";"CODE"
"parameters are validated in fit transform";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"sample weight is always a ndarray never none";"-"
"header fields and line format str";"CODE"
"do oob";"CODE"
"print the header line";"CODE"
"plot verbose info each time i verbose mod 0";"IRRE"
"we need to take into account if we fit additional estimators";"TASK"
"i j self begin at stage iteration relative to the start iter";"CODE"
"adjust verbose frequency powers of 10";"IRRE"
"todo without oob i e with self subsample 1 0 we could call";"CODE"
"self loss loss gradient and use it to set train score";"IRRE"
"but note that train score i is the score after fitting the i th tree";"TASK"
"note we need the negative gradient";"TASK"
"ample weight none we pass sample weights to the tree directly";"CODE"
"2 d views of shape n samples n trees per iteration or n samples 1";"-"
"on neg gradient to simplify the loop over n trees per iteration";"IRRE"
"induce regression tree on the negative gradient";"-"
"no inplace multiplication";"-"
"update tree leaves";"CODE"
"add tree to ensemble";"TASK"
"do oob";"CODE"
"self n estimators is the number of additional est to fit";"TASK"
"if do oob resize arrays or create new if not available";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"ignore missing values when computing bin thresholds";"IRRE"
"the data will be sorted anyway in np unique and again in percentile so we do it";"CODE"
"here sorting also returns a contiguous array";"IRRE"
"we could compute approximate midpoint percentiles using the output of";"IRRE"
"np unique col data return counts instead but this is more";"CODE"
"work and the performance benefit will be limited because we";"CODE"
"work on a fixed size subsample of the full data";"-"
"we avoid having inf thresholds inf thresholds are only allowed in";"CODE"
"a split on nan situation";"-"
"min is 3 at least 2 distinct bins and a missing values bin";"IRRE"
"validate is categorical and known categories parameters";"IRRE"
"since categories are assumed to be encoded in";"-"
"0 n cats and since n cats max bins";"-"
"the thresholds are the unique categorical values this will";"IRRE"
"lead to the correct mapping in transform";"CODE"
"todo complexity is o n categorical features 255 maybe this is";"TASK"
"worth cythonizing";"-"
"if there is a preprocessor we let the preprocessor handle the validation";"CODE"
"otherwise we validate the data ourselves";"-"
"at this point reset is false which runs during fit";"CODE"
"check categories found by the ordinalencoder and get their encoded values";"IRRE"
"the columntransformer s output places the categorical features at the";"TASK"
"beginning";"-"
"ordinalencoder always puts np nan as the last category if the";"-"
"training data has missing values here we remove it because it is";"IRRE"
"already added by the binmapper";"TASK"
"special code for pandas because of a bug in recent pandas which is";"CODE"
"fixed in main and maybe included in 2 2 1 see";"CODE"
"https github com pandas dev pandas pull 57173";"CODE"
"also pandas versions 1 5 1 do not support the dataframe interchange";"CODE"
"at this point validate data was not called yet because we use the original";"IRRE"
"dtypes to discover the categorical features thus feature names in";"TASK"
"is not defined yet";"CODE"
"check for feature names";"TASK"
"check for categorical features as indices";"TASK"
"todo incorporate sample weights here in resample";"TASK"
"a higher score is always better higher tol means that it will be";"-"
"harder for subsequent iteration to be considered an improvement upon";"TASK"
"the reference score and therefore it is more likely to early stop";"CODE"
"because of the lack of significant improvement";"TASK"
"x binned self bin mapper fit transform x f aligned array";"CODE"
"x binned self bin mapper transform x f aligned array";"CODE"
"we convert the array to c contiguous since predicting is faster";"-"
"with this layout training is faster on f arrays though";"CODE"
"we intentionally decouple the number of threads used at prediction";"CODE"
"time from the number of threads used at fit time because the model";"CODE"
"can be deployed on a different machine for prediction purposes";"CODE"
"note that the learning rate is already accounted for in the leaves";"CODE"
"values";"IRRE"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"todo this could be done in parallel";"CODE"
"np argmax 0 5 0 5 is 0 not 1 therefore 0 not 0 to be";"CODE"
"consistent with the multiclass case";"CODE"
"np argmax 0 0 is 0 not 1 therefore 0 not 0";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"start and stop indices of the node in the splitter partition";"CODE"
"array concretely";"-"
"self sample indices view self splitter partition start stop";"CODE"
"please see the comments about splitter partition and";"-"
"splitter split indices for more info about this design";"CODE"
"these 2 attributes are only used in update raw prediction because we";"META"
"need to iterate over the leaves and i don t know how to efficiently";"TASK"
"store the sample indices views because they re all of different sizes";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"the dtype of feature idx is np intp which is platform dependent here we";"CODE"
"make sure that saving and loading on different bitness systems works without";"CODE"
"errors for instance on a 64 bit python runtime np intp np int64";"CODE"
"while on 32 bit np intp np int32";"CODE"
"todo consider always using platform agnostic dtypes for fitted";"CODE"
"estimator attributes for this particular estimator this would";"CODE"
"mean replacing the intp field of predictor record dtype by an int32";"CODE"
"field ideally this should be done consistently throughout";"CODE"
"scikit learn along with a common test";"IRRE"
"assert bin thresholds i shape 254 255 1";"CODE"
"assert bin thresholds i shape 127 128 1";"CODE"
"max bins is the number of bins for non missing values";"IRRE"
"check that the binned data is approximately balanced across bins";"-"
"max bins is the number of bins for non missing values";"IRRE"
"max bins is the number of bins for non missing values";"IRRE"
"adding more bins to the mapper yields the same results same thresholds";"TASK"
"max bins is the number of bins for non missing values";"IRRE"
"check that n bins non missing is n unique values when";"IRRE"
"there are not a lot of unique values else n bins 1";"IRRE"
"make sure bin thresholds are different when applying subsampling";"CODE"
"0 0 0 255 missing value";"IRRE"
"0 0 0 2 missing value";"IRRE"
"check for missing values make sure nans are mapped to the last bin";"IRRE"
"and that the binmapper attributes are correct";"META"
"make sure infinite values are properly handled";"IRRE"
"basic test for categorical features";"TASK"
"we make sure that categories are mapped into 0 n categories 1 and";"CODE"
"that nans are mapped to the last bin";"-"
"negative categories are mapped to the missing values bin";"IRRE"
"i e the bin of index missing values bin idx n bins 1";"IRRE"
"unknown positive categories does not happen in practice and tested";"IRRE"
"for illustration purpose";"CODE"
"make sure sklearn has the same predictions as lightgbm for easy targets";"CODE"
"in particular when the size of the trees are bound and the number of";"-"
"samples is large enough the structure of the prediction trees found by";"CODE"
"lightgbm and sklearn should be exactly identical";"-"
"notes";"TASK"
"several candidate splits may have equal gains when the number of";"-"
"samples in a node is low and because of float errors therefore the";"CODE"
"predictions on the test set might differ if the structure of the tree";"IRRE"
"is not exactly the same to avoid this issue we only compare the";"CODE"
"predictions on the test set when the number of samples is large enough";"IRRE"
"and max leaf nodes is low enough";"-"
"to ignore discrepancies caused by small differences in the binning";"CODE"
"strategy data is pre binned if n samples 255";"-"
"we don t check the absolute error loss here this is because";"CODE"
"lightgbm s computation of the median used for the initial value of";"IRRE"
"raw prediction is a bit off they ll e g return midpoints when there";"CODE"
"is no need to since these tests only run 1 iteration the";"IRRE"
"discrepancy between the initial values leads to biggish differences in";"IRRE"
"the predictions these differences are much smaller with more";"-"
"iterations";"-"
"make the target positive";"-"
"bin data and convert it to float32 so that the estimator doesn t";"CODE"
"treat it as pre binned";"-"
"we need x to be treated an numerical data not pre binned data";"-"
"more than 65 of the predictions must be close up to the 2nd decimal";"CODE"
"todo we are not entirely satisfied with this lax comparison but the root";"TASK"
"cause is not clear maybe algorithmic differences one such example is the";"META"
"poisson max delta step parameter of lightgbm which does not exist in hgbt";"TASK"
"less than 1 of the predictions may deviate more than 1e 3 in relative terms";"-"
"less than 1 of the predictions may deviate more than 1e 4 in relative terms";"-"
"same as test same predictions regression but for classification";"IRRE"
"bin data and convert it to float32 so that the estimator doesn t";"CODE"
"treat it as pre binned";"-"
"we need x to be treated an numerical data not pre binned data";"-"
"same as test same predictions regression but for classification";"IRRE"
"bin data and convert it to float32 so that the estimator doesn t";"CODE"
"treat it as pre binned";"-"
"we need x to be treated an numerical data not pre binned data";"-"
"assert more than 75 of the predicted probabilities are the same up to";"CODE"
"the second decimal";"-"
"assert more than 75 of the predicted probabilities are the same up";"CODE"
"to the second decimal";"-"
"take care that x coef intercept 0";"CODE"
"for an exponential distribution with rate lambda e g exp lambda x";"CODE"
"the quantile at level q is";"-"
"quantile q log 1 q lambda";"CODE"
"scale 1 lambda quantile q log 1 q";"CODE"
"we are overfitting";"-"
"test that valueerror is raised if either one y i 0 or sum y i 0";"IRRE"
"for poisson distributed target poisson loss should give better results";"IRRE"
"than least squares measured in poisson deviance as metric";"-"
"we create a log linear poisson model and downscale coef as it will get";"IRRE"
"exponentiated";"-"
"squared error might produce non positive predictions clip";"CODE"
"make sure training and validation data are binned separately";"-"
"see issue 13926";"-"
"note that since the data is small there is no subsampling and the";"TASK"
"random state doesn t matter";"IRRE"
"sanity check for missing values support with only one feature and";"IRRE"
"y isnan x the gbdt is supposed to reach perfect accuracy on the";"-"
"training set";"IRRE"
"make sure the estimators can deal with missing values and still yield";"IRRE"
"decent predictions";"-"
"non regression test for issue 14018";"IRRE"
"make sure we avoid zero division errors when computing the leaves values";"IRRE"
"if the learning rate is too high the raw predictions are bad and will";"-"
"saturate the softmax or sigmoid in binary classif this leads to";"CODE"
"probabilities being exactly 0 or 1 gradients being constant and";"CODE"
"hessians being zero";"-"
"make sure that the small trainset is stratified and has the expected";"IRRE"
"length 10k samples";"-"
"compute the small training set";"IRRE"
"compute the class distribution in the small training set";"IRRE"
"test that the small training set has the expected length";"IRRE"
"test that the class distributions in the whole dataset and in the small";"IRRE"
"training set are identical";"IRRE"
"compare the buit in missing value handling of histogram gbc with an";"IRRE"
"a priori missing value imputation strategy that should yield the same";"IRRE"
"results in terms of decision function";"CODE"
"each feature containing nans is replaced by 2 features";"TASK"
"one where the nans are replaced by min feature 1";"TASK"
"one where the nans are replaced by max feature 1";"TASK"
"a split where nans go to the left has an equivalent split in the";"CODE"
"first min feature and a split where nans go to the right has an";"TASK"
"equivalent split in the second max feature";"TASK"
"assuming the data is such that there is never a tie to select the best";"CODE"
"feature to split on during training the learned decision trees should be";"TASK"
"strictly equivalent learn a sequence of splits that encode the same";"-"
"decision function";"CODE"
"the minmaximputer transformer is meant to be a toy implementation of the";"TASK"
"missing in attributes mia missing value handling for decision trees";"IRRE"
"https www sciencedirect com science article abs pii s0167865508000305";"CODE"
"the implementation of mia as an imputation transformer was suggested by";"TASK"
"remark 3 in arxiv 1902 06931";"-"
"pre bin the data to ensure a deterministic handling by the 2";"CODE"
"strategies and also make it easier to insert np nan in a structured";"CODE"
"way";"-"
"first feature has missing values completely at random";"IRRE"
"second and third features have missing values for extreme values";"IRRE"
"censoring missingness";"-"
"make the last feature nan pattern very informative";"TASK"
"check that there is at least one missing value in each feature";"TASK"
"let s use a test set to check that the learned decision function is";"IRRE"
"the same as evaluated on unseen data otherwise it could just be the";"CODE"
"case that we find two independent ways to overfit the training set";"IRRE"
"n samples need to be large enough to minimize the likelihood of having";"TASK"
"several candidate splits with the same gain value in a given tree";"IRRE"
"use a small number of leaf nodes and iterations so as to keep";"-"
"under fitting models to minimize the likelihood of ties when training the";"CODE"
"model";"-"
"check that the model reach the same score";"-"
"check the individual prediction match as a finer grained";"-"
"decision function check";"CODE"
"basic test for infinite values";"IRRE"
"high level test making sure that inf and nan values are properly handled";"IRRE"
"when both are present this is similar to";"CODE"
"test split on nan with infinite values in test grower py though we";"IRRE"
"cannot check the predictions for binned values here";"IRRE"
"regression tests for 14709 where the targets need to be encoded before";"CODE"
"to compute the score";"-"
"make sure setting a sw to zero amounts to ignoring the corresponding";"IRRE"
"sample";"-"
"ignore the first 2 training samples by setting their weight to 0";"IRRE"
"make sure setting a sw to zero amounts to ignoring the corresponding";"IRRE"
"sample";"-"
"ignore the first 2 training samples by setting their weight to 0";"IRRE"
"ignore the first 2 training samples by setting their weight to 0";"IRRE"
"high level test to make sure that duplicating a sample is equivalent to";"IRRE"
"giving it weight of 2";"-"
"fails for n samples 255 because binning does not take sample weights";"CODE"
"into account keeping n samples 255 makes";"CODE"
"sure only unique values are used so sw have no effect on binning";"IRRE"
"this test can t pass if min samples leaf 1 because that would force 2";"IRRE"
"samples to be in the same node in est sw while these samples would be";"CODE"
"free to be separate in est dup est dup would just group together the";"CODE"
"duplicated samples";"-"
"create dataset with duplicate and corresponding sample weights";"IRRE"
"checking raw predict is stricter than just predict for classification";"CODE"
"for losses with constant hessians the sum hessians field of the";"CODE"
"histograms must be equal to the sum of the sample weight of samples at";"TASK"
"the corresponding bin";"-"
"while sample weights are supposed to be positive this still works";"CODE"
"build sum sample weight which contains the sum of the sample weights at";"-"
"each bin for each feature this must be equal to the sum hessians";"CODE"
"field of the corresponding histogram";"CODE"
"build histogram";"-"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 16179";"CODE"
"there was a bug when the max depth and the max leaf nodes criteria were";"-"
"met at the same time which would lead to max leaf nodes not being";"-"
"respected";"-"
"assert tree get n leaf nodes 3 would be 4 prior to bug fix";"CODE"
"non regression test for 16661 where second fit fails with";"IRRE"
"warm start true early stopping is on and no validation set";"IRRE"
"does not raise on second call";"CODE"
"raw predict should never be called with scoring as a string";"IRRE"
"for scorer is called twice train and val for the baseline score and twice";"CODE"
"per iteration train and val after that so 6 times in total for max iter 2";"CODE"
"non regression test for 22907";"IRRE"
"class weight is the same as sample weights with the corresponding class";"IRRE"
"check that sample weight and class weight are multiplicative";"IRRE"
"make imbalanced dataset";"IRRE"
"class weight balanced is the same as sample weights to be";"IRRE"
"inversely proportional to n samples n classes np bincount y";"IRRE"
"check that negative values from the second column are treated like a";"IRRE"
"missing category";"-"
"make f cat an informative feature";"TASK"
"check categories are correct and sorted";"-"
"construct a target with some noise";"CODE"
"construct categorical where 0 a and 1 b and 1 a and 0 b";"CODE"
"field names in node struct with np intp types see";"CODE"
"sklearn ensemble hist gradient boosting common pyx";"-"
"simulate loading a pickle of the same model trained on a platform with different";"CODE"
"bitness that than the platform it will be used to make predictions on";"OUTD"
"make sure that a platform specific pickle generated on a 64 bit";"CODE"
"platform can be converted at pickle load time into an estimator";"CODE"
"with cython code that works with the host s native integer precision";"CODE"
"to index nodes in the tree data structure when the host is a 32 bit";"CODE"
"platform and vice versa";"CODE"
"this is in particular useful to be able to train a model on a 64 bit linux";"CODE"
"server and deploy the model as part of a 32 bit wasm in browser";"IRRE"
"application using pyodide";"-"
"non regression test for https github com scikit learn scikit learn issues 28317";"CODE"
"generate some test data directly binned so as to test the grower code";"IRRE"
"independently of the binning logic";"CODE"
"assume a square loss applied to an initial model that always predicts 0";"IRRE"
"hardcoded for this test";"CODE"
"make sure the samples are correctly dispatched from a parent to its";"CODE"
"children";"-"
"each sample from the parent is propagated to one of the two children";"CODE"
"samples are sent either to the left or the right node never to both";"-"
"the root node is not yet split but the best possible split has";"TASK"
"already been evaluated";"CODE"
"calling split next applies the next split and computes the best split";"IRRE"
"for each of the two newly introduced children nodes";"CODE"
"all training samples have ben split in the two nodes approximately";"CODE"
"50 50";"-"
"the left node is too pure there is no gain to split it further";"CODE"
"the right node can still be split further this time on feature 1";"TASK"
"the right split has not been applied yet let s do it now";"CODE"
"all the leafs are pure it is not possible to split any further";"CODE"
"check the values of the leaves";"IRRE"
"build a tree on the toy 3 leaf dataset to extract the predictor";"IRRE"
"assert grower n nodes 5 2 decision nodes 3 leaves";"CODE"
"check that the node structure can be converted into a predictor";"CODE"
"object to perform predictions at scale";"CODE"
"we pass undefined binning thresholds because we won t use predict anyway";"CODE"
"probe some predictions for each leaf of the tree";"CODE"
"each group of 3 samples corresponds to a condition in make training data";"-"
"check that training set can be recovered exactly";"IRRE"
"data linear target 3 features 1 irrelevant";"TASK"
"make sure root node isn t split if n samples is not at least twice";"-"
"min samples leaf";"-"
"data linear target 3 features 1 irrelevant";"TASK"
"to assert that stumps are created when max depth 1";"IRRE"
"make sure max depth parameter works as expected";"IRRE"
"data linear target 3 features 1 irrelevant";"TASK"
"make sure that missing values are supported at predict time even if they";"IRRE"
"were not encountered in the training data the missing values are";"IRRE"
"assigned to whichever child has the most samples";"IRRE"
"we pass undefined binning thresholds because we won t use predict anyway";"CODE"
"go from root to a leaf always following node with the most samples";"CODE"
"that s the path nans are supposed to take";"-"
"now build x test with only nans and make sure all predictions are equal";"IRRE"
"to prediction main path";"CODE"
"make sure the split on nan situations are respected even when there are";"-"
"samples with inf values we set the threshold to inf when we have a";"IRRE"
"split on nan so this test makes sure this does not introduce edge case";"CODE"
"bugs we need to use the private api so that we can also test";"IRRE"
"predict binned";"-"
"the gradient values will force a split on nan situation";"IRRE"
"sanity check this was a split on nan";"CODE"
"make sure in particular that the inf sample is mapped to the left child";"-"
"note that lightgbm fails here and will assign the inf sample to the";"TASK"
"right child even though it s a split on nan situation";"-"
"check that the grower produces the right predictor tree when a split is";"-"
"categorical";"-"
"arbitrary validation but this means ones go to the left";"META"
"check binned category value 1";"IRRE"
"check raw category value 9";"IRRE"
"note that since there was no missing values during training the missing";"IRRE"
"values aren t part of the bitsets however we expect the missing values";"IRRE"
"to go to the biggest child i e the left one";"-"
"the left child has a value of 1 negative gradient";"IRRE"
"make sure binned missing values are mapped to the left child during";"IRRE"
"prediction";"-"
"assert allclose prediction binned 1 negative gradient";"CODE"
"make sure raw missing values are mapped to the left child during";"IRRE"
"prediction";"-"
"known cat bitsets np zeros 1 8 dtype np uint32 ignored anyway";"IRRE"
"make sure that native categorical splits are equivalent to using a ohe";"-"
"when given enough depth";"-"
"we pass undefined bin thresholds because we won t use predict";"CODE"
"ohe needs more splits to achieve the same predictions";"TASK"
"small sample indices below unrolling threshold";"-"
"larger sample indices above unrolling threshold";"-"
"make sure the order of the samples has no impact on the histogram";"-"
"computations";"-"
"make sure the different unrolled histogram computations give the same";"-"
"results as the naive one";"IRRE"
"make sure the histogram subtraction trick gives the same result as the";"IRRE"
"classical method";"IRRE"
"make sure leaves values from left to right are either all increasing";"IRRE"
"or all decreasing or neither depending on the monotonic constraint";"CODE"
"init gradients and hessians to that of least squares loss";"IRRE"
"make sure infinite values and infinite thresholds are handled properly";"IRRE"
"in particular if a value is inf and the threshold is almost inf the";"IRRE"
"sample should go to the right child if the threshold is inf split on";"-"
"nan the inf sample will go to the left child";"-"
"we just construct a simple tree with 1 root and 2 children";"CODE"
"parent node";"-"
"left child";"-"
"right child";"-"
"test predictor outputs are correct with categorical features";"IRRE"
"we just construct a simple tree with 1 root and 2 children";"CODE"
"parent node";"-"
"left child";"-"
"right child";"-"
"check binned data gives correct predictions";"-"
"manually construct bitset";"CODE"
"check with un binned data";"-"
"check missing goes left because missing values bin idx 6";"IRRE"
"missing and unknown go left";"-"
"constant hessian 1 per sample";"CODE"
"this test checks that the values of gradients and hessians are";"IRRE"
"consistent in different places";"-"
"in split info si sum gradient left si sum gradient right must be";"TASK"
"equal to the gradient at the node same for hessians";"CODE"
"in the histograms summing sum gradients over the bins must be";"TASK"
"constant across all features and those sums must be equal to the";"TASK"
"node s gradient same for hessians";"CODE"
"make sure that si sum gradient left si sum gradient right have their";"-"
"expected value same for hessians";"IRRE"
"make sure sum of gradients in histograms are the same for all features";"TASK"
"and make sure they re equal to their expected value";"IRRE"
"note gradients and hessians have shape n features";"TASK"
"we re comparing them to scalars this has the benefit of also";"CODE"
"making sure that all the entries are equal across features";"TASK"
"gradients hists sum gradients sum axis 1 shape n features";"TASK"
"expected gradient all gradients indices sum scalar";"-"
"0 is not the actual hessian but it s not computed in this case";"CODE"
"check that split indices returns the correct splits and that";"IRRE"
"splitter partition is consistent with what is returned";"IRRE"
"split will happen on feature 1 and on bin 3";"TASK"
"sanity checks for best split";"CODE"
"check that the resulting split indices sizes are consistent with the";"IRRE"
"count statistics anticipated when looking for the best split";"CODE"
"try to split a pure node all gradients are equal same for hessians";"CODE"
"with min gain to split 0 and make sure that the node is not split best";"CODE"
"possible gain 1 note before the strict inequality comparison this";"CODE"
"test would fail because the node would be split with a gain of 0";"IRRE"
"basic sanity check with no missing values given the gradient";"IRRE"
"values the split must occur on bin idx 3";"IRRE"
"0 1 2 3 4 5 6 7 8 9 x binned";"-"
"1 1 1 1 5 5 5 5 5 5 gradients";"-"
"false no missing values";"IRRE"
"10 n bins non missing";"-"
"false don t split on nans";"CODE"
"3 expected bin idx";"-"
"we replace 2 samples by nans bin idx 8";"-"
"these 2 samples were mapped to the left node before so they should";"CODE"
"be mapped to left node again";"-"
"notice how the bin idx threshold changes from 3 to 1";"CODE"
"8 0 1 8 2 3 4 5 6 7 8 missing";"-"
"true missing values";"IRRE"
"8 n bins non missing";"-"
"false don t split on nans";"CODE"
"1 cut on bin idx 1";"-"
"missing values go to left";"IRRE"
"same as above but with non consecutive missing values bin";"IRRE"
"9 0 1 9 2 3 4 5 6 7 9 missing";"-"
"true missing values";"IRRE"
"8 n bins non missing";"-"
"false don t split on nans";"CODE"
"1 cut on bin idx 1";"-"
"missing values go to left";"IRRE"
"this time replacing 2 samples that were on the right";"CODE"
"0 1 2 3 8 4 8 5 6 7 8 missing";"-"
"true missing values";"IRRE"
"8 n bins non missing";"-"
"false don t split on nans";"CODE"
"3 cut on bin idx 3 like in first case";"CODE"
"missing values go to right";"IRRE"
"same as above but with non consecutive missing values bin";"IRRE"
"0 1 2 3 9 4 9 5 6 7 9 missing";"-"
"true missing values";"IRRE"
"8 n bins non missing";"-"
"false don t split on nans";"CODE"
"3 cut on bin idx 3 like in first case";"CODE"
"missing values go to right";"IRRE"
"for the following cases split on nans is true we replace all of";"CODE"
"the samples with nans instead of just 2";"CODE"
"0 1 2 3 4 4 4 4 4 4 4 missing";"-"
"true missing values";"IRRE"
"4 n bins non missing";"-"
"true split on nans";"-"
"3 cut on bin idx 3";"-"
"missing values go to right";"IRRE"
"same as above but with non consecutive missing values bin";"IRRE"
"0 1 2 3 9 9 9 9 9 9 9 missing";"-"
"true missing values";"IRRE"
"4 n bins non missing";"-"
"true split on nans";"-"
"3 cut on bin idx 3";"-"
"missing values go to right";"IRRE"
"6 6 6 6 0 1 2 3 4 5 6 missing";"-"
"true missing values";"IRRE"
"6 n bins non missing";"-"
"true split on nans";"-"
"5 cut on bin idx 5";"-"
"missing values go to right";"IRRE"
"same as above but with non consecutive missing values bin";"IRRE"
"9 9 9 9 0 1 2 3 4 5 9 missing";"-"
"true missing values";"IRRE"
"6 n bins non missing";"-"
"true split on nans";"-"
"5 cut on bin idx 5";"-"
"missing values go to right";"IRRE"
"make sure missing values are properly supported";"IRRE"
"we build an artificial example with gradients such that the best split";"-"
"is on bin idx 3 when there are no missing values";"IRRE"
"then we introduce missing values and";"IRRE"
"make sure the chosen bin is correct find best bin it s";"IRRE"
"still the same split even though the index of the bin may change";"TASK"
"make sure the missing values are mapped to the correct child";"IRRE"
"split indices";"-"
"make sure the split is properly computed";"-"
"this also make sure missing values are properly assigned to the correct";"IRRE"
"child in split indices";"-"
"when we don t split on nans the split should always be the same";"CODE"
"when we split on nans samples with missing values are always mapped";"IRRE"
"to the right child";"-"
"one category";"-"
"all categories appear less than min cat support hardcoded to 10";"-"
"only one category appears more than min cat support";"-"
"missing values category appear less than min cat support";"IRRE"
"9 is missing";"-"
"no non missing category";"-"
"checks categorical splits are correct when the min cat support constraint";"CODE"
"isn t respected there are no splits";"-"
"no split found";"-"
"assert that the bitset exactly corresponds to the categories";"IRRE"
"bitset is assumed to be an array of 8 uint32 elements";"IRRE"
"form bitset from threshold";"CODE"
"check for equality";"CODE"
"4 categories";"-"
"0 1 2 3 11 x binned";"-"
"10 1 10 10 11 all gradients";"-"
"1 expected categories left";"-"
"4 n bins non missing";"-"
"4 missing values bin idx";"IRRE"
"false has missing values";"IRRE"
"expected missing go to left unchecked";"-"
"make sure that the categories that are on the right second half of";"-"
"the sorted categories array can still go in the left child in this";"CODE"
"case the best split was found when scanning from right to left";"CODE"
"0 1 2 3 11 x binned";"-"
"10 10 10 1 11 all gradients";"-"
"3 expected categories left";"-"
"4 n bins non missing";"-"
"4 missing values bin idx";"IRRE"
"false has missing values";"IRRE"
"expected missing go to left unchecked";"-"
"categories that don t respect min cat support cat 4 are always";"CODE"
"mapped to the right child";"-"
"0 1 2 3 11 4 5 x binned";"-"
"10 10 10 1 11 10 5 all gradients";"-"
"3 expected categories left";"-"
"4 n bins non missing";"-"
"4 missing values bin idx";"IRRE"
"false has missing values";"IRRE"
"expected missing go to left unchecked";"-"
"categories that don t respect min cat support are always mapped to";"CODE"
"the right child in this case a more sensible split could have been";"CODE"
"3 4 0 1 2";"-"
"but the split is still 3 0 1 2 4 this is because we only scan";"TASK"
"up to the middle of the sorted category array 0 1 2 3 and";"-"
"because we exclude cat 4 in this array";"CODE"
"0 1 2 3 11 4 5 x binned";"-"
"10 10 10 1 11 1 5 all gradients";"-"
"3 expected categories left";"-"
"4 n bins non missing";"-"
"4 missing values bin idx";"IRRE"
"false has missing values";"IRRE"
"expected missing go to left unchecked";"-"
"4 categories with missing values that go to the right";"IRRE"
"0 1 2 11 9 11 x binned";"-"
"10 1 10 11 10 11 all gradients";"-"
"1 expected categories left";"-"
"3 n bins non missing";"-"
"9 missing values bin idx";"IRRE"
"true has missing values";"IRRE"
"expected missing go to left";"-"
"4 categories with missing values that go to the left";"IRRE"
"0 1 2 11 9 11 x binned";"-"
"10 1 10 11 1 11 all gradients";"-"
"1 9 expected categories left";"-"
"3 n bins non missing";"-"
"9 missing values bin idx";"IRRE"
"true has missing values";"IRRE"
"expected missing go to left";"-"
"split is on the missing value";"IRRE"
"0 1 2 3 4 11 255 12 x binned";"-"
"10 10 10 10 10 11 1 12 all gradients";"-"
"255 expected categories left";"-"
"5 n bins non missing";"-"
"255 missing values bin idx";"IRRE"
"true has missing values";"IRRE"
"expected missing go to left";"-"
"split on even categories";"-"
"list range 60 12 x binned";"-"
"10 1 360 all gradients";"-"
"list range 1 60 2 expected categories left";"-"
"59 n bins non missing";"-"
"59 missing values bin idx";"IRRE"
"true has missing values";"IRRE"
"expected missing go to left";"-"
"split on every 8 categories";"-"
"list range 256 12 x binned";"-"
"10 10 10 10 10 10 10 1 384 all gradients";"-"
"list range 7 256 8 expected categories left";"-"
"255 n bins non missing";"-"
"255 missing values bin idx";"IRRE"
"true has missing values";"IRRE"
"expected missing go to left";"-"
"tests various combinations of categorical splits";"IRRE"
"if there is no missing value during training the flag missing go to left";"IRRE"
"is set later in the grower";"IRRE"
"make sure samples are split correctly";"-"
"unmapped xgb parameters";"IRRE"
"min samples leaf";"-"
"min data in bin";"-"
"min split gain there is min split loss though";"CODE"
"unmapped catboost parameters";"IRRE"
"max leaves";"-"
"min";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"note we use threading here as the predict method is not cpu bound";"TASK"
"note we use threading here as the decision function method is";"CODE"
"not cpu bound";"-"
"note we use threading here as the score samples method is not cpu bound";"TASK"
"preds is here a list of n targets 2d ndarrays of";"-"
"n classes columns the k th column contains the";"IRRE"
"probabilities of the samples belonging the k th class";"IRRE"
"since those probabilities must sum to one for each sample";"CODE"
"we can work with probabilities of n classes 1 classes";"IRRE"
"hence we drop the first column";"-"
"some estimator return a 1d array for predictions";"CODE"
"which must be 2 dimensional arrays";"TASK"
"remove the first column when using probabilities in";"-"
"binary classification because both features preds are perfectly";"TASK"
"collinear";"-"
"estimators in stacking estimators are not validated yet";"TASK"
"all estimators contains all estimators the one to be fitted and the";"-"
"drop string";"CODE"
"fit the base estimators on the whole training data those";"CODE"
"base estimators will be used in transform predict and";"CODE"
"predict proba they are exposed publicly";"CODE"
"generate predictions from prefit models";"CODE"
"to train the meta classifier using the most data as possible we use";"CODE"
"a cross validation to obtain the output of the stacked estimators";"IRRE"
"to ensure that the data provided to each estimator are the same";"-"
"we need to set the random state of the cv if there is one and we";"IRRE"
"need to take a copy";"TASK"
"only not none or not drop estimators will be used in transform";"CODE"
"remove the none from the method as well";"CODE"
"final estimator is wrapped in a parallel block to show the label";"CODE"
"final estimator in the html repr";"CODE"
"self estimators is a list of name est tuples";"CODE"
"todo slep6 remove when metadata routing cannot be disabled";"TASK"
"handle the multilabel indicator case";"CODE"
"handle the multilabel indicator cases";"CODE"
"if final estimator s default changes then this should be";"CODE"
"updated";"CODE"
"todo slep6 remove when metadata routing cannot be disabled";"TASK"
"if final estimator s default changes then this should be";"CODE"
"updated";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"for consistency with other estimators we raise an attributeerror so";"CODE"
"that hasattr fails if the estimator isn t fitted";"-"
"self estimators is a list of name est tuples";"CODE"
"estimators in votingclassifier estimators are not validated yet";"TASK"
"raise a specific valueerror for non classification tasks";"CODE"
"raise a notimplementederror for backward compatibility for non supported";"CODE"
"classification tasks";"TASK"
"else hard voting";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"only called to validate x in non fit methods therefore reset false";"IRRE"
"adaboost estimator is not validated yet";"TASK"
"check parameters";"IRRE"
"clear any previous fit results";"IRRE"
"initialization of the random number instance that will be used to";"IRRE"
"generate a seed at each iteration";"-"
"avoid extremely small sample weight for details see issue 20320";"CODE"
"do not clip sample weights that were exactly zero originally";"CODE"
"boosting step";"-"
"early termination";"-"
"stop if error is zero";"-"
"stop if the sum of sample weights has become non positive";"-"
"normalize";"-"
"weighted sampling of the training set with replacement";"IRRE"
"fit on the bootstrapped sample and obtain a prediction";"CODE"
"for all samples in the training set";"CODE"
"calculate the average loss";"-"
"stop if fit is perfect";"-"
"discard current estimator only if it isn t the only one";"IRRE"
"boost weight using adaboost r2 alg";"-"
"evaluate predictions of all estimators";"-"
"sort the predictions";"-"
"find index of median prediction for each sample";"CODE"
"return median predictions";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"also load the iris dataset";"IRRE"
"and randomly permute it";"IRRE"
"also load the diabetes dataset";"IRRE"
"and randomly permute it";"IRRE"
"check classification for various parameter settings";"IRRE"
"try different parameter settings with different base classifiers without";"IRRE"
"doing the full cartesian product to keep the test durations low";"CODE"
"check classification for various parameter settings on sparse input";"IRRE"
"trained on sparse format";"CODE"
"trained on dense format";"CODE"
"test that bootstrapping samples generate non perfect base estimators";"IRRE"
"without bootstrap all trees are perfect on the training set";"IRRE"
"with bootstrap trees are no longer perfect on the training set";"IRRE"
"check that each sampling correspond to a complete bootstrap resample";"CODE"
"the size of each bootstrap should be the same as the input data but";"META"
"the data should be different checked using the hash of the data";"-"
"test that bootstrapping features may generate duplicate features";"TASK"
"predict probabilities";"-"
"normal case";"CODE"
"degenerate case where some classes are missing";"CODE"
"check that oob prediction is a good estimation of the generalization";"-"
"error";"-"
"test with few estimators";"IRRE"
"check that oob prediction is a good estimation of the generalization";"-"
"error";"-"
"test with few estimators";"IRRE"
"check singleton ensembles";"CODE"
"test support of decision function";"IRRE"
"check parallel classification";"IRRE"
"predict proba";"-"
"decision function";"CODE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"check parallel regression";"-"
"check that bagging ensembles can be grid searched";"-"
"transform iris into a binary classification task";"CODE"
"grid search with scoring based on decision function";"CODE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"check estimator and its default values";"IRRE"
"classification";"IRRE"
"regression";"-"
"test if fitting incrementally with warm start gives a forest of the";"IRRE"
"right size and the same results as a normal fit";"IRRE"
"test if warm start ed second fit with smaller n estimators raises error";"IRRE"
"test that nothing happens when fitting without increasing n estimators";"IRRE"
"modify x to nonsense values this should not change anything";"IRRE"
"warm started classifier with 5 5 estimators should be equivalent to";"IRRE"
"one classifier with 10 estimators";"IRRE"
"check using oob score and warm start simultaneously fails";"-"
"case 1 small weights and fractional max samples would lead to sampling";"CODE"
"less than 1 sample which is not allowed";"-"
"case 2 large weights and bootstrap false would lead to sampling without";"CODE"
"replacement more than the number of samples which is not allowed";"-"
"all indices except 4 and 5 have zero weight";"CODE"
"max samples passed as a fraction of the input data since";"CODE"
"sample weight are provided the effective number of samples is the";"-"
"sum of the sample weights";"-"
"todo slep006 remove block when default routing is implemented";"CODE"
"only indices 4 and 5 should appear";"-"
"sampled indices represented through weighting";"-"
"sampled indices represented through indexing";"-"
"make sure oob scores are identical when random state estimator and";"IRRE"
"training data are fixed and fitting is done twice";"CODE"
"check that format of estimators samples is correct and that results";"IRRE"
"generated at fit time can be identically reproduced at a later time";"IRRE"
"using data saved in object attributes";"CODE"
"get relevant attributes";"META"
"test for correct formatting";"CODE"
"re fit single estimator to test for consistent sampling";"IRRE"
"this test is a regression test to check that with a random step";"IRRE"
"e g sparserandomprojection and a given random state the results";"IRRE"
"generated at fit time can be identically reproduced at a later time using";"IRRE"
"data saved in object attributes check issue 9524 for full discussion";"CODE"
"make sure validated max samples and original max samples are identical";"CODE"
"when valid integer max samples supplied by user";"CODE"
"make sure the oob score doesn t change when the labels change";"CODE"
"see https github com scikit learn scikit learn issues 8933";"CODE"
"check that baggingregressor can accept x with missing infinite data";"IRRE"
"verify that exceptions can be raised by wrapper regressor";"CODE"
"check that baggingclassifier can accept x with missing infinite data";"IRRE"
"verify that exceptions can be raised by wrapper classifier";"CODE"
"check that bagging estimator can accept low fractional max features";"TASK"
"check that bagging estimator can generate sample indices properly";"-"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 16436";"CODE"
"metadata routing tests";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"check baseensemble methods";"-"
"ensemble estimators empty the list and create estimators manually";"IRRE"
"linear discriminant analysis doesn t have random state smoke test";"IRRE"
"check random state is none still sets";"IRRE"
"check random state fixes results in consistent initialisation";"IRRE"
"nested random state";"IRRE"
"ensure multiple random state parameters are invariant to get params";"IRRE"
"iteration order";"-"
"check that the behavior of estimators estimators";"-"
"named estimators named estimators is consistent across all";"-"
"ensemble classes and when using set params";"IRRE"
"estimator clone estimator avoid side effects from shared instances";"CODE"
"before fit";"CODE"
"check fitted attributes";"META"
"check that set params does not add a new attribute";"CODE"
"check the behavior when setting and dropping an estimator";"IRRE"
"check that the correspondence is correct";"-"
"check that we can set the parameters of the underlying classifier";"IRRE"
"check that ensemble will fail during validation if the underlying";"-"
"estimators are not of the same type i e classifier or regressor";"IRRE"
"stackingclassifier can have an underlying regresor so it s not checked";"IRRE"
"raise an error when the name contains dunder";"CODE"
"raise an error when the name is not unique";"CODE"
"raise an error when the name conflicts with the parameters";"IRRE"
"check that we raise a consistent error when all estimators are";"CODE"
"dropped";"-"
"fixme we should move this test in estimator checks once we are able";"IRRE"
"to construct meta estimator instances";"CODE"
"check that voting and stacking predictor delegate the missing values";"IRRE"
"validation to the underlying estimator";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"toy sample";"-"
"larger classification sample used for testing feature importances";"CODE"
"also load the iris dataset";"IRRE"
"and randomly permute it";"IRRE"
"make regression dataset";"IRRE"
"also make a hastie 10 2 dataset";"IRRE"
"get the default backend in joblib to test parallelism and interaction with";"CODE"
"different backends";"CODE"
"regression models should not have a classes attribute";"IRRE"
"predict probabilities";"-"
"cast as dtype";"-"
"the forest estimator can detect that only the first 3 features of the";"TASK"
"dataset are informative";"IRRE"
"check with parallel";"-"
"check with sample weights";"-"
"check whether variable importances of totally randomized trees";"CODE"
"converge towards their theoretical values see louppe et al";"IRRE"
"understanding variable importances in forests of randomized trees 2013";"CODE"
"weight of each b of size k";"-"
"for all b of size k";"CODE"
"for all values b b";"IRRE"
"1 0 n samples b n samples p b b";"-"
"compute true importances";"CODE"
"estimate importances with totally randomized trees";"CODE"
"check correctness";"-"
"check pickability";"-"
"check estimators on multi output problems";"IRRE"
"check estimators on multi output problems with string outputs";"IRRE"
"test that n classes and classes have proper shape";"IRRE"
"classification single output";"IRRE"
"classification multi output";"IRRE"
"test that the sparse output parameter of randomtreesembedding";"IRRE"
"works by returning a dense array";"IRRE"
"create the rte with sparse false";"IRRE"
"assert that type is ndarray not scipy sparse csr matrix";"IRRE"
"test that the sparse output parameter of randomtreesembedding";"IRRE"
"works by returning the same array for both argument values";"IRRE"
"create the rtes";"IRRE"
"assert that dense and sparse hashers have same array";"IRRE"
"test random forest hashing on circles dataset";"IRRE"
"make sure that it is linearly separable";"-"
"even after projected to two svd dimensions";"-"
"note not all random states produce perfect results";"IRRE"
"test fit and transform";"IRRE"
"one leaf active per data point per forest";"CODE"
"single variable with 4 values";"IRRE"
"on a single variable problem where x 0 has 4 equiprobable values there";"IRRE"
"are 5 ways to build a random tree the more compact 0 1 0 0 0 2 of";"IRRE"
"them has probability 1 3 while the 4 others have probability 1 6";"CODE"
"assert 0 20 uniques 0 0 rough approximation of 1 6";"CODE"
"two variables one with 2 values one with 3 values";"IRRE"
"test precedence of max leaf nodes over max depth";"IRRE"
"test if leaves contain more than leaf count training examples";"IRRE"
"drop inner nodes";"-"
"drop inner nodes";"-"
"test if leaves contain at least min weight fraction leaf of the";"IRRE"
"training set";"IRRE"
"test both depthfirsttreebuilder and bestfirsttreebuilder";"IRRE"
"by setting max leaf nodes";"IRRE"
"drop inner nodes";"-"
"test that it works no matter the memory layout";"IRRE"
"dense";"-"
"np asarray nothing";"-"
"np asarray order c c order";"-"
"np asarray order f f order";"CODE"
"np ascontiguousarray contiguous";"-"
"sparse if applicable";"IRRE"
"strided";"-"
"check class weights resemble sample weights behavior";"IRRE"
"iris is balanced so no effect expected for using balanced weights";"CODE"
"make a multi output problem with three copies of iris";"IRRE"
"create user defined weights that should balance over the outputs";"IRRE"
"check against multi output balanced which should also have no effect";"IRRE"
"inflate importance of class 1 check against user defined weights";"CODE"
"check that sample weight and class weight are multiplicative";"IRRE"
"note nodes with indices 0 1 and 4 are internal split nodes and";"TASK"
"therefore do not appear in the expected output feature names";"CODE"
"test repeated calls result in same set of indices";"IRRE"
"the bootstrap should be a resampling with replacement";"-"
"toy sample";"-"
"also make regression dataset";"IRRE"
"also load the iris dataset";"IRRE"
"and randomly permute it";"IRRE"
"check classification on a toy dataset";"IRRE"
"test gradientboostingclassifier on synthetic dataset used by";"IRRE"
"hastie et al in eslii figure 10 9";"-"
"note that figure 10 9 reuses the dataset generated for figure 10 2";"TASK"
"and should have 2 000 train data points and 10 000 test data points";"CODE"
"here we intentionally use a smaller variant to make the test run faster";"CODE"
"but the conclusions are still the same despite the smaller datasets";"TASK"
"increasing the number of trees should decrease the test error";"IRRE"
"decision stumps are better suited for this dataset with a large number of";"CODE"
"estimators";"-"
"check consistency on regression dataset with least squares";"IRRE"
"and least absolute deviation";"-"
"learning rate max depth and n estimators were adjusted to get a mode";"CODE"
"that is accurate enough to reach a low mse on the training set while";"IRRE"
"keeping the resource used to execute this test low enough";"CODE"
"fixme we temporarily bypass this test this is due to the fact";"CODE"
"that gbrt with and without sample weight do not use the same";"IRRE"
"implementation of the median during the initialization with the";"TASK"
"dummyregressor in the future we should make sure that both";"TASK"
"implementations should be the same see pr 17377 for more";"TASK"
"assert allclose last y pred y pred";"CODE"
"check consistency on dataset iris";"IRRE"
"test on synthetic regression datasets used in leo breiman";"IRRE"
"bagging predictors machine learning 24 2 123 140 1996";"-"
"friedman1";"-"
"friedman2";"-"
"friedman3";"-"
"smoke test to check that the gradient boosting expose an attribute";"IRRE"
"feature importances";"CODE"
"predict probabilities";"-"
"check if probabilities are in 0 1";"IRRE"
"derive predictions from probabilities";"CODE"
"check that predict stages through an error if the type of x is not";"-"
"supported";"-"
"test to make sure random state is set properly";"IRRE"
"the most important feature is the median income by far";"CODE"
"the three subsequent features are the following their relative ordering";"TASK"
"might change a bit depending on the randomness of the trees and the";"IRRE"
"train test split";"IRRE"
"test if max features is set properly for floats and str";"IRRE"
"test whether staged decision function eventually gives";"IRRE"
"the same prediction";"-"
"test raise valueerror if not fitted";"IRRE"
"test if prediction for last stage equals predict";"IRRE"
"test whether staged predict proba eventually gives";"IRRE"
"the same prediction";"-"
"test raise notfittederror if not";"IRRE"
"test if prediction for last stage equals predict";"IRRE"
"test if prediction for last stage equals predict proba";"IRRE"
"test that staged functions make defensive copies";"CODE"
"y 4 x 0 astype int 1 don t predict zeros";"CODE"
"regressor has no staged predict proba";"-"
"check model serialization";"-"
"check if we can fit even though all targets are equal";"IRRE"
"classifier should raise exception";"CODE"
"check if quantile loss with alpha 0 5 equals absolute error";"IRRE"
"test with non integer class labels";"IRRE"
"test with float class labels";"IRRE"
"test with float class labels";"IRRE"
"this will raise a dataconversionwarning that we want to";"CODE"
"always raise elsewhere the warnings gets ignored in the";"CODE"
"later tests and the tests that check for this warning fail";"IRRE"
"test with different memory layouts of x and y";"IRRE"
"test if oob improvement has correct shape and regression test";"IRRE"
"hard coded regression test change if modification in oob computation";"IRRE"
"test if oob scores has correct shape and regression test";"IRRE"
"check oob improvement on multi class dataset";"IRRE"
"hard coded regression test change if modification in oob computation";"IRRE"
"fixme the following snippet does not yield the same results on 32 bits";"IRRE"
"assert array almost equal estimator oob improvement 5";"TASK"
"np array 12 68 10 45 8 18 6 43 5 13";"-"
"decimal 2";"-"
"pytest mark thread unsafe manually captured stdout";"CODE"
"check verbose 1 does not cause error";"IRRE"
"check output";"IRRE"
"with oob";"-"
"one for 1 10 and then 9 for 20 100";"CODE"
"pytest mark thread unsafe manually captured stdout";"CODE"
"check verbose 2 does not cause error";"IRRE"
"check output";"IRRE"
"no oob";"-"
"100 lines for n estimators 100";"CODE"
"test if warm start equals fit";"IRRE"
"random state is preserved and hence predict proba must also be";"IRRE"
"same";"-"
"test if warm start equals fit set n estimators";"IRRE"
"test if possible to fit trees of different depth in ensemble";"IRRE"
"last 10 trees have different depth";"-"
"test if fit clears state";"IRRE"
"est 2 fit x y inits state";"IRRE"
"est 2 fit x y clears old state and equals est";"-"
"test if warm start with smaller n estimators raises error";"IRRE"
"test if warm start with equal n estimators does nothing";"IRRE"
"test if oob can be turned on during warm start";"IRRE"
"the last 10 are not zeros";"-"
"test if warm start oob equals fit";"IRRE"
"test that all sparse matrix types are supported";"IRRE"
"test that feeding a x in fortran ordered is giving the same results as";"IRRE"
"in c ordered";"-"
"we want an asymmetric distribution";"META"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load iris diabetes dataset";"IRRE"
"trained on sparse format";"CODE"
"trained on dense format";"CODE"
"generate train test data";"CODE"
"generate some abnormal novel observations";"-"
"fit the model";"-"
"predict scores the lower the more normal";"-"
"check that there is at most 6 errors false positive or false negative";"-"
"toy sample the last two samples are outliers";"CODE"
"test isolationforest";"IRRE"
"assert detect outliers";"CODE"
"make sure validated max samples in iforest and basebagging are identical";"CODE"
"it tests non regression for 5732 which failed at predict";"IRRE"
"it tests non regression for 8549 which used the wrong formula";"CODE"
"for average path length strictly for the integer case";"CODE"
"updated to check average path length when input is 2 issue 11839";"CODE"
"average path length is increasing";"CODE"
"2 d array of all 1s";"-"
"2 d array where columns contain the same value across rows";"IRRE"
"single row";"-"
"mock out fit and stack method to be asserted later";"CODE"
"mocking a method will not provide a name while python methods";"CODE"
"do and we are using it in get response method";"CODE"
"fit was not called again";"IRRE"
"stack method is called with the proper inputs";"IRRE"
"check that notfittederror is raised";"CODE"
"if base estimators are not fitted when cv prefit";"-"
"stacking supports estimators without n features in regression test";"TASK"
"for 17353";"CODE"
"access sub estimator in name est with estimator 1";"-"
"access final estimator";"CODE"
"check that an estimator can be set to drop and passing some weight";"IRRE"
"regression test for";"IRRE"
"https github com scikit learn scikit learn issues 13777";"CODE"
"scaled to solve convergencewarning throw by logistic regression";"CODE"
"access sub estimator in name est with estimator 1";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"use settattr to avoid mypy errors when monkeypatching";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"don t remove this file we don t want to break users code just because the";"CODE"
"feature isn t experimental anymore";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"use settattr to avoid mypy errors when monkeypatching";"IRRE"
"from sklearn experimental import enable hist gradient boosting noqa";"CODE"
"federal university of rio grande do sul ufrgs";"IRRE"
"connectionist artificial intelligence laboratory liac";"CODE"
"renato de pontes pereira rppereira inf ufrgs br";"-"
"copyright c 2011 renato de pontes pereira renato ppontes at gmail dot com";"CODE"
"permission is hereby granted free of charge to any person obtaining a copy";"CODE"
"of this software and associated documentation files the software to deal";"CODE"
"in the software without restriction including without limitation the rights";"CODE"
"to use copy modify merge publish distribute sublicense and or sell";"META"
"copies of the software and to permit persons to whom the software is";"-"
"furnished to do so subject to the following conditions";"TASK"
"the above copyright notice and this permission notice shall be included in";"CODE"
"all copies or substantial portions of the software";"-"
"the software is provided as is without warranty of any kind express or";"-"
"implied including but not limited to the warranties of merchantability";"META"
"fitness for a particular purpose and noninfringement in no event shall the";"CODE"
"authors or copyright holders be liable for any claim damages or other";"OUTD"
"liability whether in an action of contract tort or otherwise arising from";"CODE"
"out of or in connection with the software or the use or other dealings in the";"CODE"
"software";"-"
"org doc scipy reference generated scipy sparse coo matrix html scipy sparse coo matrix";"IRRE"
"constants";"CODE"
"typing extensions is available when mypy is installed";"-"
"open quote followed by zero or more of";"CODE"
"no additional backslash";"TASK"
"maybe escaped backslashes";"-"
"escaped quote";"-"
"escaping a non quote";"CODE"
"non quote char";"CODE"
"close quote";"CODE"
"a value is surrounded by or by or contains no quotables";"IRRE"
"a value may be surrounded by";"IRRE"
"or by";"-"
"s or may contain no characters requiring quoting";"CODE"
"this captures value error groups because empty values are allowed";"IRRE"
"we cannot just look for empty values to handle syntax errors";"IRRE"
"we presume the line has had prepended";"CODE"
"may follow";"-"
"value re empty or value";"IRRE"
"s error";"-"
"this captures key value groups and will have an empty key value";"IRRE"
"in case of syntax errors";"CODE"
"it does not ensure that the line starts with or ends with";"CODE"
"s may follow or at line start";"-"
"d attribute key";"META"
"value re s value";"IRRE"
"s not an error if it s";"-"
"s s s not an error if it s";"-"
"s error";"-"
"fast path for trivial cases unfortunately we have to handle missing";"CODE"
"values because of the empty string case";"IRRE"
"re dense values tokenizes despite quoting whitespace etc";"IRRE"
"an arff syntax error in sparse data";"IRRE"
"an arff syntax error";"-"
"dense 0 constant value representing a dense matrix";"IRRE"
"coo 1 constant value representing a sparse matrix in coordinate format";"IRRE"
"lod 2 constant value representing a sparse matrix in list of";"IRRE"
"dictionaries format";"CODE"
"dense gen 3 generator of dictionaries";"-"
"lod gen 4 generator of dictionaries";"-"
"exceptions";"CODE"
"internal";"CODE"
"sparse decode";"IRRE"
"see issue 52 nominals should take their first value when";"IRRE"
"unspecified in a sparse matrix naturally this is consistent";"IRRE"
"with encodednominalconversor";"-"
"xxx int 0 is used for implicit values not 0";"IRRE"
"do not rename this file";"CODE"
"this is a hook for array api extra lib compat py";"CODE"
"to co vendor array api compat and potentially override its functions";"CODE"
"from array api compat import noqa f403";"CODE"
"elf str data split n store string as list of lines";"CODE"
"elf l 0 current line nr";"-"
"l1 self doc peek strip e g parameters";"CODE"
"l2 self doc peek 1 strip or";"CODE"
"if not self doc peek 1 strip previous line was empty";"CODE"
"if name startswith index section";"-"
"note param line with single element should never have a";"TASK"
"a before the description line so this should probably";"CODE"
"warn";"-"
"see also supports the following formats";"CODE"
"funcname";"-"
"funcname space colon space desc space";"-"
"funcname comma space funcname comma period space";"-"
"funcname comma space funcname space colon space desc space";"-"
"funcname is one of";"-"
"plain funcname";"-"
"colon role colon backtick plain funcname backtick";"-"
"where";"-"
"plain funcname is a legal function name and";"CODE"
"role is any nonempty sequence of word characters";"CODE"
"examples func f1 meth func h1 obj baz obj r class class j";"IRRE"
"desc is a string describing the function";"CODE"
"funcname group for all function names";"CODE"
"r p trailing end of allfuncs";"CODE"
"description some function lists have a trailing comma or period s";"CODE"
"empty desc elements are replaced with";"-"
"if several signatures present take the last one";"-"
"we could do more tests but we are not arbitrarily";"IRRE"
"we know where the docs came from";"CODE"
"make userwarning more descriptive via object introspection";"CODE"
"skip if introspection fails";"CODE"
"string conversion routines";"META"
"copyright c donald stufft and individual contributors";"META"
"all rights reserved";"-"
"redistribution and use in source and binary forms with or without";"META"
"modification are permitted provided that the following conditions are met";"-"
"1 redistributions of source code must retain the above copyright notice";"META"
"this list of conditions and the following disclaimer";"CODE"
"2 redistributions in binary form must reproduce the above copyright";"META"
"notice this list of conditions and the following disclaimer in the";"CODE"
"documentation and or other materials provided with the distribution";"CODE"
"this software is provided by the copyright holders and contributors as is and";"OUTD"
"any express or implied warranties including but not limited to the implied";"META"
"warranties of merchantability and fitness for a particular purpose are";"CODE"
"disclaimed in no event shall the copyright holder or contributors be liable";"OUTD"
"for any direct indirect incidental special exemplary or consequential";"CODE"
"damages including but not limited to procurement of substitute goods or";"META"
"services loss of use data or profits or business interruption however";"CODE"
"caused and on any theory of liability whether in contract strict liability";"CODE"
"or tort including negligence or otherwise arising in any way out of the use";"-"
"of this software even if advised of the possibility of such damage";"CODE"
"copyright c donald stufft and individual contributors";"META"
"all rights reserved";"-"
"redistribution and use in source and binary forms with or without";"META"
"modification are permitted provided that the following conditions are met";"-"
"1 redistributions of source code must retain the above copyright notice";"META"
"this list of conditions and the following disclaimer";"CODE"
"2 redistributions in binary form must reproduce the above copyright";"META"
"notice this list of conditions and the following disclaimer in the";"CODE"
"documentation and or other materials provided with the distribution";"CODE"
"this software is provided by the copyright holders and contributors as is and";"OUTD"
"any express or implied warranties including but not limited to the implied";"META"
"warranties of merchantability and fitness for a particular purpose are";"CODE"
"disclaimed in no event shall the copyright holder or contributors be liable";"OUTD"
"for any direct indirect incidental special exemplary or consequential";"CODE"
"damages including but not limited to procurement of substitute goods or";"META"
"services loss of use data or profits or business interruption however";"CODE"
"caused and on any theory of liability whether in contract strict liability";"CODE"
"or tort including negligence or otherwise arising in any way out of the use";"-"
"of this software even if advised of the possibility of such damage";"CODE"
"please keep the duplicated isinstance check";"-"
"in the six comparisons hereunder";"CODE"
"unless you find a way to avoid adding overhead function calls";"CODE"
"pad for numeric comparison";"CODE"
"ensure that alpha beta candidate are before final";"CODE"
"we hardcode an epoch of 1 here a pep 440 version can only have a epoch";"META"
"greater than or equal to 0 this will effectively put the legacyversion";"META"
"which uses the defacto standard originally implemented by setuptools";"CODE"
"as before all pep 440 versions";"META"
"this scheme is taken from pkg resources parse version setuptools prior to";"IRRE"
"it s adoption of the packaging library";"CODE"
"remove before a prerelease tag";"IRRE"
"remove trailing zeros from each series of numeric parts";"CODE"
"deliberately not anchored to the start and end of the string to make it";"CODE"
"easier for 3rd party code to reuse";"CODE"
"validate the version and parse it into pieces";"IRRE"
"store the parsed out pieces of the version";"IRRE"
"generate a key which will be used for sorting";"CODE"
"epoch";"-"
"release segment";"-"
"pre release";"-"
"post release";"-"
"development release";"-"
"local version segment";"META"
"epoch";"-"
"release segment";"-"
"we consider there to be an implicit 0 in a pre release if there is";"-"
"not a numeral associated with it";"-"
"we normalize any letters to their lower case form";"CODE"
"we consider some words to be alternate spellings of other words and";"-"
"in those cases we want to normalize the spellings to our preferred";"CODE"
"spelling";"-"
"we assume if we are given a number but we are not given a letter";"META"
"then this is using the implicit post release syntax e g 1 0 1";"CODE"
"when we compare a release version we want to compare it with all of the";"IRRE"
"trailing zeros removed so we ll use a reverse the list drop all the now";"IRRE"
"leading zeros until we come to something non zero then take the rest";"-"
"re reverse it back into the correct order and make it a tuple and use";"IRRE"
"that for our sorting key";"CODE"
"we need to trick the sorting algorithm to put 1 0 dev0 before 1 0a0";"TASK"
"we ll do this by abusing the pre segment but we only want to do this";"CODE"
"if there is not a pre or a post segment if we have one of those then";"IRRE"
"the normal sorting rules will handle this case correctly";"CODE"
"versions without a pre release except as noted above should sort after";"TASK"
"those with one";"-"
"versions without a post segment should sort before those with one";"META"
"versions without a development segment should sort after those with one";"META"
"versions without a local segment should sort before those with one";"META"
"versions with a local segment need that segment parsed to implement";"TASK"
"the sorting rules in pep440";"-"
"alpha numeric segments sort before numeric segments";"CODE"
"alpha numeric segments sort lexicographically";"IRRE"
"numeric segments sort numerically";"IRRE"
"shorter versions sort before longer versions when the prefixes";"META"
"match exactly";"-"
"spdx license identifier bsd 3 clause";"-"
"graph laplacian";"-"
"the keyword argument copy is unused and has no effect here";"OUTD"
"the keyword argument form is unused and has no effect here";"OUTD"
"from common import noqa f401 f403";"CODE"
"wrapped f signature new sig pyright ignore reportattributeaccessissue";"META"
"return wrapped f pyright ignore reportreturntype";"IRRE"
"from helpers import noqa f403";"CODE"
"todo import from typing requires python 3 13";"CODE"
"these functions are modified from the numpy versions";"CODE"
"creation functions add the device keyword which does nothing for numpy and dask";"CODE"
"np unique is split into four functions in the array api";"CODE"
"unique all unique counts unique inverse and unique values this is done";"IRRE"
"to remove polymorphic return types";"IRRE"
"the functions here return namedtuples np unique returns a normal";"CODE"
"tuple";"-"
"note that these named tuples aren t actually part of the standard namespace";"TASK"
"but i don t see any issue with exporting the names here regardless";"META"
"older versions of numpy and cupy do not have equal nan rather than";"OUTD"
"trying to parse version numbers just check if equal nan is in the";"IRRE"
"signature";"-"
"np unique flattens inverse indices but they need to share x s shape";"TASK"
"see https github com numpy numpy issues 20638";"CODE"
"xp unique flattens inverse indices but they need to share x s shape";"TASK"
"see https github com numpy numpy issues 20638";"CODE"
"these functions have different keyword argument names";"CODE"
"correction float 0 0 correction instead of ddof";"CODE"
"correction float 0 0 correction instead of ddof";"CODE"
"cumulative sum is renamed from cumsum and adds the include initial keyword";"CODE"
"argument";"-"
"todo the standard is not clear about what should happen when x ndim 0";"TASK"
"np cumsum does not support include initial";"CODE"
"np cumprod does not support include initial";"CODE"
"the min and max argument names in clip are different and not optional in numpy and type";"CODE"
"promotion behavior is different";"-"
"todo np clip has other ufunc kwargs";"CODE"
"np clip does type promotion but the array api clip requires that the";"CODE"
"output have the same dtype as x we do this instead of just downcasting";"CODE"
"the result of xp clip to handle some corner cases better e g";"CODE"
"avoiding uint64 float64 promotion";"CODE"
"note cases where min or max overflow integer or round float in the";"CODE"
"wrong direction when downcasting to x dtype are unspecified this code";"CODE"
"just does whatever numpy does when it downcasts in the assignment but";"CODE"
"other behavior could be preferred especially for integers for example";"CODE"
"this code produces";"CODE"
"clip asarray 0 dtype int8 asarray 128 dtype int16 none";"CODE"
"128";"-"
"but an answer of 0 might be preferred see";"META"
"https github com numpy numpy issues 24976 for more discussion on this issue";"CODE"
"at least handle the case of python integers correctly see";"CODE"
"https github com numpy numpy pull 26892";"CODE"
"assert out is not none workaround for a type narrowing issue in pyright";"CODE"
"return a scalar for 0 d";"CODE"
"unlike transpose the axes argument to permute dims is required";"IRRE"
"np reshape calls the keyword argument newshape instead of shape";"CODE"
"the descending keyword is new in sort and argsort and kind replaced with";"CODE"
"stable";"-"
"note this keyword argument is different and the default is different";"CODE"
"we set it in kwargs like this because numpy sort uses kind quicksort";"IRRE"
"as the default whereas cupy sort uses kind none";"CODE"
"as numpy has no native descending sort we imitate it here note that";"TASK"
"simply flipping the results of xp argsort x would not";"IRRE"
"respect the relative order like it would in native descending sorts";"CODE"
"rely on flip argsort to validate axis";"IRRE"
"note this keyword argument is different and the default is different";"CODE"
"we set it in kwargs like this because numpy sort uses kind quicksort";"IRRE"
"as the default whereas cupy sort uses kind none";"CODE"
"nonzero should error for zero dimensional arrays";"CODE"
"ceil floor and trunc return integers for integer inputs";"CODE"
"linear algebra functions";"CODE"
"unlike transpose matrix transpose only transposes the last two axes";"-"
"isdtype is a new function in the 2022 12 array api specification";"CODE"
"tuple bool true disallow nested tuples";"CODE"
"this will allow things that aren t required by the spec like";"CODE"
"isdtype np float64 float or isdtype np int64 l should we be";"CODE"
"more strict here to match the type annotation note that the";"TASK"
"array api strict implementation will be very strict";"TASK"
"unstack is a new function in the 2023 12 array api standard";"CODE"
"numpy 1 26 does not use the standard definition for sign on complex numbers";"IRRE"
"sign 0 0 but the above formula would give nan";"META"
"cupy sign does not propagate nans see";"CODE"
"https github com data apis array api compat issues 136";"CODE"
"it is surprisingly difficult to recognize a dtype apart from an array";"CODE"
"np int64 is not the same as np asarray 1 dtype";"CODE"
"note numpy fft functions improperly upcast float32 and complex64 to";"CODE"
"complex128 which is why we require wrapping them all here";"META"
"import sparse pyright ignore reportmissingtypestubs";"CODE"
"todo import from typing requires python 3 13";"CODE"
"cupyarray typealias any cupy has no py typed";"-"
"fast exit";"-"
"dtype x dtype type ignore attr defined";"CODE"
"jax float0 is a np dtype float0 v";"CODE"
"todo should we reject ndarray subclasses";"CODE"
"todo account for other backends";"CODE"
"def is array api obj x object typeis arrayapiobj pyright ignore reportunknownparametertype";"CODE"
"todo drop support for numpy 2 which didn t have array namespace";"CODE"
"todo drop support for jax 0 4 32 which didn t have array namespace";"CODE"
"numpy 2 0 have array namespace however they are not yet fully array api";"TASK"
"compatible";"-"
"import cupy as cp pyright ignore reportmissingtypestubs";"CODE"
"jax v0 4 32 and newer implements the array api directly in jax numpy";"CODE"
"for older jax versions it is available via jax experimental array api";"CODE"
"import jax experimental array api as jnp pyright ignore reportmissingimports";"CODE"
"import sparse pyright ignore reportmissingtypestubs";"CODE"
"sparse is already an array namespace we do not have a wrapper";"CODE"
"submodule for it";"CODE"
"todo support python scalars";"CODE"
"backwards compatibility alias";"-"
"def check device bare xp namespace device device none pyright ignore reportunusedfunction";"CODE"
"placeholder object to represent the dask device";"IRRE"
"when the array backend is not the cpu";"CODE"
"since it is not easy to tell which device a dask array is on";"TASK"
"device is not on numpy ndarray or dask array and to device is not on numpy ndarray";"-"
"or cupy ndarray they are not included in array objects of this library";"CODE"
"because this library just reuses the respective ndarray classes without";"CODE"
"wrapping or subclassing them these helper functions can be used instead of";"CODE"
"the wrapper functions for libraries that need to support both numpy cupy and";"CODE"
"other libraries that use devices";"-"
"peek at the metadata of the dask array to determine type";"-"
"if is numpy array x meta pyright ignore";"-"
"must be on cpu since backed by numpy";"TASK"
"fixme jitted jax arrays do not have a device attribute";"META"
"https github com jax ml jax issues 26000";"CODE"
"return none in this case note that this workaround breaks";"CODE"
"the standard and will result in new arrays being created on the";"IRRE"
"default device instead of the same device as the input array s";"CODE"
"older jax releases had device as a method which has been replaced";"TASK"
"with a property in accordance with the standard";"-"
"sparse will gain device so check for this first";"CODE"
"everything but dok has this attr";"CODE"
"inner x data pyright ignore";"-"
"return the device of the constituent array";"CODE"
"return device inner pyright ignore";"IRRE"
"return x device pyright ignore";"IRRE"
"prevent shadowing used below";"CODE"
"based on cupy array api array to device";"CODE"
"allowing us to use to device x cpu";"IRRE"
"is useful for portable test swapping between";"CODE"
"host and device backends";"CODE"
"stream can be an int as specified in dlpack or a cupy stream";"CODE"
"cupy does not yet have to device";"TASK"
"return torch to device x device stream stream pyright ignore reportargumenttype";"IRRE"
"todo what if our array is on the gpu already";"CODE"
"in jax v0 4 31 and older this import adds to device method to x";"CODE"
"import jax experimental array api noqa f401 pyright ignore";"CODE"
"but only on eager jax it won t work inside jax jit";"META"
"perform trivial check to return the same array if";"IRRE"
"device is same instead of err ing";"CODE"
"return x to device device stream stream pyright ignore";"IRRE"
"lazy api compliant arrays such as ndonnx can contain none in their shape";"CODE"
"dask array array shape can contain nan";"-"
"jax note while it is possible to determine if you re inside or outside";"CODE"
"jax jit by testing the subclass of a jax array object as well as testing bool";"IRRE"
"as we do below for unknown arrays this is not recommended by jax best practices";"CODE"
"dask note dask eagerly computes the graph on bool float and so on";"CODE"
"this behaviour while impossible to change without breaking backwards";"CODE"
"compatibility is highly detrimental to performance as the whole graph will end";"CODE"
"up being computed multiple times";"CODE"
"note skipping reclassification of jax zero gradient arrays as one will";"CODE"
"exclusively get them once they leave a jax grad jit context";"-"
"unknown array api compatible object note that this test may have dire consequences";"CODE"
"in terms of performance e g for a lazy object that eagerly computes the graph";"CODE"
"on bool dask is one such example which however is special cased above";"CODE"
"select a single point of the array";"CODE"
"cast to dtype bool and deal with size 0 arrays";"CODE"
"the array api standard dictactes that bool should raise typeerror if the";"CODE"
"output cannot be defined";"IRRE"
"here we allow for it to raise arbitrary exceptions e g like dask does";"CODE"
"these are in the main numpy namespace but not in numpy linalg";"CODE"
"these functions are the same as their numpy counterparts except they return";"CODE"
"a namedtuple";"-"
"these functions have additional keyword arguments";"TASK"
"the upper keyword argument is new from numpy";"CODE"
"u xp conj u pyright ignore reportconstantredefinition";"IRRE"
"the rtol keyword argument of matrix rank and pinv is new from numpy";"CODE"
"note that it has a different semantic meaning from tol and rcond";"TASK"
"this is different from xp linalg matrix rank which supports 1";"CODE"
"dimensional arrays";"-"
"this is different from xp linalg matrix rank which does not";"CODE"
"multiply the tolerance by the largest singular value";"IRRE"
"this is different from xp linalg pinv which does not multiply the";"CODE"
"default tolerance by max m n";"CODE"
"these functions are new in the array api spec";"CODE"
"svdvals is not in numpy but it is in scipy it is equivalent to";"META"
"xp linalg svd compute uv false";"-"
"xp linalg norm tries to do a matrix norm whenever axis is a 2 tuple or";"TASK"
"when axis none and the input is 2 d so to force a vector norm we make";"CODE"
"it so the input is 1 d for axis none or reshape so that norm is done";"CODE"
"on a single dimension";"-"
"note xp linalg norm doesn t handle 0 d arrays";"TASK"
"note the axis argument supports any number of axes whereas";"TASK"
"xp linalg norm only supports a single axis for vector norm";"CODE"
"normalize axis tuple axis x ndim pyright ignore reportcallissue";"IRRE"
"we can t reuse xp linalg norm keepdims because of the reshape hacks";"-"
"above to avoid matrix norm logic";"CODE"
"normalize axis tuple pyright ignore reportcallissue";"IRRE"
"xp diagonal and xp trace operate on the first two axes whereas these";"-"
"operates on the last two";"-"
"these just types are equivalent to the just type from the optype library";"CODE"
"apart from them not being runtime checkable";"CODE"
"docs https github com jorenham optype blob master readme md just";"CODE"
"code https github com jorenham optype blob master optype core just py";"CODE"
"def class self value type int none pyright ignore reportincompatiblemethodoverride";"CODE"
"def class self value type float none pyright ignore reportincompatiblemethodoverride";"CODE"
"def class self value type complex none pyright ignore reportincompatiblemethodoverride";"CODE"
"return type of array namespace info default dtypes";"CODE"
"return type of array namespace info default dtypes";"CODE"
"type of the kind parameter in array namespace info dtypes";"IRRE"
"array namespace info dtypes kind bool";"CODE"
"array namespace info dtypes kind signed integer";"CODE"
"array namespace info dtypes kind unsigned integer";"CODE"
"array namespace info dtypes kind integral";"CODE"
"array namespace info dtypes kind real floating";"CODE"
"array namespace info dtypes kind complex floating";"META"
"array namespace info dtypes kind numeric";"-"
"array namespace info dtypes kind none default";"CODE"
"array namespace info dtypes kind fallback";"-"
"from cupy import noqa f403";"CODE"
"from cupy import doesn t overwrite these builtin names";"CODE"
"from cupy import abs max min round noqa f401";"CODE"
"these imports may overwrite names from the import above";"CODE"
"from aliases import noqa f403";"CODE"
"see the comment in the numpy init py";"IRRE"
"basic renames";"-"
"asarray also adds the copy keyword which is not present in numpy 1 0";"TASK"
"cupy count nonzero does not have keepdims";"CODE"
"take along axis axis defaults to 1 but in cupy and numpy axis is a required arg";"CODE"
"these functions are completely new here if the library already has them";"CODE"
"i e numpy 2 0 use the library version instead of our wrapper";"CODE"
"todo does this depend on device";"CODE"
"todo does this depend on device";"CODE"
"numpy 1 x on python 3 10 fails to parse np dtype";"IRRE"
"from cupy fft import noqa f403";"CODE"
"cupy fft doesn t have all if it is added replace this with";"CODE"
"from cupy fft import all as linalg all";"CODE"
"from cupy linalg import noqa f403";"CODE"
"cupy linalg doesn t have all if it is added replace this with";"CODE"
"from cupy linalg import all as linalg all";"CODE"
"these functions are in both the main and linalg namespaces";"CODE"
"from aliases import matmul matrix transpose tensordot vecdot noqa f401";"CODE"
"these functions are completely new here if the library already has them";"CODE"
"i e numpy 2 0 use the library version instead of our wrapper";"CODE"
"from dask array import noqa f403";"CODE"
"these imports may overwrite names from the import above";"CODE"
"from aliases import noqa f403";"CODE"
"see the comment in the numpy init py";"IRRE"
"pyright reportprivateusage false";"CODE"
"pyright reportunknownargumenttype false";"-"
"pyright reportunknownmembertype false";"-"
"pyright reportunknownvariabletype false";"CODE"
"da astype doesn t respect copy true";"CODE"
"todo respect device keyword";"TASK"
"common aliases";"-"
"this arange func is modified from the common one to";"CODE"
"not pass stop step as keyword arguments which will cause";"-"
"an error with dask";"-"
"todo respect device keyword";"TASK"
"stop is none so start is actually stop";"META"
"prepend the default value for start which is 0";"CODE"
"asarray also adds the copy keyword which is not present in numpy 1 0";"TASK"
"todo respect device keyword";"TASK"
"return obj copy if copy else obj pyright ignore reportattributeaccessissue";"IRRE"
"copy none to be uniform across dask 2024 12 and 2024 12";"CODE"
"see https github com dask dask pull 11524";"CODE"
"element wise aliases";"-"
"other";"-"
"dask array clip does not work unless all three arguments are provided";"CODE"
"furthermore the masking workaround in common aliases clip cannot work with";"CODE"
"dask meaning uint64 promoting to float64 is going to just be unfixed for";"CODE"
"now";"-"
"todo this won t handle dask unknown shapes";"CODE"
"break chunks on other axes in an attempt to keep chunk size low";"CODE"
"rather than reconstructing the original chunks which can be a";"CODE"
"very expensive affair just break down oversized chunks without";"CODE"
"incurring in any transfers over the network";"-"
"this has the downside of a risk of overchunking if the array is";"CODE"
"then used in operations against other arrays that match the";"-"
"original chunking pattern";"-"
"dask array count nonzero does not have keepdims";"CODE"
"fmt skip";"-"
"pyright reportprivateusage false";"CODE"
"if isinstance kind tuple type ignore reportunnecessaryisinstancecall";"IRRE"
"from dask array fft import noqa f403";"CODE"
"dask array fft doesn t have all if it is added replace this with";"CODE"
"from dask array fft import all as linalg all";"CODE"
"the matmul and tensordot functions are in both the main and linalg namespaces";"CODE"
"exports";"-"
"from dask array linalg import noqa f403";"CODE"
"dask array linalg doesn t have all if it is added replace this with";"CODE"
"from dask array linalg import all as linalg all";"CODE"
"todo use the qr wrapper once dask";"TASK"
"supports the mode keyword on qr";"CODE"
"https github com dask dask issues 10388";"CODE"
"qr get xp da linalg qr";"-"
"wrap the svd functions to not pass full matrices to dask";"CODE"
"when full matrices false as that is the default behavior for dask";"CODE"
"and dask doesn t have the full matrices keyword";"CODE"
"todo can t avoid computing u or v for dask";"CODE"
"ruff noqa plc0414";"-"
"from numpy import noqa f403 pyright ignore reportwildcardimportfromlibrary";"CODE"
"from numpy import doesn t overwrite these builtin names";"CODE"
"these imports may overwrite names from the import above";"CODE"
"from aliases import noqa f403";"CODE"
"don t know why but we have to do an absolute import to import linalg if we";"CODE"
"instead do";"TASK"
"from import linalg";"CODE"
"it doesn t overwrite np linalg from above the import is generated";"CODE"
"dynamically so that the library can be vendored";"CODE"
"from linalg import matrix transpose vecdot type ignore no redef noqa f401";"CODE"
"pyright reportprivateusage false";"CODE"
"the values of the copymode enum can be either false true or 2";"IRRE"
"https github com numpy numpy blob 5a8a6a79d9c2fff8f07dcab5d41e14f8508d673f numpy globals pyi l7 l10";"CODE"
"basic renames";"-"
"def supports buffer protocol obj object typeis buffer pyright ignore reportunusedfunction";"CODE"
"memoryview obj pyright ignore reportargumenttype";"-"
"asarray also adds the copy keyword which is not present in numpy 1 0";"TASK"
"asarray is different enough between numpy cupy and dask the logic";"-"
"complicated enough that it s easier to define it separately for each module";"CODE"
"rather than trying to combine everything into one function in common";"CODE"
"return np array obj copy copy dtype dtype kwargs pyright ignore";"IRRE"
"count nonzero returns a python int for axis none and keepdims false";"CODE"
"https github com numpy numpy issues 17562";"CODE"
"note this is currently incorrectly typed in numpy but will be fixed in";"TASK"
"numpy 2 2 5 and 2 3 0 https github com numpy numpy pull 28750";"CODE"
"result cast any np count nonzero x axis axis keepdims keepdims pyright ignore reportargumenttype reportcallissue";"IRRE"
"take along axis axis defaults to 1 but in numpy axis is a required arg";"CODE"
"these functions are completely new here if the library already has them";"CODE"
"i e numpy 2 0 use the library version instead of our wrapper";"CODE"
"numpy 1 x on python 3 10 fails to parse np dtype";"IRRE"
"pyright reportattributeaccessissue false";"META"
"pyright reportunknownargumenttype false";"-"
"pyright reportunknownmembertype false";"-"
"pyright reportunknownvariabletype false";"CODE"
"intersection of np linalg all on numpy 1 22 and 2 2 minus linalg all";"CODE"
"these functions are in both the main and linalg namespaces";"CODE"
"from aliases import matmul matrix transpose tensordot vecdot noqa f401";"CODE"
"note unlike np linalg solve the array api solve only accepts x2 as a";"TASK"
"vector when it is exactly 1 dimensional all other cases treat x2 as a stack";"CODE"
"of matrices the np linalg solve behavior of allowing stacks of both";"-"
"matrices and vectors is ambiguous c f";"META"
"https github com numpy numpy issues 15349 and";"CODE"
"https github com data apis array api issues 285";"CODE"
"to workaround this the below is the code from np linalg solve except";"CODE"
"only calling solve1 in the exactly 1d case";"CODE"
"this code is here instead of in common because it is numpy specific also";"CODE"
"note that cupy s solve does not currently support broadcasting see";"TASK"
"https github com cupy cupy blob main cupy cublas py l43";"CODE"
"this part is different from np linalg solve";"CODE"
"this does nothing currently but is left in because it will be relevant";"CODE"
"when complex dtype support is added to the spec in 2022";"TASK"
"these functions are completely new here if the library already has them";"CODE"
"i e numpy 2 0 use the library version instead of our wrapper";"CODE"
"from torch import noqa f403";"CODE"
"several names are not included in the above import";"CODE"
"these imports may overwrite names from the import above";"CODE"
"from aliases import noqa f403";"CODE"
"see the comment in the numpy init py";"IRRE"
"torch 2 3";"-"
"ints";"CODE"
"ints and uints mixed sign";"CODE"
"floats";"CODE"
"complexes";"META"
"mixed float and complex";"META"
"if an argument is 0 d pytorch downcasts the other argument";"CODE"
"sort scalars so that they are treated last";"-"
"combine left to right";"-"
"this doesn t result type dtype dtype for non array api dtypes";"CODE"
"because torch result type only accepts tensors this does however allow";"IRRE"
"cross kind promotion";"-"
"basic renames";"-"
"torch conj sets the conjugation bit which breaks conversion to other";"IRRE"
"libraries see https github com data apis array api compat issues 173";"CODE"
"two arg elementwise functions";"CODE"
"these require a wrapper to do the correct type promotion on 0 d tensors";"CODE"
"also a rename torch equal does not broadcast";"CODE"
"logical functions are not included here because they only accept bool in the";"CODE"
"spec so type promotion is irrelevant";"-"
"torch asarray does not respect input output device propagation";"CODE"
"https github com pytorch pytorch issues 150199";"CODE"
"these wrappers are mostly based on the fact that pytorch uses dim instead";"CODE"
"of axis";"-"
"torch min and torch max return a tuple and don t support multiple axes https github com pytorch pytorch issues 58745";"CODE"
"https github com pytorch pytorch issues 29137";"CODE"
"https github com pytorch pytorch issues 29137";"CODE"
"torch sort also returns a tuple";"IRRE"
"https github com pytorch pytorch issues 70921";"CODE"
"better error message in this case";"CODE"
"match torch error message e g from sum";"CODE"
"use indexerror instead of runtimeerror and axis instead of dim";"CODE"
"apply keepdims when axis none";"-"
"https github com pytorch pytorch issues 71209";"CODE"
"note that this is only valid for the axis none case";"CODE"
"some reductions don t support multiple axes";"CODE"
"https github com pytorch pytorch issues 56586";"CODE"
"we can t upcast uint8 according to the spec because there is no";"IRRE"
"torch uint64 so at least upcast to int64 which is what prod does";"CODE"
"when axis none";"-"
"torch prod doesn t support multiple axes";"CODE"
"https github com pytorch pytorch issues 56586";"CODE"
"torch doesn t support keepdims with axis none";"CODE"
"https github com pytorch pytorch issues 71209";"CODE"
"torch doesn t support keepdims with axis none";"CODE"
"https github com pytorch pytorch issues 71209";"CODE"
"torch any doesn t support multiple axes";"CODE"
"https github com pytorch pytorch issues 56586";"CODE"
"torch doesn t support keepdims with axis none";"CODE"
"https github com pytorch pytorch issues 71209";"CODE"
"torch any doesn t return bool for uint8";"CODE"
"torch all doesn t support multiple axes";"CODE"
"https github com pytorch pytorch issues 56586";"CODE"
"torch doesn t support keepdims with axis none";"CODE"
"https github com pytorch pytorch issues 71209";"CODE"
"torch all doesn t return bool for uint8";"CODE"
"https github com pytorch pytorch issues 29137";"CODE"
"torch doesn t support keepdims with axis none";"CODE"
"https github com pytorch pytorch issues 71209";"CODE"
"note float correction is not supported";"TASK"
"https github com pytorch pytorch issues 61492 we don t try to";"CODE"
"implement it here for now";"TASK"
"https github com pytorch pytorch issues 29137";"CODE"
"torch doesn t support keepdims with axis none";"CODE"
"https github com pytorch pytorch issues 71209";"CODE"
"note float correction is not supported";"TASK"
"https github com pytorch pytorch issues 61492 we don t try to";"CODE"
"implement it here for now";"TASK"
"if isinstance correction float";"CODE"
"correction int correction";"CODE"
"https github com pytorch pytorch issues 29137";"CODE"
"torch doesn t support keepdims with axis none";"CODE"
"https github com pytorch pytorch issues 71209";"CODE"
"torch concat doesn t support dim none";"CODE"
"https github com pytorch pytorch issues 70925";"CODE"
"torch squeeze only accepts int dim and doesn t require it";"CODE"
"https github com pytorch pytorch issues 70924 support for tuple dim was";"CODE"
"added at https github com pytorch pytorch pull 89017";"TASK"
"remove this once pytorch 1 14 is released with the above pr 89017";"OUTD"
"torch broadcast to uses size instead of shape";"CODE"
"torch permute uses dims instead of axes";"CODE"
"the axis parameter doesn t work for flip and roll";"CODE"
"https github com pytorch pytorch issues 71210 also torch flip doesn t";"CODE"
"accept axis none";"-"
"torch flip doesn t accept dim as an int but the method does";"CODE"
"https github com pytorch pytorch issues 18095";"CODE"
"torch uses dim instead of axis";"CODE"
"torch uses dim instead of axis does not have keepdims";"CODE"
"repeat is torch repeat interleave also the dim argument";"CODE"
"torch reshape doesn t have the copy keyword";"CODE"
"torch arange doesn t support returning empty arrays";"CODE"
"https github com pytorch pytorch issues 70915 and doesn t support some";"CODE"
"keyword argument combinations";"-"
"https github com pytorch pytorch issues 70914";"CODE"
"torch eye does not accept none as a default for the second argument and";"CODE"
"doesn t support off diagonals https github com pytorch pytorch issues 70910";"CODE"
"torch linspace doesn t have the endpoint parameter";"CODE"
"torch full does not accept an int size";"CODE"
"https github com pytorch pytorch issues 70906";"CODE"
"ones zeros and empty do not accept shape as a keyword argument";"CODE"
"tril and triu do not call the keyword argument k";"IRRE"
"functions that aren t in torch https github com pytorch pytorch issues 58742";"CODE"
"note that these named tuples aren t actually part of the standard namespace";"TASK"
"but i don t see any issue with exporting the names here regardless";"META"
"https github com pytorch pytorch issues 70920";"CODE"
"torch unique doesn t support returning indices";"CODE"
"https github com pytorch pytorch issues 36748 the workaround";"CODE"
"suggested in that issue doesn t actually function correctly it relies";"CODE"
"on non deterministic behavior of scatter";"-"
"values inverse indices counts torch unique x return counts true return inverse true";"IRRE"
"torch unique incorrectly gives a 0 count for nan values";"IRRE"
"https github com pytorch pytorch issues 94106";"CODE"
"counts torch isnan values 1";"IRRE"
"return uniqueallresult values indices inverse indices counts";"IRRE"
"torch unique incorrectly gives a 0 count for nan values";"IRRE"
"https github com pytorch pytorch issues 94106";"CODE"
"torch matmul doesn t type promote but differently from fix promotion";"CODE"
"torch tensordot uses dims instead of axes";"CODE"
"note torch tensordot fails with integer dtypes when there is only 1";"CODE"
"element in the axis https github com pytorch pytorch issues 84530";"CODE"
"tuple true disallow nested tuples";"CODE"
"torch sign does not support complex numbers and does not propagate";"CODE"
"nans see https github com data apis array api compat issues 136";"CODE"
"sign 0 0 but the above formula would give nan";"META"
"enforce the default of xy";"CODE"
"todo is the return type a list or a tuple";"CODE"
"note if the default is set to float64 the devices like mps that";"CODE"
"don t support float64 will error we still return the default dtype";"CODE"
"value here because this error doesn t represent a different default";"CODE"
"per device";"-"
"uint16 uint32 and uint64 are present in newer versions of pytorch";"CODE"
"but they aren t generally supported by the array api functions so";"CODE"
"we omit them from this function";"CODE"
"torch doesn t have a straightforward way to get the list of all";"CODE"
"currently supported devices to do this we first parse the error";"IRRE"
"message of torch device to get the list of all possible types of";"CODE"
"device";"-"
"raise assertionerror unreachable pragma nocover";"CODE"
"the error message is something like";"-"
"expected one of cpu cuda ipu xpu mkldnn opengl opencl ideep hip ve fpga ort xla lazy vulkan mps meta hpu mtia privateuseone device type at start of device string notadevice";"CODE"
"next we need to check for different indices for different devices";"CODE"
"device device name index index doesn t actually check if the";"IRRE"
"device name or index is valid we have to try to create a tensor";"IRRE"
"with it which is why this function is cached";"CODE"
"from torch fft import noqa f403";"CODE"
"several torch fft functions do not map axes to dim";"CODE"
"from torch linalg import noqa f403";"CODE"
"torch linalg doesn t define all";"CODE"
"from torch linalg import all as linalg all";"CODE"
"outer is implemented in torch but aren t in the linalg namespace";"CODE"
"these functions are in both the main and linalg namespaces";"CODE"
"note torch linalg cross does not default to axis 1 it defaults to the";"CODE"
"first axis with size 3 see https github com pytorch pytorch issues 58743";"CODE"
"torch cross also does not support broadcasting when it would add new";"CODE"
"dimensions https github com pytorch pytorch issues 39656";"CODE"
"torch linalg vecdot incorrectly allows broadcasting along the contracted dimension";"CODE"
"torch linalg vecdot doesn t support integer dtypes";"CODE"
"torch tries to emulate numpy 1 solve behavior by using batched 1 d solve";"-"
"whenever";"-"
"1 x1 ndim 1 x2 ndim";"-"
"2 x1 shape 1 x2 shape";"-"
"see linalg solve is vector rhs in";"-"
"aten src aten native linearalgebrautils h and";"-"
"torch meta func linalg solve ex in";"CODE"
"aten src aten native batchlinearalgebra cpp in the pytorch source code";"CODE"
"the easiest way to work around this is to prepend a size 1 dimension to";"CODE"
"x2 since x2 is already one dimension less than x1";"CODE"
"see https github com pytorch pytorch issues 52915";"CODE"
"torch trace doesn t support the offset argument and doesn t support stacking";"CODE"
"use our wrapped sum to make sure it does upcasting correctly";"CODE"
"justfloat stands for inf inf which are not valid for literal";"CODE"
"torch vector norm incorrectly treats axis the same as axis none";"-"
"the norm of a single scalar works out to abs x in every case except";"CODE"
"for ord 0 which is x 0";"CODE"
"array 1 79769313e 308 1 79769313e 308 0 00000000e 000 may vary";"CODE"
"array 1 79769313e 308 1 79769313e 308 0 00000000e 000 may vary";"CODE"
"array 1 79769313e 308 0 00000000e 000j may vary";"CODE"
"override from python 3 12";"CODE"
"def str self str pyright ignore reportimplicitoverride";"CODE"
"xpx at x idx set value or add value etc";"IRRE"
"y xpx at x 0 set 2 copy true never updates x";"IRRE"
"z xpx at x 1 set 3 may or may not update x in place";"IRRE"
"del x avoid accidental reuse of x as we don t know its state anymore";"CODE"
"x xpx at x mask set f x mask crash on dask and jax jit";"IRRE"
"array numpydoc ignore pr01 rt01";"CODE"
"array numpydoc ignore pr01 rt01";"CODE"
"array numpydoc ignore pr01 rt01";"CODE"
"on dask this function runs on the chunks so we need to determine the";"CODE"
"namespace that dask is wrapping";"-"
"note that da minimum incidentally works on numpy cupy and sparse";"TASK"
"thanks to all these meta namespaces implementing the array ufunc";"TASK"
"interface but there s no guarantee that it will work for other";"CODE"
"wrapped libraries in the future";"TASK"
"array numpydoc ignore pr01 rt01";"CODE"
"def like self others backend bool numpydoc ignore pr01 rt01";"CODE"
"jax jit does not support assignment by boolean mask";"CODE"
"float in signature to accept math nan for dask";"CODE"
"int s are still accepted as float is a superclass of int in typing";"CODE"
"return match numpy output";"IRRE"
"dask uses nan for unknown shape which predates the array api spec for none";"CODE"
"none size none in sizes or math nan in sizes noqa plw0177";"-"
"array numpydoc ignore pr01 rt01";"CODE"
"prevent warnings on numpy and dask on inf inf";"-"
"lambda a b mxp isinf a mxp isinf b mxp sign a mxp sign b pyright ignore reportunknownargumenttype";"CODE"
"note inf inf is true";"TASK"
"lambda a b mxp abs a b atol rtol mxp abs b pyright ignore reportunknownargumenttype";"CODE"
"integer types";"CODE"
"don t rely on overflowerror as it is not guaranteed by the array api";"CODE"
"rtol max int 1 so it s inconsequential";"CODE"
"equalise the shapes by prepending smaller one with 1s";"CODE"
"insert empty dimensions";"CODE"
"compute the product";"-"
"reshape back and return";"IRRE"
"def nan to num numpydoc ignore pr01 rt01";"CODE"
"convert infinities to finite values";"IRRE"
"size is jax specific";"-"
"https github com data apis array api issues 883";"CODE"
"there are 3 general use cases";"CODE"
"1 backend has unique counts and it returns an array with known shape";"CODE"
"2 backend has unique counts and it returns a none sized array";"CODE"
"e g dask ndonnx";"CODE"
"3 backend does not have unique counts e g wrapped jax";"CODE"
"xp has unique counts o n complexity";"META"
"xp does not have unique counts o n logn complexity";"META"
"special cases";"CODE"
"array is size 0";"-"
"array has all elements equal to each other";"-"
"array numpydoc ignore pr01 rt01";"CODE"
"weisstein eric w sinc function from mathworld a wolfram web";"CODE"
"wikipedia sinc function";"CODE"
"https sparse pydata org en stable operations html package configuration";"IRRE"
"import jax pylint disable import outside toplevel";"CODE"
"def lazy apply wrapper numpydoc ignore pr01 rt01";"CODE"
"on dask wraps causes the graph key to contain the wrapped function s name";"CODE"
"tuple array numpydoc ignore gl08";"CODE"
"arg cast array np asarray arg pyright ignore reportinvalidcast noqa plw2901";"OUTD"
"moduletype numpydoc ignore rt03";"CODE"
"actual xp array namespace actual raises on scalars and lists";"CODE"
"dask uses nan instead of none for unknown shapes";"CODE"
"assert none not in actual shape requires explicit support";"CODE"
"actual shape actual compute shape type ignore attr defined pyright ignore reportattributeaccessissue";"CODE"
"desired shape desired compute shape type ignore attr defined pyright ignore reportattributeaccessissue";"CODE"
"ignore shape but check flattened size this is normally done by";"CODE"
"np testing assert array equal etc even when strict false but not for";"CODE"
"non materializable arrays";"-"
"actual size math prod actual shape pyright ignore reportunknownargumenttype";"-"
"desired size math prod desired shape pyright ignore reportunknownargumenttype";"-"
"only numpy distinguishes between scalars and arrays we do if check scalar";"CODE"
"important here we assume that we re not tracing";"CODE"
"e g we re not inside jax jit nor cupy cuda stream begin capture";"-"
"return not is torch array x or x device type meta type ignore attr defined pyright ignore reportattributeaccessissue";"CODE"
"return array todense type ignore attr defined pyright ignore reportattributeaccessissue";"IRRE"
"note only needed if the transfer guard is enabled";"TASK"
"multiplier of 4 is used as for np float64 this puts the default rtol";"CODE"
"roughly half way between sqrt eps and the default for";"CODE"
"numpy testing assert allclose 1e 7";"CODE"
"np testing assert allclose pyright ignore reportcallissue";"IRRE"
"rtol rtol pyright ignore reportargumenttype";"-"
"https github com numpy numpy blob v1 26 0 numpy lib arraysetops py l524 l758";"IRRE"
"isinstance x float returns true for np float64";"CODE"
"isinstance x complex returns true for np complex128";"META"
"bool is a subclass of int";"CODE"
"this includes misc malformed input e g str";"CODE"
"return a b type ignore return value";"IRRE"
"a is an array api object";"CODE"
"b is a int float complex bool";"CODE"
"https data apis org array api draft api specification type promotion html mixing arrays with python scalars";"CODE"
"undefined behaviour let the function deal with it if it can";"CODE"
"neither a nor b are array api objects";"IRRE"
"note we can only reach this point when one explicitly passes";"CODE"
"xp xp to the calling function otherwise we fail earlier on";"IRRE"
"array namespace a b";"-"
"dask arrays uses non standard nan instead of none";"CODE"
"quietly skip scalars and none s";"-"
"fixme https github com pydata sparse issues 876";"IRRE"
"boolean indexing is supported but not when the index is a sparse array";"IRRE"
"boolean indexing by list or numpy array is not part of the array api";"CODE"
"if out boolean indexing pragma no cover";"CODE"
"backwards compatibility with jax 0 6 0";"-"
"https github com jax ml jax issues 27418";"CODE"
"fixme https github com data apis array api issues 945";"CODE"
"if device type meta type ignore union attr pyright ignore reportattributeaccessissue reportoptionalmemberaccess";"META"
"fmt skip";"-"
"fmt skip";"-"
"class pickler pickle pickler numpydoc ignore gl08";"CODE"
"literal 0 1 none numpydoc ignore gl08";"CODE"
"instances append obj type ignore arg type";"CODE"
"if typ in basic pickled types no subclasses";"IRRE"
"if obj is a collection recursively descend inside it";"CODE"
"note a class that defines slots without defining getstate";"CODE"
"cannot be pickled with reduce but can with reduce ex 5";"META"
"except exception pylint disable broad exception caught";"CODE"
"object can be pickled let the pickler recursively descend inside it";"CODE"
"class unpickler pickle unpickler numpydoc ignore gl08";"CODE"
"numpydoc ignore gl08";"CODE"
"pylint disable missing module docstring duplicate code";"CODE"
"if type checking pragma no cover";"-"
"todo import override from typing requires python 3 12";"CODE"
"sphinx hacks";"-"
"when xp jax numpy this is similar to b jax jit myfunc a";"CODE"
"when xp dask array crash on compute or persist";"-"
"b myfunc a this is wrapped when xp jax numpy or xp dask array";"CODE"
"c mymodule myfunc a this is not";"CODE"
"b mymodule myfunc a this is wrapped when xp jax numpy or xp dask array";"CODE"
"c naked myfunc a this is not";"CODE"
"do not collect any tests in externals this is more robust than using";"CODE"
"ignore because ignore needs a path and it is not convenient to pass in";"TASK"
"the externals path very long install dependent path in site packages when";"CODE"
"using pyargs";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"this isn t something that people should be routing using in a pipeline";"CODE"
"dtype no validation validation delegated to numpy";"OUTD"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"this list of english stop words is taken from the glasgow information";"CODE"
"retrieval group the original list can be found at";"-"
"http ir dcs gla ac uk resources linguistic utils stop words";"CODE"
"use the array data from the first image in this dataset";"CODE"
"here are just two of these patches";"-"
"use the array data from the second image in this dataset";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"csr matrices can t be compared for equality";"IRRE"
"make two feature dicts with two useful features and a bunch of useless";"TASK"
"ones in terms of chi2";"CODE"
"generate equal dictionaries with different memory layouts";"-"
"check that the memory layout does not impact the resulting vocabulary";"IRRE"
"for vectorizers n features in does not make sense and does not exist";"CODE"
"mix byte and unicode strings note that foo is a duplicate in row 0";"TASK"
"it x for x in raw x iterable";"CODE"
"check the influence of the seed when computing the hashes";"-"
"assert that no zeros are materialized in the output";"CODE"
"check that some of the hashed tokens are added";"TASK"
"with an opposite sign and cancel out";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"negative elements are the diagonal the elements of the original";"-"
"image positive elements are the values of the gradient they";"IRRE"
"should all be equal on grad x and grad y";"-"
"check that the edges are in the right position";"CODE"
"when using a sparse image with a singleton component";"IRRE"
"checking that the function works with graphs containing no edges";"CODE"
"generating two convex parts with one vertex";"-"
"thus edges will be empty in to graph";"CODE"
"check ordering";"-"
"checking that the function works whatever the type of mask is";"CODE"
"checking dtype of the graph";"-"
"subsample by 4 to reduce run time";"CODE"
"subsample by 4 to reduce run time";"CODE"
"make a collection of faces";"-"
"request patches of the same size as image";"CODE"
"should return just the single patch a k a the image";"IRRE"
"this is 3185";"CODE"
"test same patch size for all dimensions";"IRRE"
"width and height of the patch should be less than the image";"-"
"check some classical latin accentuated symbols";"IRRE"
"check some arabic";"-"
"a u0625 alef with a hamza below";"-"
"expected u0627 simple alef";"-"
"mix letters accentuated and not";"CODE"
"strings that are already decomposed";"CODE"
"a o u0308 o with diaeresis";"-"
"combining marks by themselves";"-"
"multiple combining marks on one character";"CODE"
"check some classical latin accentuated symbols";"IRRE"
"check some arabic";"-"
"a u0625 halef with a hamza below";"-"
"expected halef has no direct ascii match";"-"
"mix letters accentuated and not";"CODE"
"with custom preprocessor";"-"
"with custom tokenizer";"-"
"decode error default to strict so this should fail";"CODE"
"first encode as bytes a unicode string";"CODE"
"then let the analyzer try to decode it as ascii it should fail";"CODE"
"because we have given it an incorrect encoding";"-"
"try a few of the supported types";"CODE"
"fit on stopwords only";"-"
"check that the check for uppercase in the provided vocabulary is only done at fit";"CODE"
"time and not at transform time 21251";"CODE"
"normalize white spaces";"-"
"no need to do any slicing for unigrams";"CODE"
"iterate through the string";"CODE"
"bind method outside of loop to reduce overhead";"IRRE"
"normalize white spaces";"-"
"bind method outside of loop to reduce overhead";"IRRE"
"if offset 0 count a short word w len n only once";"IRRE"
"accent stripping";"-"
"stop words are were previously validated";"-"
"nb stop words is validated unlike self stop words";"CODE"
"failed to check stop words consistency e g because a custom";"-"
"preprocessor or tokenizer was used";"-"
"dtype no validation delegate to numpy";"OUTD"
"triggers a parameter validation";"IRRE"
"add a new value when a new vocabulary item is seen";"CODE"
"ignore out of vocabulary items for fixed vocab true";"CODE"
"disable defaultdict behaviour";"CODE"
"if indptr 1 np iinfo np int32 max 2 31 1";"CODE"
"we intentionally don t call the transform method to make";"CODE"
"fit transform overridable without unwanted side effects in";"CODE"
"tfidfvectorizer";"-"
"use the same matrix building strategy as fit transform";"IRRE"
"we need csr format for fast row manipulations";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"mask 2 true select the first two features";"TASK"
"input features in which an element is true iff its";"TASK"
"true this is an integer array of shape output features whose";"CODE"
"upport boolean array of shape input features";"CODE"
"insert additional entries in indptr";"TASK"
"e g if transform changed indptr from 0 2 6 7 to 0 2 3";"CODE"
"col nonzeros here will be 2 0 1 so indptr becomes 0 2 2 3";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"selectfrommodel estimator is not validated yet";"CODE"
"todo slep6 remove when metadata routing cannot be disabled";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"here we rely on nearestneighbors to select the fastest algorithm";"IRRE"
"kdtree is explicitly fit to allow for the querying of number of";"CODE"
"neighbors within a specified radius";"-"
"ignore points with unique labels";"CODE"
"add small noise to continuous features as advised in kraskov et al";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"features in the friedman 1 dataset";"TASK"
"informative features in the friedman 1 dataset";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"sequentialfeatureselector estimator is not validated yet";"TASK"
"with auto feature selection n features to select will be updated";"CODE"
"to support sum after features are selected";"TASK"
"the current mask corresponds to the set of features";"TASK"
"that we have already selected if we do forward selection";"CODE"
"that we have already excluded if we do backward selection";"CODE"
"we only need to verify the routing here and not use the routed params";"TASK"
"because internally the actual routing will also take place inside the";"CODE"
"cross val score function";"CODE"
"return the best new feature and its score to add to the current mask";"TASK"
"i e return the best new feature and its score to add resp remove";"TASK"
"when doing forward selection resp backward selection";"CODE"
"feature will be added if the current score and past score are greater";"TASK"
"than tol when n feature is auto";"TASK"
"fixes issue 1240 nans can t be properly compared so change them to the";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"if hasattr x toarray sparse matrix";"IRRE"
"use peak to peak to avoid numeric precision issues";"CODE"
"for constant features";"CODE"
"check dtype matches";"-"
"check 1d list and other dtype";"CODE"
"check wrong shape raises error";"META"
"check dtype matches";"-"
"check wrong shape raises error";"META"
"check dtype matches";"-"
"check 1d list and other dtype";"CODE"
"check wrong shape raises error";"META"
"check dtype matches";"-"
"check wrong shape raises error";"META"
"feature 0 is highly informative for class 1";"CODE"
"feature 1 is the same everywhere";"TASK"
"feature 2 is a bit informative for class 2";"CODE"
"test the score functions";"IRRE"
"test that our f oneway gives the same result as scipy stats";"IRRE"
"smoke test f oneway on integers that it does raise casting errors";"CODE"
"with recent numpys";"-"
"test that is gives the same result as with float";"IRRE"
"test whether the f test yields meaningful results";"IRRE"
"on a simple simulated classification problem";"IRRE"
"testing against numpy for reference";"CODE"
"test whether the f test yields meaningful results";"IRRE"
"on a simple simulated regression problem";"-"
"with centering compare with sparse";"IRRE"
"again without centering compare with sparse";"IRRE"
"test whether f regression returns the same value";"IRRE"
"for any numeric data type";"CODE"
"test whether f regression preserves dof according to center argument";"IRRE"
"we use two centered variates so we have a simple relationship between";"IRRE"
"f score with variates centering and f score without variates centering";"CODE"
"create toy example";"IRRE"
"x np arange 5 6 reshape 1 1 x has zero mean";"-"
"y 0 0 0 have y mean being null";"-"
"assert almost equal f2 0 0 232558139 value from statsmodels ols";"CODE"
"a feature in x is constant forcing finite";"CODE"
"the target y is constant forcing finite";"CODE"
"a feature in x is constant not forcing finite";"CODE"
"the target y is constant not forcing finite";"CODE"
"a feature in x is constant forcing finite";"CODE"
"the target y is constant forcing finite";"CODE"
"feature in x correlated with y forcing finite";"CODE"
"feature in x anti correlated with y forcing finite";"CODE"
"a feature in x is constant not forcing finite";"CODE"
"the target y is constant not forcing finite";"CODE"
"feature in x correlated with y not forcing finite";"CODE"
"feature in x anti correlated with y not forcing finite";"CODE"
"test whether the f test yields meaningful results";"IRRE"
"on a simple simulated classification problem";"IRRE"
"test whether the relative univariate feature selection";"CODE"
"gets the correct items in a simple classification problem";"IRRE"
"with the percentile heuristic";"-"
"test whether the relative univariate feature selection";"CODE"
"gets the correct items in a simple classification problem";"IRRE"
"with the percentile heuristic";"-"
"check other columns are empty";"-"
"test univariate selection in classification settings";"IRRE"
"test whether the relative univariate feature selection";"CODE"
"gets the correct items in a simple classification problem";"IRRE"
"with the k best heuristic";"-"
"test whether k all correctly returns all features";"IRRE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 24949";"CODE"
"test whether k 0 correctly returns no features";"IRRE"
"test whether the relative univariate feature selection";"CODE"
"gets the correct items in a simple classification problem";"IRRE"
"with the fdr fwe and fpr heuristics";"-"
"test univariate selection in regression settings";"IRRE"
"test whether the relative univariate feature selection";"CODE"
"gets the correct items in a simple regression problem";"-"
"with the percentile heuristic";"-"
"check inverse transform respects dtype";"IRRE"
"test whether the relative univariate feature selection";"CODE"
"selects all features when 100 is asked";"TASK"
"test whether the relative univariate feature selection";"CODE"
"gets the correct items in a simple regression problem";"-"
"with the k best heuristic";"-"
"test whether the relative univariate feature selection";"CODE"
"gets the correct items in a simple regression problem";"-"
"with the fpr fdr or fwe heuristics";"-"
"test boundary case and always aim to select 1 feature";"CODE"
"test that fdr heuristic actually has low fdr";"IRRE"
"warnings can be raised when no features are selected";"CODE"
"low alpha or very noisy data";"-"
"as per benjamini hochberg the expected false discovery rate";"-"
"should be lower than alpha";"-"
"fdr e fp tp fp alpha";"-"
"make sure that the empirical false discovery rate increases";"-"
"with alpha";"-"
"test whether the relative univariate feature selection";"CODE"
"gets the correct items in a simple regression problem";"-"
"with the fwe heuristic";"-"
"test whether selectkbest actually selects k features in case of ties";"CODE"
"prior to 0 11 selectkbest would return more features than requested";"CODE"
"test if selectpercentile selects the right n features in case of ties";"CODE"
"test whether k best and percentiles work with tied pvalues from chi2";"IRRE"
"chi2 will return the same p values for the following features but it";"IRRE"
"will return different scores";"IRRE"
"test whether k best and percentiles works with multilabels with chi2";"IRRE"
"test for stable sorting in k best with tied scores";"IRRE"
"assert that selectkbest and selectpercentile can handle nans";"CODE"
"first feature has zero variance to confuse f classif anova and";"CODE"
"make it return a nan";"IRRE"
"test that f classif warns if a feature is constant throughout";"IRRE"
"generate random uncorrelated data a strict univariate test should";"IRRE"
"rejects all the features";"TASK"
"test in kbest mode";"IRRE"
"test in percentile mode";"IRRE"
"test in kbest mode";"IRRE"
"test in percentile mode";"IRRE"
"test that selectfrommodel fits on a clone of the estimator";"CODE"
"case 1 an error should be raised at transform if fit was not called to";"CODE"
"validate the attributes";"META"
"case 2 max features is not validated and different from an integer";"CODE"
"fixme we cannot validate the upper bound of the attribute at transform";"CODE"
"and we should force calling fit if we intend to force the attribute";"CODE"
"to have such an upper bound";"-"
"non regression test for 21949";"IRRE"
"linearregression does not implement partial fit and should raise an";"CODE"
"attributeerror";"META"
"in discrete case computations are straightforward and can be done";"CODE"
"by hand on given vectors";"CODE"
"for two continuous variables a good approach is to test on bivariate";"CODE"
"normal distribution where mutual information is known";"META"
"mean of the distribution irrelevant for mutual information";"CODE"
"setup covariance matrix with correlation coeff equal 0 5";"IRRE"
"true theoretical mutual information";"CODE"
"theory and computed values won t be very close";"IRRE"
"we here check with a large relative tolerance";"-"
"to test define a joint distribution as follows";"CODE"
"p x y p x p y x";"-"
"x bernoulli p";"-"
"y x 0 uniform 1 1";"CODE"
"y x 1 uniform 0 2";"CODE"
"use the following formula for mutual information";"CODE"
"i x y h y h y x";"-"
"two entropies can be computed by hand";"-"
"h y 1 p 2 ln 1 p 2 p 2 log p 2 1 2 log 1 2";"-"
"h y x ln 2";"-"
"now we need to implement sampling from out distribution which is";"TASK"
"done easily using conditional distribution logic";"META"
"assert the same tolerance";"CODE"
"test that adding unique label doesn t change mi";"TASK"
"we are going test that feature ordering by mi matches our expectations";"TASK"
"here x 0 is the most informative feature and x 1 is weakly";"TASK"
"informative";"CODE"
"we generate sample from multivariate normal distribution using";"CODE"
"transformation from initially uncorrelated variables the zero";"CODE"
"variables after transformation is selected as the target vector";"CODE"
"it has the strongest correlation with the variable 2 and";"IRRE"
"the weakest correlation with the variable 1";"IRRE"
"xxx should mutual info regression be fixed to avoid";"CODE"
"up casting float32 inputs to float64";"CODE"
"here the target is discrete and there are two continuous and one";"CODE"
"discrete feature the idea of this test is clear from the code";"CODE"
"check that the continuous values have a higher mi with greater";"IRRE"
"n neighbors";"-"
"the n neighbors should not have any effect on the discrete value";"IRRE"
"the mi should be the same";"-"
"add some irrelevant features random seed is set to make sure that";"IRRE"
"irrelevant features are always irrelevant";"TASK"
"check if the supports are equal";"IRRE"
"add some irrelevant features random seed is set to make sure that";"IRRE"
"irrelevant features are always irrelevant";"TASK"
"dense model";"-"
"sparse model";"IRRE"
"make sure rfe passes the metadata down to fit and score methods of the";"CODE"
"underlying estimator";"-"
"test that the results are the same";"IRRE"
"add some irrelevant features random seed is set to make sure that";"IRRE"
"irrelevant features are always irrelevant";"TASK"
"there are 10 features in the data we select 40";"CODE"
"add some irrelevant features random seed is set to make sure that";"IRRE"
"irrelevant features are always irrelevant";"TASK"
"dense model";"-"
"add some irrelevant features random seed is set to make sure that";"IRRE"
"irrelevant features are always irrelevant";"TASK"
"y list iris target regression test list should be supported";"IRRE"
"test using the score function";"IRRE"
"non regression test for missing worst feature";"TASK"
"all the noisy variable were filtered out";"CODE"
"same in sparse";"IRRE"
"test using a customized loss function";"IRRE"
"test using a scorer";"IRRE"
"test fix on cv results";"IRRE"
"in the event of cross validation score ties the expected behavior of";"CODE"
"rfecv is to return the fewest features that maximize the cv score";"TASK"
"because test scorer always returns 1 0 in this example rfecv should";"IRRE"
"reduce the dimensionality to a single feature i e n features 1";"TASK"
"same as the first two tests but with step 2";"IRRE"
"verifying that steps 1 don t blow up";"CODE"
"y list iris target regression test list should be supported";"IRRE"
"test using the score function";"IRRE"
"non regression test for missing worst feature";"TASK"
"check verbose 1 is producing an output";"IRRE"
"y list iris target regression test list should be supported";"IRRE"
"non regression test for varying combinations of step and";"CODE"
"min features to select";"TASK"
"make sure that cross validation is stratified";"-"
"test when floor step n features 0";"TASK"
"test when step is between 0 1 and floor step n features 0";"TASK"
"test when step is an integer";"IRRE"
"in rfe number of subsets of features";"TASK"
"the number of iterations in fit";"-"
"max ranking";"-"
"1 n features step n features to select 1 step";"TASK"
"after optimization 4534 this number";"CODE"
"1 np ceil n features n features to select float step";"TASK"
"this test case is to test their equivalence refer to 4534 and 3824";"IRRE"
"rfe";"-"
"case 1 n features n features to select is divisible by step";"TASK"
"case 2 n features n features to select is not divisible by step";"TASK"
"this number also equals to the maximum of ranking";"CODE"
"in rfecv fit calls rfe fit";"IRRE"
"number of subsets of features of rfe";"TASK"
"the size of each score in cv results of rfecv";"IRRE"
"the number of iterations of the for loop before optimization 4534";"CODE"
"rfecv n features to select 1";"TASK"
"case 1 n features 1 is divisible by step";"TASK"
"case 2 n features 1 is not divisible by step";"TASK"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 15312";"CODE"
"add nan and inf value to x";"TASK"
"linearregression does not implement decision function and should raise an";"CODE"
"attributeerror";"META"
"create rfe rfecv with n features to select min features to select";"TASK"
"larger than the number of features present in the x variable";"CODE"
"make sure n features to select is respected";"TASK"
"test passing a float as n features to select";"CODE"
"2 0 2 f1 is dropped since it has no predictive power";"-"
"1 2 f2 is more predictive than f0 so it s kept";"-"
"basic sanity check 3 features only f0 and f2 are correlated with the";"TASK"
"target f2 having a stronger correlation than f0 we expect f1 to be";"-"
"dropped and f2 to always be selected";"CODE"
"make sure sparse data is supported";"IRRE"
"make sure nans are ok if the underlying estimator supports nans";"-"
"linearregression does not support nans";"CODE"
"make sure that pipelines can be passed into sfs and that sfs can be";"CODE"
"passed into a pipeline";"CODE"
"pipeline in sfs";"CODE"
"sfs in pipeline";"CODE"
"make sure that models without classification labels are not being";"IRRE"
"validated";"-"
"make sure that other non conventional y labels are not accepted";"-"
"non regression test for 25525";"IRRE"
"non regression test for 25957";"IRRE"
"test variancethreshold with default setting zero variance";"CODE"
"test variancethreshold with default setting zero variance error cases";"CODE"
"test variancethreshold with custom variance";"CODE"
"test that variancethreshold 0 0 fit eliminates features that have";"TASK"
"the same value in every sample even when floating point errors";"CODE"
"cause np var not to be 0 for the feature";"CODE"
"see 13691";"-"
"add single nan and feature should still be included";"TASK"
"make all values in feature nan and feature should be rejected";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"raise original attributeerror if attr does not exist";"CODE"
"estimator s attributes are now accessible except fit predict and";"META"
"fit transform";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"this should be no op";"CODE"
"only here to test that it doesn t get called";"IRRE"
"pragma no cover";"-"
"only here to test that it doesn t get called";"IRRE"
"pragma no cover";"-"
"obj func is the objective function to be maximized which";"CODE"
"takes the hyperparameters theta as parameter and an";"IRRE"
"optional flag eval gradient which determines if the";"-"
"gradient is returned additionally to the function value";"IRRE"
"initial theta the initial value for theta which can be";"IRRE"
"used by local optimizers";"-"
"bounds the bounds on the values of theta";"IRRE"
"returned are the best found hyperparameters theta and";"IRRE"
"the corresponding value of the target function";"IRRE"
"obj func is the objective function to be maximized which";"CODE"
"takes the hyperparameters theta as parameter and an";"IRRE"
"optional flag eval gradient which determines if the";"-"
"gradient is returned additionally to the function value";"IRRE"
"initial theta the initial value for theta which can be";"IRRE"
"used by local optimizers";"-"
"bounds the bounds on the values of theta";"IRRE"
"returned are the best found hyperparameters theta and";"IRRE"
"the corresponding value of the target function";"IRRE"
"if theta shape 0 n dims use same theta for all sub kernels";"CODE"
"theta for compound kernel";"CODE"
"obj func the objective function to be minimized which";"CODE"
"takes the hyperparameters theta as a parameter and an";"IRRE"
"optional flag eval gradient which determines if the";"-"
"gradient is returned additionally to the function value";"IRRE"
"initial theta the initial value for theta which can be";"IRRE"
"used by local optimizers";"-"
"bounds the bounds on the values of theta";"IRRE"
"returned are the best found hyperparameters theta and";"IRRE"
"the corresponding value of the target function";"IRRE"
"convert from upper triangular matrix to square matrix";"CODE"
"hyperparameter l kept fixed";"IRRE"
"we need to recompute the pairwise dimension wise distances";"TASK"
"else isotropic";"-"
"else general case expensive to evaluate";"CODE"
"k k 0 0 np finfo float eps strict zeros result in nan";"IRRE"
"convert from upper triangular matrix to square matrix";"CODE"
"hyperparameter l kept fixed";"IRRE"
"we need to recompute the pairwise dimension wise distances";"TASK"
"approximate gradient numerically";"IRRE"
"def f theta helper function";"CODE"
"gradient with respect to length scale";"-"
"else l is kept fixed";"-"
"gradient with respect to alpha";"-"
"else alpha is kept fixed";"-"
"adapted from scipy optimize optimize py for functions with 2d output";"CODE"
"approximate gradient numerically";"IRRE"
"def f gamma helper function";"CODE"
"we have to fall back to slow way of computing diagonal";"-"
"check that the latent mean and variance have the right shape";"CODE"
"larger than unity this test was made in response to issue 15612";"IRRE"
"by convention single output data is squeezed upon prediction";"IRRE"
"number of spatial locations to predict at";"-"
"number of sample predictions per test point";"IRRE"
"by convention single output data is squeezed upon prediction";"IRRE"
"fixme before fitting the estimator does not have information regarding";"CODE"
"the number of targets and default to 1 this is inconsistent with the shape";"CODE"
"provided after fit this assert should be made once the following issue";"CODE"
"is fixed";"-"
"https github com scikit learn scikit learn issues 22430";"CODE"
"y samples model sample y x test n samples n samples y test";"IRRE"
"assert y samples shape y test shape";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"allow using pd na as missing values to impute numerical arrays";"IRRE"
"raise pragma no cover";"CODE"
"compute the most frequent value in array only";"IRRE"
"scipy stats mode is slow with object dtype array";"IRRE"
"python counter is more efficient";"CODE"
"tie breaking similarly to scipy stats mode";"CODE"
"compare to array extra value n repeat";"IRRE"
"tie breaking similarly to scipy stats mode";"CODE"
"sp hstack may result in different formats between sparse arrays and";"IRRE"
"matrices specify the format to keep consistent behavior";"CODE"
"fill value no validation any object is valid";"IRRE"
"if input is a list of strings dtype object";"CODE"
"otherwise valueerror is raised in simpleimputer";"IRRE"
"with strategy most frequent or constant";"CODE"
"because the list is converted to unicode numpy array";"IRRE"
"use object dtype if fitted on object dtypes";"CODE"
"use the dtype seen in fit for non fit conversion";"IRRE"
"missing values 0 not allowed with sparse data as it would";"IRRE"
"force densification";"CODE"
"by default fill value none and the replacement is always";"IRRE"
"compatible with the input data";"CODE"
"make sure we can safely cast fill value dtype to the input data dtype";"IRRE"
"default fill value is 0 for numerical input and missing value";"CODE"
"otherwise";"-"
"mean";"-"
"avoid the warning warning converting a masked element to nan";"CODE"
"median";"-"
"avoid the warning warning converting a masked element to nan";"CODE"
"most frequent";"-"
"avoid use of scipy stats mstats mode due to the required";"CODE"
"additional overhead and slow benchmarking performance";"TASK"
"see issue 14325 and pr 14399 for full discussion";"CODE"
"to be able access the elements by columns";"-"
"constant";"CODE"
"for constant strategy self statistcs is used to store";"CODE"
"fill value in each column or np nan for columns to drop";"IRRE"
"custom";"-"
"compute mask before eliminating invalid features";"TASK"
"decide whether to keep missing features";"TASK"
"same as np isnan but also works for object dtypes";"CODE"
"use feature names warning if features are provided";"TASK"
"do actual imputation";"CODE"
"if no invalid statistics are found use the mask computed";"IRRE"
"before else recompute mask";"CODE"
"use mask computed before eliminating invalid mask";"OUTD"
"count number of true values in each row";"IRRE"
"missing values 0 not allowed with sparse data as it would";"IRRE"
"force densification";"CODE"
"need not validate x again as it would have already been validated";"CODE"
"in the imputer calling missingindicator";"IRRE"
"only create n features in in the precomputed case";"CODE"
"need not validate x again as it would have already been validated";"CODE"
"in the imputer calling missingindicator";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"if hasattr x1 mask pandas dataframes";"-"
"else ndarrays";"CODE"
"fill value no validation any object is valid";"IRRE"
"if no missing values don t predict";"IRRE"
"get posterior samples if there is at least one missing value";"IRRE"
"two types of problems 1 non positive sigmas";"-"
"2 mus outside legal range of min value and max value";"IRRE"
"results in inf sample";"IRRE"
"the rest can be sampled without statistical issues";"-"
"update the feature";"TASK"
"if a feature in the neighborhood has only a single value";"TASK"
"e g categorical feature the std dev will be null and";"TASK"
"np corrcoef will raise a warning due to a division by zero";"CODE"
"np corrcoef is not defined for features with zero std";"CODE"
"ensures exploration i e at least some probability of sampling";"-"
"features are not their own neighbors";"TASK"
"needs to sum to 1 for np random choice sampling";"IRRE"
"drop empty features";"TASK"
"mark empty features as not missing and keep the original";"TASK"
"imputation";"-"
"make sure to remove the empty feature elements from the bounds";"TASK"
"iterativeimputer estimator is not validated yet";"TASK"
"edge case a single feature we return the initial imputation";"IRRE"
"order in which to impute";"-"
"note this is probably too slow for large feature data d 100000";"TASK"
"and a better way would be good";"-"
"see https goo gl kycnwj and subsequent comments";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"get donors";"CODE"
"get weight matrix from distance matrix";"CODE"
"fill nans with zeros";"-"
"retrieve donor values and calculate knn average";"IRRE"
"check data integrity and calling arguments";"IRRE"
"removes columns where the training data is all nan";"-"
"no missing values in x";"IRRE"
"even if there are no missing values in x we still concatenate xc";"IRRE"
"with the missing value indicator matrix x indicator";"IRRE"
"this is to ensure that the output maintains consistency in terms";"CODE"
"of columns regardless of whether missing values exist in x or not";"IRRE"
"maps from indices from x to indices in dist matrix";"CODE"
"find and impute missing by column";"-"
"column was all missing during training";"-"
"column has no missing values";"IRRE"
"receivers idx are indices in x";"-"
"distances for samples that needed imputation for column";"CODE"
"receivers with all nan distances impute with mean";"-"
"all receivers imputed with mean";"-"
"receivers with at least one defined distance";"CODE"
"process in fixed memory chunks";"-"
"process chunk modifies x in place no return value";"IRRE"
"from sklearn experimental import enable iterative imputer noqa f401";"CODE"
"convergencewarning will be raised by the iterativeimputer";"CODE"
"non regression test for issue 13968 missing value in test set should";"IRRE"
"not throw an error and return a finite dataset";"IRRE"
"convergencewarning will be raised by the iterativeimputer";"CODE"
"convergencewarning will be raised by the iterativeimputer";"CODE"
"imputer clone imputer avoid side effects from shared instances";"CODE"
"convergencewarning will be raised by the iterativeimputer";"CODE"
"test pandas integerarray with pd na";"IRRE"
"fit on numpy array";"-"
"creates dataframe with integerarrays with pd na";"IRRE"
"fit on pandas dataframe with integerarrays";"CODE"
"test data where missing value test variable can be set to np nan or 1";"IRRE"
"make iterativeimputer available";"-"
"from sklearn experimental import enable iterative imputer noqa f401";"CODE"
"normal matrix";"-"
"sparse matrix";"IRRE"
"verify the shapes of the imputed matrix for different strategies";"CODE"
"check simpleimputer returning feature name attribute correctly";"TASK"
"ensure that skipped feature warning includes feature name";"TASK"
"check that error are raised when missing values 0 and input is sparse";"IRRE"
"np median raises a typeerror for numpy 1 10 1";"CODE"
"np mean raises a runtimewarning for numpy 1 10 1";"CODE"
"test imputation using the mean and median strategies when";"IRRE"
"missing values 0";"IRRE"
"create a matrix x with columns";"IRRE"
"with only zeros";"-"
"with only missing values";"IRRE"
"with zeros missing values and values";"IRRE"
"and a matrix x true containing all true values";"IRRE"
"create the columns";"IRRE"
"xxx unreached code as of v0 22";"-"
"shuffle them the same way";"CODE"
"mean doesn t support columns containing nans median does";"CODE"
"test median imputation with sparse boundary cases";"IRRE"
"0 np nan np nan odd implicit zero";"-"
"5 np nan np nan odd explicit nonzero";"-"
"0 0 np nan even average two zeros";"-"
"5 0 np nan even avg zero and neg";"-"
"0 5 np nan even avg zero and pos";"-"
"4 5 np nan even avg nonzeros";"-"
"4 5 np nan even avg negatives";"-"
"1 2 np nan even crossing neg and pos";"-"
"test imputation on non numeric data using most frequent and constant";"IRRE"
"strategy";"-"
"test imputation using the most frequent strategy";"IRRE"
"scipy stats mode used in simpleimputer doesn t return the first most";"CODE"
"frequent as promised in the doc but the lowest most frequent when this";"CODE"
"test will fail after an update of scipy simpleimputer will need to be";"TASK"
"updated to be consistent with the new correct behaviour";"CODE"
"test imputation using the most frequent strategy";"IRRE"
"test imputation using the most frequent strategy on pandas df";"IRRE"
"verify that exceptions are raised on invalid fill value type";"CODE"
"test imputation using the constant strategy on integers";"CODE"
"test imputation using the constant strategy on floats";"CODE"
"test imputation using the constant strategy on objects";"IRRE"
"test imputation using the constant strategy on pandas df";"IRRE"
"check we exit early when there is a single feature";"TASK"
"test imputation within a pipeline gridsearch";"CODE"
"test imputation with copy";"IRRE"
"copy true dense copy";"-"
"copy true sparse csr copy";"IRRE"
"copy false dense no copy";"-"
"copy false sparse csc no copy";"IRRE"
"copy false sparse csr copy";"IRRE"
"note if x is sparse and if missing values 0 then a dense copy of x is";"IRRE"
"made even if copy false";"-"
"with max iter 0 only initial imputation is performed";"IRRE"
"repeat but force n iter to 0";"META"
"transformed should not be equal to initial imputation";"IRRE"
"now they should be equal as only initial imputation is done";"IRRE"
"x 0 1 this column should not be discarded by iterativeimputer";"CODE"
"check that types are correct for estimators";"CODE"
"check that each estimator is unique";"-"
"test that the values that are imputed using sample posterior true";"IRRE"
"with boundaries min value and max value are not none are drawn";"IRRE"
"from a distribution that looks gaussian via the kolmogorov smirnov test";"IRRE"
"note that starting from the wrong random seed will make this test fail";"IRRE"
"because random sampling doesn t occur at all when the imputation";"IRRE"
"is outside of the min value max value range";"IRRE"
"generate multiple imputations for the single missing value";"IRRE"
"we want to fail to reject null hypothesis";"-"
"null hypothesis distributions are the same";"META"
"x train 0 1 definitely no missing values in 0th column";"IRRE"
"x test 0 0 0 definitely missing value in 0th column";"IRRE"
"if there were no missing values at time of fit then imputer will";"IRRE"
"only use the initial imputer for that feature at transform";"IRRE"
"when sample posterior true two transforms shouldn t be equal";"CODE"
"sufficient to assert that the means are not the same";"CODE"
"when sample posterior false and n nearest features none";"TASK"
"and imputation order is not random";"IRRE"
"the two transforms should be identical even if rng are different";"CODE"
"should exclude the first column entirely";"-"
"fit and fit transform should both be identical";"CODE"
"split up data in half";"-"
"a quarter is randomly missing";"IRRE"
"split up data";"-"
"check that we catch a runtimewarning due to a division by zero when a";"CODE"
"feature is constant in the dataset";"CODE"
"simulate that a feature only contain one category during fit";"TASK"
"add some missing values";"IRRE"
"check that passing scalar or array like";"-"
"for min value and max value in iterativeimputer works";"IRRE"
"check that passing scalar or array like";"-"
"for min value and max value in iterativeimputer works";"IRRE"
"test that none inf and scalar vector give the same imputation";"IRRE"
"check the imputing strategy when missing data are present in the";"CODE"
"testing set only";"IRRE"
"taken from https github com scikit learn scikit learn issues 14383";"CODE"
"impute with the initial strategy mean";"IRRE"
"convert the input to the right array format and right dtype";"CODE"
"test for sparse input and missing value 0";"IRRE"
"convert the input to the right array format";"CODE"
"check the format of the output with different sparse parameter";"IRRE"
"regression test for issue 11390 comparison between incoherent dtype";"IRRE"
"for x and missing values was not raising a proper error";"IRRE"
"check that all features are dropped if there are no missing values when";"IRRE"
"features missing only 13491";"TASK"
"check that non missing values don t become explicit zeros in the mask";"IRRE"
"generated by missing indicator when x is sparse 13491";"IRRE"
"regression test for 15393";"IRRE"
"test inverse transform feature for np nan";"IRRE"
"x 2 trans imputer transform x 2 test on new data";"CODE"
"array of object dtype";"IRRE"
"array of numeric dtype";"-"
"impute pandas array of string types";"CODE"
"impute pandas array of string types without any missing values";"IRRE"
"impute pandas array of integer types";"CODE"
"use np nan also works";"-"
"impute pandas array of integer types with median strategy";"CODE"
"impute pandas array of integer types with mean strategy";"CODE"
"impute pandas array of float types";"CODE"
"impute pandas array of float types with median strategy";"CODE"
"non regression test for 19572";"IRRE"
"sparse matrix";"IRRE"
"cannot cast fill value at fit";"IRRE"
"cannot cast fill value at transform";"IRRE"
"check that no error is raised when having the same kind of dtype";"CODE"
"np array 1 2 3 4 5 6 7 8 without empty feature";"TASK"
"np array np nan 2 3 4 np nan 6 7 8 empty feature at column 0";"TASK"
"np array 1 2 3 np nan 5 6 7 np nan empty feature at column 3";"TASK"
"verify the shapes of the imputed matrix for different weights and";"CODE"
"number of neighbors";"-"
"test imputation with default values and invalid input";"IRRE"
"test with inf present";"IRRE"
"test with inf present in matrix passed in transform";"CODE"
"test with missing values 0 when nan present";"IRRE"
"test with an imputable matrix and compare with different missing values";"IRRE"
"test with an imputable matrix";"IRRE"
"test when there is not enough neighbors";"TASK"
"not enough neighbors use column mean from training";"TASK"
"test when data in fit and transform are different";"CODE"
"test with uniform weight or unweighted";"CODE"
"test with callable weight";"IRRE"
"test with callable uniform weight";"IRRE"
"test with distance weight";"IRRE"
"manual calculation";"-"
"nearestneighbor calculation";"-"
"test with weights distance and n neighbors 2";"IRRE"
"neighbors are rows 1 2 the nan euclidean distances are";"CODE"
"test with varying missingness patterns";"IRRE"
"get weights of donor neighbors";"CODE"
"collect donor values";"IRRE"
"final imputed values";"IRRE"
"calculate weights";"-"
"calculate weighted averages";"-"
"define callable metric that returns the l1 norm";"IRRE"
"note that we use working memory 0 to ensure that chunking is tested even";"TASK"
"for a small dataset however it should raise a userwarning that we ignore";"CODE"
"samples with needed feature has nan distance";"TASK"
"samples with nan distance should be excluded from the mean computation";"CODE"
"grid resolution 2 doctest skip";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"get the column names for a pandas dataframe";"CODE"
"define a list of numbered indices for a numpy array";"CODE"
"convert numpy array or pandas index to a list";"-"
"work on a copy of x to ensure thread safety in case of threading based";"CODE"
"parallelism furthermore making a copy is also useful when the joblib";"-"
"backend is loky default or the old multiprocessing in those cases";"CODE"
"if x is large it will be automatically be backed by a readonly memory map";"CODE"
"memmap x copy on the other hand is always guaranteed to return a";"IRRE"
"writable data structure whose columns can be shuffled inplace";"CODE"
"precompute random seed from the random state to be used";"IRRE"
"to get a fresh independent randomstate instance for each";"CODE"
"parallel call to calculate permutation scores irrespective of";"IRRE"
"the fact that variables are shared or not depending on the active";"CODE"
"joblib backend sequential thread based or process based";"CODE"
"unpack the permuted scores";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"else self response ndim 3";"CODE"
"special case for the tab10 and tab20 colormaps that encode a";"CODE"
"discrete set of colors that are easily distinguishable";"IRRE"
"contrary to other colormaps that are continuous";"-"
"for linearsegmentedcolormap";"CODE"
"plot only argmax map for contour";"CODE"
"re raise a more informative error message since pos label is unknown";"CODE"
"to our user when interacting with";"CODE"
"decisionboundarydisplay from estimator";"CODE"
"convert classes predictions into integers";"CODE"
"for the multiclass case get response values returns the response";"CODE"
"as is thus we have a column per class and we need to select the";"CODE"
"column corresponding to the positive class";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"set target idx for multi class estimators";"IRRE"
"regression and binary classification";"IRRE"
"use check array only on lists and other non array likes sparse do not";"CODE"
"convert dataframe into a numpy array";"CODE"
"expand kind to always be a list of str";"-"
"convert features into a seq of int tuples";"CODE"
"store the information if 2 way pd was requested with ice to later";"CODE"
"raise a valueerror with an exhaustive list of problematic";"IRRE"
"settings";"IRRE"
"raise an error and be specific regarding the parameter values";"IRRE"
"when 1 and 2 way pd were requested";"CODE"
"we need to create a boolean indicator of which features are";"TASK"
"categorical from the categorical features list";"TASK"
"categorical features provided as a list of boolean";"TASK"
"categorical features provided as a list of indices or feature names";"TASK"
"collect the indices of the categorical features targeted by the partial";"TASK"
"dependence computation";"CODE"
"early exit if the axes does not have the correct number of axes";"CODE"
"compute predictions and or averaged predictions";"CODE"
"for multioutput regression we can only check the validity of target";"IRRE"
"now that we have the predictions";"-"
"also note as multiclass multioutput classifiers are not supported";"IRRE"
"multiclass and multioutput scenario are mutually exclusive so there is";"IRRE"
"no risk of overwriting target idx here";"-"
"pd result pd results 0 checking the first result is enough";"IRRE"
"change plotting method for second plot";"CODE"
"change plotting method for second plot";"CODE"
"pandas column names are used by default";"CODE"
"second call to plot will have the names";"IRRE"
"axes with a label will not get overridden";"-"
"labels get overridden only if provided to the plot method";"-"
"labels do not get inferred if provided to from estimator";"CODE"
"in matplotlib v3 5 default value of pcolormesh shading is flat which";"IRRE"
"results in the last row and column being dropped thus older versions produce";"IRRE"
"a 99x99 grid while newer versions produce a 100x100 grid";"CODE"
"get which class has highest response and check it is plotted";"CODE"
"note quadmesh mask is true i e masked when idx is not the highest class";"TASK"
"diabetes dataset subsampled for speed";"IRRE"
"test partial dependence plot function";"CODE"
"use columns 0 2 as 1 is not quantitative sex";"-"
"deciles lines always show on xaxis only show on yaxis if 2 way pdp";"-"
"two feature position";"TASK"
"check with str features and array feature names and single column";"TASK"
"line";"-"
"contour";"-"
"contour";"-"
"with axes object";"IRRE"
"the first call to plot partial dependence will create two new axes to";"IRRE"
"place in the space of the passed in axes which results in a total of";"IRRE"
"three axes in the figure";"CODE"
"currently the api does not allow for the second call to";"CODE"
"plot partial dependence to use the same axes again because it will";"IRRE"
"create two new axes in the space resulting in five axes to get the";"IRRE"
"expected behavior one needs to pass the generated axes into the second";"TASK"
"call";"IRRE"
"disp1 plot partial dependence";"CODE"
"disp2 plot partial dependence ax disp1 axes";"CODE"
"second call to plot does not change the feature names from the first";"CODE"
"call";"IRRE"
"test partial dependence plot function on multi class input";"CODE"
"now with symbol labels";"-"
"check that the pd plots are different for another target";"CODE"
"test partial dependence plot function on multi output input";"CODE"
"non regression test to be sure to not override the ylabel if it has been";"IRRE"
"see https github com scikit learn scikit learn issues 15772";"CODE"
"single feature";"TASK"
"interaction between two features";"TASK"
"check that the subsampling is properly working";"-"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn pull 18359";"CODE"
"check that we anchor to zero x axis when centering";"-"
"kind average len kind len features";"TASK"
"alter kind to be a list with a length different from length of features";"TASK"
"toy sample";"-"
"x y n targets as expected in the output of partial dep";"IRRE"
"iris";"-"
"check that partial dependence has consistent output shape for different";"CODE"
"kinds of estimators";"-"
"classifiers with binary and multiclass settings";"IRRE"
"regressors";"-"
"multi task regressors";"TASK"
"est set params n estimators 2 speed up computations";"IRRE"
"n target corresponds to the number of classes 1 for binary classif or";"CODE"
"the number of tasks outputs in multi task settings it s equal to 1 for";"TASK"
"classical regression data";"IRRE"
"else both";"-"
"tests for grid from x sanity check for output and for shapes";"CODE"
"make sure that the grid is a cartesian product of the input it will use";"CODE"
"the unique values instead of the percentiles";"IRRE"
"test shapes of returned objects depending on the number of unique values";"IRRE"
"for a feature";"TASK"
"n unique values grid resolution";"IRRE"
"n unique values grid resolution will use actual values";"IRRE"
"rng shuffle x just to make sure the order is irrelevant";"-"
"axes is a list of arrays of different shapes";"-"
"check that uses custom range";"-"
"axes is a list of arrays of different shapes";"-"
"check that grid resolution does not impact custom range";"CODE"
"axes is a list of arrays of different shapes";"-"
"2 since n categories 2 we should not use quantiles resampling";"-"
"check that what is returned by partial dependence brute or";"CODE"
"partial dependence recursion is equivalent to manually setting a target";"IRRE"
"feature to a given value and computing the average prediction over all";"TASK"
"samples";"-"
"this also checks that the brute and recursion methods give the same";"CODE"
"output";"IRRE"
"note that even on the trainset the brute and the recursion methods";"TASK"
"aren t always strictly equivalent in particular when the slow method";"-"
"generates unrealistic samples that have low mass in the joint";"CODE"
"distribution of the input features and when some of the features are";"TASK"
"dependent hence the high tolerance on the checks";"CODE"
"the init estimator for gbdt here the average prediction isn t taken";"IRRE"
"into account with the recursion method for technical reasons we set";"CODE"
"the mean to 0 to that this bug doesn t have any effect";"CODE"
"clone is necessary to make the test thread safe";"IRRE"
"target feature will be set to 5 and then to 123";"TASK"
"pdp pdp 0 shape is 1 2 so make it 2";"-"
"allow for greater margin for error with recursion method";"CODE"
"make sure that the recursion method gives the same results on a";"IRRE"
"decisiontreeregressor and a gradientboostingregressor or a";"IRRE"
"randomforestregressor with 1 tree and equivalent parameters";"IRRE"
"purely random dataset to avoid correlated features";"IRRE"
"the init estimator for gbdt here the average prediction isn t taken";"IRRE"
"into account with the recursion method for technical reasons we set";"CODE"
"the mean to 0 to that this bug doesn t have any effect";"CODE"
"set max depth not too high to avoid splits with same gain but different";"IRRE"
"features";"TASK"
"the forest will use ensemble base set random states to set the";"IRRE"
"random state of the tree sub estimator we simulate this here to have";"IRRE"
"equivalent estimators";"IRRE"
"sanity check if the trees aren t the same the pd values won t be equal";"IRRE"
"for some reason the trees aren t exactly equal on 32bits so the pds";"CODE"
"cannot be equal either see";"-"
"https github com scikit learn scikit learn issues 8853";"CODE"
"make sure the recursion method implicitly uses decision function has";"CODE"
"the same result as using brute method with";"IRRE"
"response method decision function";"CODE"
"assert np mean y 0 5 make sure the init estimator predicts 0 anyway";"IRRE"
"if the target y only depends on one feature in an obvious way linear or";"TASK"
"quadratic then the partial dependence for that feature should reflect";"CODE"
"we here fit a linear regression data model with polynomial features if";"TASK"
"needed and compute r squared to check that the partial dependence";"CODE"
"correctly reflects the target";"-"
"add polynomial features if needed";"TASK"
"make sure error is raised for multiclass multioutput classifiers";"CODE"
"make multiclass multioutput dataset";"IRRE"
"simulate that we have some classes";"IRRE"
"check that array like objects are accepted";"IRRE"
"make sure that passing a non constant init parameter to a gbdt and using";"IRRE"
"recursion method yields a warning";"-"
"test near perfect correlation between partial dependence and diagonal";"IRRE"
"when sample weights emphasize y x predictions";"-"
"non regression test for 13193";"IRRE"
"todo extend to histgradientboosting once sample weight is supported";"CODE"
"set y x on mask and y x outside";"IRRE"
"sample weights to emphasize data points where y x";"CODE"
"todo remove fix when pdp supports hgbt with sample weights";"TASK"
"check that the partial dependence support pipeline";"CODE"
"check that the partial dependence support dataframe and pipeline";"CODE"
"including a column transformer";"CODE"
"the column transformer will reorder the column when transforming";"CODE"
"we mixed the index to be sure that we are computing the partial";"-"
"dependence of the right columns";"CODE"
"check all possible features type supported in pdp";"TASK"
"with pytest raises valueerror match the column 0 contains mixed data types";"IRRE"
"the following should not raise as we do not compute numerical partial";"CODE"
"dependence on integer columns";"CODE"
"the following should not raise as we do not compute numerical partial";"CODE"
"dependence on integer columns";"CODE"
"make sure that feature highly correlated to the target have a higher";"TASK"
"importance";"CODE"
"the correlated feature with y was added as the last column and should";"TASK"
"have the highest importance";"CODE"
"make sure that feature highly correlated to the target have a higher";"TASK"
"importance";"CODE"
"adds feature correlated with y as the last column";"TASK"
"the correlated feature with y was added as the last column and should";"TASK"
"have the highest importance";"CODE"
"permutation variable importance should not be affected by the high";"CODE"
"cardinality bias of traditional feature importances especially when";"CODE"
"computed on a held out test set";"IRRE"
"generate a multiclass classification dataset and a set of informative";"IRRE"
"binary features that can be used to predict some classes of y exactly";"TASK"
"while leaving some classes unexplained to make the problem harder";"CODE"
"not all target classes are explained by the binary class indicator";"IRRE"
"features";"TASK"
"add 10 other noisy features with high cardinality numerical values";"TASK"
"that can be used to overfit the training data";"OUTD"
"split the dataset to be able to evaluate on a held out test set the";"IRRE"
"test size should be large enough for importance measurements to be";"CODE"
"stable";"-"
"variable importances computed by impurity decrease on the tree node";"CODE"
"splits often use the noisy features in splits this can give misleading";"TASK"
"impression that high cardinality noisy variables are the most important";"CODE"
"let s check that permutation based feature importances do not have this";"CODE"
"problem";"-"
"split the importances between informative and noisy features";"CODE"
"because we do not have a binary variable explaining each target classes";"CODE"
"the rf model will have to use the random variable to make some";"IRRE"
"overfitting splits as max depth is not set therefore the noisy";"CODE"
"variables will be non zero but with small values oscillating around";"IRRE"
"zero";"-"
"the binary features correlated with y should have a higher importance";"CODE"
"than the high cardinality noisy features";"TASK"
"the maximum test accuracy is 2 5 0 4 each informative feature";"TASK"
"contributing approximately a bit more than 0 2 of accuracy";"META"
"last column is correlated with y";"-"
"the correlated feature with y is the last column and should";"TASK"
"have the highest importance";"CODE"
"use another random state";"IRRE"
"the correlated feature with y is the last column and should";"TASK"
"have the highest importance";"CODE"
"last column is correlated with y";"-"
"the correlated feature with y is the last column and should";"TASK"
"have the highest importance";"CODE"
"this relationship can be computed in closed form";"CODE"
"regression test to make sure that sequential and parallel calls will";"IRRE"
"output the same results";"IRRE"
"also tests that max samples equal to number of samples is equivalent to 1 0";"IRRE"
"first check that the problem is structured enough and that the model is";"CODE"
"complex enough to not yield trivial constant importances";"CODE"
"the actually check that parallelism does not impact the results";"IRRE"
"either with shared memory threading or without isolated memory";"CODE"
"via process based parallelism using the default backend";"CODE"
"loky or multiprocessing depending on the joblib version";"TASK"
"process based parallelism by default";"CODE"
"thread based parallelism";"CODE"
"this test checks that the column shuffling logic has the same behavior";"IRRE"
"both a dataframe and a simple numpy array";"-"
"regression test to make sure that sequential and parallel calls will";"IRRE"
"output the same results";"IRRE"
"add a categorical feature that is statistically linked to y";"TASK"
"concatenate the extra column to the numpy array integers will be";"CODE"
"cast to float values";"IRRE"
"insert extra column as a non numpy native dtype";"CODE"
"stich an arbitrary index to the dataframe";"-"
"first check that the problem is structured enough and that the model is";"CODE"
"complex enough to not yield trivial constant importances";"CODE"
"now check that importances computed on dataframe matche the values";"CODE"
"of those computed on the array with the same data";"CODE"
"smoke non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 15810";"CODE"
"assert x nbytes 1e6 trigger joblib memmaping";"CODE"
"actual smoke test should not raise any error";"IRRE"
"auxiliary check dummyclassifier is feature independent";"CODE"
"permutating feature should not change the predictions";"TASK"
"creating data with 2 features and 1000 samples where the target";"TASK"
"variable is a linear combination of the two features such that";"TASK"
"in half of the samples the impact of feature 1 is twice the impact of";"TASK"
"feature 2 and vice versa on the other half of the samples";"TASK"
"fitting linear regression with perfect prediction";"-"
"when all samples are weighted with the same weights the ratio of";"-"
"the two features importance should equal to 1 on expectation when using";"CODE"
"mean absolutes error as the loss function";"CODE"
"when passing a vector of ones as the sample weight results should be";"IRRE"
"the same as in the case that sample weight none";"CODE"
"when the ratio between the weights of the first half of the samples and";"-"
"the second half of the samples approaches to infinity the ratio of";"IRRE"
"the two features importance should equal to 2 on expectation when using";"CODE"
"mean absolutes error as the loss function";"CODE"
"creating a scorer function that does not takes sample weight";"CODE"
"creating some data and estimator for the permutation test";"IRRE"
"test that permutation importance does not return error when";"CODE"
"sample weight is none";"-"
"test that permutation importance raise exception when sample weight is";"CODE"
"not none";"-"
"test permutation importance when scoring contains multiple scorers";"CODE"
"creating some data and estimator for the permutation test";"IRRE"
"single y constant prediction";"CODE"
"remove interpolation method";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"todo bayesian ridge regression and bayesian regression ard";"TASK"
"should be squashed into its respective objects";"CODE"
"for sparse data intercept updates are scaled by this decay factor to avoid";"CODE"
"intercept oscillation";"CODE"
"seed should never be 0 in sequentialdataset64";"IRRE"
"x scale is no longer needed it is a historic artifact from the";"OUTD"
"time where linear model exposed the normalize parameter";"IRRE"
"sample weight can be implemented via a simple rescaling";"TASK"
"for sparse x and y it triggers copies anyway";"IRRE"
"for dense x and y that already have been copied we safely do inplace";"CODE"
"rescaling";"-"
"assume that validate data and check sample weight have been called by";"IRRE"
"the caller";"IRRE"
"y 1 x 0 2 x 1 3";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"bayesianridge regression";"-"
"sample weight can be implemented via a simple rescaling";"TASK"
"initialization of the values of the parameters";"IRRE"
"add eps in the denominator to omit division by zero";"TASK"
"avoid unintended type promotion to float64 with numpy 2";"CODE"
"let m n n samples n features and k min m n";"TASK"
"the posterior covariance matrix needs vh full n n";"TASK"
"the full svd is only required when n samples n features";"TASK"
"when n samples n features k m and full matrices true";"TASK"
"u m m s m vh full n n vh m n";"-"
"when n samples n features k n and full matrices false";"TASK"
"u m n s n vh full n n vh n n";"-"
"convergence loop of the bayesian ridge regression";"IRRE"
"update posterior mean coef based on alpha and lambda and";"CODE"
"compute corresponding sse sum of squared errors";"-"
"compute the log marginal likelihood";"-"
"update alpha and lambda according to mackay 1992";"CODE"
"check for convergence";"CODE"
"return regularization parameters and corresponding posterior mean";"IRRE"
"log marginal likelihood and posterior covariance";"CODE"
"compute the log marginal likelihood";"-"
"posterior covariance";"CODE"
"note we do not need to explicitly use the weights in this sum because";"CODE"
"y and x were preprocessed by rescale data to handle the weights";"CODE"
"http www utstat toronto edu rsalakhu sta4273 notes lecture2 pdf page 15";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"mypy error module sklearn linear model has no attribute cd fast";"META"
"from sklearn linear model import cd fast as cd fast type ignore attr defined";"CODE"
"paths functions";"CODE"
"todo for y ndim 1 think about avoiding memory of y y y mean";"CODE"
"avoid copy of x i e avoid explicitly computing x x mean";"CODE"
"compute np max np sqrt np sum xyw 2 axis 1 we switch sqrt and max to avoid";"CODE"
"many computations of sqrt this however needs an additional np abs";"TASK"
"we expect x and y to be already fortran ordered when bypassing";"CODE"
"checks";"-"
"xy should be a 1d contiguous array or a 2d c ordered array";"-"
"multitaskelasticnet does not support sparse matrices";"TASK"
"as sparse matrices are not actually centered we need this to be passed to";"IRRE"
"the cd solver";"-"
"x should have been passed through pre fit already if function is called";"CODE"
"from elasticnet fit";"CODE"
"fit intercept and sample weight have already been dealt with in calling";"CODE"
"methods like elasticnet fit";"-"
"alphas np sort alphas 1 make sure alphas are properly ordered";"-"
"account for n samples scaling in objectives between here and cd fast";"CODE"
"we expect precompute to be already fortran ordered when bypassing";"CODE"
"checks";"-"
"we correct the scale of the returned dual gap as the objective";"IRRE"
"in cd fast is n samples the objective in this docstring";"CODE"
"elasticnet model";"-"
"check input is used for optimisation and isn t something to be passed";"CODE"
"around in a pipeline";"CODE"
"remember if x is copied";"-"
"we expect x and y to be float64 or float32 fortran ordered arrays";"CODE"
"when bypassing checks";"-"
"tldr rescale sw to sum up to n samples";"-"
"long the objective function of enet";"CODE"
"1 2 np average squared error weights sw";"-"
"alpha penalty 1";"-"
"is invariant under rescaling of sw";"CODE"
"but enet path coordinate descent minimizes";"META"
"1 2 sum squared error alpha penalty 2";"-"
"and therefore sets";"IRRE"
"alpha n samples alpha 3";"-"
"inside its function body which results in objective 2 being";"IRRE"
"equivalent to 1 in case of no sw";"CODE"
"with sw however enet path should set";"IRRE"
"alpha sum sw alpha 4";"-"
"therefore we use the freedom of eq 1 to rescale sw before";"CODE"
"calling enet path i e";"IRRE"
"sw n samples sum sw";"-"
"such that sum sw n samples this way 3 and 4 are the same";"CODE"
"note alternatively we could also have rescaled alpha instead";"TASK"
"of sample weight";"-"
"alpha np sum sample weight n samples";"-"
"ensure copying happens only once don t do it again if done above";"CODE"
"x and y will be rescaled if sample weight is not none order f";"-"
"ensures that the returned x and y are still f contiguous";"TASK"
"coordinate descent needs f ordered arrays and pre fit might have";"TASK"
"called rescale data";"IRRE"
"from here on params";"CODE"
"check for finiteness of coefficients";"IRRE"
"return self for chaining fit and predict calls";"CODE"
"todo 1 9 remove warn and none options";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we allow for newtonsolver classes for the solver parameter but do not";"CODE"
"make them public in the docstrings this facilitates testing and";"CODE"
"benchmarking";"-"
"required by losses";"CODE"
"lbfgs will force coef and therefore raw prediction to be float64 the";"CODE"
"base loss needs y x coef and sample weight all of same dtype";"TASK"
"and contiguous";"-"
"note that check sample weight calls check array order c required by";"TASK"
"losses";"-"
"todo if alpha 0 check that x is not rank deficient";"CODE"
"note rescaling of sample weight";"TASK"
"we want to minimize";"-"
"obj 1 2 sum sample weight sum sample weight deviance";"-"
"1 2 alpha l2";"-"
"with";"-"
"deviance 2 loss";"-"
"the objective is invariant to multiplying sample weight by a constant we";"CODE"
"could choose this constant such that sum sample weight 1 in order to end";"CODE"
"up with";"-"
"obj sum sample weight loss 1 2 alpha l2";"-"
"but linearmodelloss loss already computes";"META"
"average loss weights sample weight";"-"
"thus without rescaling we have";"-"
"obj linearmodelloss loss";"-"
"linearmodelloss needs intercept at the end of coefficient array";"CODE"
"algorithms for optimization";"CODE"
"note again that our losses implement 1 2 deviance";"TASK"
"maxls 50 default is 20";"CODE"
"the constant 64 was found empirically to pass the test suite";"IRRE"
"the point is that ftol is very small but a bit larger than";"META"
"machine precision for float64 which is the dtype used by lbfgs";"CODE"
"set intercept to zero as the other linear models do";"CODE"
"check array is done in linear predictor";"CODE"
"todo adapt link to user guide in the docstring once";"CODE"
"https github com scikit learn scikit learn pull 22118 is merged";"CODE"
"note default score defined in regressormixin is r 2 score";"CODE"
"todo make d 2 a score function in module metrics and thereby get";"CODE"
"input validation and so on";"CODE"
"raw prediction self linear predictor x validates x";"CODE"
"required by losses";"CODE"
"note that check sample weight calls check array order c required by";"TASK"
"losses";"-"
"missing factor of 2 in deviance cancels out";"-"
"create instance of baseloss if fit wasn t called yet this is necessary as";"IRRE"
"tweedieregressor might set the used loss during fit different from";"IRRE"
"self base loss";"CODE"
"this happens when the link or power parameter of tweedieregressor is";"IRRE"
"invalid we fallback on the default tags in that case";"CODE"
"pass pragma no cover";"-"
"identity link";"-"
"log link";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"find good starting point by nelder mead";"CODE"
"now refine via root finding on the gradient of the function which is";"CODE"
"more precise than minimizing the function itself";"CODE"
"tweedieregressor power 3 0 too difficult";"-"
"tweedieregressor power 0 link log too difficult";"-"
"make larger dim more than double as big as the smaller one";"CODE"
"this helps when constructing singular matrices like x x";"CODE"
"x 1 1 last columns acts as intercept";"CODE"
"assert np all s 1e 3 to be sure";"IRRE"
"assert np max s np min s 100 condition number of x";"CODE"
"minimum norm solution min w 2 such that raw prediction x w";"-"
"w x xx 1 raw prediction v s 1 u raw prediction";"-"
"add penalty l2 reg strength coef 2 2 for l2 reg strength 1 and solve with";"TASK"
"optimizer note that the problem is well conditioned such that we get accurate";"TASK"
"results";"IRRE"
"to be sure";"-"
"x x 1 remove intercept";"CODE"
"same with sample weight";"-"
"x x 1 remove intercept";"CODE"
"xxx investigate if the convergencewarning that can appear in some";"-"
"cases should be considered a bug or not in the mean time we don t";"CODE"
"fail when the assertions below pass irrespective of the presence of";"CODE"
"the warning";"-"
"x x 1 remove intercept";"CODE"
"alpha 0 unpenalized";"-"
"x x 1 remove intercept";"CODE"
"the newton solvers should warn and automatically fallback to lbfgs";"IRRE"
"in this case the model should still converge";"CODE"
"xxx investigate if the convergencewarning that can appear in some";"-"
"cases should be considered a bug or not in the mean time we don t";"CODE"
"fail when the assertions below pass irrespective of the presence of";"CODE"
"the warning";"-"
"fixme assert allclose model coef coef should work for all cases but fails";"CODE"
"for the wide fat case with n features n samples most current glm solvers do";"CODE"
"not return the minimum norm solution with fit intercept true";"CODE"
"as it is an underdetermined problem prediction y the following shows that";"-"
"we get a solution i e a non unique minimum of the objective function";"CODE"
"xxx this solver shows random behaviour sometimes it finds solutions";"IRRE"
"with norm model norm solution so we check conditionally";"-"
"but it is not the minimum norm solution otherwise the norms would be";"IRRE"
"equal";"-"
"see https github com scikit learn scikit learn issues 23670";"CODE"
"note even adding a tiny penalty does not give the minimal norm solution";"TASK"
"xxx we could have naively expected lbfgs to find the minimal norm";"-"
"solution by adding a very small penalty even that fails for a reason we";"TASK"
"do not properly understand at this point";"CODE"
"when fit intercept false lbfgs naturally converges to the minimum norm";"CODE"
"solution on this problem";"CODE"
"xxx do we have any theoretical guarantees why this should be the case";"CODE"
"alpha 0 unpenalized";"-"
"x x 1 remove intercept";"CODE"
"to know the minimum norm solution we keep one intercept column and do";"CODE"
"not divide by 2 later on we must take special care";"-"
"the newton solvers should warn and automatically fallback to lbfgs";"IRRE"
"in this case the model should still converge";"CODE"
"xxx investigate if the convergencewarning that can appear in some";"-"
"cases should be considered a bug or not in the mean time we don t";"CODE"
"fail when the assertions below pass irrespective of the presence of";"CODE"
"the warning";"-"
"here we take special care";"-"
"model coef 2 model coef 1 exclude the other intercept term";"CODE"
"for minimum norm solution we would have";"CODE"
"assert model intercept pytest approx model coef 1";"CODE"
"as it is an underdetermined problem prediction y the following shows that";"-"
"we get a solution i e a non unique minimum of the objective function";"CODE"
"same as in test glm regression unpenalized";"IRRE"
"but it is not the minimum norm solution otherwise the norms would be";"IRRE"
"equal";"-"
"for minimum norm solution we would have";"CODE"
"assert model intercept pytest approx model coef 1";"CODE"
"alpha 0 unpenalized";"-"
"x x 1 remove intercept";"CODE"
"the newton solvers should warn and automatically fallback to lbfgs";"IRRE"
"in this case the model should still converge";"CODE"
"xxx investigate if the convergencewarning that can appear in some";"-"
"cases should be considered a bug or not in the mean time we don t";"CODE"
"fail when the assertions below pass irrespective of the presence of";"CODE"
"the warning";"-"
"as it is an underdetermined problem prediction y the following shows that";"-"
"we get a solution i e a non unique minimum of the objective function";"CODE"
"xxx this solver shows random behaviour sometimes it finds solutions";"IRRE"
"with norm model norm solution so we check conditionally";"-"
"same as in test glm regression unpenalized";"IRRE"
"but it is not the minimum norm solution otherwise the norms would be";"IRRE"
"equal";"-"
"y np abs y poisson requires non negative targets";"CODE"
"as we intentionally set max iter 1 such that the solver should raise a";"CODE"
"convergencewarning";"-"
"the two models are not exactly identical since the lbfgs solver";"TASK"
"computes the approximate hessian from previous iterations which";"IRRE"
"will not be strictly identical in the case of a warm start";"CODE"
"library glmnet";"CODE"
"options digits 10";"-"
"df data frame a c 2 1 1 2 b c 0 0 1 1 y c 0 1 1 2";"-"
"x data matrix df c a b";"-"
"y df y";"-"
"fit glmnet x x y y alpha 0 intercept t family poisson";"CODE"
"standardize f thresh 1e 10 nlambda 10000";"CODE"
"coef fit s 1";"-"
"intercept 0 12889386979";"CODE"
"a 0 29019207995";"-"
"b 0 03741173122";"-"
"y np array 0 1 0 5 in range of all distributions";"META"
"use at least 20 samples to reduce the likelihood of getting a degenerate";"CODE"
"dataset for any global random seed";"IRRE"
"collinear variation of the same input features";"CODE"
"let s consider the deviance of a constant baseline on this problem";"CODE"
"no warning raised on well conditioned design even without regularization";"CODE"
"on this dataset we should have enough data points to not make it";"CODE"
"possible to get a near zero deviance for the any of the admissible";"CODE"
"random seeds this will make it easier to interpret meaning of rtol in";"CODE"
"the subsequent assertions";"CODE"
"we check that the model could successfully fit information in x orig to";"CODE"
"improve upon the constant baseline by a large margin when evaluated on";"CODE"
"the traing set";"IRRE"
"lbfgs is robust to a collinear design because its approximation of the";"-"
"hessian is symmeric positive definite by construction let s record its";"CODE"
"solution";"-"
"the lbfgs solution on the collinear is expected to reach a comparable";"-"
"solution to the newton solution on the original data";"CODE"
"fitting a newton solver on the collinear version of the training data";"META"
"without regularization should raise an informative warning and fallback";"CODE"
"to the lbfgs solver";"-"
"as a result we should still automatically converge to a good solution";"IRRE"
"increasing the regularization slightly should make the problem go away";"-"
"the slightly penalized model on the collinear data should be close enough";"CODE"
"to the unpenalized model on the original data";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"calculate the values where y x w c sigma epsilon";"IRRE"
"the values above this threshold are outliers";"IRRE"
"calculate the linear loss due to the outliers";"-"
"this is equal to 2 m y x w c sigma m 2 sigma";"CODE"
"n sq outliers includes the weight give to the outliers while";"CODE"
"num outliers is just the number of outliers";"-"
"calculate the quadratic loss due to the non outliers";"-"
"this is equal to y x w c 2 sigma 2 sigma";"CODE"
"gradient due to the squared loss";"-"
"gradient due to the linear loss";"-"
"gradient due to the penalty";"-"
"gradient due to sigma";"-"
"gradient due to the intercept";"CODE"
"make sure to initialize the scale parameter to a strictly";"IRRE"
"positive value";"IRRE"
"sigma or the scale factor should be non negative";"-"
"setting it to be zero might cause undefined bounds hence we set it";"IRRE"
"to a value close to zero";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"mypy error module sklearn utils has no attribute arrayfuncs";"META"
"force copy setting the array to be fortran ordered";"CODE"
"speeds up the calculation of the partial gram matrix";"-"
"and allows to easily swap columns";"-"
"use the precision level of input data if it is consistent";"IRRE"
"fallback to double precision otherwise";"CODE"
"above better ideas";"-"
"holds the sign of covariance";"CODE"
"will hold the cholesky factorization only lower part is";"CODE"
"referenced";"CODE"
"tiny32 np finfo np float32 tiny to avoid division by 0 warning";"CODE"
"if alpha 0 alpha min equality tolerance early stopping";"-"
"interpolation factor 0 ss 1";"CODE"
"in the first iteration all alphas are zero the formula";"CODE"
"below would make ss a nan";"-"
"append x j to the cholesky factorization of xa xa";"CODE"
"l 0";"-"
"l where l w xa x j";"-"
"w z and z x j";"-"
"cov cov 1 remove cov 0";"-"
"swap does only work inplace if matrix is fortran";"CODE"
"contiguous";"-"
"update the cholesky decomposition for the gram matrix";"CODE"
"the system is becoming too ill conditioned";"CODE"
"we have degenerate vectors in our active set";"IRRE"
"we ll drop for good the last regressor added";"TASK"
"xxx need to figure a drop for good way";"CODE"
"alpha is increasing this is because the updates of cov are";"CODE"
"bringing in too much numerical error that is greater than";"CODE"
"than the remaining correlation with the";"CODE"
"regressors time to bail out";"-"
"least squares solution";"-"
"this happens because sign active n active 0";"CODE"
"is this really needed";"CODE"
"l is too ill conditioned";"-"
"equiangular direction of variables in the active set";"IRRE"
"correlation between each unactive variables and";"IRRE"
"eqiangular vector";"-"
"if huge number of features this takes 50 of time i";"TASK"
"think could be avoided if we just update it using an";"CODE"
"orthogonal qr decomposition of x";"-"
"explicit rounding can be necessary to avoid np argmax cov yielding";"CODE"
"unstable results because of rounding errors";"IRRE"
"todo better names for these variables z";"CODE"
"some coefficients have changed sign";"-"
"update the sign important for lar";"CODE"
"resize the coefs and alphas array";"-"
"mimic the effect of incrementing n iter on the array references";"CODE"
"update correlations";"CODE"
"see if any coefficient has changed sign";"-"
"handle the case when idx is not length of 1";"CODE"
"handle the case when idx is not length of 1";"CODE"
"propagate dropped variable";"IRRE"
"yeah this is stupid";"CODE"
"todo this could be updated";"CODE"
"cov n cov j x j x increment betas todo";"TASK"
"will this still work with multiple drops";"TASK"
"recompute covariance probably could be done better";"CODE"
"wrong as xy is not swapped with the rest of variables";"IRRE"
"todo this could be updated";"CODE"
"ign active np append sign active 0 0 just to maintain size";"CODE"
"resize coefs in case of early stop";"CODE"
"estimator classes";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"grad pointwise shape n samples n classes";"CODE"
"gradient shape n samples n classes";"IRRE"
"allocate gradient";"-"
"allocate hessian";"-"
"n coef size for multinomial this equals n dof n classes";"CODE"
"for non canonical link functions and far away from the optimum the";"CODE"
"pointwise hessian can be negative we take care that 75 of the hessian";"CODE"
"entries are positive";"-"
"exit early without computing the hessian";"-"
"the l2 penalty enters the hessian on the diagonal only to add those";"TASK"
"terms we use a flattened view of the array";"-"
"with intercept included as added column to x the hessian becomes";"CODE"
"hess x 1 diag h x 1";"-"
"x diag h x x h";"-"
"h x sum h";"-"
"the left upper part has already been filled it remains to compute";"CODE"
"the last row and the last column";"-"
"here we may safely assume halfmultinomialloss aka categorical";"-"
"cross entropy";"-"
"halfmultinomialloss computes only the diagonal part of the hessian i e";"-"
"diagonal in the classes here we want the full hessian therefore we";"CODE"
"call gradient proba";"IRRE"
"the full hessian matrix i e not only the diagonal part dropping most";"-"
"indices is given by";"-"
"hess x h x";"-"
"here h is a priori a 4 dimensional matrix of shape";"-"
"n samples n samples n classes n classes it is diagonal its first";"IRRE"
"two dimensions the ones with n samples i e it is";"-"
"effectively a 3 dimensional matrix n samples n classes n classes";"IRRE"
"h diag p p p";"-"
"or with indices k and l for classes";"CODE"
"h kl p k delta kl p k p l";"-"
"with p k the predicted probability for class k only the dimension in";"CODE"
"n samples multiplies with x";"META"
"for 3 classes and n samples 1 this looks like is a bit misused";"CODE"
"here";"-"
"hess x h00 h10 h20 x";"-"
"h10 h11 h12";"-"
"h20 h12 h22";"-"
"x diag h00 x x diag h10 x diag h20";"-"
"x diag h10 x x diag h11 x diag h12";"-"
"x diag h20 x x diag h12 x diag h22";"-"
"now coef of shape n classes n dof is contiguous in n classes";"CODE"
"therefore we want the hessian to follow this convention too i e";"CODE"
"hess n classes n classes x0 h00 x0 x0 h10 x0";"IRRE"
"x0 h10 x0 x0 h11 x0";"-"
"x0 h20 x0 x0 h12 x0";"-"
"is the first feature x0 for all classes in our implementation we";"TASK"
"still want to take advantage of blas x t x therefore we have some";"TASK"
"index slicing battle to fight";"CODE"
"diagonal terms in classes hess kk";"IRRE"
"note that this also writes to some of the lower triangular part";"TASK"
"see above in the non multiclass case";"CODE"
"off diagonal terms in classes hess kl";"IRRE"
"upper triangle in classes";"IRRE"
"fill lower triangle in classes";"IRRE"
"see above in the non multiclass case";"CODE"
"the pointwise hessian is always non negative for the multinomial loss";"CODE"
"precompute as much as possible hx hx sum and hessian sum";"-"
"calculate the double derivative with respect to intercept";"CODE"
"note in case hx is sparse hx sum is a matrix object";"IRRE"
"prevent squeezing to zero dim array if n features 1";"TASK"
"with intercept included and l2 reg strength 0 hessp returns";"CODE"
"res x 1 diag h x 1 s";"-"
"x 1 hx s n features sum h s 1";"TASK"
"res n features x hx s n features sum h s 1";"TASK"
"res 1 1 hx s n features sum h s 1";"TASK"
"here we may safely assume halfmultinomialloss aka categorical";"-"
"cross entropy";"-"
"halfmultinomialloss computes only the diagonal part of the hessian i e";"-"
"diagonal in the classes here we want the matrix vector product of the";"CODE"
"full hessian therefore we call gradient proba";"IRRE"
"full hessian vector product i e not only the diagonal part of the";"-"
"hessian derivation with some index battle for input vector s";"CODE"
"sample index i";"-"
"feature indices j m";"TASK"
"class indices k l";"IRRE"
"1 k l is one if k l else 0";"-"
"p i k is the predicted probability that sample i belongs to class k";"IRRE"
"for all i sum k p i k 1";"CODE"
"s l m is input vector for class l and feature m";"CODE"
"x x transposed";"-"
"note hessian with dropping most indices is just";"TASK"
"x p k 1 k l p l x";"-"
"result k j sum i l m hessian i k j m l s l m";"IRRE"
"sum i l m x ji p i k 1 k l p i l";"-"
"x im s l m";"-"
"sum i m x ji p i k";"-"
"x im s k m sum l p i l x im s l m";"-"
"see also https github com scikit learn scikit learn pull 3646 discussion r17461411";"CODE"
"s reshape n classes 1 order f shape n classes n dof";"CODE"
"s 1 shape n classes n features";"TASK"
"tmp x s t s intercept x im s k m";"CODE"
"tmp proba tmp sum axis 1 np newaxis sum l";"CODE"
"tmp proba p i k";"CODE"
"hess prod empty like grad but we ravel grad below and this";"META"
"function is run after that";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"logistic regression";"-"
"preprocessing";"-"
"halfbinomialloss used for those solvers represents y in 0 1 instead";"CODE"
"of in 1 1";"-"
"all solvers capable of a multinomial need labelencoder not labelbinarizer";"-"
"i e y as a 1d array of integers labelencoder also saves memory";"CODE"
"compared to labelbinarizer especially when n classes is large";"IRRE"
"it is important that w0 is f contiguous";"CODE"
"important note";"CODE"
"all solvers relying on linearmodelloss need to scale the penalty with n samples";"TASK"
"or the sum of sample weights because the implemented logistic regression";"TASK"
"objective here is unfortunately";"CODE"
"c sum pointwise loss penalty";"CODE"
"instead of as linearmodelloss does";"CODE"
"mean pointwise loss 1 c penalty";"CODE"
"this needs to be calculated after sample weight is multiplied by";"TASK"
"class weight it is even tested that passing class weight is equivalent to";"IRRE"
"passing sample weights according to class weight";"IRRE"
"hess loss gradient hessian product hess gradient hessp";"-"
"else multinomial";"-"
"scipy optimize minimize and newton cg accept only ravelled parameters";"IRRE"
"i e 1d arrays linearmodelloss expects classes to be contiguous and";"IRRE"
"reconstructs the 2d array via w0 reshape n classes 1 order f";"CODE"
"as w0 is f contiguous ravel order f also avoids a copy";"CODE"
"hess loss gradient hessian product hess gradient hessp";"-"
"maxls 50 default is 20";"CODE"
"n iter i is an array for each class however target is always encoded";"CODE"
"in 1 1 so we only take the first element of n iter i";"-"
"alpha is for l2 norm beta is for l1 norm";"CODE"
"else elastic net penalty";"-"
"helper function for logisticcv";"CODE"
"note we pass classes for the whole dataset to avoid inconsistencies i e";"CODE"
"different number of classes in different folds this way if a class is empty";"CODE"
"in a fold logistic regression path will initialize it to zero and not change";"IRRE"
"the score method of logistic regression has a classes attribute";"CODE"
"if self c 1 0 default values";"IRRE"
"note that check for l1 ratio is done right above";"CODE"
"todo enable multi threading if benchmarks show a positive effect";"CODE"
"see https github com scikit learn scikit learn issues 32162";"CODE"
"encode for string labels";"CODE"
"the original class labels";"IRRE"
"init cross validation generator";"IRRE"
"compute the class weights for the entire dataset y";"IRRE"
"the sag solver releases the gil so it s more efficient to use";"-"
"threads for this solver";"CODE"
"fold coefs is a list and would have shape n folds n l1 ratios";"-"
"after reshaping";"CODE"
"coefs paths is of shape n classes n folds n cs n l1 ratios n features";"TASK"
"scores is of shape n classes n folds n cs n l1 ratios";"IRRE"
"n iter is of shape 1 n folds n cs n l1 ratios";"-"
"elf cs cs 0 the same for all folds and l1 ratios";"CODE"
"coefs paths shape n folds n l1 ratios n cs n features";"TASK"
"coefs paths shape n folds n l1 ratios n cs n classes n features";"TASK"
"n iter shape n folds n l1 ratios n cs";"-"
"scores shape n folds n l1 ratios n cs";"-"
"repeat same scores across all classes";"IRRE"
"all scores are the same across classes";"IRRE"
"best index over folds";"-"
"cores sum scores sum axis 0 shape n cs n l1 ratios";"-"
"note that y is label encoded";"TASK"
"take the best scores across every fold and the average of";"-"
"all coefficients corresponding to the best scores";"-"
"cores scores reshape n folds 1 n folds n cs n l1 ratios";"-"
"best indices np argmax scores axis 1 n folds";"-"
"best indices list zip best indices n folds 2";"-"
"each row of best indices has the 2 indices for cs and l1 ratios";"CODE"
"if elasticnet was not used remove the l1 ratios dimension of some";"OUTD"
"attributes";"META"
"same for all classes";"CODE"
"newpaths shape n classes n folds n cs n l1 ratios n dof";"CODE"
"self coefs paths shape should be";"CODE"
"n folds n l1 ratios n cs n classes n dof";"CODE"
"newscores shape n folds n cs n l1 ratios";"CODE"
"self scores shape should be n folds n l1 ratios n cs";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"todo 1 10 remove";"TASK"
"for an explanation see";"CODE"
"https github com scikit learn scikit learn pull 1259 issuecomment 9818044";"CODE"
"todo 1 10 remove";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"note that centering y and x with preprocess data does not work";"TASK"
"for quantile regression";"CODE"
"the objective is defined as 1 n sum pinball loss alpha l1";"CODE"
"so we rescale the penalty term which is equivalent";"CODE"
"make default solver more stable";"CODE"
"after rescaling alpha the minimization problem is";"-"
"min sum pinball loss alpha l1";"-"
"use linear programming formulation of quantile regression";"CODE"
"min x c x";"-"
"a eq x b eq";"-"
"0 x";"-"
"x s0 s t0 t u v slack variables 0";"IRRE"
"intercept s0 t0";"CODE"
"coef s t";"-"
"c 0 alpha 1 p 0 alpha 1 p quantile 1 n 1 quantile 1 n";"-"
"residual y x coef intercept u v";"CODE"
"a eq 1 n x 1 n x diag 1 n diag 1 n";"-"
"b eq y";"-"
"p n features";"TASK"
"n n samples";"-"
"1 n vector of length n with entries equal one";"-"
"see https stats stackexchange com questions 384909";"CODE"
"filtering out zero sample weights from the beginning makes life";"CODE"
"easier for the linprog solver";"CODE"
"n indices len indices use n mask instead of n samples";"CODE"
"do not penalize the intercept";"CODE"
"note that highs methods always use a sparse csc memory layout internally";"TASK"
"even for optimization problems parametrized using dense numpy arrays";"CODE"
"therefore we work with csc matrices as early as possible to limit";"CODE"
"unnecessary repeated memory copies";"IRRE"
"positive slack negative slack";"-"
"solution is an array with params pos params neg u v";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"ransacregressor estimator is not validated yet";"TASK"
"need to validate separately here we can t pass multi output true";"TASK"
"because that would allow y to be csr delay expensive finiteness";"IRRE"
"check to the estimator s own input validation";"CODE"
"mad median absolute deviation";"-"
"try not all estimator accept a random state";"IRRE"
"number of data samples";"-"
"choose random sample set";"IRRE"
"check if random sample set is valid";"IRRE"
"cut fit params down to subset idxs";"IRRE"
"fit model for current random sample set";"IRRE"
"check if estimated model is valid";"IRRE"
"residuals of all data for current random sample model";"IRRE"
"classify data into inliers and outliers";"CODE"
"less inliers skip current random sample";"IRRE"
"extract inlier data set";"IRRE"
"cut fit params down to inlier idxs subset";"IRRE"
"score of inlier data set";"IRRE"
"same number of inliers but worse score skip current random";"IRRE"
"sample";"-"
"save current random sample as best sample";"IRRE"
"break if sufficient number of inliers or score is reached";"CODE"
"if none of the iterations met the required criteria";"CODE"
"estimate final model using all inliers";"CODE"
"tags input tags sparse true default estimator is linearregression";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"kernel ridge";"-"
"w x t inv x x t alpha id y";"-"
"linear ridge";"-"
"w inv x t x alpha id x t y";"-"
"no need to touch anything";"TASK"
"according to the lsqr documentation alpha damp 2";"CODE"
"w inv x t x alpha id x t y";"-"
"dual coef inv x x t alpha id y";"-"
"unlike other solvers we need to support sample weight directly";"TASK"
"because k might be a pre computed kernel";"-"
"only one penalty we can solve multi target problems in one time";"-"
"note we must use overwrite a false in order to be able to";"TASK"
"use the fall back solution below in case a linalgerror";"IRRE"
"is raised";"CODE"
"k is expensive to compute and store in memory so change it back in";"-"
"case it was user given";"CODE"
"one penalty per target we need to solve each target separately";"TASK"
"idx s 1e 15 same default value as scipy linalg pinv";"IRRE"
"sag supports sample weight directly for other solvers";"CODE"
"we implement sample weight via a simple rescaling";"TASK"
"some callers of this method might pass alpha as single";"IRRE"
"element array which already has been validated";"CODE"
"there should be either 1 or n targets penalties";"-"
"use svd solver if matrix is singular";"-"
"use svd solver if matrix is singular";"-"
"precompute max squared sum for all targets";"CODE"
"at the moment array api dispatch only supports the svd solver";"CODE"
"sag supports fitting intercept directly";"CODE"
"when x is sparse we only remove offset from y";"IRRE"
"add the offset which was subtracted by preprocess data";"TASK"
"required to fit intercept with sparse cg and lbfgs solver";"CODE"
"for dense matrices or when intercept is set to 0";"CODE"
"todo update this line to avoid calling convert to numpy";"CODE"
"once labelbinarizer has been updated to accept non numpy array api";"CODE"
"compatible inputs";"CODE"
"threshold such that the negative label is 1 and positive label";"-"
"is 1 to use the inverse transform of the label binarizer fitted";"IRRE"
"during fit";"-"
"if x has more rows than columns use decomposition of x t x";"-"
"otherwise x x t";"-"
"if x is dense it has already been centered in preprocessing";"CODE"
"to emulate centering x with sample weights";"-"
"ie removing the weighted average we add a column";"TASK"
"containing the square roots of the sample weights";"-"
"by centering it is orthogonal to the other columns";"-"
"the vector containing the square roots of the sample weights 1";"-"
"when no sample weights is the eigenvector of xx t which";"-"
"corresponds to the intercept we cancel the regularization on";"CODE"
"this dimension the corresponding eigenvalue is";"IRRE"
"sum sample weight";"-"
"w intercept dim 0 cancel regularization for the intercept";"CODE"
"handle case where y is 2 d";"CODE"
"to emulate centering x with sample weights";"-"
"ie removing the weighted average we add a column";"TASK"
"containing the square roots of the sample weights";"-"
"by centering it is orthogonal to the other columns";"-"
"when all samples have the same weight we add a column of 1";"TASK"
"remove eigenvalues and vectors in the null space of x t x";"IRRE"
"handle case where y is 2 d";"CODE"
"the vector 0 0 0 1";"-"
"is the eigenvector of x tx which";"-"
"corresponds to the intercept we cancel the regularization on";"CODE"
"this dimension the corresponding eigenvalue is";"IRRE"
"sum sample weight e g n when uniform sample weights";"CODE"
"add a column to x containing the square roots of sample weights";"TASK"
"return 1 hat diag y y hat";"IRRE"
"handle case where y is 2 d";"CODE"
"x already centered";"CODE"
"to emulate fit intercept true situation add a column";"TASK"
"containing the square roots of the sample weights";"-"
"by centering the other columns are orthogonal to that one";"-"
"detect intercept column";"CODE"
"cancel the regularization for the intercept";"CODE"
"handle case where y is 2 d";"CODE"
"for x that does not have a simple dtype e g pandas dataframe";"CODE"
"the attributes will be stored in the dtype chosen by";"CODE"
"validate data i e np float64";"CODE"
"using float32 can be numerically unstable for this estimator so if";"CODE"
"the array api namespace and device allow convert the input values";"IRRE"
"to float64 whenever possible before converting the results back to";"CODE"
"float32";"CODE"
"alpha per target cannot be used in classifier mode all subclasses";"IRRE"
"of ridgegcv that are classifiers keep alpha per target at its";"IRRE"
"default value false so the condition below should never happen";"IRRE"
"rescale predictions back to original scale";"-"
"if sample weight is not none avoid the unnecessary division by ones";"IRRE"
"keep track of the best model";"-"
"initialize";"IRRE"
"update";"CODE"
"avoid torch warning about x t for x with ndim 2";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"default value of epsilon parameter";"IRRE"
"todo consider whether pa1 and pa2 could also work for other losses";"CODE"
"raises valueerror if not registered";"IRRE"
"plain sgd expects a float any value is fine since at this point";"CODE"
"penalty can t be elsaticnet so l1 ratio is not used";"OUTD"
"allocate coef for multi class";"CODE"
"allocate intercept for multi class";"CODE"
"allocate coef";"-"
"allocate intercept";"CODE"
"initialize average parameters";"IRRE"
"use the full set for training with an empty validation set";"IRRE"
"y in 0 1";"-"
"y in 1 1";"-"
"if average is not true average coef and average intercept will be";"META"
"unused";"OUTD"
"numpy mtrand expects a c long which is a signed 32 bit integer under";"CODE"
"windows";"CODE"
"allocate datastructures from input arguments";"CODE"
"delegate to concrete training procedure";"-"
"delete the attribute otherwise partial fit thinks it s not the first call";"CODE"
"labels can be encoded as float int or string literals";"CODE"
"np unique sorts in asc order largest class id is positive class";"IRRE"
"clear iteration count for multiple call to fit";"IRRE"
"always scale the input the most convenient way is to use a pipeline";"CODE"
"always scale the input the most convenient way is to use a pipeline";"CODE"
"the one class svm uses the sgd implementation with";"TASK"
"y np ones n samples";"-"
"early stopping is set to false for the one class svm thus";"IRRE"
"validation mask and validation score cb will be set to values";"IRRE"
"associated to early stopping false in make validation split and";"-"
"make validation score cb respectively";"-"
"numpy mtrand expects a c long which is a signed 32 bit integer under";"CODE"
"windows";"CODE"
"there are no class weights for the one class svm and they are";"CODE"
"therefore set to 1";"IRRE"
"average coef none not used";"OUTD"
"average intercept 0 not used";"OUTD"
"made enough updates for averaging to be taken into account";"CODE"
"allocate datastructures from input arguments";"CODE"
"we use intercept 1 offset where intercept is the intercept of";"CODE"
"the sgd implementation and offset is the offset of the one class svm";"IRRE"
"optimization problem";"-"
"delegate to concrete training procedure";"-"
"clear iteration count for multiple call to fit";"IRRE"
"y y 0 1 for consistency with outlier detectors";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"x old equals one of our samples";"-"
"if quotient norm epsilon to avoid division by zero";"CODE"
"tol 2 we are computing the tol on the squared norm";"-"
"gelss need to pad y subpopulation to be of the max dim of x subpopulation";"TASK"
"target type should be integral but can accept real for backward compatibility";"CODE"
"else if n samples n features";"TASK"
"determine indices of subpopulation";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"test linearregression on a simple dataset";"IRRE"
"a simple dataset";"IRRE"
"test it also for degenerate input";"CODE"
"it would not work with under determined systems";"CODE"
"linearregression with explicit sample weight";"-"
"assert reg coef shape x shape 1 sanity checks";"CODE"
"closed form of the weighted least square";"CODE"
"theta x t w x 1 x t w y";"-"
"x must not be sparse if positive true";"IRRE"
"sample weights must be either scalar or 1d";"TASK"
"make sure the ok sample weights actually work";"META"
"test assertions on betas shape";"IRRE"
"test that linear regression also works with sparse data";"IRRE"
"test that linear regression agrees between sparse and dense";"IRRE"
"test multiple outcome linear regressions";"IRRE"
"test multiple outcome linear regressions with sparse data";"IRRE"
"test nonnegative linearregression on a simple dataset";"IRRE"
"test it also for degenerate input";"CODE"
"test multiple outcome nonnegative linear regressions";"IRRE"
"test differences with linearregression when positive false";"IRRE"
"test linearregression fitted coefficients";"IRRE"
"when the problem is positive";"-"
"check that the data is not modified inplace by the linear regression";"-"
"estimator";"-"
"xxx note hat y sparse is not supported broken in the current";"TASK"
"implementation of linearregression";"TASK"
"do not allow inplace preprocessing of x and y";"CODE"
"allow inplace preprocessing of x and y";"-"
"no optimization relying on the inplace modification of sparse input";"IRRE"
"data has been implemented at this time";"TASK"
"x has been offset and optionally rescaled by sample weights";"IRRE"
"inplace the 0 42 threshold is arbitrary and has been found to be";"-"
"robust to any random seed in the admissible range";"IRRE"
"y should not have been modified inplace by linearregression fit";"-"
"sample weights have no reason to ever be modified inplace";"-"
"warning is raised only when some of the columns is sparse";"CODE"
"all columns but the first column is sparse";"IRRE"
"does not warn when the whole dataframe is sparse";"IRRE"
"generate random data with 50 of zero values to make sure";"IRRE"
"that the sparse variant of this test is actually sparse this also";"IRRE"
"shifts the mean value for each columns in x further away from";"CODE"
"zero";"-"
"scale the first feature of x to be 10 larger than the other to";"TASK"
"better check the impact of feature scaling";"TASK"
"constant non zero feature";"TASK"
"constant zero feature non materialized in the sparse case";"CODE"
"near constant features should not be scaled";"TASK"
"simplifies asserts";"CODE"
"test output format of preprocess data when input is csr";"IRRE"
"ample weight sw 32 sample weight must have same dtype as x";"-"
"ample weight sw 64 sample weight must have same dtype as x";"-"
"array";"-"
"csr";"-"
"reason known to fail for csr arrays see issue 30131";"CODE"
"1 sample weight np ones must be equivalent to sample weight none";"TASK"
"a special case of check sample weight equivalence name reg but we also";"META"
"test with sparse input";"IRRE"
"2 sample weight none should be equivalent to sample weight number";"-"
"3 scaling of sample weight should have no effect cf np average";"-"
"4 setting elements of sample weight to 0 is equivalent to removing these samples";"IRRE"
"y 5 1000 to make excluding those samples important";"CODE"
"5 check that multiplying sample weight by 2 is equivalent to repeating";"-"
"corresponding samples twice";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"spdx license identifier bsd 3 clause";"-"
"note gammaregressor and tweedieregressor power 1 have a non canonical link";"IRRE"
"todo fix saga which fails badly with sample weights";"TASK"
"this is a known limitation see";"CODE"
"https github com scikit learn scikit learn issues 21305";"CODE"
"tweedieregressor power 0 same as ridge";"-"
"test that sum y predicted sum y observed on the training set";"IRRE"
"this must hold for all linear models with deviance of an exponential disperson";"CODE"
"family as loss and the corresponding canonical link if fit intercept true";"IRRE"
"examples";"-"
"squared error and identity link most linear models";"IRRE"
"poisson deviance with log link";"-"
"log loss with logit link";"-"
"this is known as balance property or unconditional calibration unbiasedness";"CODE"
"for reference see corollary 3 18 3 20 and chapter 5 1 5 of";"CODE"
"m v wuthrich and m merz statistical foundations of actuarial learning and its";"-"
"applications june 3 2022 http doi org 10 2139 ssrn 3822407";"CODE"
"model clone model avoid side effects from shared instances";"CODE"
"rel 2e 4 test precision";"IRRE"
"y rng poisson lam expectation 1 strict positive i e y 0";"-"
"model set params fit intercept true to be sure";"IRRE"
"assert balance property";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"from sklearn linear model import cd fast as cd fast type ignore attr defined";"CODE"
"for alpha max coefficients must all be zero";"CODE"
"without gap safe screening rules";"-"
"at least 2 coefficients are non zero";"-"
"with gap safe screening rules";"-"
"sparse";"IRRE"
"gram";"-"
"check that the lasso can handle zero data without crashing";"-"
"pytest mark filterwarnings ignore runtimewarning overflow and similar";"IRRE"
"check elasticnet throws valueerror when dealing with non finite parameter";"IRRE"
"values";"IRRE"
"test lasso on a toy example for various values of alpha";"IRRE"
"when validating this against glmnet notice that glmnet divides it";"CODE"
"against nobs";"-"
"y 1 0 1 just a straight line";"-"
"t 2 3 4 test sample";"IRRE"
"test elasticnet for various parameters of alpha and l1 ratio";"IRRE"
"actually the parameters alpha 0 should not be allowed however";"IRRE"
"we test it as a border case";"IRRE"
"elasticnet is tested with and without precomputed gram matrix";"IRRE"
"y 1 0 1 just a straight line";"-"
"t 2 0 3 0 4 0 test sample";"IRRE"
"this should be the same as lasso";"CODE"
"clf fit x y with gram";"-"
"clf fit x y with gram";"-"
"dual pt r n samples dual constraint norm x t theta inf alpha";"CODE"
"check that the lars and the coordinate descent implementation";"TASK"
"select a similar alpha";"CODE"
"for this we check that they don t fall in the grid of";"CODE"
"clf alphas further than 1";"-"
"check that they also give a similar mse";"-"
"test set";"IRRE"
"ensure the unconstrained fit has a negative coefficient";"CODE"
"on same data constrained fit has non negative coefficients";"CODE"
"set initial coefficients to very bad values";"IRRE"
"check that the model converges w o convergence warnings";"-"
"check that the model converges w o convergence warnings";"-"
"x x astype int make it explicit that x is int";"CODE"
"1 sample weight np ones should be equivalent to sample weight none";"-"
"2 sample weight none should be equivalent to sample weight number";"-"
"3 scaling of sample weight should have no effect cf np average";"-"
"4 setting elements of sample weight to 0 is equivalent to removing these samples";"IRRE"
"y 5 1000 to make excluding those samples important";"CODE"
"5 check that multiplying sample weight by 2 is equivalent to repeating";"-"
"corresponding samples twice";"-"
"assign random integer weights only to the first cross validation group";"IRRE"
"the samples in the other cross validation groups are left with unit";"CODE"
"weights";"-"
"check that the alpha selection process is the same";"CODE"
"check that the final model coefficients are the same";"CODE"
"sample weight np ones should be equivalent to sample weight none";"-"
"sample weight none should be equivalent to sample weight number";"-"
"scaling of sample weight should have no effect cf np average";"-"
"test alpha max makes coefs zero";"IRRE"
"test smaller alpha makes coefs nonzero";"IRRE"
"linearmodelscv fit performs operations on fancy indexed memmapped";"CODE"
"data when using the loky backend causing an error due to unexpected";"CODE"
"behavior of fancy indexing of read only memmaps cf numpy 14132";"CODE"
"create a problem sufficiently large to cause memmapping 1mb";"IRRE"
"unfortunately the scikit learn and joblib apis do not make it possible to";"CODE"
"change the max nbyte of the inner parallel call";"IRRE"
"assert x nbytes 1e6 1 mb";"CODE"
"asses warning message raised by linearmodelcv when n alphas is used";"CODE"
"asses no warning message raised when n alphas is not used";"OUTD"
"todo 1 9 remove";"TASK"
"asses no warning message raised when n alphas is not used";"OUTD"
"todo 1 9 remove";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"generate data with outliers by replacing 10 of the samples with noise";"CODE"
"replace 10 of the sample with noise";"-"
"test that ridge matches linearregression for large epsilon";"IRRE"
"test that the gradient calculated by huber loss and gradient is correct";"IRRE"
"check using optimize check grad that the gradients are equal";"-"
"check for both fit intercept and otherwise";"CODE"
"the ridge regressor should be influenced by the outliers and hence";"-"
"give a worse score on the non outliers as compared to the huber";"IRRE"
"regressor";"-"
"the huber model should also fit poorly on the outliers";"-"
"test that it does not crash with bool data";"CODE"
"todo use another dataset that has multiple drops";"TASK"
"principle of lars is to keep covariances tied and decreasing";"CODE"
"also test verbose output";"IRRE"
"no more than max pred variables can go into the active set";"IRRE"
"the same with precomputed gram matrix";"-"
"no more than max pred variables can go into the active set";"IRRE"
"test that lars path with no x and gram raises exception";"CODE"
"test that lars path with precomputed gram and xy gives the right answer";"IRRE"
"todo remove warning filter when numpy min version 2 0 0";"TASK"
"test that lars gives least square solution at the end";"IRRE"
"of the path";"-"
"x1 3 x use un normalized dataset";"IRRE"
"todo remove warning filter when numpy min version 2 0 0";"TASK"
"test that lars lasso gives least square solution at the end";"IRRE"
"of the path";"-"
"check that lars path is robust to collinearity in input";"CODE"
"assert residual 2 sum 1 0 just make sure it s bounded";"CODE"
"test that the return path false option returns the correct output";"IRRE"
"test that the return path false option with gram remains correct";"IRRE"
"test that the return path false option with gram and xy remains";"IRRE"
"correct";"-"
"check for different values of precompute";"IRRE"
"test when input is a singular matrix";"IRRE"
"consistency test that checks that lars lasso is handling rank";"IRRE"
"deficient input data with n features rank in the same way";"CODE"
"as coordinate descent lasso";"-"
"to be able to use the coefs to compute the objective function";"CODE"
"we need to turn off normalization";"TASK"
"test that lassolars and lasso using coordinate descent give the";"IRRE"
"same results";"IRRE"
"similar test with the classifiers";"IRRE"
"same test with normalized data";"IRRE"
"test that lassolars and lasso using coordinate descent give the";"IRRE"
"same results when early stopping is used";"IRRE"
"test before in the middle and in the last part of the path";"CODE"
"same test with normalization";"IRRE"
"test that the path length of the lassolars is right";"IRRE"
"also check that the sequence of alphas is always decreasing";"-"
"test lasso lars on a very ill conditioned design and check that";"IRRE"
"it does not blow up and stays somewhat close to a solution given";"CODE"
"by the coordinate descent solver";"-"
"also test that lasso path using lars path output style gives";"IRRE"
"the same result as lars path and previous lasso output style";"IRRE"
"under these conditions";"-"
"generate data";"-"
"create an ill conditioned situation in which the lars has to go";"IRRE"
"far in the path to converge and check that lars and coordinate";"CODE"
"descent give the same answers";"-"
"note it used to be the case that lars had to use the drop for good";"IRRE"
"strategy for this but this is no longer the case with the";"CODE"
"equality tolerance checks";"-"
"assure that at least some features get added if necessary";"TASK"
"test for 6d2b4c";"IRRE"
"hilbert matrix";"-"
"the path should be of length 6 1 in a lars going down to 6";"CODE"
"non zero coefs";"-"
"assure that estimators receiving multidimensional y do the right thing";"IRRE"
"regression test for gh 1615";"IRRE"
"test the lassolarscv object by checking that the optimal alpha";"IRRE"
"increases as the number of samples increases";"-"
"this property is not actually guaranteed in general and is just a";"META"
"property of the given dataset with the given steps chosen";"IRRE"
"x np c x x x add correlated features";"TASK"
"check that there is no warning in general and no convergencewarning";"-"
"in particular";"-"
"materialize the string representation of the warning to get a more";"CODE"
"informative error message in case of assertionerror";"CODE"
"test the lassolarsic object by checking that";"IRRE"
"some good features are selected";"TASK"
"alpha bic alpha aic";"-"
"n nonzero bic n nonzero aic";"-"
"x np c x rng randn x shape 0 5 add 5 bad features";"TASK"
"when using automated memory mapping on large input the";"CODE"
"fold data is in read only mode";"CODE"
"this is a non regression test for";"CODE"
"https github com scikit learn scikit learn issues 4597";"CODE"
"the following should not fail despite copy false";"-"
"this is the main test for the positive parameter on the lars path method";"CODE"
"the estimator classes just make use of this function";"CODE"
"we do the test on the diabetes dataset";"IRRE"
"ensure that we get negative coefficients when positive false";"-"
"and all positive when positive true";"-"
"for method lar default and lasso";"CODE"
"now we gonna test the positive option for all estimator classes";"IRRE"
"testing the transmissibility for the positive option of all estimator";"IRRE"
"classes in this same function here";"CODE"
"test that lassolars and lasso using coordinate descent give the";"IRRE"
"same results when using the positive option";"IRRE"
"this test is basically a copy of the above with additional positive";"IRRE"
"option however for the middle part the comparison of coefficient values";"IRRE"
"for a range of alphas we had to make an adaptations see below";"CODE"
"not normalized data";"-"
"the range of alphas chosen for coefficient comparison here is restricted";"CODE"
"as compared with the above test without the positive option this is due";"IRRE"
"to the circumstance that the lars lasso algorithm does not converge to";"CODE"
"the least squares solution for small alphas see least angle regression";"CODE"
"by efron et al 2004 the coefficients are typically in congruence up to";"IRRE"
"the smallest alpha reached by the lars lasso algorithm and start to";"-"
"diverge thereafter see";"-"
"https gist github com michigraber 7e7d7c75eca694c7a6ff";"CODE"
"normalized data";"-"
"for c a in zip lasso path t 1 alphas 1 don t include alpha 0";"CODE"
"test that sklearn lassolars implementation agrees with the lassolars";"TASK"
"implementation available in r lars library when fit intercept false";"TASK"
"let s generate the data used in the bug report 7778";"CODE"
"the r result was obtained using the following code";"IRRE"
"library lars";"CODE"
"model lasso lars lars x t y type lasso intercept false";"CODE"
"trace true normalize false";"-"
"r t model lasso lars beta";"-"
"est clone est avoid side effects from previous tests";"CODE"
"test that a small amount of jitter helps stability";"IRRE"
"using example provided in issue 2746";"-"
"set to fit intercept to false since target is constant and we want check";"CODE"
"the value of coef coef would be all zeros otherwise";"IRRE"
"non regression test for 17789 copy x true and gram auto does not";"CODE"
"overwrite x";"TASK"
"x did not change";"-"
"max iter 5 is for avoiding convergencewarning";"CODE"
"the test ensures that the fit method preserves input dtype";"IRRE"
"max iter 5 is for avoiding convergencewarning";"CODE"
"the test ensures numerical consistency between trained coefficients";"IRRE"
"of float32 and float64";"CODE"
"we do not need to test all losses just what linearmodelloss does on top of the";"CODE"
"base losses";"-"
"x 1 1 make last column of 1 to mimic intercept term";"CODE"
"exclude intercept column as it is added automatically by loss inter";"CODE"
"note that intercept gets no l2 penalty";"TASK"
"coef coef ravel order f this is important only for multinomial loss";"CODE"
"1 check gradients numerically";"IRRE"
"use a trick to get central finite difference of accuracy 4 five point stencil";"IRRE"
"https en wikipedia org wiki numerical differentiation";"CODE"
"https en wikipedia org wiki finite difference coefficient";"IRRE"
"approx g1 f x eps f x eps 2 eps";"-"
"approx g2 f x 2 eps f x 2 eps 4 eps";"-"
"five point stencil approximation";"CODE"
"see https en wikipedia org wiki five point stencil 1d first derivative";"CODE"
"2 check hessp numerically along the second direction of the gradient";"IRRE"
"computation of the hessian is particularly fragile to numerical errors when doing";"CODE"
"simple finite differences here we compute the grad along a path in the direction";"IRRE"
"of the vector and then use a least square regression to estimate the slope";"IRRE"
"cs 2 has the highest score 0 8 from mockscorer";"CODE"
"scorer called 8 times cv len cs";"IRRE"
"reset mock scorer";"IRRE"
"clf clone clf avoid side effects from shared instances";"CODE"
"for numerical labels let y values be taken from set 1 0 1";"IRRE"
"test for string labels";"CODE"
"the predictions should be in original labels";"-"
"cv does not necessarily predict all labels";"CODE"
"we use explicit cs parameter to make sure all labels are predicted for each c";"CODE"
"make sure class weights can be given with string labels";"CODE"
"todo 1 12 remove deprecated use legacy attributes";"OUTD"
"test that multinomial logisticregressioncv is correct using the iris dataset";"IRRE"
"the cv indices from stratified kfold";"CODE"
"train clf on the original dataset";"IRRE"
"test the shape of various attributes";"IRRE"
"test that for the iris data multinomial gives a better accuracy than ovr";"IRRE"
"lbfgs requires scaling to avoid convergence warnings";"CODE"
"test attributes of logisticregressioncv";"IRRE"
"norm of coefficients should increase with increasing c";"-"
"with use legacy attributes true coefs paths is a dict whose keys";"IRRE"
"are classes and each value has shape";"IRRE"
"n folds n l1 ratios n cs n features";"TASK"
"note that we have to exclude the intercept hence the 1";"TASK"
"on the last dimension";"CODE"
"norms np sum coefs coefs axis 1 l2 norm for each c";"CODE"
"norm of coefficients should increase with increasing c";"-"
"with use legacy attributes false coefs paths has shape";"META"
"n folds n l1 ratios n cs n classes n features 1";"TASK"
"note that we have to exclude the intercept hence the 1";"TASK"
"on the last dimension";"CODE"
"norms np sum coefs coefs axis 1 l2 norm for each c";"CODE"
"override max iteration count for specific solvers to allow for";"CODE"
"proper convergence";"-"
"xxx lbfgs line search can fail and cause a convergencewarning for some";"CODE"
"10 of the random seeds but only on specific platforms in particular";"IRRE"
"when using atlas blas lapack implementation doubling the maxls internal";"TASK"
"parameter of the solver does not help however this lack of proper";"CODE"
"convergence does not seem to prevent the assertion to pass so we ignore";"CODE"
"the warning for now";"CODE"
"see https github com scikit learn scikit learn pull 27649";"CODE"
"todo 1 10 remove filterwarnings with deprecation period of use legacy attributes";"CODE"
"we weight the first fold 2 times more";"-"
"lbfgs has convergence issues on the data but this should not impact";"META"
"the quality of the results";"IRRE"
"test that passing class weight as 1 2 is the same as";"IRRE"
"passing class weight 1 1 but adjusting sample weights";"IRRE"
"to be 2 for all instances of class 1";"CODE"
"test the above for l1 penalty and l2 penalty with dual true";"IRRE"
"since the patched liblinear code is different";"-"
"helper for returning a dictionary instead of an array";"CODE"
"scale data to avoid convergence warnings with the lbfgs solver";"CODE"
"multinomial case remove 90 of class 0";"CODE"
"same as appropriate sample weight";"-"
"binary case remove 90 of class 0 and 100 of class 2";"CODE"
"tests for the multinomial option in logistic regression";"IRRE"
"some basic attributes of logistic regression";"META"
"lbfgs solver is used as a reference it s the default";"CODE"
"compare solutions between lbfgs and the other solvers";"IRRE"
"test that the path give almost the same results however since in this";"IRRE"
"case we take the average of the coefs after fitting across all the";"CODE"
"folds it need not be exactly the same";"-"
"test negative prediction when decision function values are zero";"IRRE"
"liblinear predicts the positive class when decision function values";"IRRE"
"are zero this is a test to verify that we do not do the same";"CODE"
"see issue https github com scikit learn scikit learn issues 3600";"CODE"
"and the pr https github com scikit learn scikit learn pull 3623";"CODE"
"dummy data such that the decision function becomes zero";"CODE"
"test logregcv with solver liblinear works for sparse matrices";"IRRE"
"test logregcv with solver liblinear works for sparse matrices";"IRRE"
"test that intercept scaling is ignored when fit intercept is false";"CODE"
"because liblinear penalizes the intercept and saga does not we do not";"CODE"
"fit the intercept to make it possible to compare the coefficients of";"CODE"
"the two models at convergence";"-"
"because liblinear penalizes the intercept and saga does not we do not";"CODE"
"fit the intercept to make it possible to compare the coefficients of";"CODE"
"the two models at convergence";"-"
"noise and constant features should be regularized to zero by the l1";"TASK"
"penalty";"-"
"check that solving on the sparse and dense data yield the same results";"IRRE"
"test that when refit true logistic regression cv with the saga solver";"IRRE"
"converges to the same solution as logistic regression with a fixed";"-"
"regularization parameter";"IRRE"
"internally the logisticregressioncv model uses a warm start to refit on";"CODE"
"the full data model with the optimal c found by cv as the penalized";"-"
"logistic regression loss is convex we should still recover exactly";"TASK"
"the same solution as long as the stopping criterion is strict enough and";"-"
"that there are no exactly duplicated features when penalty l1";"TASK"
"predicted probabilities using the true entropy loss should give a";"-"
"smaller loss than those using the ovr method";"-"
"predicted probabilities using the soft max function should give a";"CODE"
"smaller loss than those using the logistic function";"CODE"
"test that the maximum number of iteration is reached";"IRRE"
"test that self n iter has the correct format";"CODE"
"lbfgs requires scaling to avoid convergence warnings";"CODE"
"also generate a binary classification sub problem";"IRRE"
"binary classification case";"CODE"
"multinomial case";"CODE"
"this solver only supports one vs rest multiclass classification";"CODE"
"when using the multinomial objective function there is a single";"CODE"
"optimization problem to solve for all classes at once";"CODE"
"a 1 iteration second fit on same data should give almost same result";"IRRE"
"with warm starting and quite different result without warm starting";"IRRE"
"warm starting does not work with liblinear solver";"CODE"
"reproduce the exact same split as default logisticregressioncv";"CODE"
"some combinations of fold and value of c";"IRRE"
"train fold 0 folds idx fold 0 0 is training fold";"-"
"coefficients without intecept";"CODE"
"intercepts";"CODE"
"compare elasticnet penalty in logisticregression and sgd loss log";"IRRE"
"make sure that the returned coefs by logistic regression path on a";"IRRE"
"multiclass multinomial don t override each other used to be a";"CODE"
"bug";"-"
"for n class 3 coef should be of shape";"CODE"
"n classes features int fit intercept";"CODE"
"for the binary case coef should be of shape";"CODE"
"1 features int fit intercept or";"CODE"
"features int fit intercept";"CODE"
"make sure warning is raised if penalty none and c is set to a";"IRRE"
"non default value";"IRRE"
"make sure setting penalty none is equivalent to setting c np inf with";"IRRE"
"l2 penalty";"-"
"xxx investigate thread safety bug that might be related to";"CODE"
"https github com scikit learn scikit learn issues 31883";"CODE"
"check that we support sample weight with liblinear in all possible cases";"CODE"
"l1 primal l2 primal l2 dual";"-"
"non regression test for issue 14955";"IRRE"
"when penalty is elastic net the scores attribute has shape";"META"
"n classes n cs n l1 ratios";"IRRE"
"we here make sure that the second dimension indeed corresponds to cs and";"-"
"the third dimension corresponds to l1 ratios";"-"
"avg scores lrcv lrcv scores 1 mean axis 0 average over folds";"CODE"
"test logistic regression with the iris dataset";"IRRE"
"scaling x to ease convergence";"-"
"axis 0 is sum over classes";"IRRE"
"solvers either accept large sparse matrices or raise helpful error";"IRRE"
"non regression test for pull request 21093";"CODE"
"generate sparse matrix with int64 indices";"IRRE"
"liblinear freezes when x max 1e100 see issue 7486";"-"
"we preemptively raise an error when x max 1e30";"CODE"
"generate sparse matrix with int64 indices";"IRRE"
"test that newton cg works with a single feature and intercept";"CODE"
"non regression test for issue 23605";"IRRE"
"non regression https github com scikit learn scikit learn issues 18264";"CODE"
"test that the fit does not raise a convergencewarning";"CODE"
"make sure we can inspect the state of logisticregression right after";"-"
"initialization before the first weight update";"CODE"
"xxx sag and saga have n iter 1";"-"
"xxx lbfgs has already started to update the coefficients";"CODE"
"wide data matrix should lead to a rank deficient hessian matrix";"CODE"
"hence make the newton cholesky solver raise a warning and fallback to";"CODE"
"lbfgs";"-"
"c 1e30 very high c to nearly disable regularization";"-"
"check that lbfgs can converge without any warning on this problem";"CODE"
"check that the newton cholesky solver raises a warning and falls back to";"CODE"
"lbfgs this should converge with the same number of iterations as the";"CODE"
"above call of lbfgs since the newton cholesky triggers the fallback";"IRRE"
"before completing the first iteration for the problem setting at hand";"CODE"
"trying to fit the same model again with a small iteration budget should";"CODE"
"therefore raise a convergencewarning";"CODE"
"todo 1 10 remove filterwarnings with deprecation period of use legacy attributes";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"make x not of norm 1 for testing";"IRRE"
"this makes x n samples n features";"TASK"
"and y n samples 3";"-"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 5956";"CODE"
"todo 1 10 move to test sgd py";"TASK"
"mimic sgd s behavior for intercept";"CODE"
"classifier can be retrained on different labels and features";"CODE"
"todo 1 10 move to test sgd py";"TASK"
"test class weights";"IRRE"
"we give a small weights to class 1";"IRRE"
"now the hyperplane should rotate clock wise and";"-"
"the prediction on this point should shift";"CODE"
"partial fit with class weight balanced not supported";"IRRE"
"already balanced so balanced weights should have no effect";"CODE"
"should be similar up to some epsilon due to learning rate schedule";"-"
"valueerror due to wrong class weight label";"IRRE"
"todo 1 10 move to test sgd py";"TASK"
"todo 1 10 remove";"TASK"
"check that we raise the proper deprecation warning";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"for 50 quantile w o regularization any slope in 1 10 is okay";"CODE"
"if positive error costs more the slope is maximal";"-"
"if negative error costs more the slope is minimal";"-"
"for a small lasso penalty the slope is also minimal";"CODE"
"for a large lasso penalty the model predicts the constant median";"CODE"
"test how different parameters affect a small intuitive example";"IRRE"
"check that we still predict fraction";"TASK"
"test that model estimates percentage of points below the prediction";"IRRE"
"test that with unequal sample weights we still estimate weighted fraction";"TASK"
"when we increase weight of upper observations";"-"
"estimate of quantile should go up";"-"
"generate coordinates of line";"-"
"add some faulty data";"TASK"
"estimate parameters of corrupted data";"IRRE"
"ground truth reference inlier mask";"CODE"
"there is a 1e 9 chance it will take these many trials no good reason";"-"
"1e 2 isn t enough can still happen";"TASK"
"2 is the what ransac defines as min samples x shape 1 1";"CODE"
"gh 19390";"-"
"3 d target values";"IRRE"
"estimate parameters of corrupted data";"IRRE"
"ground truth reference inlier mask";"CODE"
"multi dimensional";"-"
"one dimensional";"-"
"estimate parameters of corrupted data";"IRRE"
"ground truth reference inlier mask";"CODE"
"numbers hand calculated and confirmed on page 119 table 4 3 in";"CODE"
"hartley r i and zisserman a 2004";"-"
"multiple view geometry in computer vision second edition";"CODE"
"cambridge university press isbn 0521540518";"-"
"e 0 min samples x";"-"
"e 5 min samples 2";"-"
"e 10 min samples 2";"-"
"e 30 min samples 2";"-"
"e 50 min samples 2";"-"
"e 5 min samples 8";"-"
"e 10 min samples 8";"-"
"e 30 min samples 8";"-"
"e 50 min samples 8";"-"
"e 0 min samples 10";"-"
"sanity check";"-"
"check that mask is correct";"-"
"check that fit x fit x1 x2 x3 sample weight n1 n2 n3 where";"-"
"x x1 repeated n1 times x2 repeated n2 times and so forth";"CODE"
"check that if estimator fit doesn t support";"CODE"
"sample weight raises error";"CODE"
"make larger dim more than double as big as the smaller one";"CODE"
"this helps when constructing singular matrices like x x";"CODE"
"x 1 1 last columns acts as intercept";"CODE"
"assert np all s 1e 3 to be sure";"IRRE"
"add a term that vanishes in the product x y";"TASK"
"w x xx 1 y v s 1 u y";"-"
"add penalty alpha coef 2 2 for alpha 1 and solve via normal equations";"TASK"
"note that the problem is well conditioned such that we get accurate results";"TASK"
"d 1 1 0 intercept gets no penalty";"CODE"
"to be sure";"-"
"alpha 1 0 because ols ridge dataset uses this";"IRRE"
"calculate residuals and r2";"-"
"x x 1 remove intercept";"CODE"
"same with sample weight";"-"
"alpha 1 0 because ols ridge dataset uses this";"IRRE"
"x x 1 remove intercept";"CODE"
"coefficients are not all on the same magnitude adding a small atol to";"TASK"
"make this test less brittle";"IRRE"
"alpha 1 0 because ols ridge dataset uses this";"IRRE"
"x x 1 remove intercept";"CODE"
"coefficients are not all on the same magnitude adding a small atol to";"TASK"
"make this test less brittle";"IRRE"
"alpha 0 ols";"-"
"note that cholesky might give a warning singular matrix in solving dual";"TASK"
"problem using least squares solution instead";"-"
"x x 1 remove intercept";"CODE"
"fixme assert allclose model coef coef should work for all cases but fails";"CODE"
"for the wide fat case with n features n samples the current ridge solvers do";"CODE"
"not return the minimum norm solution with fit intercept true";"CODE"
"as it is an underdetermined problem residuals 0 this shows that we get";"CODE"
"a solution to x w y";"-"
"but it is not the minimum norm solution this should be equal";"META"
"alpha 0 ols";"-"
"x x 1 remove intercept";"CODE"
"cholesky is a bad choice for singular x";"CODE"
"fixme same as in test ridge regression unpenalized";"IRRE"
"as it is an underdetermined problem residuals 0 this shows that we get";"CODE"
"a solution to x w y";"-"
"but it is not the minimum norm solution this should be equal";"META"
"alpha 0 ols";"-"
"x x 1 remove intercept";"CODE"
"fixme same as in test ridge regression unpenalized";"IRRE"
"as it is an underdetermined problem residuals 0 this shows that we get";"CODE"
"a solution to x w y";"-"
"but it is not the minimum norm solution this should be equal";"META"
"x x 1 remove intercept";"CODE"
"test shape of coef and intercept";"IRRE"
"test intercept with multiple targets gh issue 708";"CODE"
"on alpha 0 ridge and ols yield the same solution";"CODE"
"we need more samples than features";"TASK"
"tests the ridge object using individual penalties";"IRRE"
"test error is raised when number of targets and penalties do not match";"CODE"
"manually scale the data to avoid pathological cases we use";"CODE"
"minmax scale to deal with the sparse case without breaking";"CODE"
"the sparsity pattern";"-"
"avoid convergencewarning for sag and saga solvers";"CODE"
"checking on asymmetric scoring";"-"
"test that can work with both dense or sparse matrices";"IRRE"
"check best alpha";"-"
"check that we get same best alpha with custom loss func";"-"
"check that we get same best alpha with custom score func";"-"
"check that we get same best alpha with a scorer";"-"
"check that we get same best alpha with sample weights";"-"
"simulate several responses";"CODE"
"check that cv results is not stored when store cv results is false";"IRRE"
"check that the best score is store";"-"
"ridge clone ridge avoid side effects from shared instances";"CODE"
"tests the ridge cv object optimizing individual penalties for each target";"CODE"
"create random dataset with multiple targets each target should have";"IRRE"
"a different optimal alpha";"-"
"find optimal alpha for each target";"CODE"
"find optimal alphas for all targets simultaneously";"CODE"
"the resulting regression weights should incorporate the different";"IRRE"
"alpha values";"IRRE"
"test shape of alpha and cv results";"IRRE"
"test edge case of there being only one alpha value";"IRRE"
"test edge case of there being only one target";"IRRE"
"try with a custom scoring function";"CODE"
"using a custom cv object should throw an error in combination with";"CODE"
"alpha per target true";"-"
"simulate several responses";"CODE"
"non regression test for 14672";"IRRE"
"check that ridgeclassifiercv works with all sort of scoring and";"IRRE"
"cross validation";"-"
"smoke test to check that fit predict does not raise error";"CODE"
"check that custom scoring is working as expected";"-"
"check the tie breaking strategy keep the first alpha tried";"CODE"
"in case of tie score the first alphas will be kept";"CODE"
"ridgegcv is not very numerically stable with float32 it casts the";"IRRE"
"input to float64 unless the device and namespace combination does";"CODE"
"not allow float64 specifically torch with mps";"IRRE"
"all numpy namespaces are compatible with all solver in particular";"-"
"solvers that support positive true like lbfgs should work";"-"
"test dense matrix";"IRRE"
"test sparse matrix";"IRRE"
"test that the outputs are the same";"IRRE"
"test class weights";"IRRE"
"we give a small weights to class 1";"IRRE"
"now the hyperplane should rotate clock wise and";"-"
"the prediction on this point should shift";"CODE"
"check if class weight balanced can handle negative labels";"IRRE"
"class weight balanced and class weight none should return";"IRRE"
"same values when y has equal number of all labels";"IRRE"
"there are different algorithms for n samples n features";"TASK"
"and the opposite so test them both";"IRRE"
"check using gridsearchcv directly";"-"
"sample weights must be either scalar or 1d";"TASK"
"make sure the ok sample weights actually work";"META"
"sample weights must work with sparse matrices";"IRRE"
"integers";"CODE"
"test excludes svd solver because it raises exception for sparse inputs";"CODE"
"check type consistency 32bits";"-"
"check type consistency 64 bits";"-"
"do the actual checks at once for easier debug";"CODE"
"test different alphas in cholesky solver to ensure full coverage";"IRRE"
"this test is separated from test dtype match for clarity";"CODE"
"check type consistency 32bits";"-"
"check type consistency 64 bits";"-"
"do all the checks at once like this is easier to debug";"CODE"
"xxx sparse cg seems to be far less numerically stable than the";"IRRE"
"others maybe we should not enable float32 for this one";"CODE"
"check that fortran array are converted when using sag solver";"CODE"
"for the order of x and y to not be c ordered arrays";"CODE"
"case of multioutput";"IRRE"
"single output this part of the code should not be reached in the case of";"CODE"
"multioutput scoring";"IRRE"
"return mean errors pragma no cover";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"this is used for sag classification";"CODE"
"approximately equal and saves the computation of the log";"CODE"
"this is used for sag regression";"CODE"
"function for measuring the log loss";"CODE"
"sparse data has a fixed decay of 01";"IRRE"
"idx k";"-"
"sparse data has a fixed decay of 01";"IRRE"
"idx k";"-"
"y must be 0 or 1";"TASK"
"saga variance w r t stream order is higher";"CODE"
"2 y 1 y must be 1 or 1";"TASK"
"2 y 1 y must be 1 or 1";"TASK"
"simple linear function without noise";"CODE"
"simple linear function with noise";"CODE"
"note the very crude accuracy i e high rtol";"TASK"
"xxx untested as of v0 22";"IRRE"
"test data";"IRRE"
"test sample 1";"IRRE"
"test sample 2 string class labels";"IRRE"
"test sample 3";"IRRE"
"test sample 4 two more or less redundant feature groups";"IRRE"
"test sample 5 test sample 1 as binary classification problem";"IRRE"
"common test case to classification and regression";"IRRE"
"a simple implementation of asgd to use for testing";"TASK"
"uses squared loss to find the gradient";"-"
"sparse data has a fixed decay of 01";"IRRE"
"test that explicit warm restart";"IRRE"
"and implicit warm restart are equivalent";"-"
"input format tests";"CODE"
"todo 1 10 remove this test";"CODE"
"multi class test case";"IRRE"
"multi class average test case";"CODE"
"multi class test case";"IRRE"
"multi class test case with multi core support";"IRRE"
"checks coef init and intercept init shape for multi class";"IRRE"
"problems";"-"
"provided coef does not match dataset";"IRRE"
"provided coef does match dataset";"IRRE"
"provided intercept does not match dataset";"CODE"
"provided intercept does match dataset";"CODE"
"checks that sgdclassifier predict proba and predict log proba methods";"IRRE"
"can either be accessed or raise an appropriate error message";"CODE"
"otherwise see";"-"
"https github com scikit learn scikit learn issues 10938 for more";"CODE"
"details";"-"
"check sgd predict proba";"-"
"hinge loss does not allow for conditional prob estimate";"CODE"
"we cannot use the factory here because it defines predict proba";"IRRE"
"anyway";"-"
"log and modified huber losses can output probability estimates";"IRRE"
"binary case";"CODE"
"if predict proba is 0 we get runtimewarning divide by zero encountered";"CODE"
"in log we avoid it here";"CODE"
"log loss multiclass probability estimates";"IRRE"
"modified huber multiclass probability estimates requires a separate";"CODE"
"test because the hard zero one probabilities may destroy the";"IRRE"
"ordering present in decision function output";"IRRE"
"else xxx the sparse test gets a different x2";"IRRE"
"the following sample produces decision function values 1";"IRRE"
"which would cause naive normalization to fail see comment";"-"
"in sgdclassifier predict proba";"IRRE"
"if np all d 1 xxx not true in sparse test case why";"IRRE"
"test l1 regularization";"IRRE"
"test sparsify with dense inputs";"IRRE"
"pickle and unpickle with sparse coef";"IRRE"
"test class weights";"IRRE"
"we give a small weights to class 1";"IRRE"
"now the hyperplane should rotate clock wise and";"-"
"the prediction on this point should shift";"CODE"
"test if equal class weights approx equals no class weights";"IRRE"
"should be similar up to some epsilon due to learning rate schedule";"-"
"valueerror due to not existing class label";"IRRE"
"tests that class weight and sample weight are multiplicative";"IRRE"
"non regression test for 23255";"IRRE"
"check that the sparse coef property works";"IRRE"
"check that the sparse lasso can handle zero data without crashing";"IRRE"
"test elasticnet for various values of alpha and l1 ratio with list x";"IRRE"
"y 1 0 1 just a straight line";"-"
"t np array 2 3 4 test sample";"IRRE"
"this should be the same as unregularized least squares";"CODE"
"catch warning about alpha 0";"CODE"
"this is discouraged but should work";"META"
"test elasticnet for various values of alpha and l1 ratio with sparse x";"IRRE"
"training samples";"-"
"x 1 0 0";"-"
"y 1 0 1 just a straight line the identity function";"CODE"
"test samples";"IRRE"
"this should be the same as lasso";"CODE"
"build an ill posed linear regression problem with many noisy features and";"TASK"
"comparatively few samples";"-"
"generate a ground truth model";"-"
"w n informative 0 0 only the top features are impacting the model";"TASK"
"x rnd 0 5 0 0 50 of zeros in input signal";"CODE"
"generate training ground truth labels";"-"
"check the convergence is the same as the dense version";"META"
"check that the coefs are sparse";"IRRE"
"check the convergence is the same as the dense version";"META"
"check that the coefs are sparse";"IRRE"
"xxx there is a bug when precompute is not false";"-"
"compare with dense data";"IRRE"
"balance property";"-"
"make x data read only";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"linear model y 3 x n 2 0 1 2";"-"
"add some outliers";"TASK"
"linear model y 5 x 1 10 x 2 n 1 0 1 2";"-"
"add some outliers";"TASK"
"linear model y 5 x 1 10 x 2 42 x 3 7 x 4 n 1 0 1 2";"-"
"add some outliers";"TASK"
"check startvalue is element of x and solution";"IRRE"
"check startvalue is not the solution";"IRRE"
"check startvalue is not the solution but element of x";"IRRE"
"check that a single vector is identity";"-"
"check first two iterations";"-"
"check fix point";"CODE"
"test larger problem and for exact solution in 1d case";"CODE"
"check if median is solution of the fermat weber location problem";"IRRE"
"check when maximum iteration is exceeded a warning is emitted";"-"
"check that least squares fails";"-"
"check that theil sen works";"-"
"check that least squares fails";"-"
"check that theil sen works";"-"
"non regression test for 18104";"IRRE"
"check that least squares fails";"-"
"check that theil sen works";"-"
"check for exact the same results as least squares";"IRRE"
"pytest mark thread unsafe manually captured stdout";"CODE"
"check that theil sen can be verbose";"IRRE"
"check that least squares fails";"-"
"check that theil sen works";"-"
"check that theil sen falls back to least squares if fit intercept false";"CODE"
"check fit intercept true case this will not be equal to the least";"CODE"
"squares solution since the intercept is calculated differently";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"double centering";"CODE"
"eigendecomposition";"CODE"
"reversing the order of the eigenvalues eigenvectors to put";"IRRE"
"the eigenvalues in decreasing order";"IRRE"
"set the signs of eigenvectors to enforce deterministic output";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"randomly choose initial configuration";"IRRE"
"overrides the parameter p";"IRRE"
"out of bounds condition cannot happen because we are transforming";"CODE"
"the training set here but does sometimes get triggered in";"IRRE"
"practice due to machine precision issues hence clip";"CODE"
"compute distance and monotonic regression";"-"
"dissimilarities with 0 are considered as missing values";"IRRE"
"compute the disparities using isotonic regression";"-"
"for the first smacof iteration use scaled original dissimilarities";"CODE"
"this choice follows the r implementation described in this paper";"CODE"
"https www jstatsoft org article view v102i10";"CODE"
"update x using the guttman transform";"CODE"
"compute stress";"IRRE"
"if verbose 2 pragma no cover";"IRRE"
"if verbose pragma no cover";"IRRE"
"todo 1 9 change default n init to 1 see pr 31117";"CODE"
"todo 1 9 change default n init to 1 see pr 31117";"CODE"
"todo 1 10 change default init to classical mds see pr 32229";"CODE"
"todo 1 10 drop support for boolean metric see pr 32229";"CODE"
"todo 1 10 drop support for dissimilarity see pr 32229";"CODE"
"make the matrix symmetric";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"this is the exact and barnes hut t sne implementation there are other";"TASK"
"modifications of the algorithm";"-"
"fast optimization for t sne";"CODE"
"https cseweb ucsd edu lvdmaaten workshops nips2010 papers vandermaaten pdf";"CODE"
"mypy error module sklearn manifold has no attribute utils";"META"
"mypy error module sklearn manifold has no attribute barnes hut tsne";"IRRE"
"from sklearn manifold import barnes hut tsne utils type ignore attr defined";"CODE"
"compute conditional probabilities such that they approximately match";"-"
"the desired perplexity";"-"
"compute conditional probabilities such that they approximately match";"-"
"the desired perplexity";"-"
"symmetrize the joint probability distribution using sparse operations";"IRRE"
"normalize the joint probability distribution";"META"
"q is a heavy tailed distribution student s t distribution";"META"
"optimization trick below np dot x y is faster than";"CODE"
"np sum x y because it calls blas";"IRRE"
"objective c kullback leibler divergence of p and q";"IRRE"
"gradient dc dy";"-"
"pdist always returns double precision distances thus we need to take";"CODE"
"only compute the error when needed";"-"
"we set the diagonal to np inf to exclude the points themselves from";"CODE"
"their own neighborhood";"-"
"ind x i is the index of sorted distances between i and other samples";"CODE"
"we build an inverted index of neighbors in the input space for sample i";"CODE"
"we define inverted index i as the inverted index of sorted distances";"CODE"
"inverted index i ind x i np arange 1 n sample 1";"-"
"control the number of exploration iterations with early exaggeration on";"-"
"control the number of iterations between progress checks";"-"
"t sne minimizes the kullback leiber divergence of the gaussians p";"-"
"and the student s t distributions q the optimization algorithm that";"META"
"we use is batch gradient descent with two stages";"-"
"initial optimization with early exaggeration and momentum at 0 5";"IRRE"
"final optimization with momentum at 0 8";"CODE"
"repeat verbose argument for kl divergence bh";"IRRE"
"get the number of threads for gradient computation here to";"CODE"
"avoid recomputing it at each iteration";"CODE"
"learning schedule part 1 do 250 iteration with lower momentum but";"META"
"higher learning rate controlled via the early exaggeration parameter";"IRRE"
"learning schedule part 2 disable early exaggeration and finish";"TASK"
"optimization with a higher momentum at 0 8";"-"
"save the final number of iterations";"CODE"
"tsne metric is not validated yet";"TASK"
"tsne metric is not validated yet";"TASK"
"swap the signs if necessary";"-"
"non symmetric input";"CODE"
"non square input";"CODE"
"grid of equidistant points in 2d n components n dim";"CODE"
"add noise in a third dimension";"TASK"
"isomap should preserve distances when all neighbors are used";"-"
"distances from each point to all others";"CODE"
"same setup as in test isomap simple grid with an added dimension";"IRRE"
"compute input kernel";"CODE"
"compute output kernel";"IRRE"
"make sure error agrees";"-"
"create s curve dataset";"IRRE"
"compute isomap embedding";"-"
"re embed a noisy version of the points";"META"
"make sure the rms error on re embedding is comparable to noise scale";"CODE"
"check that isomap works fine as a transformer in a pipeline";"CODE"
"only checks that no error is raised";"CODE"
"todo check that it actually does something useful";"CODE"
"test chaining nearestneighborstransformer and isomap with";"IRRE"
"neighbors algorithm precomputed";"-"
"compare the chained version and the compact version";"META"
"isomap must work on various metric parameters work correctly";"IRRE"
"and must default to euclidean";"CODE"
"regression test for bug reported in 6062";"IRRE"
"todo compare results on dense and sparse data as proposed in";"IRRE"
"https github com scikit learn scikit learn pull 23585 discussion r968388186";"CODE"
"isomap fit transform must yield similar result when using";"IRRE"
"a precomputed distance matrix";"-"
"test utility routines";"IRRE"
"check that columns sum to one";"-"
"test lle by computing the reconstruction error on some manifolds";"CODE"
"note arpack is numerically unstable so this test will fail for";"IRRE"
"some random seeds we choose 42 because the tests pass";"IRRE"
"for arm64 platforms 2 makes the test fail";"CODE"
"todo rewrite this test to make less sensitive to the random seed";"CODE"
"irrespective of the platform";"CODE"
"grid of equidistant points in 2d n components n dim";"CODE"
"re embed a noisy version of x using the transform method";"META"
"similar test on a slightly more complex manifold";"IRRE"
"check that locallylinearembedding works fine as a pipeline";"CODE"
"only checks that no error is raised";"CODE"
"todo check that it actually does something useful";"CODE"
"test the error raised when the weight matrix is singular";"IRRE"
"regression test for 6033";"IRRE"
"clf fit x this previously raised a typeerror";"CODE"
"test metric smacof using the data of modern multidimensional scaling";"IRRE"
"borg groenen p 154";"-"
"testing that nonmetric mds results in lower normalized stress compared";"IRRE"
"compared to metric mds non regression test for issue 27028";"IRRE"
"a metric mds solution local minimum of the raw stress can be rescaled to";"-"
"decrease the stress 1 which is returned with normalized stress true";"IRRE"
"the optimal rescaling can be computed analytically see borg groenen";"IRRE"
"modern multidimensional scaling chapter 11 1 after rescaling stress 1";"-"
"becomes sqrt s 2 1 s 2 where s is the value of stress 1 before";"IRRE"
"rescaling";"-"
"test that stress is decreasing during nonmetric mds optimization";"IRRE"
"non regression test for issue 27028";"IRRE"
"not symmetric similarity matrix";"-"
"not squared similarity matrix";"-"
"init not none and not correct format";"IRRE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"todo 1 10 remove warning filter";"TASK"
"from pyamg import smoothed aggregation solver noqa f401";"CODE"
"non centered sparse centers to check the";"IRRE"
"connect all elements within the group at least once via an";"CODE"
"arbitrary path that spans the group";"-"
"add some more random connections within the group";"CODE"
"build a symmetric affinity matrix";"IRRE"
"we should retrieve the same component mask by starting by both ends";"CODE"
"of the group";"-"
"todo investigate why this test is seed sensitive on 32 bit python";"CODE"
"runtimes is this revealing a numerical stability problem or is it";"CODE"
"expected from the test numerical design in the latter case the test";"IRRE"
"should be made less seed sensitive instead";"-"
"test spectral embedding with two components";"IRRE"
"first component";"-"
"second component";"-"
"test of internal graph connected component before connection";"CODE"
"connection";"CODE"
"thresholding on the first components using 0";"-"
"test spectral embedding with precomputed kernel";"IRRE"
"test precomputed graph filtering when containing too many neighbors";"IRRE"
"test spectral embedding with callable affinity";"IRRE"
"same with special case in which amg is not actually used";"META"
"regression test for 10715";"IRRE"
"affinity between nodes";"IRRE"
"check that passing a sparse matrix with np int64 indices dtype raises an error";"CODE"
"or is successful based on the version of scipy which is installed";"META"
"use a csr matrix to avoid any conversion during the validation";"META"
"pr https github com scipy scipy pull 18913";"CODE"
"first integration in 1 11 3 https github com scipy scipy pull 19279";"CODE"
"non regression test for amg solver failure issue 13393 on github";"IRRE"
"check that the learned embedding is stable w r t random solver init";"IRRE"
"test using pipeline to do spectral clustering";"CODE"
"test that graph connectivity test works as expected";"IRRE"
"test that spectral embedding is deterministic";"IRRE"
"test that spectral embedding is also processing unnormalized laplacian";"IRRE"
"correctly";"-"
"verify using manual computation with dense eigh";"-"
"test that the first eigenvector of spectral embedding";"IRRE"
"is constant and that the second is not for a connected graph";"CODE"
"mypy error module sklearn manifold has no attribute barnes hut tsne";"IRRE"
"from sklearn manifold import type ignore attr defined";"CODE"
"test stopping conditions of gradient descent";"IRRE"
"gradient norm";"-"
"maximum number of iterations without improvement";"TASK"
"maximum number of iterations";"-"
"test if the binary search finds gaussians with desired perplexity";"IRRE"
"test if the binary search finds gaussians with desired perplexity";"IRRE"
"a more challenging case than the one above producing numeric";"CODE"
"underflow in float precision see issue 19471 and pr 19472";"CODE"
"binary perplexity search approximation";"-"
"should be approximately equal to the slow method when we use";"-"
"all points as neighbors";"CODE"
"test that when we use all the neighbors the results are identical";"IRRE"
"test that the highest p ij are the same when fewer neighbors are used";"IRRE"
"topn k 10 check the top 10 k entries out of k k entries";"-"
"binary perplexity search should be stable";"-"
"the binary search perplexity had a bug wherein the p array";"CODE"
"was uninitialized leading to sporadically failing tests";"IRRE"
"convert the sparse matrix to a dense one for testing";"IRRE"
"test gradient of kullback leibler divergence";"IRRE"
"test trustworthiness score";"IRRE"
"affine transformation";"CODE"
"randomly shuffled";"IRRE"
"completely different";"CODE"
"nearest neighbors should be preserved approximately";"-"
"computed distance matrices must be positive";"TASK"
"negative computed distances should be caught even if result is squared";"IRRE"
"initialize tsne with ndarray and test fit";"IRRE"
"initialize tsne with ndarray and metric precomputed";"IRRE"
"make sure no futurewarning is thrown from fit";"CODE"
"precomputed distance matrices cannot use pca initialization";"IRRE"
"barnes hut method should only be used with n components 3";"IRRE"
"check that the early exaggeration parameter has an effect";"IRRE"
"check that the max iter parameter has an effect";"IRRE"
"test the tree with only a single set of children";"IRRE"
"these tests answers have been checked against the reference";"IRRE"
"implementation by lvdm";"TASK"
"four points tests the tree with multiple levels of children";"IRRE"
"these tests answers have been checked against the reference";"IRRE"
"implementation by lvdm";"TASK"
"test the kwargs option skip num points";"IRRE"
"skip num points should make it such that the barnes hut gradient";"IRRE"
"is not calculated for indices below skip num point";"CODE"
"aside from skip num points 2 and the first two gradient rows";"CODE"
"being set to zero these data points are the same as in";"IRRE"
"test answer gradient four points";"IRRE"
"pytest mark thread unsafe manually captured stdout";"CODE"
"verbose options write to stdout";"CODE"
"t sne should allow metrics that cannot be squared issue 3526";"-"
"t sne should allow reduction to one component issue 4154";"-"
"ensure 64bit arrays are handled correctly";"-"
"tsne cython code is only single precision so the output will";"IRRE"
"always be single precision irrespectively of the input dtype";"CODE"
"ensure kl divergence is computed at last iteration";"-"
"even though max iter n iter check 0 i e 1003 50 0";"-"
"when barnes hut s angle 0 this corresponds to the exact method";"CODE"
"use a dummy negative n iter without progress and check output on stdout";"CODE"
"the output needs to contain the value of n iter without progress";"IRRE"
"make sure that the parameter min grad norm is used correctly";"IRRE"
"extract the gradient norm from the verbose output";"IRRE"
"when the computation is finished just an old gradient norm value";"TASK"
"is repeated that we do not need to store";"TASK"
"compute how often the gradient norm is smaller than min grad norm";"-"
"the gradient norm can be smaller than min grad norm at most once";"-"
"because in the moment it becomes smaller the optimization stops";"CODE"
"ensures that the accessible kl divergence matches the computed value";"IRRE"
"the output needs to contain the accessible kl divergence as the error at";"TASK"
"the last iteration";"-"
"if the test fails a first time re run with init y to see if";"IRRE"
"this was caused by a bad initialization note that this will";"CODE"
"also run an early exaggeration step";"CODE"
"ensure that the resulting embedding leads to approximately";"IRRE"
"uniformly spaced points the distance to the closest neighbors";"CODE"
"should be non zero and approximately constant";"CODE"
"check that the barnes hut method match the exact one when";"IRRE"
"angle 0 and perplexity n samples 3";"-"
"kill the early exaggeration";"-"
"check that the bh gradient with different num threads gives the same";"CODE"
"results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"swap average weight score weight";"-"
"average the results";"IRRE"
"scores with 0 weights are forced to be 0 preventing the average";"CODE"
"score from being affected by 0 weighted nan elements";"CODE"
"compute scores treating a as positive class and b as negative class";"IRRE"
"then b as positive class and a as negative class";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"else np isnan zero division";"-"
"we can t have more than one value on y type the set is no more needed";"IRRE"
"no metrics support multiclass multioutput format";"IRRE"
"we expect y true and y pred to be of the same data type";"-"
"if y true was provided to the classifier as strings";"CODE"
"y pred given by the classifier will also be encoded with";"IRRE"
"strings so we raise a meaningful error";"CODE"
"xxx do we really want to sparse encode multilabel indicators when";"IRRE"
"they are passed as a dense arrays this is not possible for array";"CODE"
"api inputs in general hence we only do it for numpy inputs but even";"CODE"
"for numpy the usefulness is questionable";"CODE"
"for classification metrics both array api compatible and non array api";"CODE"
"compatible inputs are allowed for y true this is because arrays that";"CODE"
"store class labels as strings cannot be represented in namespaces other";"CODE"
"than numpy thus to avoid unnecessary complexity we always convert";"IRRE"
"y true to a numpy array so that it can be processed appropriately by";"-"
"labelbinarizer and then transfer the integer encoded output back to the";"IRRE"
"target namespace and device";"-"
"labelbinarizer does not respect the order implied by labels which";"CODE"
"can be misleading";"META"
"if y prob is of single dimension assume y true to be binary";"-"
"and then check";"-"
"make sure y prob is normalized";"-"
"check if dimensions are consistent";"IRRE"
"compute accuracy for each possible representation";"CODE"
"convert the input arrays to numpy on cpu irrespective of the original";"CODE"
"namespace and device so as to be able to leverage the the efficient";"CODE"
"counting operations implemented by scipy in the coo matrix constructor";"CODE"
"the final results will be converted back to the input namespace and device";"CODE"
"for the sake of consistency with other metric functions with array api support";"CODE"
"this is needed to handle the special case where y true y pred and";"CODE"
"sample weight are all empty";"-"
"in this case we don t pass sample weight to check targets that would";"CODE"
"check that sample weight is not empty and we don t reuse the returned";"IRRE"
"sample weight";"-"
"if labels are not consecutive integers starting from zero then";"CODE"
"y true and y pred must be converted into index form";"CODE"
"intersect y pred y true with labels eliminate items not in labels";"CODE"
"also eliminate weights of eliminated items";"-"
"choose the accumulator dtype to always have high precision";"IRRE"
"labels are now from 0 to len labels 1 use bincount";"CODE"
"pathological case";"CODE"
"retain only selected labels";"CODE"
"all labels are index integers for multilabel";"CODE"
"select labels";"CODE"
"calculate weighted counts";"-"
"array api strict only supports floating point dtypes for truediv";"CODE"
"which is used below to compute expected as well as k therefore";"CODE"
"we use the maximum floating point dtype available for relevant arrays";"CODE"
"to avoid running into this problem";"CODE"
"else linear or quadratic";"-"
"numerator is 0 and warning should have already been issued";"CODE"
"denominator mask 1 avoid infs nans";"CODE"
"set those with 0 denominator to zero division and 0 when warn";"IRRE"
"we assume the user will be removing warnings if zero division is set";"IRRE"
"to something different than warn if we are computing only f score";"-"
"the warning will be raised only if precision and recall are ill defined";"CODE"
"build appropriate warning";"-"
"convert to python primitive type to avoid numpy type python str";"CODE"
"comparison see https github com numpy numpy issues 6784";"CODE"
"calculate tp sum pred sum true sum";"-"
"finally we have all our sufficient statistics divide";"CODE"
"divide and on zero division set scores and or warn according to";"CODE"
"zero division";"-"
"the score is defined as";"CODE"
"score 1 beta 2 precision recall beta 2 precision recall";"IRRE"
"therefore we can express the score in terms of confusion matrix entries as";"CODE"
"score 1 beta 2 tp 1 beta 2 tp beta 2 fn fp";"-"
"array api strict requires all arrays to be of the same type so we";"CODE"
"need to convert true sum pred sum and tp sum to the max supported";"TASK"
"float dtype because beta2 is a float";"CODE"
"average the results";"IRRE"
"true sum none return no support";"IRRE"
"todo 1 9 when raise warning is removed the following changes need to be made";"TASK"
"the checks for raise warning true need to be removed and we will always warn";"CODE"
"remove futurewarning and the warns section in the docstring should not mention";"CODE"
"raise warning anymore";"OUTD"
"if support pos 0 a division by zero will occur";"-"
"if fp 0 a division by zero will occur";"-"
"replace undefined by is a dict and";"CODE"
"isinstance replace undefined by get lr none real this includes";"CODE"
"np inf and np nan";"-"
"if tn 0 a division by zero will occur";"-"
"replace undefined by is a dict and";"CODE"
"isinstance replace undefined by get lr none real this includes";"CODE"
"np nan";"-"
"array api strict only supports floating point dtypes for truediv";"CODE"
"which is used below to compute per class";"IRRE"
"labelled micro average";"-"
"compute per class results without averaging";"IRRE"
"compute all applicable averages";"-"
"compute averages with specified averaging method";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"pairwise distances reductions";"-"
"overview";"-"
"this module provides routines to compute pairwise distances between a set";"CODE"
"of row vectors of x and another set of row vectors of y and apply a";"IRRE"
"reduction on top the canonical example is the brute force computation";"CODE"
"of the top k nearest neighbors by leveraging the arg k min reduction";"-"
"the reduction takes a matrix of pairwise distances between rows of x and y";"-"
"as input and outputs an aggregate data structure for each row of x the";"CODE"
"aggregate values are typically smaller than the number of rows in y hence";"IRRE"
"the term reduction";"-"
"for computational reasons the reduction are performed on the fly on chunks";"CODE"
"of rows of x and y so as to keep intermediate data structures in cpu cache";"CODE"
"and avoid unnecessary round trips of large distance arrays with the ram";"IRRE"
"that would otherwise severely degrade the speed by making the overall";"CODE"
"processing memory bound";"-"
"finally the routines follow a generic parallelization template to process";"CODE"
"chunks of data with openmp loops via cython prange either on rows of x";"IRRE"
"or rows of y depending on their respective sizes";"TASK"
"dispatching to specialized implementations";"TASK"
"dispatchers are meant to be used in the python code under the hood a";"CODE"
"dispatcher must only define the logic to choose at runtime to the correct";"CODE"
"dtype specialized class basedistancesreductiondispatcher implementation based";"TASK"
"on the dtype of x and of y";"CODE"
"high level diagram";"-"
"legend";"CODE"
"a b a inherits from b";"CODE"
"a x b a dispatches to b";"-"
"base dispatcher";"-"
"basedistancesreductiondispatcher";"-"
"dispatcher dispatcher";"-"
"argkmin radiusneighbors";"-"
"float 32 64 implem";"CODE"
"basedistancesreduction 32 64";"-"
"dispatcher dispatcher";"-"
"argkminclassmode radiusneighborsclassmode";"IRRE"
"x x";"-"
"argkmin 32 64 radiusneighbors 32 64";"-"
"x x";"-"
"argkminclassmode 32 64 radiusneighborsclassmode 32 64";"IRRE"
"specializations";"-"
"x x";"-"
"euclideanargkmin 32 64 euclideanradiusneighbors 32 64";"CODE"
"for instance class argkmin dispatches to";"CODE"
"class argkmin64 if x and y are two float64 array likes";"CODE"
"class argkmin32 if x and y are two float32 array likes";"CODE"
"in addition if the metric parameter is set to euclidean or sqeuclidean";"IRRE"
"then some direct subclass of basedistancesreduction 32 64 further dispatches";"IRRE"
"to one of their subclass for euclidean specialized implementation for instance";"CODE"
"class argkmin64 dispatches to class euclideanargkmin64";"CODE"
"those euclidean specialized implementations relies on optimal implementations of";"TASK"
"a decomposition of the squared euclidean distance matrix into a sum of three terms";"CODE"
"see class middletermcomputer 32 64";"CODE"
"ruff noqa e501";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"pyfunc cannot be supported because it necessitates interacting with";"CODE"
"the cpython interpreter to call user defined functions";"CODE"
"mahalanobis is numerically unstable";"IRRE"
"in order to support discrete distance metrics we need to have a";"TASK"
"stable simultaneous sort which preserves the order of the indices";"-"
"because there generally is a lot of occurrences for a given values";"IRRE"
"of distances in this case";"CODE"
"todo implement a stable simultaneous sort";"TASK"
"fixme the current cython implementation is too slow for a large number of";"TASK"
"features we temporarily disable it to fallback on scipy s implementation";"TASK"
"see https github com scikit learn scikit learn issues 28191";"CODE"
"todo support csr matrices without non zeros elements";"TASK"
"todo support csr matrices with int64 indices and indptr";"CODE"
"see https github com scikit learn scikit learn issues 23653";"CODE"
"euclidean is technically usable for argkminclassmode";"CODE"
"but its current implementation would not be competitive";"TASK"
"todo implement euclidean specialization using gemm";"TASK"
"euclidean is technically usable for radiusneighborsclassmode";"CODE"
"but it would not be competitive";"META"
"todo implement euclidean specialization using gemm";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"print text with appropriate color depending on background";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we have the following bounds";"-"
"sp stats norm ppf 0 0 np inf";"-"
"sp stats norm ppf 1 0 np inf";"-"
"we therefore clip to eps and 1 eps to not provide infinity to matplotlib";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"force to have a squared axis";"CODE"
"else kind residual vs predicted";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"return single artist if only one curve is plotted";"IRRE"
"case 1 multiclass classifier with multiclass target";"CODE"
"case 2 multiclass classifier with binary target";"CODE"
"case 3 binary classifier with multiclass target";"CODE"
"clone since we parametrize the test and the classifier will be fitted";"IRRE"
"when testing the second and subsequent plotting function";"IRRE"
"safe guard for the binary if else construction";"CODE"
"diagonal text is black";"-"
"off diagonal text is white";"-"
"diagonal text is white";"-"
"off diagonal text is black";"-"
"regression test for 15920";"IRRE"
"from estimator passes the font size";"CODE"
"plot adjusts plot to new font size";"CODE"
"from predictions passes the font size";"CODE"
"binarize the data with only the two first classes";"IRRE"
"safe guard for the binary if else construction";"CODE"
"cannot fail thanks to pyplot fixture";"-"
"check the default name display in the figure when name is not provided";"CODE"
"binarize the data with only the two first classes";"IRRE"
"todo 1 10 remove";"TASK"
"checking for chance level line styles";"CODE"
"check that we can provide the positive label and display the proper";"-"
"statistics";"-"
"create a highly imbalanced version of the breast cancer dataset";"IRRE"
"only use 2 features to make the problem even harder";"TASK"
"sanity check to be sure the positive class is classes 0 and that we";"IRRE"
"are betrayed by the class imbalance";"IRRE"
"we select the corresponding probability columns or reverse the decision";"IRRE"
"function otherwise";"CODE"
"we should obtain the statistics of the cancer class";"CODE"
"otherwise we should obtain the statistics of the not cancer class";"CODE"
"check that even if one passes plot chance level false the first time";"IRRE"
"one can still call disp plot with plot chance level true and get the";"TASK"
"chance level line";"-"
"when calling from estimator or from predictions";"CODE"
"prevalence pos label should have been set so that directly";"IRRE"
"calling plot chance level true should plot the chance level line";"IRRE"
"check that raises correctly when plotting chance level with";"CODE"
"no prvelance pos label is provided";"-"
"check that the despine keyword is working correctly";"-"
"safe guard for the binary if else construction";"CODE"
"todo 1 10 remove";"TASK"
"list of length 1 is always allowed";"CODE"
"initialize display with test inputs";"IRRE"
"default alpha used";"CODE"
"alpha from dict used for all curves";"CODE"
"different alpha used for each curve";"CODE"
"other default kwargs should be the same";"IRRE"
"todo 1 9 remove in 1 9";"TASK"
"default alpha for from cv results";"CODE"
"each individual curve labelled";"-"
"single aggregate label";"-"
"multiple labels in legend";"CODE"
"name is a list of different strings";"CODE"
"single label in legend";"CODE"
"name is single string";"CODE"
"should be estimator classes 1";"IRRE"
"checking for legend behaviour";"CODE"
"assert legend is not none legend should be present if any label is set";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"reductions such as sum used internally in trapezoid do not return a";"CODE"
"scalar by default for numpy memmap instances contrary to";"CODE"
"regular numpy ndarray instances";"-"
"return the step function integral";"CODE"
"the following works because the last entry of precision is";"IRRE"
"guaranteed to be 1 as returned by precision recall curve";"IRRE"
"due to numerical error we can get 0 0 and we therefore clip it";"CODE"
"convert to python primitive type to avoid numpy type python str";"CODE"
"comparison see https github com numpy numpy issues 6784";"CODE"
"add a threshold at inf where the clf always predicts the negative class";"TASK"
"i e tps fps 0";"-"
"drop thresholds where true positives tp do not change from the";"CODE"
"previous or subsequent threshold as tp fn is fixed for a dataset";"IRRE"
"this means the false negative rate fnr remains constant while the";"CODE"
"false positive rate fpr changes producing horizontal line segments";"CODE"
"in the transformed normal deviate scale these intermediate points";"CODE"
"can be dropped to create lighter det curve plots";"IRRE"
"start with false positives zero which may be at a finite threshold";"IRRE"
"stop with false negatives zero";"-"
"reverse the output such that list of false positives is decreasing";"IRRE"
"is 00 04 stern school of business new york university";"CODE"
"get a list of n output containing probability arrays of shape";"IRRE"
"n samples n classes";"IRRE"
"extract the positive columns for each output";"CODE"
"we have ground truth relevance of some answers to a query";"CODE"
"we predict scores for the answers";"CODE"
"we can set k to truncate the sum only top k answers contribute";"IRRE"
"now we have some ties in our prediction";"-"
"by default ties are averaged so here we get the average true";"CODE"
"relevance of our top predictions 10 5 2 7 5";"-"
"we can choose to ignore ties for faster results but only";"IRRE"
"if we know there aren t ties in our scores otherwise we get";"-"
"wrong results";"IRRE"
"we have ground truth relevance of some answers to a query";"CODE"
"we predict some scores relevance for the answers";"CODE"
"we can set k to truncate the sum only top k answers contribute";"IRRE"
"the normalization takes k into account so a perfect answer";"CODE"
"would still get 1 0";"TASK"
"now we have some ties in our prediction";"-"
"by default ties are averaged so here we get the average normalized";"CODE"
"true relevance of our top predictions 10 10 5 10 2 75";"-"
"we can choose to ignore ties for faster results but only";"IRRE"
"if we know there aren t ties in our scores otherwise we get";"-"
"wrong results";"IRRE"
"y score np array 0 5 0 2 0 2 0 is in top 2";"CODE"
"0 3 0 4 0 2 1 is in top 2";"CODE"
"0 2 0 4 0 3 2 is in top 2";"CODE"
"0 7 0 2 0 1 2 isn t in top 2";"CODE"
"not normalizing gives the number of correctly classified samples";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"pass none as weights to average uniform mean";"CODE"
"average across the outputs if needed";"IRRE"
"the second call to average should always return";"IRRE"
"a scalar array that we convert to a python float to";"CODE"
"consistently return the same eager evaluated value";"IRRE"
"therefore axis none";"CODE"
"pass none as weights to average uniform mean";"CODE"
"average across the outputs if needed";"IRRE"
"the second call to average should always return";"IRRE"
"a scalar array that we convert to a python float to";"CODE"
"consistently return the same eager evaluated value";"IRRE"
"therefore axis none";"CODE"
"pass none as weights to average uniform mean";"CODE"
"average across the outputs if needed";"IRRE"
"the second call to average should always return";"IRRE"
"a scalar array that we convert to a python float to";"CODE"
"consistently return the same eager evaluated value";"IRRE"
"therefore axis none";"CODE"
"pass none as weights to average uniform mean";"CODE"
"average across the outputs if needed";"IRRE"
"the second call to average should always return";"IRRE"
"a scalar array that we convert to a python float to";"CODE"
"consistently return the same eager evaluated value";"IRRE"
"therefore axis none";"CODE"
"pass none as weights to average uniform mean";"CODE"
"average across the outputs if needed";"IRRE"
"the second call to average should always return";"IRRE"
"a scalar array that we convert to a python float to";"CODE"
"consistently return the same eager evaluated value";"IRRE"
"therefore axis none";"CODE"
"pass none as weights to np average uniform mean";"CODE"
"extreme stable y any real number y pred 0";"-"
"normal distribution y and y pred any real number";"META"
"poisson distribution";"META"
"gamma distribution";"META"
"extreme stable y any real number y pred 0";"-"
"normal y and y pred can be any real number";"-"
"poisson and compound poisson distribution y 0 y pred 0";"META"
"gamma and extreme stable distribution y and y pred 0";"META"
"else pragma nocover";"-"
"unreachable statement";"-"
"return scores individually";"IRRE"
"else multioutput uniform average";"IRRE"
"passing none as weights to np average results in uniform mean";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"todo slep006 remove when metadata routing is the only way";"TASK"
"https scikit learn org stable glossary html term scoring";"CODE"
"standard regression scores";"-"
"standard classification scores";"IRRE"
"score functions that need decision values";"IRRE"
"score function for probabilistic classification";"CODE"
"clustering scores";"-"
"cluster metrics that use supervised evaluation";"CODE"
"heuristic to ensure user has not passed a metric";"-"
"transfer the metadata request";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"todo 1 10 remove";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"input checks";"CODE"
"wk https en wikipedia org wiki rand index adjusted rand index";"IRRE"
"b contains 2 of the 3 biclusters in a so score should be 2 3";"-"
"dictionaries of metrics";"-"
"the goal of having those dictionaries is to have an easy way to call a";"IRRE"
"particular metric and associate a name to each function";"CODE"
"supervised metrics all supervised cluster metrics when given a";"CODE"
"ground truth value";"IRRE"
"unsupervised metrics all unsupervised cluster metrics";"CODE"
"those dictionaries will be used to test systematically some invariance";"IRRE"
"properties e g invariance toward several input layout";"CODE"
"lists of metrics with common properties";"-"
"lists of metrics with common properties are used to test systematically some";"IRRE"
"functionalities and invariance e g symmetric metrics lists all metrics";"CODE"
"that are symmetric with respect to their input argument y true and y pred";"CODE"
"symmetric with respect to their input arguments y true and y pred";"CODE"
"symmetric metrics only apply to supervised clusters";"CODE"
"metrics whose upper bound is 1";"-"
"all clustering metrics do not change score due to permutations of labels";"CODE"
"that is when 0 and 1 exchanged";"-"
"for all clustering metrics input parameters can be both";"CODE"
"in the form of arrays lists positive negative or string";"CODE"
"only the supervised metrics support single sample";"CODE"
"non regression tests for https github com scikit learn scikit learn issues 30950";"CODE"
"homogeneous but not complete clustering";"META"
"complete but not homogeneous clustering";"META"
"neither complete nor homogeneous but not so bad either";"META"
"test for when beta passed to";"IRRE"
"homogeneity completeness v measure";"CODE"
"and v measure score";"-"
"regression tests for labels with gaps";"IRRE"
"compute score for random uniform cluster labelings";"CODE"
"check that adjusted scores are almost zero on random labels";"IRRE"
"compute the adjusted mutual information and test against known values";"IRRE"
"mutual information";"CODE"
"with provided sparse contingency";"IRRE"
"with provided dense contingency";"-"
"expected mutual information";"CODE"
"adjusted mutual information";"CODE"
"test with a very large array";"IRRE"
"test for regression where contingency cell exceeds 2 16";"IRRE"
"leading to overflow in np outer resulting in emi 1";"IRRE"
"test overflow in mutual info classif and fowlkes mallows score";"IRRE"
"todo 1 10 remove";"TASK"
"check numerical stability when information is exactly zero";"CODE"
"check relation between v measure entropy and mutual information";"CODE"
"general case";"CODE"
"perfect match but where the label names changed";"META"
"worst case";"CODE"
"handcrafted example";"-"
"fmi tp sqrt tp fp tp fn";"-"
"symmetric property";"-"
"permutation property";"-"
"symmetric and permutation both together";"-"
"check that mi 0 when one or both labelling are constant";"CODE"
"non regression test for 16355";"IRRE"
"test warning message for continuous values";"IRRE"
"edge case every element is its own cluster";"CODE"
"edge case only one cluster";"CODE"
"regular case different non trivial clusterings";"IRRE"
"basic quadratic implementation";"TASK"
"edge case 1 every element is its own cluster";"CODE"
"edge case 2 only one cluster";"CODE"
"regular case different non trivial clusterings";"IRRE"
"pair confusion matrix";"-"
"d11 2 2 ordered pairs 1 3 5 6";"CODE"
"d10 2 4 ordered pairs 1 2 2 3 4 5 4 6";"CODE"
"d01 2 1 ordered pair 2 4";"CODE"
"d00 5 6 d11 d01 d10 the remaining pairs";"CODE"
"rand score";"IRRE"
"labels1 is constant the mutual info between labels1 and any other labelling is 0";"CODE"
"non constant non perfect matching labels";"CODE"
"todo 1 9 remove";"TASK"
"tests the silhouette coefficient";"IRRE"
"given that the actual labels are used we can assume that s would be positive";"-"
"assert silhouette coefficient 0 when there is 1 sample in a cluster";"CODE"
"cluster 0 we also test the case where there are identical samples";"IRRE"
"as the only members of a cluster cluster 2 to our knowledge this case";"CODE"
"is not discussed in reference material and we choose for it a sample";"CODE"
"score of 1";"-"
"cluster 0 1 sample score of 0 by rousseeuw s convention";"-"
"cluster 1 intra cluster 5 5 1";"CODE"
"inter cluster 1 1 1";"CODE"
"silhouette 5 5 0";"-"
"cluster 2 intra cluster 0 0";"CODE"
"inter cluster arbitrary arbitrary";"CODE"
"silhouette 1 1";"-"
"explicitly check per sample results against rousseeuw 1987";"IRRE"
"data from table 1";"CODE"
"data from figure 2";"CODE"
"data from figure 3";"CODE"
"we check to 2dp because that s what s in the paper";"IRRE"
"assert 1 n labels n samples";"CODE"
"n labels n samples";"-"
"n labels 1";"-"
"make sure silhouette samples requires diagonal to be zero";"CODE"
"non regression test for 12178";"IRRE"
"construct a zero diagonal matrix";"CODE"
"small values on the diagonal are ok";"IRRE"
"values bigger than eps 100 are not";"IRRE"
"non regression test for 22107";"IRRE"
"only check the number of features if 2d arrays are enforced otherwise";"CODE"
"validation is left to the user for custom metrics";"CODE"
"pairwise distances";"-"
"to minimize precision issues with float32 we compute the distance";"CODE"
"matrix on chunks of x and y upcast to float64";"CODE"
"if dtype is already float64 no need to chunk and upcast";"CODE"
"ensure that distances between vectors and themselves are set to 0 0";"IRRE"
"this may not be the case due to floating point rounding errors";"CODE"
"get missing mask for x";"CODE"
"get missing mask for y";"CODE"
"set missing values to zero";"IRRE"
"adjust distances for missing values";"IRRE"
"ensure that distances between vectors and themselves are set to 0 0";"IRRE"
"this may not be the case due to floating point rounding errors";"CODE"
"avoid divide by zero";"CODE"
"allow 10 more memory than x y and the distance matrix take at";"-"
"least 10mib";"-"
"the increase amount of memory in 8 byte blocks is";"-"
"x density batch size n features copy of chunk of x";"TASK"
"y density batch size n features copy of chunk of y";"TASK"
"batch size batch size chunk of distance matrix";"-"
"hence x xd yd kx m where x batch size k n features m maxmem";"TASK"
"xd x density and yd y density";"-"
"when x is y the distance matrix is symmetric so we only need";"-"
"to compute half of it";"-"
"start is specified in the signature but not used this is because the higher";"CODE"
"order pairwise distances chunked function needs reduction functions that are";"CODE"
"passed as argument to have a two arguments signature";"-"
"start is specified in the signature but not used this is because the higher";"CODE"
"order pairwise distances chunked function needs reduction functions that are";"CODE"
"passed as argument to have a two arguments signature";"-"
"if sp base version parse version 1 17 pragma no cover";"META"
"deprecated in scipy 1 15 and removed in scipy 1 17";"OUTD"
"if sp base version parse version 1 11 pragma no cover";"META"
"deprecated in scipy 1 9 and removed in scipy 1 11";"OUTD"
"deprecated in scipy 1 0 and removed in scipy 1 9";"OUTD"
"prefer skip nested validation false metric is not validated yet";"TASK"
"this is an adaptor for one sqeuclidean specification";"CODE"
"for this backend we can directly use sqeuclidean";"CODE"
"joblib based backend which is used when user defined callable";"CODE"
"are passed for metric";"CODE"
"this won t be used in the future once pairwisedistancesreductions support";"CODE"
"distancemetrics which work on supposedly binary data";"-"
"csr dense and dense csr case if euclidean in metric";"CODE"
"turn off check for finiteness because this is costly and because arrays";"IRRE"
"have already been validated";"CODE"
"prefer skip nested validation false metric is not validated yet";"TASK"
"this is an adaptor for one sqeuclidean specification";"CODE"
"for this backend we can directly use sqeuclidean";"CODE"
"joblib based backend which is used when user defined callable";"CODE"
"are passed for metric";"CODE"
"this won t be used in the future once pairwisedistancesreductions support";"CODE"
"distancemetrics which work on supposedly binary data";"-"
"csr dense and dense csr case if euclidean in metric";"CODE"
"turn off check for finiteness because this is costly and because arrays";"IRRE"
"have already been validated";"CODE"
"this returns a np ndarray generator whose arrays we need";"CODE"
"to flatten into one";"CODE"
"x sum duplicates this also sorts indices in place";"IRRE"
"array api support";"CODE"
"1 0 cosine similarity x y without copy";"-"
"ensure that distances between vectors and themselves are set to 0 0";"IRRE"
"this may not be the case due to floating point rounding errors";"CODE"
"paired distances";"-"
"check the matrix first it is usually done by the metric";"CODE"
"kernels";"-"
"compute tanh in place for numpy";"CODE"
"exponentiate k in place when using numpy";"-"
"np exp k k exponentiate k in place";"-"
"helper functions distance";"CODE"
"if updating this dictionary update the doc in both distance metrics";"CODE"
"and also in pairwise distances";"-"
"precomputed none hack precomputed is always allowed never called";"IRRE"
"no input dimension checking done for custom metrics left to user";"CODE"
"todo below 2 lines can be removed once min scipy 1 14 support for";"CODE"
"1d shapes in scipy sparse arrays coo dok and csr formats only";"CODE"
"added in 1 14 we must return 2d array until min scipy 1 14";"TASK"
"when metric is a callable 1d input arrays allowed in which case";"CODE"
"scalar should be returned";"IRRE"
"only calculate metric for upper triangle";"CODE"
"make symmetric";"-"
"nb out out t will produce incorrect results";"IRRE"
"calculate diagonal";"-"
"nb nonzero diagonals are allowed for both metrics and kernels";"CODE"
"calculate all cells";"-"
"prefer skip nested validation false metric is not validated yet";"TASK"
"we get as many rows as possible within our working memory budget to";"-"
"store len y distances in each row of output";"IRRE"
"note";"TASK"
"this will get at least 1 row even if 1 row of distances will";"CODE"
"exceed working memory";"-"
"this does not account for any temporary memory usage while";"CODE"
"calculating distances e g difference of vectors in manhattan";"-"
"distance";"-"
"precompute data derived metric params";"-"
"x chunk x enable optimised paths for x is y";"CODE"
"zeroing diagonal taking care of aliases of euclidean";"CODE"
"i e l2";"-"
"precompute data derived metric params";"-"
"these distances require boolean arrays when using scipy spatial distance";"CODE"
"deprecated in scipy 1 15 and removed in scipy 1 17";"OUTD"
"deprecated in scipy 1 9 and removed in scipy 1 11";"OUTD"
"deprecated in scipy 1 0 and removed in scipy 1 9";"OUTD"
"helper functions distance";"CODE"
"if updating this dictionary update the doc in both distance metrics";"CODE"
"and also in pairwise distances";"-"
"import gpkernel locally to prevent circular imports";"CODE"
"utilities for testing";"IRRE"
"import some data to play with";"CODE"
"restrict to a binary classification task";"TASK"
"add noisy features to make the problem harder and avoid perfect results";"TASK"
"run classifier get class probabilities and label predictions";"CODE"
"only interested in probabilities of the positive case";"CODE"
"xxx do we really want a special api for the binary case";"CODE"
"tests";"IRRE"
"test performance report with dictionary output";"IRRE"
"print classification report with class names";"CODE"
"assert the 2 dicts are equal";"CODE"
"assert the 2 dicts are equal";"CODE"
"we need always instead of once for free threaded with";"CODE"
"pytest run parallel to capture all the warnings in the";"CODE"
"zero division warn case";"CODE"
"else accuracy should be shown";"-"
"dense label indicator matrix format";"CODE"
"test precision recall and f1 score for binary classification task";"IRRE"
"detailed measures for each class";"CODE"
"individual scoring function that can be used for grid search in the";"CODE"
"binary class case the score is the value of the measure for the positive";"IRRE"
"class e g label 1 this is deprecated for average binary";"CODE"
"test precision recall and f scores behave with a single positive or";"IRRE"
"negative class";"IRRE"
"such a case may occur with non stratified cross validation";"CODE"
"test handling of explicit additional not in input labels to prf";"TASK"
"no average zeros in array";"-"
"macro average is changed";"CODE"
"no effect otherwise";"-"
"error when introducing invalid label in multilabel case";"CODE"
"although it would only affect performance if average macro none";"CODE"
"tests non regression on issue 10307";"IRRE"
"test a subset of labels may be requested for prf";"IRRE"
"ensure the above were meaningful tests";"IRRE"
"when fp 0 and tp 0 lr is undefined";"CODE"
"when fp 0 and tp 0 lr is undefined";"CODE"
"when tn 0 lr is undefined";"CODE"
"when tp fn 0 both ratios are undefined";"CODE"
"likelihood ratios must raise warnings when at";"CODE"
"least one of the ratios is ill defined";"CODE"
"likelihood ratios must raise error when attempting";"CODE"
"non binary classes to avoid simpson s paradox";"CODE"
"build confusion matrix with tn 9 fp 8 fn 1 tp 2";"-"
"sensitivity 2 3 specificity 9 17 prevalence 3 20";"-"
"lr 34 24 lr 17 27";"-"
"build limit case with y pred y true";"CODE"
"ignore last 5 samples to get tn 9 fp 3 fn 1 tp 2";"-"
"sensitivity 2 3 specificity 9 12 prevalence 3 20";"-"
"lr 24 9 lr 12 27";"-"
"todo 1 9 remove test";"TASK"
"this data causes fp 0 0 false positives in the confusion matrix and a division";"CODE"
"by zero that affects the positive likelihood ratio";"-"
"this data causes tn 0 0 true negatives in the confusion matrix and a division";"CODE"
"by zero that affects the negative likelihood ratio";"-"
"this data causes fp 0 0 false positives in the confusion matrix and a division";"CODE"
"by zero that affects the positive likelihood ratio";"-"
"this data causes tn 0 0 true negatives in the confusion matrix and a division";"CODE"
"by zero that affects the negative likelihood ratio";"-"
"these label vectors reproduce the contingency matrix from artstein and";"CODE"
"poesio 2008 table 1 np array 20 20 10 50";"-"
"add spurious labels and ignore them";"TASK"
"multiclass example artstein and poesio table 4";"IRRE"
"weighting example none linear quadratic";"-"
"this should not raise an error";"CODE"
"test for inconsistency between multiclass problem and pred decision";"IRRE"
"argument";"-"
"test for inconsistency between pred decision shape and labels number";"IRRE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 17630";"CODE"
"check that we can compute the hinge loss when providing an array";"-"
"with labels allowing to not have all labels in y true";"-"
"currently invariance of string and integer labels cannot be tested";"CODE"
"in common invariance tests because invariance tests for multiclass";"CODE"
"decision functions is not implemented yet";"TASK"
"binary case with symbolic labels no yes";"CODE"
"multiclass case adapted from http bit ly rjjhwa";"CODE"
"check that we got all the shapes and axes right";"-"
"by doubling the length of y true and y pred";"CODE"
"raise error if number of classes are not equal";"CODE"
"raise error if labels do not contain all values of y true";"IRRE"
"case when y true is a string array object";"CODE"
"test labels option";"IRRE"
"works when the labels argument is used";"-"
"ensure labels work when len np unique y true y pred shape 1";"-"
"because of the clipping the result is not exactly 0";"TASK"
"case when input is a pandas series and dataframe gh 5715";"CODE"
"y pred dataframe y true series";"-"
"check brier score loss function";"CODE"
"check that using n samples 2 y prob or y true gives the same score";"-"
"check scale by half argument";"-"
"calculate correctly when there s only one class in y true";"IRRE"
"test cases for multi class";"CODE"
"check perfect predictions for 3 classes";"CODE"
"check perfectly incorrect predictions for 3 classes";"CODE"
"binary case";"CODE"
"bad length of y prob";"-"
"y pred has value greater than 1";"IRRE"
"y pred has value less than 0";"IRRE"
"multiclass case";"CODE"
"bad length of y pred";"-"
"y pred has value greater than 1";"IRRE"
"y pred has value less than 0";"IRRE"
"raise an error for multiclass y true and binary y prob";"CODE"
"raise an error for wrong number of classes";"CODE"
"raise error message when there s only one class in y true";"CODE"
"error is fixed when labels is specified";"-"
"warnings are tested in test balanced accuracy score unseen";"IRRE"
"brier score loss requires probabilities";"CODE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"check that using sample weight also gives the correct d2 score";"-"
"check if good predictions give a relatively higher value for the d2 score";"IRRE"
"check that a similar value is obtained for string labels";"CODE"
"check if poor predictions gives a relatively low value for the d2 score";"IRRE"
"check that a similar value is obtained for string labels";"CODE"
"check if simply using the average of the classes as the predictions";"IRRE"
"gives a d2 score of 0";"-"
"check if simply using the average of the classes as the predictions";"IRRE"
"gives a d2 score of 0 when the positive class has a higher proportion";"IRRE"
"check that the d2 scores seem correct when more than 2";"-"
"labels are specified";"-"
"null model consists of weighted average of the classes";"IRRE"
"given that the sum of the weights is 3";"-"
"weighted average of 0s is 0 6 0 3 3 0 3";"-"
"weighted average of 1s is 0";"-"
"weighted average of 2s is 1 4 0 7 3 0 7";"-"
"check when labels are provided and some labels may not be present inside";"-"
"y true the d2 score is 0 when we use the label proportions based on";"IRRE"
"y true as the predictions";"-"
"also confirm that the order of the labels does not affect the d2 score";"CODE"
"check that a simple model with wrong predictions gives a negative d2 score";"META"
"binary case";"CODE"
"brier score loss and d2 brier score require specifying the";"CODE"
"pos label";"-"
"multi class case";"CODE"
"note toward developers about metric testing";"TASK"
"it is often possible to write one general test for several metrics";"CODE"
"invariance properties e g invariance to sample order";"CODE"
"common behavior for an argument e g the normalize with value true";"IRRE"
"will return the mean of the metrics and with value false will return";"IRRE"
"the sum of the metrics";"-"
"in order to improve the overall metric testing it is a good idea to write";"TASK"
"first a specific test for the given metric and then add a general test for";"IRRE"
"all metrics that have the same behavior";"-"
"two types of datastructures are used in order to implement this system";"CODE"
"dictionaries of metrics and lists of metrics with common properties";"-"
"dictionaries of metrics";"-"
"the goal of having those dictionaries is to have an easy way to call a";"IRRE"
"particular metric and associate a name to each function";"CODE"
"regression metrics all regression metrics";"-"
"classification metrics all classification metrics";"IRRE"
"which compare a ground truth and the estimated targets as returned by a";"IRRE"
"classifier";"IRRE"
"continuous classification metrics all classification metrics which";"IRRE"
"compare a ground truth and a continuous score e g estimated";"IRRE"
"probabilities or decision function format might vary";"CODE"
"those dictionaries will be used to test systematically some invariance";"IRRE"
"properties e g invariance toward several input layout";"CODE"
"confusion matrix returns absolute values and hence behaves unnormalized";"IRRE"
"naming it with an unnormalized prefix is necessary for this module to";"CODE"
"skip sample weight scaling checks which will fail for unnormalized";"CODE"
"metrics";"-"
"these are needed to test averaging";"IRRE"
"roc auc score roc auc score default average macro";"CODE"
"average precision score average precision score default average macro";"CODE"
"lists of metrics with common properties";"-"
"lists of metrics with common properties are used to test systematically some";"IRRE"
"functionalities and invariance e g symmetric metrics lists all metrics that";"CODE"
"are symmetric with respect to their input argument y true and y pred";"CODE"
"when you add a new metric or functionality check if a general test";"IRRE"
"is already written";"CODE"
"those metrics don t support binary inputs";"CODE"
"those metrics don t support multiclass inputs";"CODE"
"with default average binary multiclass is prohibited";"CODE"
"curves";"-"
"metric undefined with binary or multiclass input";"CODE"
"metrics with an average argument";"-"
"threshold based metrics with an average argument";"-"
"metrics with a pos label argument";"-"
"metrics with a labels argument";"-"
"todo handle multi class metrics that has a labels argument as well as a";"CODE"
"decision function argument e g hinge loss";"CODE"
"metrics with a normalize option";"-"
"threshold based metrics with multilabel indicator format support";"CODE"
"classification metrics with multilabel indicator format";"CODE"
"regression metrics with multioutput continuous format support";"IRRE"
"symmetric with respect to their input arguments y true and y pred";"CODE"
"metric y true y pred metric y pred y true";"-"
"p r f accuracy in multiclass case";"CODE"
"pinball loss is only symmetric for alpha 0 5 which is the default";"CODE"
"asymmetric with respect to their input arguments y true and y pred";"CODE"
"metric y true y pred metric y pred y true";"-"
"no sample weight support";"-"
"metrics involving y log 1 x";"-"
"we shouldn t forget any metrics";"CODE"
"test the symmetry of score and loss functions";"CODE"
"test the symmetry of score and loss functions";"CODE"
"the metric can be accidentally symmetric on a random draw";"IRRE"
"we run several random draws to check that at least of them";"IRRE"
"gives an asymmetric result";"IRRE"
"check test symmetric metric and test not symmetric metric";"IRRE"
"test symmetric metric passes on a symmetric metric";"IRRE"
"but fails on a not symmetric metric";"META"
"test not symmetric metric passes on a not symmetric metric";"IRRE"
"but fails on a symmetric metric";"META"
"generate some data";"-"
"some metrics e g log loss require y score to be probabilities sum to 1";"CODE"
"mix format support";"CODE"
"these mix representations aren t allowed";"-"
"nb we do not test for y1 row y2 row as these may be";"CODE"
"interpreted as multilabel or multioutput data";"IRRE"
"for consistency between the roc cuve and roc auc score";"CODE"
"np nan is returned and an undefinedmetricwarning is raised";"CODE"
"check invalid sample weight raises correct error";"OUTD"
"ensure that classification metrics with string labels are invariant";"CODE"
"ugly but handle case with a pos label and label";"META"
"ensure that continuous metrics with string labels are invariant under";"CODE"
"class relabeling";"IRRE"
"ugly but handle case with a pos label and label";"META"
"todo those metrics doesn t support string label yet";"CODE"
"reshape since coverage error only accepts 2d arrays";"-"
"add an additional case for classification only";"CODE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 6809";"CODE"
"non regression test scores should work with a single sample";"IRRE"
"this is important for leave one out cross validation";"CODE"
"score functions tested are those that formerly called np squeeze";"IRRE"
"which turns an array of size 1 into a 0 d array";"CODE"
"assert that no exception is thrown";"CODE"
"filter many metric specific warnings";"-"
"those metrics are not always defined with one sample";"CODE"
"or in multiclass classification";"IRRE"
"filter many metric specific warnings";"-"
"test invariance to dimension shuffling";"IRRE"
"generate some data";"-"
"to make sure at least one empty label is present";"-"
"xxx cruel hack to work with partial functions";"CODE"
"check representation invariance";"CODE"
"make sure the multilabel sequence format raises valueerror";"CODE"
"test in the binary case";"CODE"
"test in the multiclass case";"CODE"
"test in the multilabel case";"CODE"
"for both random state 0 and 1 y true and y pred has at least one";"IRRE"
"unlabelled entry";"CODE"
"to make sure at least one empty label is present";"-"
"no averaging";"-"
"micro measure";"-"
"macro measure";"CODE"
"weighted measure";"-"
"sample measure";"-"
"test average binary score for weight sum 0";"IRRE"
"top k accuracy score always lead to a perfect score for k 1 in the";"CODE"
"binary case";"CODE"
"check that unit weights gives the same score as no weight";"-"
"check that the weighted and unweighted scores are unequal";"-"
"use context manager to supply custom error message";"-"
"check that sample weight can be a list";"-"
"check that integer weights is the same as repeated samples";"CODE"
"check that ignoring a fraction of the samples is equivalent to setting";"IRRE"
"the corresponding weights to zero";"-"
"check that the score is invariant under scaling of the weights by a";"CODE"
"common factor";"-"
"due to numerical instability of floating points in cumulative sum in";"CODE"
"median absolute error it is not always equivalent when scaling by a float";"CODE"
"check that if number of samples in y true and sample weight are not";"-"
"equal meaningful error is raised";"CODE"
"regression";"-"
"xxx valueerror complex data not supported propagates via the warnings";"IRRE"
"machinery which is not thread safe at the time of cpython 3 13 at least";"CODE"
"check that sample weight with incorrect length raises error";"CODE"
"binary";"-"
"multiclass";"IRRE"
"softmax";"-"
"multilabel indicator";"-"
"some metrics e g log loss require y score to be probabilities sum to 1";"CODE"
"test labels argument when not using averaging";"IRRE"
"in multi class and multi label cases";"CODE"
"some metrics e g log loss require y score to be probabilities sum to 1";"CODE"
"makes sure all samples have at least one label this works around errors";"CODE"
"when running metrics where average sample";"CODE"
"here we are not comparing the values in case of mape because";"IRRE"
"whenever y true value is exactly zero the mape value doesn t";"IRRE"
"signify anything thus in this case we are just expecting";"CODE"
"very large finite value";"IRRE"
"check that an understable message is raised when the type between y true";"CODE"
"and y pred mismatch";"-"
"check that the error message if pos label is not specified and the";"-"
"targets is made of strings";"CODE"
"when array api dispatch is disabled and np asarray works for example pytorch";"CODE"
"with cpu device calling the metric function with such numpy compatible inputs";"CODE"
"should work albeit by implicitly converting to numpy arrays instead of";"CODE"
"dispatching to the array library";"CODE"
"pytorch with cuda device and cupy raise typeerror consistently";"IRRE"
"array api strict chose to raise runtimeerror instead numpy raises";"CODE"
"a valueerror if the array dunder does not return an array";"IRRE"
"exception type may need to be updated in the future for other libraries";"CODE"
"handle cases where multiple return values are not of the same shape";"IRRE"
"e g precision recall curve";"IRRE"
"handle cases where there are multiple return values e g roc curve";"IRRE"
"make boolean arrays ones and zeros";"CODE"
"x bool x64 0 3 astype np float64 quite sparse";"CODE"
"y bool y64 0 7 astype np float64 not too sparse";"CODE"
"computation of mahalanobis differs between";"-"
"the scipy and scikit learn implementation";"TASK"
"hence we increase the relative tolerance";"IRRE"
"todo inspect slight numerical discrepancy";"TASK"
"with scipy";"-"
"distancemetric pairwise must be consistent for all";"TASK"
"combinations of formats in sparse dense";"IRRE"
"some metrics can be deprecated depending on the scipy version";"TASK"
"but if they are present we still want to test whether";"TASK"
"scikit learn gives the same result whether or not they are";"IRRE"
"deprecated";"OUTD"
"distancemetric pairwise must be consistent";"TASK"
"on all combinations of format in sparse dense";"IRRE"
"computation of mahalanobis differs between";"-"
"the scipy and scikit learn implementation";"TASK"
"hence we increase the relative tolerance";"IRRE"
"todo inspect slight numerical discrepancy";"TASK"
"with scipy";"-"
"distancemetric must return similar distances for both float32 and float64";"CODE"
"input data";"CODE"
"choose rtol to make sure that this test is robust to changes in the random";"IRRE"
"seed in the module level test data generation code";"CODE"
"assert allclose introspects the dtype of the input arrays to decide";"CODE"
"which rtol value to use by default but in this case we know that d32";"CODE"
"is not computed with the same precision so we set rtol manually";"IRRE"
"some metrics can be deprecated depending on the scipy version";"TASK"
"but if they are present we still want to test whether";"TASK"
"scikit learn gives the same result whether or not they are";"IRRE"
"deprecated";"OUTD"
"the haversine distancemetric only works on 2 features";"TASK"
"haversine is not supported by scipy special distance cdist pdist";"-"
"so we reimplement it to have a reference";"TASK"
"check if both callable metric and predefined metric initialized";"IRRE"
"distancemetric object is picklable";"IRRE"
"regression test for 6288";"IRRE"
"previously a metric requiring a particular input dimension would fail";"CODE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 21685";"CODE"
"those distances metrics have to support readonly buffers";"CODE"
"we don t need the entire grid just one for a sanity check";"CODE"
"test the pairwise distance helper function";"IRRE"
"euclidean distance should be equivalent to calling the function";"CODE"
"euclidean distance with y x";"CODE"
"check to ensure nans work with pairwise distances";"-"
"test with tuples as x and y";"IRRE"
"test haversine distance";"IRRE"
"the data should be valid latitude and longitude";"-"
"haversine converts to float64 currently so we don t check dtypes";"CODE"
"test haversine distance with y x";"IRRE"
"cityblock uses scikit learn metric cityblock function is";"CODE"
"scipy spatial";"-"
"the metric functions from scipy converts to float64 so we don t check the dtypes";"CODE"
"the manhattan metric should be equivalent to cityblock";"-"
"test cosine as a string metric versus cosine callable";"IRRE"
"the string cosine uses sklearn metric";"CODE"
"while the function cosine is scipy spatial";"CODE"
"test array api support in pairwise distances";"IRRE"
"euclidean distance should be equivalent to calling the function";"CODE"
"test with y none";"IRRE"
"test with y y np y xp";"IRRE"
"test the pairwise distance helper function";"IRRE"
"test with sparse x and y";"IRRE"
"currently only supported for euclidean l1 and cosine";"CODE"
"todo fix manhattan distances to preserve dtype";"TASK"
"currently pairwise distances uses manhattan distances but converts the result";"IRRE"
"back to the input dtype";"CODE"
"todo fix manhattan distances to preserve dtype";"TASK"
"currently pairwise distances uses manhattan distances but converts the result";"IRRE"
"back to the input dtype";"CODE"
"test with scipy spatial distance metric with a kwd";"IRRE"
"same with y none";"-"
"test that scipy distance metrics throw an error if sparse matrix given";"IRRE"
"some scipy metrics are deprecated depending on the scipy version but we";"META"
"still want to test them";"TASK"
"test that we convert to boolean arrays for boolean distances";"CODE"
"ignore conversion to boolean in pairwise distances";"META"
"non boolean arrays are converted to boolean for boolean";"CODE"
"distance metrics with a data conversion warning";"META"
"check that the warning is raised if x is boolean by y is not boolean";"CODE"
"check that no warning is raised if x is already boolean and y is none";"CODE"
"no warnings issued if metric is not a boolean distance function";"CODE"
"test correct shape";"IRRE"
"with two args";"IRRE"
"even if shape 1 agrees although thus second arg is spurious";"-"
"test not copied if appropriate dtype";"IRRE"
"with two args";"IRRE"
"test always returns float dtype";"IRRE"
"test converts list to array like";"IRRE"
"test non negative values";"IRRE"
"callable version of pairwise rbf kernel";"IRRE"
"unpack the output since this is a scalar packed in a 0 dim array";"IRRE"
"note below is array api version of numpys item";"TASK"
"paired distances should allow callable metric where metric x x 0";"IRRE"
"knowing that the callable is a strict metric would allow the diagonal to";"IRRE"
"be left uncalculated and set to 0";"IRRE"
"test with all metrics that should be in pairwise kernel functions";"IRRE"
"test the pairwise kernels helper function";"IRRE"
"test with y none";"IRRE"
"test with y y";"IRRE"
"test with tuples as x and y";"IRRE"
"test with sparse x and y";"IRRE"
"these don t support sparse matrices yet";"TASK"
"test array api support in pairwise kernels";"IRRE"
"test with y none";"IRRE"
"test with y y np y xp";"IRRE"
"test the pairwise kernels helper function";"IRRE"
"with a callable function with given keywords";"IRRE"
"callable function x y";"IRRE"
"test the pairwise distance helper function";"IRRE"
"euclidean distance should be equivalent to calling the function";"CODE"
"euclidean distance with y x";"CODE"
"check the pairwise distances implementation";"TASK"
"gives the same value";"IRRE"
"test the paired distance helper function";"IRRE"
"with the callable implementation";"TASK"
"euclidean distance should be equivalent to calling the function";"CODE"
"euclidean distance with y x";"CODE"
"test that a value error is raised when the lengths of x and y should not";"IRRE"
"differ";"-"
"xxx thread safety bug tracked at";"CODE"
"https github com scikit learn scikit learn issues 31884";"CODE"
"check pairwise minimum distances computation for any metric";"CODE"
"euclidean metric";"CODE"
"sparse matrix case";"IRRE"
"we don t want np matrix here";"CODE"
"squared euclidean metric";"CODE"
"non euclidean scikit learn metric";"CODE"
"sparse matrix case";"IRRE"
"non euclidean scipy distance callable";"IRRE"
"non euclidean scipy distance string";"CODE"
"compare with naive implementation";"TASK"
"changing the axis and permuting datasets must give the same results";"IRRE"
"changing the axis and permuting datasets must give the same results";"IRRE"
"f contiguous arrays must be supported and must return identical results";"IRRE"
"reduced euclidean distance";"CODE"
"atol is for diagonal where s is explicitly zeroed on the diagonal";"CODE"
"check that the reduce func is allowed to return none";"IRRE"
"test the pairwise distance helper function";"IRRE"
"euclidean distance should be equivalent to calling the function";"CODE"
"test small amounts of memory";"IRRE"
"x as list";"-"
"euclidean distance with y x";"CODE"
"absurdly large working memory";"-"
"cityblock uses scikit learn metric cityblock function is";"CODE"
"scipy spatial";"-"
"test precomputed returns all at once";"IRRE"
"check the pairwise euclidean distances computation on known result";"IRRE"
"check that we still get the right answers with x y norm squared";"TASK"
"and that we get a wrong answer with wrong x y norm squared";"META"
"norms will only be used if their dtype is float64";"CODE"
"check we get the wrong answer with wrong x y norm squared";"META"
"non regression test for 27621";"IRRE"
"check all accepted shapes for the norms or appropriate error messages";"CODE"
"check that euclidean distances gives same result as scipy cdist";"IRRE"
"when x and y x are provided";"-"
"the default rtol 1e 7 is too close to the float32 precision";"CODE"
"and fails due to rounding errors";"-"
"check that euclidean distances gives same result as scipy pdist";"IRRE"
"when only x is provided";"-"
"the default rtol 1e 7 is too close to the float32 precision";"CODE"
"and fails due to rounding errors";"-"
"check batches handling when y x 13910";"-"
"the default rtol 1e 7 is too close to the float32 precision";"CODE"
"and fails due to rounding errors";"-"
"check batches handling when x is y 13910";"-"
"the default rtol 1e 7 is too close to the float32 precision";"CODE"
"and fails due to rounding errors";"-"
"check that euclidean distances is correct with float32 input thanks to";"CODE"
"upcasting on float64 there are still precision issues";"TASK"
"with no nan values";"IRRE"
"check for symmetry";"CODE"
"check with explicit formula and squared true";"CODE"
"check with explicit formula and squared false";"CODE"
"check when y x is explicitly passed";"-"
"check copy true against copy false";"-"
"first feature is the only feature that is non nan and in both";"TASK"
"samples the result of nan euclidean distances with squared true";"IRRE"
"should be non negative the non squared version should all be close to 0";"IRRE"
"check the pairwise cosine distances computation";"-"
"check that all elements are in 0 2";"-"
"check that diagonal elements are equal to 0";"-"
"check that all elements are in 0 2";"-"
"check that diagonal elements are equal to 0 and non diagonal to 2";"-"
"check large random matrix";"IRRE"
"check that diagonal elements are equal to 0";"-"
"check haversine distance with distances computation";"-"
"test haversine distance does not accept x where n feature 2";"TASK"
"paired distances";"-"
"check the paired euclidean distances computation";"CODE"
"check the paired manhattan distances computation";"-"
"check the paired manhattan distances computation";"-"
"check diagonal is ones for data with itself";"CODE"
"check off diagonal is 1 but 0";"META"
"check that float32 is preserved";"CODE"
"check integer type gets converted";"CODE"
"check that zeros are handled";"-"
"check that kernel of similar things is greater than dissimilar ones";"-"
"test negative input";"IRRE"
"different n features in x and y";"TASK"
"valid kernels should be symmetric";"-"
"the diagonal elements of a linear kernel are their squared norm";"-"
"the diagonal elements of a rbf kernel are 1";"-"
"the diagonal elements of a laplacian kernel are 1";"-"
"off diagonal elements are 1 but 0";"META"
"should be sparse";"IRRE"
"should be dense and equal to k1";"-"
"show the kernel output equal to the sparse toarray";"IRRE"
"test the cosine similarity";"IRRE"
"test that the cosine is kernel is equal to a linear kernel when data";"IRRE"
"has been previously normalized by l2 norm";"-"
"ensure that pairwise array check works for dense matrices";"CODE"
"check that if xb is none xb is returned as reference to xa";"CODE"
"ensure that if xa and xb are given correctly they return as equal";"IRRE"
"check that if xb is not none it is returned equal";"IRRE"
"note that the second dimension of xb is the same as xa";"TASK"
"ensure an error is raised if the dimensions are different";"CODE"
"ensure an error is raised on 1d input arrays";"CODE"
"the modified tests are not 1d in the old test the array was internally";"IRRE"
"converted to 2d anyways";"-"
"ensures that checks return valid sparse matrices";"IRRE"
"compare their difference because testing csr matrices for";"IRRE"
"equality with does not work as expected";"CODE"
"turns a numpy matrix any n dimensional array into tuples";"CODE"
"tuplify each sub array in the input";"CODE"
"single dimension input just return tuple of contents";"CODE"
"ensures that checks return valid tuples";"IRRE"
"ensures that type float32 is preserved";"CODE"
"both float32";"CODE"
"mismatched a";"-"
"mismatched b";"-"
"check that pairwise distances give the same result in sequential and";"IRRE"
"parallel when metric has data derived parameters";"IRRE"
"with config context working memory 0 1 to have more than 1 chunk";"-"
"check that pairwise distances raises an error when y is passed but";"META"
"metric has data derived params that are not provided by the user";"-"
"check that pairwise distances gives the same result as pdist and cdist";"IRRE"
"regardless of input datatype when using any scipy metric for comparing";"CODE"
"numeric vectors";"-"
"this test is necessary because pairwise distances used to throw an";"CODE"
"error when using metric seuclidean and the input data was not";"CODE"
"of type np float64 15730";"CODE"
"precompute parameters for seuclidean mahalanobis when x is not y";"CODE"
"dummy distance func using and thus relying on the input data being boolean";"CODE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"non regression test for https github com scikit learn scikit learn issues 7981";"CODE"
"joblib memory maps datasets which makes them read only";"CODE"
"the following call was reporting as failing in 7981 but this must pass";"IRRE"
"common supported metric between scipy spatial distance cdist";"-"
"and basedistancereductiondispatcher";"-"
"this allows constructing tests to check consistency of results";"CODE"
"of concrete basedistancereductiondispatcher on some metrics using apis";"CODE"
"from scipy and numpy";"CODE"
"chunk size 256 default";"CODE"
"adjusting the radius to ensure that the expected results is neither";"IRRE"
"trivially empty nor too large";"IRRE"
"create read only datasets";"IRRE"
"scaling the radius slightly with the numbers of dimensions";"-"
"utilities for testing";"IRRE"
"import some data to play with";"CODE"
"restrict to a binary classification task";"TASK"
"add noisy features to make the problem harder and avoid perfect results";"TASK"
"run classifier get class probabilities and label predictions";"CODE"
"only interested in probabilities of the positive case";"CODE"
"xxx do we really want a special api for the binary case";"CODE"
"tests";"IRRE"
"count the number of times positive samples are correctly ranked above";"-"
"negative samples";"-"
"compute precision up to document i";"TASK"
"i e percentage of relevant documents up to document i";"CODE"
"formula 5 from mcclish 1989";"CODE"
"test dropping thresholds with repeating scores";"IRRE"
"test all false keeps only endpoints";"CODE"
"test all true keeps all thresholds";"IRRE"
"check the average precision score of a constant predictor is";"CODE"
"the tpr";"-"
"generate a dataset with 25 of positives";"IRRE"
"and a constant score";"CODE"
"the precision is then the fraction of positive whatever the recall";"IRRE"
"is as there is only one threshold";"-"
"raise an error when pos label is not in binary y true";"CODE"
"raise an error for multilabel indicator y true with";"CODE"
"pos label other than 1";"-"
"raise an error for multiclass y true with pos label other than 1";"CODE"
"test that average precision score and roc auc score are invariant by";"IRRE"
"the scaling or shifting of probabilities";"-"
"this test was expanded added scaled down in response to github";"CODE"
"issue 3864 and others where overly aggressive rounding was causing";"CODE"
"problems for users with very small y score values";"IRRE"
"check on a batch of small examples";"-"
"drop when true positives do not change from the previous or subsequent point";"CODE"
"do nothing otherwise";"CODE"
"check on a batch of small examples";"-"
"exactly duplicated inputs yield the same result";"IRRE"
"input variables with inconsistent numbers of samples";"CODE"
"check that the first threshold will change depending which label we";"TASK"
"consider positive";"-"
"check for the symmetry of the fpr and fnr";"CODE"
"check on several small example that it works";"CODE"
"tie handling";"-"
"no relevant labels";"-"
"only relevant labels";"-"
"degenerate case only one label";"CODE"
"raise value error if not appropriate format";"CODE"
"check that y true shape y score shape raise the proper exception";"CODE"
"check tie handling in score";"-"
"basic check with only ties and increasing label space";"-"
"check for growing number of consecutive relevant";"CODE"
"check for a bunch of positions";"CODE"
"check that label ranking average precision works for various";"CODE"
"basic check with increasing label space size and decreasing score";"-"
"first and last";"-"
"check for growing number of consecutive relevant label";"CODE"
"check for a bunch of position";"CODE"
"check roc auc score for max fpr none";"CODE"
"tweedie deviance needs positive y pred except for p 0";"CODE"
"p 2 needs positive y true";"TASK"
"results evaluated by sympy";"IRRE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn pull 16323";"CODE"
"mean absolute error and mean squared error are equal because";"IRRE"
"it is a binary problem";"-"
"in the last case the denominator vanishes and hence we get nan";"CODE"
"but since the numerator vanishes as well the expected score is 1 0";"META"
"constant y true with force finite true leads to 1 or 0";"CODE"
"setting force finite false results in the nan for 4th output propagating";"IRRE"
"dropping the 4th output to check force finite false for nominal";"IRRE"
"constant y true with force finite false leads to nan or inf";"CODE"
"single sample case";"CODE"
"note for r2 and d2 tweedie see also test regression single sample";"IRRE"
"perfect cases";"CODE"
"non finite cases";"IRRE"
"r and explained variance have a fix by default for non finite cases";"CODE"
"tweedie deviance error";"-"
"all of length 3";"-"
"mean absolute error and mean squared error are equal because";"IRRE"
"it is a binary problem";"-"
"checking for the condition in which both numerator and denominator is";"IRRE"
"zero";"-"
"handling msle separately as it does not accept negative inputs";"CODE"
"handling msle separately as it does not accept negative inputs";"CODE"
"trigger the warning";"-"
"ws we get closer to the limit with 1e 12 difference the";"CODE"
"tolerance to pass the below check increases there are likely";"-"
"numerical precision issues on the edges of different definition";"IRRE"
"regions";"-"
"check that the pinball loss is minimized by the empirical quantile";"-"
"compute the best possible pinball loss for any constant predictor";"CODE"
"evaluate the loss on a grid of quantiles";"CODE"
"compute the pinball loss of a constant predictor";"CODE"
"check that the loss of this constant predictor is greater or equal";"CODE"
"than the loss of using the optimal quantile up to machine";"-"
"precision";"-"
"check that the value of the pinball loss matches the analytical";"IRRE"
"formula";"CODE"
"check that we can actually recover the target quantile by minimizing the";"META"
"pinball loss w r t the constant prediction quantile";"CODE"
"the minimum is not unique with limited data hence the large tolerance";"-"
"for the normal distribution and the 0 5 quantile the expected result is close to";"CODE"
"0 hence the additional use of absolute tolerance";"TASK"
"integration test to check that it is possible to use the pinball loss to";"IRRE"
"tune the hyperparameter of a quantile regressor this is conceptually";"IRRE"
"similar to the previous test but using the scikit learn estimator and";"IRRE"
"scoring api instead";"CODE"
"x rng normal size n samples 5 ignored";"-"
"test that mean pinball loss with alpha 0 5 if half of mean absolute error";"IRRE"
"all supervised cluster scorers they behave like classification metric";"CODE"
"make sure they expose the routing methods";"IRRE"
"make sure they expose the routing methods";"IRRE"
"check that by default no metadata is requested";"CODE"
"set score request should mutate the instance rather than returning a";"IRRE"
"new instance";"CODE"
"make sure the scorer doesn t request anything on methods other than";"CODE"
"score and that the requested value on score is correct";"IRRE"
"make sure putting the scorer in a router doesn t request anything by";"CODE"
"default";"CODE"
"make sure sample weight is refused if passed";"-"
"make sure sample weight is not routed even if passed";"-"
"make sure putting weighted scorer in a router requests sample weight";"CODE"
"raising scorer is raising valueerror and should return an string representation";"IRRE"
"of the error of the last scorer";"-"
"should raise an error";"CODE"
"since pos label is forwarded to the curve scorer the thresholds are not equal";"TASK"
"the min max range for the thresholds is defined by the probabilities of the";"CODE"
"pos label class the column of predict proba";"IRRE"
"the recall cannot be negative and pos label 1 should have a higher recall";"IRRE"
"since there is less samples to be considered";"-"
"spherical case";"CODE"
"for dirichlet process weight concentration will be a tuple";"CODE"
"containing the two parameters of the beta distribution";"IRRE"
"case variational gaussian mixture with dirichlet distribution";"CODE"
"warning in some bishop book there is a typo on the formula 10 63";"CODE"
"degrees of freedom k degrees of freedom 0 nk is";"CODE"
"the correct formula";"CODE"
"contrary to the original bishop book we normalize the covariances";"CODE"
"warning in some bishop book there is a typo on the formula 10 63";"CODE"
"degrees of freedom k degrees of freedom 0 nk";"CODE"
"is the correct formula";"CODE"
"contrary to the original bishop book we normalize the covariances";"CODE"
"warning in some bishop book there is a typo on the formula 10 63";"CODE"
"degrees of freedom k degrees of freedom 0 nk";"CODE"
"is the correct formula";"CODE"
"contrary to the original bishop book we normalize the covariances";"CODE"
"warning in some bishop book there is a typo on the formula 10 63";"CODE"
"degrees of freedom k degrees of freedom 0 nk";"CODE"
"is the correct formula";"CODE"
"contrary to the original bishop book we normalize the covariances";"CODE"
"case variational gaussian mixture with dirichlet distribution";"CODE"
"we remove n features np log self degrees of freedom because";"CODE"
"the precision matrix is normalized";"-"
"contrary to the original formula we have done some simplification";"CODE"
"and removed all the constant terms";"OUTD"
"we removed 5 n features np log self degrees of freedom";"CODE"
"because the precision matrix is normalized";"IRRE"
"weights computation";"-"
"precisions matrices computation";"-"
"gaussian mixture parameters estimators used by the m step";"IRRE"
"catch only numpy exceptions b c exceptions aren t part of array api spec";"CODE"
"catch only numpy exceptions since exceptions are not part of array api spec";"CODE"
"refer to 26415 for details";"CODE"
"if all the initial parameters are all provided then there is no need to run";"IRRE"
"the initialization";"IRRE"
"attributes computation";"META"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"check correct init for a given value of weight concentration prior";"IRRE"
"check correct init for the default value of weight concentration prior";"IRRE"
"check correct init for a given value of mean precision prior";"IRRE"
"check correct init for the default value of mean precision prior";"IRRE"
"check correct init for a given value of mean prior";"IRRE"
"check correct init for the default value of bemean priorta";"IRRE"
"check raise message for a bad value of degrees of freedom prior";"CODE"
"check correct init for a given value of degrees of freedom prior";"IRRE"
"check correct init for the default value of degrees of freedom prior";"CODE"
"check correct init for a given value of covariance prior";"IRRE"
"check correct init for the default value of covariance prior";"CODE"
"check raise message";"CODE"
"case dirichlet distribution for the weight concentration prior type";"CODE"
"case dirichlet process for the weight concentration prior type";"CODE"
"we check that each step of the each step of variational inference without";"CODE"
"regularization improve monotonically the training set of the bound";"IRRE"
"do one training iteration at a time so we can make sure that the";"CODE"
"training log likelihood increases after each iteration";"-"
"we can compare the full precision with the other cov type if we apply";"IRRE"
"1 iter of the m step done during initialize parameters";"IRRE"
"computation of the full covariance";"CODE"
"check tied covariance mean full covariances 0";"CODE"
"check diag covariance diag full covariances";"CODE"
"check spherical covariance np mean diag covariances 0";"CODE"
"we check that the dot product of the covariance and the precision";"CODE"
"matrices is identity";"-"
"computation of the full covariance";"CODE"
"we check here that adding a constant in the data change correctly the";"CODE"
"parameters of the mixture";"IRRE"
"0 2 1e 7 strict non convergence";"-"
"1 2 1e 1 loose non convergence";"-"
"3 300 1e 7 strict convergence";"-"
"4 300 1e 1 loose convergence";"-"
"check that fit predict is equivalent to fit predict when n init 1";"IRRE"
"this is the same test as test gaussian mixture predict predict proba";"IRRE"
"check a warning message arrive if we don t do fit";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"test bad parameters";"IRRE"
"test good parameters";"IRRE"
"check bad shape";"-"
"check bad range";"-"
"check bad normalization";"-"
"check good weights matrix";"-"
"check means bad shape";"-"
"check good means matrix";"-"
"define the bad precisions for each covariance type";"CODE"
"define not positive definite precisions";"CODE"
"check precisions with bad shapes";"-"
"check not positive precisions";"-"
"check the correct init of precisions init";"IRRE"
"compare the precision matrix compute from the";"IRRE"
"empiricalcovariance covariance fitted on x sqrt resp";"CODE"
"with sufficient sk full n components 1";"-"
"special case 1 assuming data is centered";"CODE"
"check the precision computation";"-"
"special case 2 assuming resp are all ones";"CODE"
"check the precision computation";"-"
"use equation nk sk n s tied";"-"
"check the precision computation";"-"
"test against full case";"IRRE"
"check the precision computation";"-"
"computing spherical covariance equals to the variance of one dimension";"CODE"
"data after flattening n components 1";"-"
"check the precision computation";"-"
"we compute the cholesky decomposition of the covariance matrix";"CODE"
"test against with naive lmvnpdf diag";"IRRE"
"full covariances";"CODE"
"diag covariances";"CODE"
"tied";"-"
"spherical";"-"
"skip tests on weighted log probabilities log weights";"IRRE"
"test whether responsibilities are normalized";"IRRE"
"check a warning message arrive if we don t do fit";"CODE"
"0 2 1e 7 strict non convergence";"-"
"1 2 1e 1 loose non convergence";"-"
"3 300 1e 7 strict convergence";"-"
"4 300 1e 1 loose convergence";"-"
"check if fit predict x is equivalent to fit x predict x";"IRRE"
"check that fit predict is equivalent to fit predict when n init 1";"IRRE"
"recover the ground truth";"-"
"needs more data to pass the test with rtol 1e 7";"TASK"
"the accuracy depends on the number of data and randomness rng";"IRRE"
"test that multiple inits does not much worse than a single one";"IRRE"
"test that the right number of parameters is estimated";"IRRE"
"test all of the covariance types return the same bic score for";"CODE"
"1 dimensional 1 component fits";"-"
"test the aic and bic criteria";"IRRE"
"standard gaussian entropy";"-"
"assert the warm start give the same result for the same number of iter";"CODE"
"assert that by using warm start we can converge to a good solution";"CODE"
"depending on the data there is large variability in the number of";"CODE"
"refit necessary to converge due to the complete randomness of the";"IRRE"
"data";"-"
"we check that convergence is detected when warm start true";"-"
"check the error message if we don t call fit";"IRRE"
"check score value";"IRRE"
"check if the score increase";"IRRE"
"check the error message if we don t call fit";"IRRE"
"we check that each step of the em without regularization improve";"-"
"monotonically the training set likelihood";"IRRE"
"do one training iteration at a time so we can make sure that the";"CODE"
"training log likelihood increases after each iteration";"-"
"we train the gaussianmixture on degenerate data by defining two clusters";"CODE"
"of a 0 covariance";"CODE"
"to sample we need that gaussianmixture is fitted";"-"
"just to make sure the class samples correctly";"IRRE"
"check shapes of sampled data see";"-"
"https github com scikit learn scikit learn issues 7701";"CODE"
"we check that by increasing the n init number we have a better solution";"IRRE"
"following initialization parameters were found to lead to divergence";"IRRE"
"ensure that no error is thrown during fit";"CODE"
"check that the fit did not converge";"-"
"check that parameters are set for gmm";"IRRE"
"check that all initialisations provide not duplicated starting means";"IRRE"
"check fitted means properties for all initializations";"IRRE"
"check that max iter 0 returns initialisation as expected";"IRRE"
"pick arbitrary initial means and check equal to max iter 0";"IRRE"
"generate a toy dataset";"IRRE"
"common parameters to check the consistency of precision initialization";"IRRE"
"execute the manual initialization to compute the precision matrix";"IRRE"
"run kmeans to have an initial guess";"IRRE"
"estimate the covariance";"CODE"
"compute the precision matrix from the estimated covariance";"CODE"
"the initial gaussian parameters are not estimated they are estimated for every";"IRRE"
"m step";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"check that n iter is the number of iteration performed";"CODE"
"estimator clone estimator avoid side effects from shared instances";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"else prefit estimator only a validation set is provided";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"else score type both";"-"
"plot the mean score";"-"
"elf fill between none overwritten below by fill between";"-"
"we found that a ratio smaller or bigger than 5 between the largest and";"-"
"smallest gap of the x values is a good indicator to choose between linear";"IRRE"
"and log scale";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"wrap dictionary in a singleton list to support either dict";"CODE"
"or list of dicts";"-"
"check if all entries are dictionaries of lists";"IRRE"
"always sort the keys of a dictionary for reproducibility";"CODE"
"raise an attributeerror if attr does not exist";"CODE"
"raise an attributeerror if attr does not exist";"CODE"
"this can happen when param list contains lists of different";"CODE"
"lengths for example";"CODE"
"param list 1 2 3";"-"
"there are two cases when we don t use the automatically inferred";"IRRE"
"dtype when creating the array and we use object instead";"IRRE"
"string dtype";"CODE"
"when array ndim 1 that means that param list was something";"-"
"like a list of same size sequences which gets turned into a";"CODE"
"multi dimensional array but we want a 1d array";"META"
"use one maskedarray and mask all the places where the param is not";"-"
"applicable for that candidate which may not contain all the params";"CODE"
"setting the value at an index unmasks that index";"IRRE"
"when iterated first by splits then by parameters";"IRRE"
"we want array to have n candidates rows and n splits cols";"-"
"uses closure to alter the results";"IRRE"
"weighted std is not directly available in numpy";"CODE"
"when the fit scoring fails array means contains nans we";"-"
"will exclude them from the ranking process and consider them";"CODE"
"as tied with the worst performers";"CODE"
"all fit scoring routines failed";"-"
"store a list of param dicts at the key params";"-"
"computed the weighted mean and std for test scores alone";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"explicitly require this experimental feature";"CODE"
"from sklearn experimental import enable halving search cv noqa";"CODE"
"now you can import normally from model selection";"CODE"
"from sklearn experimental import enable halving search cv noqa";"CODE"
"search best params doctest skip";"IRRE"
"explicitly require this experimental feature";"CODE"
"from sklearn experimental import enable halving search cv noqa";"CODE"
"now you can import normally from model selection";"CODE"
"from sklearn experimental import enable halving search cv noqa";"CODE"
"search best params doctest skip";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"if not shuffle and random state is not none none is the default";"IRRE"
"split and shuffle unique groups across n splits";"-"
"weight groups by their number of occurrences";"-"
"distribute the most frequent groups first";"META"
"total weight of each fold";"-"
"mapping from group index to fold index";"CODE"
"distribute samples by adding the largest weight to the lightest fold";"TASK"
"xxx as of now cross validation splitters only operate in numpy land";"-"
"without attempting to leverage array api namespace features however";"TASK"
"they might be fed by array api inputs e g in cv enabled estimators so";"CODE"
"we need the following explicit conversion";"META"
"y inv encodes y according to lexicographic order we invert y idx to";"-"
"map the classes so that they are encoded by order of appearance";"IRRE"
"0 represents the first label appearing in y 1 the second etc";"-"
"determine the optimal number of samples from each class in each fold";"CODE"
"using round robin over the sorted y this can be done direct from";"CODE"
"counts but that code is unreadable";"META"
"to maintain the data order dependencies as best as possible within";"CODE"
"the stratification constraint we assign samples from each class in";"CODE"
"blocks and then mess that up when shuffle true";"CODE"
"since the kth column of allocation stores the number of samples";"-"
"of class k in each test set this generates blocks of fold";"IRRE"
"indices corresponding to the allocation for class k";"CODE"
"implementation is based on this kaggle kernel";"TASK"
"https www kaggle com jakubwasikowski stratified group k fold cross validation";"CODE"
"and is a subject to apache 2 0 license you may obtain a copy of the";"-"
"license at http www apache org licenses license 2 0";"CODE"
"changelist";"-"
"refactored function to a class following scikit learn kfold";"CODE"
"interface";"CODE"
"added heuristic for assigning group to the least populated fold in";"TASK"
"cases when all other criteria are equal";"CODE"
"swtch from using python counter to np unique to get class";"CODE"
"distribution";"META"
"added scikit learn checks for input checking that target is binary";"CODE"
"or multiclass checking passed random state checking that number";"IRRE"
"of splits is less than number of members in each class checking";"IRRE"
"that least populated class has more members than there are splits";"IRRE"
"stable sort to keep shuffled order for groups with the same";"CODE"
"class distribution variance";"CODE"
"summarise the distribution over classes in each proposed fold";"IRRE"
"make sure we have enough samples for the given split parameters";"IRRE"
"we make a copy of groups to avoid side effects during iteration";"CODE"
"this indicates that by default cv splitters don t have a groups kwarg";"CODE"
"unless indicated by inheriting from groupsconsumermixin";"CODE"
"this also prevents set split request to be generated for splitters";"CODE"
"which don t support groups";"CODE"
"this indicates that by default cv splitters don t have a groups kwarg";"CODE"
"unless indicated by inheriting from groupsconsumermixin";"CODE"
"this also prevents set split request to be generated for splitters";"CODE"
"which don t support groups";"CODE"
"specify train and test size";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"todo slep6 to be removed when set config enable metadata routing false is not";"TASK"
"possible";"-"
"prefer skip nested validation false estimator is not validated yet";"TASK"
"for estimators a metadatarouter is created in get metadata routing";"IRRE"
"methods for these router methods we create the router to use";"IRRE"
"process routing on it";"-"
"todo slep6 also pass metadata to the predict method for";"CODE"
"scoring";"-"
"the default exception would mention fit since in the above";"CODE"
"process routing code we pass fit as the caller however";"IRRE"
"the user is not calling fit directly so we change the message";"IRRE"
"to make it more suitable for this case";"CODE"
"materialize the indices since we need to store them in the returned dict";"CODE"
"we clone the estimator to make sure that all the folds are";"-"
"independent and that it is pickle able";"CODE"
"for callable scoring the return type is only know after calling if the";"IRRE"
"return type is a dictionary the error scores can now be inserted with";"CODE"
"the correct key";"-"
"adjust length of sample weights";"-"
"prefer skip nested validation false estimator is not validated yet";"TASK"
"the default exception would mention fit since in the above";"CODE"
"process routing code we pass fit as the caller however";"IRRE"
"the user is not calling fit directly so we change the message";"IRRE"
"to make it more suitable for this case";"CODE"
"note do not change order of iteration to allow one time cv splitters";"TASK"
"x y x 100 y 100 only 2 classes";"IRRE"
"create a dataset and repeat twice the sample of class 0";"IRRE"
"create a sample weight vector that is equivalent to the repeated dataset";"IRRE"
"remove feature to degrade performances";"TASK"
"make the problem completely imbalanced such that the balanced accuracy is low";"CODE"
"encode numeric targets by meaningful strings we purposely designed the class";"CODE"
"names such that the pos label is the first alphabetically sorted class and thus";"IRRE"
"encoded as 0";"-"
"scale the data to avoid any convergence issue";"CODE"
"only use 2 classes and select samples such that 2 fold cross validation";"CODE"
"split will lead to an equivalence with a sample weight of 0";"-"
"case where refit false and cv is a float the underlying estimator will be fit";"CODE"
"on the training set given by a shufflesplit we check that we get the same model";"IRRE"
"coefficients";"-"
"case where refit true then the underlying estimator is fitted on the full";"CODE"
"dataset";"IRRE"
"emulate the response method that should take into account the pos label";"CODE"
"else response method decision function";"CODE"
"check that the underlying estimator is the same";"-"
"emulate the response method that should take into account the pos label";"CODE"
"else response method decision function";"CODE"
"create a mapping from boolean values to class labels";"IRRE"
"this should not raise an error";"CODE"
"1 0 0 0 0 0 1 0 trust me it s red";"-"
"regression test for bug in refitting";"IRRE"
"simulates re fitting a broken estimator this used to break with";"CODE"
"sparse svms";"IRRE"
"fit a dummy clf with refit true to get a list of keys in";"-"
"clf cv results";"IRRE"
"ensure that best index 0 for this dummy clf";"CODE"
"assert every key matches those in cv results";"IRRE"
"ensure best score is disabled when using refit callable";"IRRE"
"ensure best score is disabled when using refit callable";"IRRE"
"pass x as list in gridsearchcv";"-"
"pass x as list in gridsearchcv";"-"
"pass y as list in gridsearchcv";"-"
"check cross val score doesn t destroy pandas dataframe";"CODE"
"x dataframe y series";"-"
"test grid search with unsupervised estimator";"IRRE"
"multi metric evaluation unsupervised";"CODE"
"both ari and fms can find the right number";"CODE"
"single metric evaluation unsupervised";"CODE"
"now without a score and without y";"-"
"test grid search with an estimator without predict";"IRRE"
"slight duplication of a test from kde";"IRRE"
"test basic properties of param sampler";"IRRE"
"test that repeated calls yield identical parameters";"IRRE"
"check if the search cv results s array are of correct types";"IRRE"
"test the search cv results contains all the required results";"IRRE"
"check if score and timing are reasonable";"IRRE"
"check cv results structure";"IRRE"
"check masking";"-"
"check results structure";"IRRE"
"test the iid parameter todo clearly this test does something else";"CODE"
"noise free simple 2d data";"-"
"split dataset into two folds that are not iid";"IRRE"
"first one contains data of all 4 blobs second only from two";"CODE"
"this leads to perfect classification on one fold and a score of 1 3 on";"CODE"
"the other";"-"
"create cv for splits";"IRRE"
"scores are the same as above";"-"
"unweighted mean std is used";"CODE"
"for the train scores we do not take a weighted mean irrespective of";"CODE"
"i i d or not";"CODE"
"if true for multi metric pass refit accuracy";"CODE"
"check if score and timing are reasonable also checks if the keys";"IRRE"
"are present";"-"
"compare the keys other than time keys among multi metric and";"IRRE"
"single metric grid search results np testing assert equal performs a";"IRRE"
"deep nested comparison of the two cv results dicts";"IRRE"
"gridsearchcv with on error raise";"CODE"
"ensures that a warning is raised and score reset where appropriate";"IRRE"
"refit false because we only want to check that errors caused by fits";"-"
"to individual folds will be caught and warnings raised instead if";"CODE"
"refit was done then an exception would be raised on refit and not";"CODE"
"caught by grid search expected behavior and this would cause an";"IRRE"
"error in this test";"CODE"
"ensure that grid scores were set to zero as required for those fits";"CODE"
"that are expected to fail";"-"
"check that succeeded estimators have lower ranks";"-"
"check that failed estimator has the highest rank";"-"
"gridsearchcv with on error raise raises the error";"CODE"
"refit false because we want to test the behaviour of the grid search part";"IRRE"
"failingclassifier issues a valueerror so this is what we look for";"CODE"
"raise warning if n iter is bigger than total parameter space";"IRRE"
"degenerates to gridsearchcv if n iter the same as grid size";"IRRE"
"test sampling without replacement in a large grid";"IRRE"
"doesn t go into infinite loops";"IRRE"
"make sure the predict proba works when loss is specified";"-"
"as one of the parameters in the param grid";"IRRE"
"when the estimator is not fitted predict proba is not available as the";"-"
"loss is hinge";"-"
"make sure predict proba is not available when setting loss hinge";"IRRE"
"in param grid";"-"
"check if a one time iterable is accepted as a cv parameter";"IRRE"
"give generator as a cv parameter";"IRRE"
"check if generators are supported as cv and";"IRRE"
"that the splits are consistent";"-"
"onetimesplitter is a non re entrant cv where split can be called only";"IRRE"
"once if cv split is called once per param setting in gridsearchcv fit";"IRRE"
"the 2nd and 3rd parameter will not be evaluated as no train test indices";"IRRE"
"will be generated for the 2nd and subsequent cv split calls";"CODE"
"this is a check to make sure cv split is not called once per param";"CODE"
"setting";"IRRE"
"check consistency of folds across the parameters";"IRRE"
"as the first two param settings c 0 1 and the next two param";"IRRE"
"settings c 0 2 are same the test and train scores must also be";"IRRE"
"same as long as the same train test indices are generated for all";"CODE"
"the cv splits for both param setting";"IRRE"
"xxx results params is a list";"IRRE"
"using regressor to make sure each score differs";"-"
"this should not raise any exceptions";"CODE"
"this should raise a notimplementederror";"CODE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"use global x y";"-"
"create cv";"IRRE"
"pop all of it this should cause the expected valueerror";"IRRE"
"cv is empty now";"-"
"assert that this raises an error";"CODE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"use global x y";"-"
"create bad cv";"IRRE"
"assert that this raises an error";"CODE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 10529";"CODE"
"check that we raise a userwarning when a non finite score is";"IRRE"
"computed in the searchcv";"CODE"
"non regression test for issue 13920";"IRRE"
"non regression test for issue 13920";"IRRE"
"non regression test for issue 13920";"IRRE"
"unfitted shows the original pipeline";"CODE"
"fitted with refit false shows the original pipeline";"CODE"
"fitted with refit true shows the best estimator";"IRRE"
"metadata routing tests";"IRRE"
"values of param grid are such that np result type gives slightly";"IRRE"
"different errors in particular valueerror and typeerror";"IRRE"
"non regression test for 29277";"IRRE"
"from sklearn experimental import enable halving search cv noqa f401";"CODE"
"update the constraints such that we accept all parameters from a to z";"CODE"
"estimators that failed during fit predict should always rank lower";"-"
"than ones where the fit predict succeeded";"-"
"some scores should be nan";"-"
"all nan scores should have the same rank";"-"
"nans should have the lowest rank";"-"
"notice how it loops at the beginning";"IRRE"
"also the number of candidates evaluated at the last iteration is";"-"
"factor";"-"
"no aggressive elimination we end up with less iterations and";"CODE"
"the number of candidates at the last iter is factor which isn t";"-"
"ideal";"-"
"when the amount of resource isn t limited aggressive elimination";"-"
"has no effect here the default min resources exhaust will take";"CODE"
"over";"-"
"test the aggressive elimination parameter";"IRRE"
"h set params verbose true just for test coverage";"IRRE"
"same number of candidates as with the grid";"-"
"with enough resources";"-"
"with enough resources but min resources set manually";"IRRE"
"without enough resources only one iteration can be done";"CODE"
"with exhaust use as much resources as possible at the last iter";"-"
"test the min resources and max resources parameters and how they affect";"IRRE"
"the number of resources used at each iteration";"-"
"h set params n candidates 6 same number as with the grid";"IRRE"
"expected n required iterations 2 given 6 combinations and factor 3";"CODE"
"auto 5 9 all resources are used";"-"
"4 1 1 max resources min resources only one iteration is";"-"
"possible";"-"
"test the number of actual iterations that were run depending on";"CODE"
"max resources";"-"
"h set params n candidates 20 same as for halvinggridsearchcv";"IRRE"
"test the resource parameter";"IRRE"
"512 exhaust 128 generate exactly as much as needed";"-"
"32 7 7 ask for less than what we could";"CODE"
"32 9 9 ask for more than reasonable";"CODE"
"test random search and make sure the number of generated candidates is";"IRRE"
"as expected";"-"
"make sure exhaust makes the last iteration use as much resources as";"-"
"we can";"-"
"a 1 2 2 all lists sample less than n candidates";"-"
"a randint 1 3 10 not all list respect n candidates";"IRRE"
"make sure random search samples the appropriate number of candidates when";"IRRE"
"we ask for more than what s possible how many parameters are sampled";"IRRE"
"depends whether the distributions are all lists or not see";"META"
"parametersampler for details this is somewhat redundant with the checks";"IRRE"
"in parametersampler but interaction bugs were discovered during";"IRRE"
"development of sh";"-"
"tests specific to halvingrandomsearchcv";"IRRE"
"make sure splits returned by subsamplemetasplitter are of appropriate";"IRRE"
"size";"-"
"make sure subsamplemetasplitter is consistent across calls to split";"IRRE"
"we re ok having training sets differ they re always sampled with a";"IRRE"
"different fraction anyway";"-"
"when we don t subsample the test set we want it to be always the same";"IRRE"
"this check is the most important this is ensured by the determinism";"CODE"
"of the base cv";"-"
"note we could force both train and test splits to be always the same if";"TASK"
"we drew an int seed in subsamplemetasplitter init";"IRRE"
"results this isn t a real world result dict";"IRRE"
"test that the cv results matches correctly the logic of the";"IRRE"
"tournament in particular that the candidates continued in each";"CODE"
"successive iteration are those that were best in the previous iteration";"IRRE"
"generate random scores we want to avoid ties which would otherwise";"CODE"
"mess with the ordering and make testing harder";"IRRE"
"same number of candidates as with the grid";"-"
"non regression check for";"CODE"
"https github com scikit learn scikit learn issues 19203";"CODE"
"just make sure we don t have ties";"CODE"
"table looks like something like this";"CODE"
"iter 0 1 2 3 4 5";"-"
"params str";"-"
"a l2 b 23 0 75 nan nan nan nan nan";"-"
"a l1 b 30 0 90 0 875 nan nan nan nan";"-"
"a l1 b 0 0 75 nan nan nan nan nan";"-"
"a l2 b 3 0 85 0 925 0 9125 0 90625 nan nan";"-"
"a l1 b 5 0 80 nan nan nan nan nan";"-"
"where a nan indicates that the candidate wasn t evaluated at a given";"IRRE"
"iteration because it wasn t part of the top k at some previous";"-"
"iteration we here make sure that candidates that aren t in the top k at";"IRRE"
"any given iteration are indeed not evaluated at the subsequent";"TASK"
"iterations";"-"
"make sure that if a candidate is already discarded we don t evaluate";"CODE"
"it later";"-"
"make sure that the number of discarded candidate is correct";"-"
"make sure that all discarded candidates have a lower score than the";"-"
"kept candidates";"-"
"we now make sure that the best candidate is chosen only from the last";"CODE"
"iteration";"-"
"we also make sure this is true even if there were higher scores in";"CODE"
"earlier rounds this isn t generally the case but worth ensuring it s";"CODE"
"possible";"-"
"make sure that the base estimators are passed the correct parameters and";"IRRE"
"number of samples at each iteration";"-"
"same number of candidates as with the grid";"-"
"lists are of length n splits n iter n candidates at i";"-"
"each chunk of size n splits corresponds to the n splits folds for the";"CODE"
"same candidate at the same iteration so they contain equal values we";"IRRE"
"subsample such that the lists are of length n iter n candidates at it";"-"
"check if valueerror when groups is none propagates to";"IRRE"
"halvinggridsearchcv and halvingrandomsearchcv";"IRRE"
"and also check if groups is correctly passed to the cv object";"IRRE"
"should not raise an error";"CODE"
"results this isn t a real world result dict";"IRRE"
"we expect the index of i";"-"
"check results structure";"IRRE"
"training score becomes worse 2 1 test error better 0 1";"IRRE"
"xxx use 2d array since 1d x is being detected as a single sample in";"-"
"check consistent length";"-"
"the number of samples per class needs to be n splits";"TASK"
"for stratifiedkfold n splits 3";"CODE"
"smoke test";"IRRE"
"test with multioutput y";"IRRE"
"test with multioutput y";"IRRE"
"test with x and y as list";"IRRE"
"test with 3d x and";"IRRE"
"regression test for 12154 cv warn with n jobs 1 trigger a copy of";"IRRE"
"the parameters leading to a failure in check cv due to cv is warn";"IRRE"
"instead of cv warn";"CODE"
"test the errors";"IRRE"
"list tuple of callables should raise a message advising users to use";"IRRE"
"dict of names to callables mapping";"IRRE"
"so should empty lists tuples";"-"
"so should duplicated entries";"-"
"nested lists should raise a generic error message";"CODE"
"empty dict should raise invalid scoring error";"OUTD"
"multiclass scorers that return multiple values are not supported yet";"IRRE"
"the warning message we re expecting to see";"-"
"non regression test to ensure that nested";"IRRE"
"estimators are properly returned in a list";"IRRE"
"https github com scikit learn scikit learn pull 17745";"CODE"
"compute train and test mse r2 scores";"IRRE"
"regression";"-"
"classification";"IRRE"
"it s okay to evaluate regression metrics on classification too";"IRRE"
"to ensure that the test does not suffer from";"CODE"
"large statistical fluctuations due to slicing small datasets";"IRRE"
"we pass the cross validation instance";"-"
"test single metric evaluation when scoring is string or singleton list";"CODE"
"single metric passed as a string";"CODE"
"single metric passed as a list";"-"
"it must be true by default deprecated";"TASK"
"test return estimator option";"IRRE"
"test multimetric evaluation when scoring is a list dict";"IRRE"
"return train score must be true by default deprecated";"CODE"
"make sure all the arrays are of np ndarray type";"-"
"ensure all the times are within sane limits";"-"
"check if valueerror when groups is none propagates to cross val score";"IRRE"
"and cross val predict";"-"
"and also check if groups is correctly passed to the cv object";"IRRE"
"check cross val score doesn t destroy pandas dataframe";"CODE"
"x dataframe y series";"-"
"3 fold cross val is used so we need at least 3 samples per class";"IRRE"
"test that cross val score works with boolean masks";"IRRE"
"test for svm with precomputed kernel";"IRRE"
"test with callable";"IRRE"
"error raised for non square x";"CODE"
"test error is raised when the precomputed kernel is not array like";"IRRE"
"or sparse";"IRRE"
"function to test that the values are passed correctly to the";"IRRE"
"classifier arguments for non array type";"CODE"
"test that score function is called only 3 times for cv 3";"CODE"
"default score should be the accuracy score";"CODE"
"correct classification score aka zero one score should be the";"IRRE"
"same as the default estimator score";"IRRE"
"f1 score class are balanced so f1 score should be equal to zero one";"IRRE"
"score";"-"
"default score of the ridge regression estimator";"CODE"
"r2 score aka determination coefficient should be the";"-"
"same as the default estimator score";"IRRE"
"mean squared error this is a loss function so scores are negative";"CODE"
"explained variance";"CODE"
"check that we obtain the same results with a sparse representation";"IRRE"
"test with custom scoring object";"IRRE"
"set random y";"IRRE"
"check that permutation test score allows input data with nans";"IRRE"
"check that cross val score allows input data with nans";"CODE"
"naive loop should be same as cross val predict";"IRRE"
"this specifically tests imbalanced splits for binary";"IRRE"
"classification with decision function this is only";"CODE"
"applicable to classifiers that can be fit on a single";"CODE"
"class";"IRRE"
"3 fold cv is used at least 3 samples per class";"IRRE"
"smoke test";"IRRE"
"test with multioutput y";"IRRE"
"test with multioutput y";"IRRE"
"test with x and y as list";"IRRE"
"test with x and y as list and non empty method";"IRRE"
"test with 3d x and";"IRRE"
"check cross val score doesn t destroy pandas dataframe";"CODE"
"x dataframe y series";"-"
"change the first sample to a new class";"CODE"
"assert y test 0 0 2 sanity check for further assertions";"CODE"
"ensure that cross val predict works when y is none";"-"
"cannot use assert array almost equal for fit and score times because";"CODE"
"the values are hardware dependent";"IRRE"
"test a custom cv splitter that can iterate only once";"IRRE"
"the mockup does not have partial fit";"CODE"
"following test case was designed this way to verify the code";"CODE"
"changes made in pull request 7506";"CODE"
"splits on these groups fail without shuffle as the first iteration";"-"
"of the learning curve doesn t contain label 4 in the training set";"CODE"
"the onetimesplitter is a non re entrant cv splitter unless the";"CODE"
"split is called for each parameter the following should produce";"IRRE"
"identical results for param setting 1 and param setting 2 as both have";"IRRE"
"the same c value";"IRRE"
"for scores2 compare the 1st and 2nd parameter s scores";"IRRE"
"since the c value for 1st two param setting is 0 1 they must be";"IRRE"
"consistent unless the train test folds differ between the param settings";"IRRE"
"onetimesplitter is basically unshuffled kfold n splits 5 sanity check";"IRRE"
"check if the additional duplicate indices are caught";"TASK"
"check that cross val predict gives same result for sparse and dense input";"IRRE"
"generate expected outputs";"IRRE"
"check actual outputs for several representations of y";"IRRE"
"create empty arrays of the correct size to hold outputs";"IRRE"
"generate expected outputs";"IRRE"
"decision function with 2 classes";"CODE"
"check actual outputs for several representations of y";"IRRE"
"this test includes the decision function with two classes";"CODE"
"this is a special case it has only one column of output";"CODE"
"regression test for issue 9639 tests that cross val predict does not";"IRRE"
"check estimator methods e g predict proba before fitting";"CODE"
"ovr does multilabel predictions but only arrays of";"META"
"binary indicator columns the output of predict proba";"IRRE"
"is a 2d array with shape n samples n classes";"IRRE"
"none of the current multioutput multiclass estimators have";"IRRE"
"decision function methods create a mock decision function";"CODE"
"to test the cross val predict function s handling of this case";"CODE"
"the randomforest allows multiple classes in each label";"IRRE"
"output of predict proba is a list of outputs of predict proba";"IRRE"
"for each individual label";"CODE"
"y 0 y 1 put three classes in the first column";"CODE"
"suppress runtimewarning divide by zero encountered in log";"CODE"
"test a multiclass problem where one class will be missing from";"IRRE"
"one of the cv training sets";"IRRE"
"suppress warning about too few examples of a class";"IRRE"
"the randomforest allows anything for the contents of the labels";"CODE"
"output of predict proba is a list of outputs of predict proba";"IRRE"
"for each individual label";"CODE"
"in this test the first label has a class with a single example";"CODE"
"we ll have one cv fold where the training data don t include it";"CODE"
"suppress runtimewarning divide by zero encountered in log";"CODE"
"to avoid 2 dimensional indexing";"CODE"
"test with n splits 3";"IRRE"
"runs a naive loop should be same as cross val predict";"IRRE"
"test with n splits 4";"IRRE"
"testing unordered labels";"IRRE"
"ensure a scalar score of memmap type is accepted";"-"
"best effort to release the mmap file handles before deleting the";"CODE"
"backing file under windows";"CODE"
"check permutation test score doesn t destroy pandas dataframe";"IRRE"
"x dataframe y series";"-"
"create a failing classifier to deliberately fail";"IRRE"
"dummy x data";"-"
"passing error score to trigger the warning message";"-"
"check if exception was raised with default error score raise";"CODE"
"assert failing clf score 0 0 failingclassifier coverage";"CODE"
"test return parameters option";"IRRE"
"create a failing classifier to deliberately fail";"IRRE"
"dummy x data";"-"
"passing error score to trigger the warning message";"-"
"check if the warning message type is as expected";"IRRE"
"create a failing classifier to deliberately fail";"IRRE"
"dummy x data";"-"
"check that an estimator can fail during scoring in cross val score and";"-"
"that we can optionally replaced it with error score";"-"
"check that an estimator can fail during scoring in cross validate and";"-"
"that we can optionally replace it with error score in the multimetric";"CODE"
"case also check the result of a non failing scorer where the other scorers";"IRRE"
"are failing";"-"
"check the test and optionally train score for the";"CODE"
"scorer that should be non failing";"-"
"check the test and optionally train score for all";"CODE"
"scorers that should be assigned to error score";"IRRE"
"test print without train score";"IRRE"
"does not error";"TASK"
"x scale x scale features for better convergence";"TASK"
"tests for metadata routing in cross val and in curve";"IRRE"
"cross val predict doesn t use scoring";"CODE"
"cross val predict doesn t need a scorer";"CODE"
"end of metadata routing tests";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"predict proba threshold";"-"
"onevsoneclassifier estimator is not validated yet";"TASK"
"we need to validate the data because we do a safe indexing later";"TASK"
"onevsoneclassifier estimator is not validated yet";"TASK"
"outputcodeclassifier estimator is not validated yet";"IRRE"
"fixme there are more elaborate methods than generating the codebook";"-"
"randomly";"IRRE"
"argkmin only accepts c contiguous array the aggregated predictions need to be";"TASK"
"transposed we therefore create a f contiguous array to avoid a copy and have";"CODE"
"a c contiguous array after the transpose operation";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"multioutput estimator is not validated yet";"IRRE"
"multioutput estimator is not validated yet";"IRRE"
"raise an attributeerror if predict proba does not exist for";"CODE"
"each estimator";"-"
"raise an attributeerror if predict proba does not exist for the";"CODE"
"unfitted estimator";"-"
"fixme";"-"
"todo 1 9 remove base estimator";"TASK"
"todo 1 9 this is a temporary getter method to validate input wrt deprecation";"CODE"
"it was only included to avoid relying on the presence of self estimator";"CODE"
"regressorchain does not have a chain method parameter so we";"IRRE"
"default to predict";"CODE"
"if x is a scipy sparse dok array we convert it to a sparse";"IRRE"
"coo array format before hstacking it s faster see";"CODE"
"https github com scipy scipy issues 20060 issuecomment 1937007039";"CODE"
"todo remove this condition check when the minimum supported scipy version";"CODE"
"doesn t support sparse matrices anymore";"IRRE"
"if x is a scipy sparse dok array we convert it to a sparse";"IRRE"
"coo array format before hstacking it s faster see";"CODE"
"https github com scipy scipy issues 20060 issuecomment 1937007039";"CODE"
"in case that x is a sparse array we create y pred chain as a";"IRRE"
"sparse array format";"IRRE"
"regressorchain does not have a chain method parameter";"IRRE"
"predict proba output is 2d we use only output for classes 1";"IRRE"
"todo 1 9 remove base estimator from init";"CODE"
"classifierchain base estimator is not validated yet";"TASK"
"fixme";"-"
"regressorchain base estimator is not validated yet";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"compute potentially weighted mean and variance of new datapoints";"CODE"
"combine mean of old and new data taking into consideration";"CODE"
"weighted number of observations";"-"
"combine variance of old and new data taking into consideration";"CODE"
"weighted number of observations this is achieved by combining";"CODE"
"the sum of squared differences ssd";"-"
"if the ratio of data variance between dimensions is too small it";"CODE"
"will cause numerical errors to address this we artificially";"TASK"
"boost the variance by epsilon a small fraction of the standard";"CODE"
"deviation of the largest dimension";"CODE"
"this is the first call to partial fit";"IRRE"
"initialize various cumulative counters";"IRRE"
"initialise the class prior";"IRRE"
"take into account the priors";"CODE"
"check that the provided prior matches the number of classes";"IRRE"
"check that the sum is 1";"-"
"check that the priors are non negative";"-"
"initialize the priors to zeros for each class";"CODE"
"put epsilon back in each time";"-"
"update if only no priors is provided";"CODE"
"empirical prior with sample weight taken into account";"CODE"
"silence the warning when count is 0 because class was not yet";"TASK"
"observed";"-"
"empirical prior with sample weight taken into account";"CODE"
"check that all alpha are positive";"-"
"this is the first call to partial fit";"IRRE"
"initialize various cumulative counters";"IRRE"
"else degenerate case just one class";"CODE"
"label binarize returns arrays with dtype np int64";"CODE"
"we convert it to np float64 to support sample weight consistently";"CODE"
"count raw events from data before updating the class log prior";"CODE"
"and feature log probas";"TASK"
"xxx optim we could introduce a public finalization method to";"CODE"
"be called by the user explicitly just once after several consecutive";"IRRE"
"calls to partial fit and prior any call to predict log proba";"IRRE"
"to avoid computing the smooth log probas at each call to partial fit";"IRRE"
"else degenerate case just one class";"CODE"
"labelbinarizer fit transform returns arrays with dtype np int64";"CODE"
"we convert it to np float64 to support sample weight consistently";"CODE"
"this means we also don t have to cast x to floating point";"CODE"
"count raw events from data before updating the class log prior";"CODE"
"and feature log probas";"TASK"
"compute neg prob 1 x t as neg prob x neg prob";"-"
"callable metric is only valid for brute force and ball tree";"CODE"
"classification targets require a specific format";"CODE"
"using dtype np intp is necessary since np bincount";"CODE"
"called in classification py fails when dealing";"IRRE"
"with a float64 array on 32bit systems";"CODE"
"for minkowski distance use more efficient methods where available";"CODE"
"use the generic minkowski metric possibly weighted";"IRRE"
"precomputed matrix x must be squared";"TASK"
"a tree approach is better for small number of neighbors or small";"CODE"
"number of features with kdtree generally faster when available";"TASK"
"minkowski with weights is not supported by kdtree but is";"META"
"supported byballtree";"-"
"for 0 p 1 minkowski distances aren t valid distance";"CODE"
"metric as they do not satisfy triangular inequality";"CODE"
"they are semi metrics";"-"
"algorithm kd tree and algorithm ball tree can t be used because";"-"
"kdtree and balltree require a proper distance metric to work properly";"CODE"
"however the brute force algorithm supports semi metrics";"CODE"
"else self fit method in kd tree ball tree";"CODE"
"for cross validation routines to split data correctly";"CODE"
"when input is precomputed metric values all those values need to be positive";"IRRE"
"joblib based backend which is used when user defined callable";"CODE"
"are passed for metric";"CODE"
"this won t be used in the future once pairwisedistancesreductions";"CODE"
"support";"-"
"distancemetrics which work on supposedly binary data";"-"
"csr dense and dense csr case if euclidean in metric";"CODE"
"for efficiency use squared euclidean distances";"CODE"
"if the query data is the same as the indexed data we would like";"CODE"
"to ignore the first nearest neighbor of every sample i e";"-"
"the sample itself";"CODE"
"check the input only in self radius neighbors";"CODE"
"construct csr matrix representation of the nn graph";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"todo implement a brute force version for testing purposes";"CODE"
"todo create a density estimation base class";"IRRE"
"given the algorithm string metric string choose the optimal";"CODE"
"algorithm to compute the result";"IRRE"
"use kd tree if possible";"-"
"else kd tree or ball tree";"-"
"kerneldensity metric is not validated yet";"TASK"
"the returned density is normalized to the number of points";"CODE"
"for it to be a probability we must scale it for this reason";"CODE"
"we ll also scale atol";"-"
"todo implement sampling for other valid kernel shapes";"TASK"
"we first draw points from a d dimensional normal distribution";"CODE"
"then use an incomplete gamma function to map them to a uniform";"CODE"
"d dimensional tophat distribution";"META"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"as fit predict would be different from fit predict fit predict is";"CODE"
"only available for outlier detection novelty false";"CODE"
"localoutlierfactor metric is not validated yet";"TASK"
"compute lof score over training samples to define offset";"IRRE"
"inliers score around 1 the higher the less abnormal";"-"
"verify if negative outlier factor values are within acceptable range";"IRRE"
"novelty must also be false to detect outliers";"IRRE"
"as bigger is better";"-"
"1e 10 to avoid nan when nb of duplicates n neighbors";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"validate the inputs x and y and converts y to numerical classes";"CODE"
"check the preferred dimensionality of the projected space";"-"
"if warm start is enabled check that the inputs are consistent";"CODE"
"check how the linear transformation should be initialized";"IRRE"
"assert that init shape 1 x shape 1";"IRRE"
"assert that init shape 0 init shape 1";"IRRE"
"assert that self n components init shape 0";"CODE"
"initialize the random generator";"IRRE"
"measure the total training time";"-"
"compute a mask that stays fixed during optimization";"-"
"n samples n samples";"-"
"initialize the transformation";"IRRE"
"create a dictionary of parameters to be passed to the optimizer";"IRRE"
"call the optimizer";"IRRE"
"reshape the solution found by the optimizer";"-"
"stop timer";"-"
"warn the user if the algorithm did not converge";"-"
"x embedded np dot x transformation t n samples n components";"CODE"
"compute softmax distances";"-"
"p ij softmax p ij n samples n samples";"-"
"compute loss";"-"
"p np sum masked p ij axis 1 keepdims true n samples 1";"-"
"compute gradient of loss w r t transform";"CODE"
"time complexity of the gradient o n components x n samples x";"META"
"n samples n features";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"if x is sparse and the metric is manhattan store it in a csc";"IRRE"
"format is easier to calculate the median";"CODE"
"if self priors empirical estimate priors from sample";"CODE"
"class counts np unique y return inverse true non negative ints";"IRRE"
"mask mapping each class to its members";"IRRE"
"number of clusters in each class";"IRRE"
"numpy does not calculate median of sparse matrices";"IRRE"
"else metric euclidean";"CODE"
"compute within class std dev with unshrunked centroids";"CODE"
"m parameter for determining deviation";"IRRE"
"calculate deviation using the standard deviation of centroids";"-"
"to deter outliers from affecting the results";"IRRE"
"mm m reshape len m 1 reshape to allow broadcasting";"-"
"soft thresholding if the deviation crosses 0 during shrinking";"-"
"it becomes zero";"-"
"now adjust the centroids using the deviation";"-"
"validate data is called here since we are not calling super";"IRRE"
"return discriminant scores see eq 18 2 p 652 of the esl";"IRRE"
"test the number of neighbors returned";"IRRE"
"with n neighbors";"-"
"with radius";"-"
"xxx duplicated in test neighbors tree test kde";"IRRE"
"draw a tophat sample";"-"
"check that samples are in the right range";"CODE"
"5 standard deviations is safe for 100 samples but there s a";"META"
"very small chance this test could fail";"IRRE"
"check unsupported kernels";"-"
"non regression test used to return a scalar";"IRRE"
"smoke test for various metrics and algorithms";"CODE"
"x rng randn 10 2 2 features required for haversine dist";"CODE"
"fixme";"-"
"rng np random randomstate 0";"IRRE"
"x rng random sample n samples n features";"IRRE"
"y rng random sample n samples n features";"IRRE"
"test that kde plays nice in pipelines and grid searches";"IRRE"
"test that adding a constant sample weight has no effect";"TASK"
"test equivalence between sampling and integer weights";"IRRE"
"test that sample weights has a non trivial effect";"IRRE"
"test invariance with respect to arbitrary scaling";"IRRE"
"make sure that predictions are the same before and after pickling used";"CODE"
"to be a bug because sample weights wasn t pickled and the resulting tree";"IRRE"
"would miss some info";"-"
"check that predict raises an exception in an unfitted estimator";"CODE"
"unfitted estimators should raise a notfittederror";"CODE"
"test that the attribute self bandwidth has the expected value";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the iris dataset";"IRRE"
"and randomly permute it";"IRRE"
"toy sample the last two samples are outliers";"CODE"
"test localoutlierfactor";"IRRE"
"assert largest outlier score is smaller than smallest inlier score";"CODE"
"assert predict works";"CODE"
"generate train test data";"CODE"
"generate some abnormal novel observations";"-"
"fit the model for novelty detection";"CODE"
"predict scores the lower the more normal";"-"
"check that roc auc is good";"-"
"toy samples";"-"
"check predict";"-"
"check predict one sample not in train";"CODE"
"check predict one sample already in train";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load and shuffle the iris dataset";"CODE"
"avoid having test data introducing dependencies between tests";"CODE"
"elf loss np inf initialize the loss to very high";"IRRE"
"initialize a fake nca and variables needed to compute the loss";"IRRE"
"initialize a fake nca and variables needed to call the loss";"IRRE"
"function";"CODE"
"check that no error is raised when parameters have numpy integer or";"CODE"
"floating types";"CODE"
"toy sample";"-"
"also load the iris dataset";"IRRE"
"and randomly permute it";"IRRE"
"check classification on a toy dataset including sparse versions";"IRRE"
"check classification on a toy dataset including sparse versions";"IRRE"
"test uniform priors";"IRRE"
"test custom priors";"IRRE"
"same test but with a sparse matrix to fit and test";"IRRE"
"fit with sparse test with non sparse";"IRRE"
"fit with non sparse test with sparse";"IRRE"
"fit and predict with non csr sparse matrices";"IRRE"
"check consistency on dataset iris";"IRRE"
"check consistency on dataset iris when using shrinkage";"IRRE"
"classification";"IRRE"
"ensure that the shrinking is correct";"-"
"the expected result is calculated by r pamr";"IRRE"
"which is implemented by the author of the original paper";"META"
"one need to modify the code to output the new centroid in pamr predict";"TASK"
"test that nearestcentroid gives same results on translated data";"IRRE"
"test the manhattan metric";"IRRE"
"test that features with 0 variance throw error";"CODE"
"load and shuffle iris dataset";"IRRE"
"load and shuffle digits";"CODE"
"filter deprecation warnings";"-"
"a list containing metrics where the string specifies the use of the";"CODE"
"distancemetric object directly as resolved in parse metric";"IRRE"
"note smaller samples may result in spurious test success";"IRRE"
"todo also test radius neighbors but requires different assertion";"CODE"
"as a feature matrix n samples by n features";"TASK"
"as a dense distance matrix n samples by n samples";"-"
"check auto works too";"-"
"check x none in prediction";"-"
"must raise a valueerror if the matrix is not of correct shape";"IRRE"
"we do not test radiusneighborsclassifier and radiusneighborsregressor";"IRRE"
"since the precomputed neighbors graph is built with k neighbors only";"-"
"we do not test kneighborsclassifier and kneighborsregressor";"IRRE"
"since the precomputed neighbors graph is built with a radius";"-"
"test that is sorted by data works as expected in csr sparse matrix";"IRRE"
"entries in each row can be sorted by indices by data or unsorted";"CODE"
"is sorted by data should return true when entries are sorted by data";"IRRE"
"and false in all other cases";"CODE"
"test with sorted single row sparse array";"IRRE"
"test with unsorted 1d array";"IRRE"
"test when the data is sorted in each sample but not necessarily";"IRRE"
"between samples";"-"
"test with duplicates entries in x indptr";"IRRE"
"test that sort graph by row values returns a graph sorted by row values";"IRRE"
"test with a different number of nonzero entries for each sample";"CODE"
"test if the sorting is done inplace if x is csr so that xt is x";"IRRE"
"sort graph by row values is done inplace if copy false";"IRRE"
"check precomputed is never done inplace";"CODE"
"do not raise if x is not csr and copy true";"CODE"
"raise if x is not csr and copy false";"IRRE"
"test that the parameter warn when not sorted works as expected";"IRRE"
"warning";"-"
"no warning";"-"
"test that sort graph by row values and check precomputed error on bad formats";"IRRE"
"ensures enough number of nearest neighbors";"-"
"checks error with inconsistent distance matrix";"META"
"ensure array is split correctly";"-"
"test unsupervised radius based query";"CODE"
"sort the results this is not done automatically for";"CODE"
"radius searches";"-"
"test k neighbors classification";"IRRE"
"test prediction with y str";"IRRE"
"test k neighbors classification";"IRRE"
"test kneighborsclassifier predict proba method";"IRRE"
"cls neighbors kneighborsclassifier n neighbors 3 p 1 cityblock dist";"IRRE"
"check that it also works with non integer labels";"CODE"
"check that it works with weights distance";"-"
"test radius based classification";"IRRE"
"test radius based classifier when no neighbors found";"IRRE"
"in this case it should rise an informative exception";"CODE"
"no outliers";"-"
"one outlier";"-"
"test radius based classifier when no neighbors found and outliers";"IRRE"
"are labeled";"-"
"no outliers";"-"
"one outlier";"-"
"test outlier labeling of using predict proba";"IRRE"
"test outlier label scalar verification";"IRRE"
"test invalid outlier label dtype";"IRRE"
"test most frequent";"IRRE"
"test manual label in y";"IRRE"
"test manual label out of y warning";"IRRE"
"test multi output same outlier label";"IRRE"
"test multi output different outlier label";"IRRE"
"test inconsistent outlier label list length";"IRRE"
"test radius based classifier when distance to a sample is zero";"IRRE"
"ignore the warning raised in weight func when making";"CODE"
"predictions with null distances resulting in np inf values";"IRRE"
"test radius based regressor when distance to a sample is zero";"IRRE"
"we don t test for weights weight func since user will be expected";"CODE"
"to handle zero distances themselves in the function";"CODE"
"check that we can pass precomputed distances to";"-"
"nearestneighbors radius neighbors";"-"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 16036";"CODE"
"for several candidates for the k th nearest neighbor position";"CODE"
"the first candidate should be chosen";"CODE"
"the 3rd and 4th points should not replace the 2nd point";"CODE"
"for the 2th nearest neighbor position";"CODE"
"test radius neighbors graph output when sort result is true";"IRRE"
"self radius neighbors";"CODE"
"sort results true and return distance false";"IRRE"
"if metric precomputed no need to raise with precomputed graph";"TASK"
"self radius neighbors graph";"CODE"
"test k nn classifier on multioutput data";"IRRE"
"stack single output prediction";"IRRE"
"multioutput prediction";"IRRE"
"test k nn classifier on sparse matrices";"IRRE"
"like the above but with various types of sparse matrices";"IRRE"
"test k nn classifier on multioutput data";"IRRE"
"stack single output prediction";"IRRE"
"multioutput prediction";"IRRE"
"check proba";"-"
"test k neighbors regression";"IRRE"
"test k neighbors in multi output regression with uniform weight";"IRRE"
"test k neighbors in multi output regression";"IRRE"
"test radius based neighbors regression";"IRRE"
"test that nan is returned when no nearby observations";"IRRE"
"test radius neighbors in multi output regression uniform weight";"IRRE"
"test k neighbors in multi output regression with various weight";"IRRE"
"test radius based regression on sparse matrices";"IRRE"
"like the above but with various types of sparse matrices";"IRRE"
"sanity checks on the iris dataset";"IRRE"
"puts three points of each label in the plane and performs a";"CODE"
"nearest neighbor query on points near the decision boundary";"CODE"
"sanity check on the digits dataset";"IRRE"
"the brute algorithm has been observed to fail if the input";"CODE"
"dtype is uint8 due to overflow in distance calculations";"CODE"
"test kneighbors graph to build the k nearest neighbor graph";"IRRE"
"n neighbors 1";"-"
"n neighbors 2";"-"
"n neighbors 3";"-"
"test kneighbors graph to build the k nearest neighbor graph";"IRRE"
"for sparse input";"CODE"
"test radius neighbors graph to build the nearest neighbor graph";"IRRE"
"test radius neighbors graph to build the nearest neighbor graph";"IRRE"
"for sparse input";"CODE"
"test computing the neighbors for various metrics";"CODE"
"some metric e g weighted minkowski are not supported by kdtree";"-"
"if tree in algorithm pragma nocover";"-"
"haversine distance only accepts 2d data";"-"
"the returned distances are always in float64 regardless of the input dtype";"CODE"
"we need to adjust the tolerance w r t the input dtype";"TASK"
"todo remove ignore warnings when minimum supported scipy version is 1 17";"TASK"
"some scipy metrics are deprecated depending on the scipy version but we";"META"
"still want to test them";"TASK"
"both backend for the brute algorithm of kneighbors must give identical results";"CODE"
"haversine distance only accepts 2d data";"-"
"use the legacy backend for brute";"CODE"
"use the pairwise distances reduction backend for brute";"CODE"
"haversine distance only accepts 2d data";"-"
"find a reasonable radius";"-"
"test kneighbors graph";"IRRE"
"test radiusneighbors graph";"IRRE"
"raise error when wrong parameters are supplied";"IRRE"
"test kneighbors et al when query is not training data";"IRRE"
"test neighbors";"IRRE"
"test the graph variants";"IRRE"
"test kneighbors et al when query is none";"IRRE"
"test the graph variants";"IRRE"
"test behavior of kneighbors when duplicates are present in query";"CODE"
"do not do anything special to duplicates";"CODE"
"mask the first duplicates when n duplicates n neighbors";"CODE"
"test that zeros are explicitly marked in kneighbors graph";"IRRE"
"test include self parameter in neighbors graph";"CODE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"non regression test which ensures the knn methods are properly working";"IRRE"
"even when forcing the global joblib backend";"CODE"
"def sparse metric x y metric accepting sparse matrix input only";"IRRE"
"1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 population matrix";"-"
"y csr container 1 1 0 1 1 1 0 0 1 1 query matrix";"CODE"
"gs indices of nearest neighbours in x for sparse metric";"IRRE"
"ignore conversion to boolean in pairwise distances";"META"
"non regression test for 4523";"IRRE"
"brute uses scipy spatial distance through pairwise distances";"-"
"ball tree uses sklearn neighbors dist metrics";"-"
"test chaining kneighborstransformer and classifiers regressors";"IRRE"
"we precompute more neighbors than necessary to have equivalence between";"-"
"k neighbors estimator after radius neighbors transformer and vice versa";"IRRE"
"compare the chained version and the compact version";"META"
"todo remove ignore warnings when minimum supported scipy version is 1 17";"TASK"
"some scipy metrics are deprecated depending on the scipy version but we";"META"
"still want to test them";"TASK"
"both backends for the brute algorithm of radius neighbors";"CODE"
"must give identical results";"IRRE"
"haversine distance only accepts 2d data";"-"
"use the legacy backend for brute";"CODE"
"use the pairwise distances reduction backend for brute";"CODE"
"set the radius for radiusneighborsregressor to some percentile of the";"IRRE"
"empirical pairwise distances to avoid trivial test cases and warnings for";"CODE"
"predictions with no neighbors within the radius";"CODE"
"evaluating nn model on its training set should lead to a higher";"IRRE"
"accuracy value than leaving out each data point in turn because the";"IRRE"
"former can overfit while the latter cannot by construction";"CODE"
"test chaining kneighborstransformer and spectralclustering";"IRRE"
"compare the chained version and the compact version";"META"
"test chaining kneighborstransformer and spectralembedding";"IRRE"
"compare the chained version and the compact version";"META"
"test chaining radiusneighborstransformer and dbscan";"IRRE"
"compare the chained version and the compact version";"META"
"test chaining kneighborstransformer and isomap with";"IRRE"
"neighbors algorithm precomputed";"-"
"compare the chained version and the compact version";"META"
"test chaining kneighborstransformer and tsne";"IRRE"
"compare the chained version and the compact version";"META"
"test chaining kneighborstransformer and localoutlierfactor";"IRRE"
"compare the chained version and the compact version";"META"
"test chaining kneighborstransformer and localoutlierfactor";"IRRE"
"compare the chained version and the compact version";"META"
"test chaining kneighborstransformer and classifiers regressors";"IRRE"
"we precompute more neighbors than necessary to have equivalence between";"-"
"k neighbors estimator after radius neighbors transformer and vice versa";"IRRE"
"compare the chained version and the compact version";"META"
"spdx license identifier bsd 3 clause";"-"
"eps 1e 15 roundoff error can cause test to fail";"IRRE"
"eps 1e 15 roundoff error can cause test to fail";"IRRE"
"simultaneous sort rows using function";"CODE"
"simultaneous sort rows using numpy";"-"
"compare gaussian kde results to scipy stats gaussian kde";"IRRE"
"don t check indices here if there are any duplicate distances";"CODE"
"the indices may not match distances should not have this problem";"CODE"
"introduce a point into a quad tree with boundaries not easy to compute";"CODE"
"check a random case";"IRRE"
"check the case where only 0 are inserted";"CODE"
"check the case where only negative are inserted";"CODE"
"check the case where only small numbers are inserted";"CODE"
"introduce a point into a quad tree where a similar point already exists";"CODE"
"test will hang if it doesn t complete";"CODE"
"check the case where points are actually different";"CODE"
"check the case where points are the same on x axis";"CODE"
"check the case where points are arbitrarily close on x axis";"CODE"
"check the case where points are the same on y axis";"CODE"
"check the case where points are arbitrarily close on y axis";"CODE"
"check the case where points are arbitrarily close on both axes";"CODE"
"check the case where points are arbitrarily close on both axes";"CODE"
"close to machine epsilon x axis";"IRRE"
"check the case where points are arbitrarily close on both axes";"CODE"
"close to machine epsilon y axis";"IRRE"
"create some duplicates";"IRRE"
"epsilon 1e 6 is defined in sklearn neighbors quad tree pyx but not";"CODE"
"accessible from python";"CODE"
"add slight noise duplicate detection should tolerate tiny numerical differences";"TASK"
"assert that the first 5 are indeed duplicated and that the next";"CODE"
"ones are single point leaf";"CODE"
"simple check for quad tree s summarize";"CODE"
"summary should contain only 1 node with size 3 and distance to";"-"
"x 1 barycenter";"IRRE"
"summary should contain all 3 node with size 1 and distance to";"-"
"each point in x 1 for angle 0";"CODE"
"input validation would remove feature names so we disable it";"TASK"
"matrix of actions to be taken under the possible combinations";"-"
"the case that incremental true and classes not defined is";"CODE"
"already checked by check partial fit first call that is called";"IRRE"
"in partial fit below";"-"
"the cases are already grouped into the respective if blocks below";"CODE"
"incremental warm start classes def action";"CODE"
"0 0 0 define classes";"CODE"
"0 1 0 define classes";"CODE"
"0 0 1 redefine classes";"CODE"
"0 1 1 check compat warm start";"-"
"1 1 1 check compat warm start";"-"
"1 0 1 check compat last fit";"-"
"note the reliance on short circuiting here so that the second";"TASK"
"or part implies that classes is defined";"CODE"
"this downcast to bool is to prevent upcasting when working with";"CODE"
"float32 data";"CODE"
"y proba is equal to one should result in a finite logloss";"IRRE"
"y proba is equal to 1 should result in a finite logloss";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"test that larger alpha yields weights closer to zero";"IRRE"
"test that the algorithm solution is equal to a worked out example";"IRRE"
"set weights";"IRRE"
"initialize parameters";"IRRE"
"compute the number of layers";"-"
"pre allocate gradient matrices";"-"
"manually worked out example";"CODE"
"h1 g x1 w i1 b11 g 0 6 0 1 0 8 0 3 0 7 0 5 0 1";"-"
"0 679178699175393";"-"
"h2 g x2 w i2 b12 g 0 6 0 2 0 8 0 1 0 7 0 0 1";"-"
"0 574442516811659";"-"
"o1 g h w2 b21 g 0 679 0 1 0 574 0 2 1";"-"
"0 7654329236196236";"-"
"d21 0 0 765 0 765";"-"
"d11 1 0 679 0 679 0 765 0 1 0 01667";"-"
"d12 1 0 574 0 574 0 765 0 2 0 0374";"-"
"w1grad11 x1 d11 alpha w11 0 6 0 01667 0 1 0 1 0 0200";"-"
"w1grad11 x1 d12 alpha w12 0 6 0 0374 0 1 0 2 0 04244";"-"
"w1grad21 x2 d11 alpha w13 0 8 0 01667 0 1 0 3 0 043336";"-"
"w1grad22 x2 d12 alpha w14 0 8 0 0374 0 1 0 1 0 03992";"-"
"w1grad31 x3 d11 alpha w15 0 6 0 01667 0 1 0 5 0 060002";"-"
"w1grad32 x3 d12 alpha w16 0 6 0 0374 0 1 0 0 02244";"-"
"w2grad1 h1 d21 alpha w21 0 679 0 765 0 1 0 1 0 5294";"-"
"w2grad2 h2 d21 alpha w22 0 574 0 765 0 1 0 2 0 45911";"-"
"b1grad1 d11 0 01667";"TASK"
"b1grad2 d12 0 0374";"TASK"
"b2grad d21 0 765";"TASK"
"w1 w1 eta w1grad11 w1grad32 0 1 0 2 0 3 0 1";"-"
"0 5 0 0 1 0 0200 0 04244 0 043336 0 03992";"-"
"0 060002 0 02244 0 098 0 195756 0 2956664";"-"
"0 096008 0 4939998 0 002244";"-"
"w2 w2 eta w2grad1 w2grad2 0 1 0 2 0 1";"-"
"0 5294 0 45911 0 04706 0 154089";"-"
"b1 b1 eta b1grad1 b1grad2 0 1 0 1 0 01667 0 0374";"-"
"0 098333 0 09626";"-"
"b2 b2 eta b2grad 1 0 0 1 0 765 0 9235";"-"
"testing output";"IRRE"
"h1 g x1 w i1 b11 g 0 6 0 098 0 8 0 2956664";"-"
"0 7 0 4939998 0 098333 0 677";"-"
"h2 g x2 w i2 b12 g 0 6 0 195756 0 8 0 096008";"-"
"0 7 0 002244 0 09626 0 572";"-"
"o1 h w2 b21 0 677 0 04706";"-"
"0 572 0 154089 0 9235 1 043";"-"
"prob sigmoid o1 0 739";"CODE"
"test gradient";"IRRE"
"this makes sure that the activation functions and their derivatives";"CODE"
"are correct the numerical and analytical computation of the gradient";"-"
"should be close";"CODE"
"analytically compute the gradients";"IRRE"
"numerically compute the gradients";"IRRE"
"test lbfgs on classification";"IRRE"
"it should achieve a score higher than 0 95 for the binary and multi class";"CODE"
"versions of the digits dataset";"IRRE"
"test lbfgs on the regression dataset";"IRRE"
"non linear models perform much better than linear bottleneck";"CODE"
"test lbfgs parameter max fun";"IRRE"
"it should independently limit the number of iterations for lbfgs";"CODE"
"classification tests";"IRRE"
"test lbfgs parameter max fun";"IRRE"
"it should independently limit the number of iterations for lbfgs";"CODE"
"regression tests";"IRRE"
"tests that warm start reuse past solutions";"IRRE"
"test that multi label classification works as expected";"IRRE"
"test fit method";"IRRE"
"test partial fit method";"IRRE"
"make sure early stopping still work now that splitting is stratified by";"TASK"
"default it is disabled for multilabel classification";"CODE"
"test that multi output regression works as expected";"IRRE"
"tests that passing different classes to partial fit raises an error";"IRRE"
"test partial fit on classification";"IRRE"
"partial fit should yield the same results as fit for binary and";"IRRE"
"multi class classification";"IRRE"
"non regression test for bug 6994";"IRRE"
"tests for labeling errors in partial fit";"IRRE"
"test partial fit on regression";"IRRE"
"partial fit should yield the same results as fit for regression";"IRRE"
"test partial fit error handling";"IRRE"
"no classes passed";"IRRE"
"lbfgs doesn t support partial fit";"CODE"
"check that mlpregressor throws valueerror when dealing with non finite";"IRRE"
"parameter values";"IRRE"
"runtimewarning overflow encountered in square";"CODE"
"test that predict proba works as expected for binary class";"IRRE"
"test that predict proba works as expected for multi class";"IRRE"
"test that predict proba works as expected for multilabel";"IRRE"
"multilabel should not use softmax which makes probabilities sum to 1";"-"
"test that the shuffle parameter affects the training process it should";"IRRE"
"the coefficients will be identical if both do or do not shuffle";"CODE"
"the coefficients will be slightly different if shuffle true";"CODE"
"test that sparse and dense input matrices output the same results";"IRRE"
"test tolerance";"IRRE"
"it should force the solver to exit the loop when it converges";"IRRE"
"test verbose";"IRRE"
"check that the attributes validation scores and best validation score";"META"
"are set to none when early stopping false";"IRRE"
"no error raised";"CODE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 16812";"CODE"
"check that the mlp estimator accomplish max iter with a";"-"
"warm started estimator";"-"
"test n iter no change using binary data set";"IRRE"
"the classifying fitting process is not prone to loss curve fluctuations";"IRRE"
"test multiple n iter no change";"IRRE"
"validate n iter no change";"-"
"test n iter no change using binary data set";"IRRE"
"the fitting process should go to max iter iterations";"-"
"set a ridiculous tolerance";"IRRE"
"this should always trigger update no improvement count";"CODE"
"fit";"-"
"validate n iter no change doesn t cause early stopping";"CODE"
"validate update no improvement count was always triggered";"TASK"
"make sure data splitting for early stopping is stratified";"CODE"
"compare predictions for different dtypes";"IRRE"
"checks if input dtype is used for network parameters";"CODE"
"and predictions";"-"
"fit on x 2 y 4";"-"
"dump and load model";"CODE"
"train for a more epochs on point x 2 y 1";"CODE"
"finetuned model learned the new target";"CODE"
"unfortunately we can t set a zero hidden layer size so we use a trick by using";"IRRE"
"just one hidden layer node with an identity activation coefficients will";"-"
"therefore be different but predictions are the same";"META"
"the same does not work with the squared error because the output activation is";"IRRE"
"the identity instead of the exponential";"CODE"
"in place tricks shouldn t have modified x";"-"
"bernoullirbm should work on small sparse matrices";"IRRE"
"bernoullirbm fit x no exception";"CODE"
"xxx this test is very seed dependent it probably needs to be rewritten";"CODE"
"gibbs on the rbm hidden layer should be able to recreate 0 1";"IRRE"
"from the same input";"CODE"
"you need that much iters";"-"
"gibbs on the rbm hidden layer should be able to recreate 0 1 from";"CODE"
"the same input even when the input is sparse and test against non sparse";"IRRE"
"check if we don t get nans sampling the full digits dataset";"IRRE"
"also check that sampling again will yield different results";"IRRE"
"test score samples pseudo likelihood method";"IRRE"
"assert that pseudo likelihood is computed without clipping";"CODE"
"see fabian s blog http bit ly 1iyefrk";"CODE"
"sparse vs dense should not affect the output also test sparse input";"IRRE"
"validation";"-"
"test numerical stability 2785 would previously generate infinities";"IRRE"
"and crash with an exception";"CODE"
"pytest mark thread unsafe manually captured stdout";"CODE"
"make sure rbm works with sparse input when verbose true";"IRRE"
"make sure the captured standard output is sound";"IRRE"
"dtype in and dtype out should be consistent";"-"
"float 64 transformer";"CODE"
"float 32 transformer";"CODE"
"results and attributes should be close enough in 32 bit and 64 bit";"IRRE"
"the pipeline can be used as any other estimator";"CODE"
"and avoids leaking the test set into the train set";"IRRE"
"an estimator s parameter can be set using syntax";"IRRE"
"warning the sparse tag can be incorrect";"IRRE"
"some pipelines accepting sparse data are wrongly tagged sparse false";"IRRE"
"for example pipeline pca estimator accepts sparse data";"CODE"
"even if the estimator doesn t as pca outputs a dense array";"IRRE"
"this happens when the steps is not a list of name estimator";"CODE"
"tuples and fit is not called yet to validate the steps";"TASK"
"this happens when the steps is not a list of name estimator";"CODE"
"tuples and fit is not called yet to validate the steps";"TASK"
"delegate to first step which will call check is fitted";"IRRE"
"first find the last step that is not passthrough";"-"
"all steps are passthrough so the pipeline is considered fitted";"CODE"
"check if the last step of the pipeline is fitted";"IRRE"
"we only check the last step since if the last step is fit it";"-"
"means the previous steps should also be fit this is faster than";"CODE"
"checking if every step of the pipeline is fit";"CODE"
"is an estimator";"-"
"first we add all steps except the last one";"TASK"
"fit fit predict and fit transform call fit transform if it";"CODE"
"exists or else fit and transform";"CODE"
"then we add the last step";"TASK"
"an estimator s parameter can be set using syntax";"IRRE"
"todo slep6 remove when metadata routing cannot be disabled";"TASK"
"all transformers are none";"CODE"
"check if xs dimensions are valid";"IRRE"
"x is passed to all transformers delegate to the first one";"CODE"
"delegate whether feature union was fitted";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"in scikit learn variance is always computed using float64 accumulators";"CODE"
"if we are fitting on 1d arrays scale might be a scalar";"-"
"scale is an array";"-"
"detect near constant values to avoid dividing by a very small";"IRRE"
"value that could lead to surprising results and numerical";"IRRE"
"stability issues";"-"
"new array to avoid side effects";"CODE"
"xr is a view on the original array that enables easy use of";"-"
"broadcasting on the axis in which we are interested in";"CODE"
"verify that mean 1 is close to zero if x contains very";"IRRE"
"large values mean 1 can also be very large due to a lack of";"IRRE"
"precision of mean in this case a pre scaling of the";"CODE"
"concerned feature is efficient for instance by its mean or";"TASK"
"maximum";"-"
"if mean 2 is not close to zero it comes from the fact that";"CODE"
"scale is very small so that mean 2 mean 1 scale 0 even";"-"
"if mean 1 was close to zero the problem is thus essentially";"IRRE"
"due to the lack of precision of mean a solution is then to";"-"
"subtract the mean again";"-"
"checking one attribute is enough because they are all set together";"IRRE"
"in partial fit";"-"
"reset internal state before fitting";"CODE"
"unlike the scaler object this function allows 1d input";"CODE"
"if copy is required it will be done inside the scaler object";"CODE"
"checking one attribute is enough because they are all set together";"IRRE"
"in partial fit";"-"
"reset internal state before fitting";"CODE"
"even in the case of with mean false we update the mean anyway";"CODE"
"this is needed for the incremental computation of the var";"CODE"
"see incr mean variance axis and incremental mean variance axis";"CODE"
"if n samples seen is an integer i e no missing values we need to";"IRRE"
"transform it to an array of shape n features required by";"CODE"
"incr mean variance axis and incremental variance axis";"CODE"
"first pass";"-"
"next passes";"-"
"we force the mean and variance to float64 for large arrays";"CODE"
"see https github com scikit learn scikit learn pull 12338";"CODE"
"elf mean none as with mean must be false for sparse";"TASK"
"first pass";"-"
"for backward compatibility reduce n samples seen to an integer";"CODE"
"if the number of samples is the same for each feature i e no";"CODE"
"missing values";"IRRE"
"extract the list of near constant features on the raw variances";"CODE"
"before taking the square root";"CODE"
"checking one attribute is enough because they are all set together";"IRRE"
"in partial fit";"-"
"reset internal state before fitting";"CODE"
"unlike the scaler object this function allows 1d input";"CODE"
"if copy is required it will be done inside the scaler object";"CODE"
"at fit convert sparse matrices to csc for optimized computation of";"IRRE"
"the quantiles";"-"
"else axis 1";"-"
"todo this should be refactored because binarize also calls";"CODE"
"check array";"-"
"x is called k in these methods";"IRRE"
"for inverse transform match a uniform distribution";"CODE"
"with np errstate invalid ignore hide nan comparison warnings";"OUTD"
"else output distribution is already a uniform distribution";"META"
"find index for lower and higher bounds";"IRRE"
"with np errstate invalid ignore hide nan comparison warnings";"OUTD"
"interpolate in one direction and in the other and take the";"CODE"
"mean this is in case of repeated values in the features";"CODE"
"and hence repeated quantiles";"-"
"if we don t do this only one extreme of the duplicated is";"CODE"
"used the upper when we do ascending and the";"CODE"
"lower for descending we take the mean of these two";"CODE"
"for forward transform match the output distribution";"CODE"
"with np errstate invalid ignore hide nan comparison warnings";"OUTD"
"find the value to clip the data to avoid mapping to";"CODE"
"infinity clip such that the inverse transform will be";"IRRE"
"consistent";"-"
"else output distribution is uniform and the ppf is the";"IRRE"
"identity function so we let x col unchanged";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"else self dtype is none";"CODE"
"take a subsample of x";"-"
"when resampling it is important to subsample with replacement to";"CODE"
"preserve the distribution in particular in the presence of a few data";"META"
"points with large weights you can check this by setting replace false";"CODE"
"in sklearn utils test test indexing test resample weighted and check that";"IRRE"
"it fails as a justification for this claim";"CODE"
"since we already used the weights when resampling when provided";"CODE"
"we set them back to none to avoid accounting for the weights twice";"CODE"
"in subsequent operations to compute weight aware bin edges with";"-"
"quantiles or k means";"-"
"todo 1 9 remove and switch to quantile method averaged inverted cdf";"CODE"
"by default";"CODE"
"prepare a mask to filter out zero weight samples when extracting";"-"
"the min and max values of each columns which are needed for the";"IRRE"
"uniform and kmeans strategies";"CODE"
"otherwise all samples are used use a slice to avoid creating a";"CODE"
"new array";"CODE"
"method linear is the implicit default for any numpy";"CODE"
"version so we keep it version independent in that case by";"CODE"
"using an empty param dict";"-"
"from sklearn cluster import kmeans fixes import loops";"CODE"
"deterministic initialization with uniform spacing";"CODE"
"1d k means procedure";"-"
"must sort centers may be unsorted even with sorted init";"IRRE"
"remove bins whose width are too small i e 1e 8";"-"
"fit the onehotencoder with toy datasets";"IRRE"
"so that it s ready for use after the kbinsdiscretizer is fitted";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"if not a dataframe do normal check array validation";"CODE"
"pandas dataframe do validation later column by column in order";"CODE"
"to keep the dtype information to be used in the encoder";"CODE"
"always convert string categories to objects to avoid";"CODE"
"unexpected string truncation for longer category labels";"CODE"
"passed in the constructor";"CODE"
"nan must be the last stated category";"TASK"
"f in column i";"-"
"if there are nans nan should be the last element";"-"
"nan values can only be placed in the latest position";"IRRE"
"set the problematic rows to an acceptable value and";"IRRE"
"continue the rows are marked x mask and will be";"CODE"
"removed later";"OUTD"
"cast xi into the largest string type necessary";"CODE"
"to handle different lengths of numpy strings";"CODE"
"categories are objects and xi are numpy strings";"CODE"
"cast xi to an object dtype to prevent truncation";"CODE"
"when setting invalid values";"IRRE"
"we use check unknown false since check unknown was";"-"
"already called above";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"dataframes can have multiple dtypes";"-"
"not all dtypes are numpy dtypes they can be pandas dtypes as well";"-"
"check the consistency between the column provided by transform and";"CODE"
"the column names provided by get feature names out";"TASK"
"we can override the column names of the output if it is inconsistent";"IRRE"
"with the column names provided by get feature names out in the";"TASK"
"following cases";"CODE"
"func preserved the column names between the input and the output";"IRRE"
"the input column names are all numbers";"CODE"
"the output is requested to be a dataframe pandas or polars";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"transform of empty array is empty array";"CODE"
"inverse transform of empty array is empty array";"IRRE"
"xxx workaround that will be removed when list of list format is";"OUTD"
"dropped";"-"
"to account for pos label 0 in the dense case";"CODE"
"pick out the known labels from y";"CODE"
"preserve label ordering";"-"
"find the argmax for each row in y where y is a csr matrix";"CODE"
"picks out all indices obtaining the maximum per row";"-"
"for corner case where last row has a max of 0";"CODE"
"gets the index of the first argmax in each row from y i all argmax";"CODE"
"first argmax of each row";"-"
"handle rows of all 0";"-"
"handles rows with max of 0 that contain negative numbers";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"pragma nocover";"-"
"we also record the number of output features for";"TASK"
"min degree 0";"-"
"edge case deal with empty matrix";"CODE"
"do as if min degree 0 and cut down array after the";"CODE"
"computation i e use n out full instead of n output features";"TASK"
"what follows is a faster implementation of";"TASK"
"for i comb in enumerate combinations";"CODE"
"xp i x comb prod 1";"-"
"this implementation uses two optimisations";"TASK"
"first one is broadcasting";"-"
"multiply x1 xn x1 x1 x1 xn x1";"-"
"multiply x2 xn x2 x2 x2 xn x2";"-"
"multiply x start end x start";"CODE"
"second optimisation happens for degrees 3";"CODE"
"xi 3 is computed reusing previous computation";"-"
"xi 3 xi 2 xi";"-"
"degree 0 term";"-"
"degree 1 term";"-"
"loop over degree 2 terms";"IRRE"
"xp start end are terms of degree d 1";"CODE"
"that exclude feature feature idx";"TASK"
"numpy performs this multiplication in place";"CODE"
"knots uniform";"CODE"
"note that the variable knots has already been validated and";"CODE"
"else is therefore safe";"CODE"
"disregard observations with zero weight";"CODE"
"number of knots for base interval";"CODE"
"number of splines basis functions";"CODE"
"periodic splines have self degree less degrees of freedom";"CODE"
"we have to add degree number of knots below and degree number knots";"TASK"
"above the base knots in order to make the spline basis complete";"CODE"
"for periodic splines the spacing of the first last degree knots";"CODE"
"needs to be a continuation of the spacing of the last first";"TASK"
"base knots";"-"
"eilers marx in flexible smoothing with b splines and";"-"
"penalties https doi org 10 1214 ss 1038425655 advice";"CODE"
"against repeating first and last knot several times which";"-"
"would have inferior behaviour at boundaries if combined with";"-"
"a penalty hence p spline we follow this advice even if our";"CODE"
"splines are unpenalized meaning we do not";"CODE"
"knots np r";"-"
"np tile base knots min axis 0 reps degree 1";"-"
"base knots";"-"
"np tile base knots max axis 0 reps degree 1";"-"
"instead we reuse the distance of the 2 fist last knots";"IRRE"
"with a diagonal coefficient matrix we get back the spline basis";"-"
"elements i e the design matrix of the spline";"-"
"note bspline appreciates c contiguous float64 arrays as c coef";"TASK"
"note that scipy bspline returns float64 arrays and converts input";"CODE"
"x x i to c contiguous float64";"CODE"
"get indicator for nan values in the current column";"IRRE"
"with periodic extrapolation we map x to the segment";"-"
"spl t k spl t n";"-"
"this is equivalent to bspline extrapolate periodic";"CODE"
"for scipy 1 0 0";"CODE"
"assign to new array to avoid inplace operation";"CODE"
"this can happen if the column has a single non nan";"CODE"
"value treat as a constant feature";"TASK"
"else self extrapolation in continue error";"CODE"
"we replace the nan values in the input column by some";"IRRE"
"arbitrary in range numerical value since";"IRRE"
"bspline design matrix would otherwise raise on any nan";"CODE"
"value in its input the spline encoded values in";"IRRE"
"the output of that function that correspond to missing";"IRRE"
"values in the original input will be replaced by 0 0";"IRRE"
"afterwards";"-"
"note that in the following we use np nanmin x as the";"TASK"
"input replacement to make sure that this code works even";"CODE"
"when extrapolation error any other choice of";"-"
"in range value would have worked work since the";"IRRE"
"corresponding values in the array are replaced by zeros";"IRRE"
"the column is all np nan valued replace it by a";"IRRE"
"constant column with an arbitrary non nan value";"IRRE"
"inside so that it is encoded as constant column";"CODE"
"x np zeros like x avoid mutation of input data";"CODE"
"x x copy avoid mutation of input data";"CODE"
"note self bsplines 0 extrapolate is true for extrapolation in";"CODE"
"periodic continue";"CODE"
"see the construction of coef in fit we need to add the last";"TASK"
"degree spline basis function to the first degree ones and";"CODE"
"then drop the last ones";"CODE"
"note see comment about sparseefficiencywarning below";"TASK"
"note see comment about sparseefficiencywarning below";"TASK"
"replace any indicated values with 0";"IRRE"
"else extrapolation in constant linear";"CODE"
"spline values at boundaries";"IRRE"
"values outside of the feature range during fit and nan values get";"IRRE"
"filtered out";"CODE"
"set to some arbitrary value within the range of values";"IRRE"
"observed on the training set before calling";"IRRE"
"bspline design matrix those transformed will be";"IRRE"
"reassigned later when handling with extrapolation";"IRRE"
"note without converting to lil matrix we would get";"TASK"
"scipy sparse base sparseefficiencywarning changing the sparsity";"IRRE"
"structure of a csr matrix is expensive lil matrix is more";"CODE"
"efficient";"-"
"note for extrapolation";"TASK"
"continue is already returned as is by scipy bsplines";"CODE"
"early convert to csr as the sparsity structure of this";"CODE"
"block should not change anymore this is needed to be able";"OUTD"
"to safely assume that data is a 1d array";"-"
"set all values beyond xmin and xmax to the value of the";"IRRE"
"spline basis functions at those two positions";"IRRE"
"only the first degree and last degree number of splines";"CODE"
"have non zero values at the boundaries";"IRRE"
"note see comment about sparseefficiencywarning above";"TASK"
"note see comment about sparseefficiencywarning above";"TASK"
"continue the degree first and degree last spline bases";"CODE"
"linearly beyond the boundaries with slope derivative at";"-"
"the boundary";"-"
"note that all others have derivative value 0 at the";"TASK"
"boundaries";"-"
"spline derivatives slopes at boundaries";"-"
"compute the linear continuation";"-"
"for degree 1 the derivative of 2nd spline is not zero at";"CODE"
"boundary for degree 0 it is the same as constant";"CODE"
"note see comment about sparseefficiencywarning above";"TASK"
"note see comment about sparseefficiencywarning above";"TASK"
"we throw away one spline basis per feature";"TASK"
"we chose the last one";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"from sklearn model selection import avoid circular import";"CODE"
"the cv splitter is voluntarily restricted to kfold to enforce non";"CODE"
"overlapping validation folds otherwise the fit transform output will";"IRRE"
"not be well specified";"-"
"if multiclass multiply axis 1 by num classes else keep shape the same";"IRRE"
"if multiclass multiply axis 1 by num of classes else keep shape the same";"IRRE"
"repeat feature indices by n classes";"TASK"
"cycle through each class";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"make some data to be used many times";"-"
"sample weights must be either scalar or 1d";"TASK"
"make sure error is raised the sample weights greater than 1d";"CODE"
"weighted standardscaler";"-"
"unweighted but with repeated samples";"META"
"n b the sample statistics for xw w sample weight should match";"CODE"
"the statistics of x w uniform sample weight";"CODE"
"test array api support and correctness";"IRRE"
"unweighted but with repeated samples";"META"
"check that both array api outputs match";"IRRE"
"test scaling of dataset along single axis";"IRRE"
"x np array x cast only after scaling done";"CODE"
"check inverse transform";"IRRE"
"constant feature";"TASK"
"ensure scaling does not affect dtype";"CODE"
"scipy sparse containers do not support float16 see";"CODE"
"https github com scipy scipy issues 7408 for more details";"CODE"
"caler clone scaler avoid side effects from previous tests";"CODE"
"the variance info should be close to zero for constant features";"CODE"
"constant features should not be scaled scale of 1";"TASK"
"assert x scaled is not x make sure we make a copy";"CODE"
"also check consistency with the standard scale function";"CODE"
"assert x scaled 2 is not x make sure we did a copy";"CODE"
"check that when the variance is too small var mean 2 the feature";"CODE"
"is considered constant and not scaled";"CODE"
"make a dataset of known var scales 2 and mean average";"IRRE"
"standardscaler uses float64 accumulators even if the data has a float32";"CODE"
"dtype";"-"
"if var bound n eps var n eps mean the feature is considered";"CODE"
"constant and the scale attribute is set to 1";"IRRE"
"check that scale min is small enough to have some scales below the";"-"
"bound and therefore detected as constant";"CODE"
"check that such features are actually treated as constant by the scaler";"TASK"
"depending the on the dtype of x some features might not actually be";"TASK"
"representable as non constant for small scales even if above the";"CODE"
"precision bound of the float64 variance estimate such feature should";"CODE"
"be correctly detected as constants with 0 variance by standardscaler";"CODE"
"the other features are scaled and scale is equal to sqrt var assuming";"TASK"
"that scales are large enough for average scale and average scale to";"CODE"
"be distinct in x depending on x s dtype";"TASK"
"1 d inputs";"CODE"
"test numerical stability of scaling";"IRRE"
"np log 1e 5 is taken because of its floating point representation";"CODE"
"was empirically found to cause numerical problems with np mean np std";"CODE"
"this does not raise a warning as the number of samples is too low";"CODE"
"to trigger the problem in recent numpy";"-"
"with 2 more samples the std computation run into numerical issues";"CODE"
"large values can cause often recoverable numerical stability issues";"IRRE"
"test scaling of 2d array along first axis";"IRRE"
"x 0 0 0 first feature is always of zero";"TASK"
"check that x has been copied";"-"
"check inverse transform";"IRRE"
"check that the data hasn t been modified";"-"
"check that x has not been copied";"-"
"x 0 1 0 first feature is a constant non zero feature";"TASK"
"check that x has not been copied";"-"
"test if the scaler will not overflow on float16 numpy arrays";"IRRE"
"float16 has a maximum of 65500 0 on the worst case 5 200000 is 100000";"CODE"
"which is enough to overflow the data type";"-"
"calculate the float64 equivalent to verify result";"IRRE"
"overflow calculations may cause inf inf or nan since there is no nan";"CODE"
"input all of the outputs should be finite this may be redundant since a";"IRRE"
"floatingpointerror exception will be thrown on overflow above";"CODE"
"the normal distribution is very unlikely to go above 4 at 4 0 8 0 the";"META"
"float16 precision is 2 8 which is around 0 004 thus only 2 decimals are";"CODE"
"checked to account for precision differences";"CODE"
"test if partial fit run over many batches of size 1 and 50";"IRRE"
"gives the same results as fit";"IRRE"
"test mean at the end of the process";"CODE"
"test std after 1 step";"IRRE"
"test std until the end of partial fits and";"CODE"
"caler incr minmaxscaler clean estimator";"TASK"
"test if partial fit run over many batches of size 1 and 50";"IRRE"
"gives the same results as fit";"IRRE"
"test mean at the end of the process";"CODE"
"assert scaler batch var scaler incr var nones";"CODE"
"test std after 1 step";"IRRE"
"no constants";"CODE"
"test std until the end of partial fits and";"CODE"
"caler incr standardscaler clean estimator";"TASK"
"test if the incremental computation introduces significative errors";"IRRE"
"for large datasets with values of large magniture";"IRRE"
"regardless of abs values they must not be more diff 6 significant digits";"IRRE"
"note be aware that for much larger offsets std is very unstable last";"CODE"
"assert while mean is ok";"CODE"
"sparse input";"IRRE"
"with mean false is required with sparse input";"CODE"
"sparse arrays can be 1d in scipy 1 14 and later while old";"IRRE"
"sparse matrix instances are always 2d";"IRRE"
"regardless of magnitude they must not differ more than of 6 digits";"-"
"check that sparsity is not destroyed";"-"
"check some postconditions after applying partial fit and transform";"CODE"
"assert array almost equal x sofar chunks copy no change";"CODE"
"assert array less zero scaler incr var epsilon as less or equal";"CODE"
"i 1 because the scaler has been already fitted";"IRRE"
"check if standardscaler inverse transform is";"IRRE"
"converting the integer array to float";"CODE"
"the of inverse transform should be converted";"IRRE"
"to a float array";"CODE"
"if not x self scale will fail";"CODE"
"default params";"CODE"
"not default params min 1 max 2";"CODE"
"min 5 max 6";"-"
"raises on invalid range";"OUTD"
"check min max scaler on toy data with zero variance features";"TASK"
"default params";"CODE"
"not default params";"CODE"
"function interface";"CODE"
"test scaling of dataset along single axis";"IRRE"
"x np array x cast only after scaling done";"CODE"
"check inverse transform";"IRRE"
"constant feature";"TASK"
"function interface";"CODE"
"x 0 0 0 first feature is always of zero";"TASK"
"check that x has not been modified copy";"-"
"test that the scaler return identity when with mean and with std are";"IRRE"
"false";"-"
"test that scaler converts integer input to floating";"CODE"
"for both sparse and dense matrices";"IRRE"
"x 0 0 first feature is always of zero";"TASK"
"check that x has not been modified copy";"-"
"check that standardscaler fit does not change input";"CODE"
"x 0 0 0 first feature is always of zero";"TASK"
"check scaling and fit with direct calls on sparse data";"IRRE"
"check transform and inverse transform after a fit on a dense array";"CODE"
"check if non finite inputs raise valueerror";"IRRE"
"check consistent type of attributes";"META"
"check that the scaler is working when there is not data materialized in a";"-"
"column of a sparse matrix";"IRRE"
"test robust scaling of 2d array along first axis";"IRRE"
"x 0 0 0 first feature is always of zero";"TASK"
"check the equivalence of the fitting with dense and sparse matrices";"IRRE"
"check robustscaler on transforming csr matrix with one row";"CODE"
"uniform output distribution";"IRRE"
"normal output distribution";"IRRE"
"make sure it is possible to take the inverse of a sparse matrix";"IRRE"
"which contain negative value this is the case in the iris dataset";"CODE"
"check that an error is raised if input is scalar";"CODE"
"check that a warning is raised is n quantiles n samples";"CODE"
"dense case warning raise";"CODE"
"consider the case where sparse entries are missing values and user given";"IRRE"
"zeros are to be considered";"-"
"check in conjunction with subsampling";"-"
"using a uniform output each entry of x should be map between 0 and 1";"CODE"
"and equally spaced";"-"
"test that subsampling the input yield to a consistent results we check";"IRRE"
"that the computed quantiles are almost mapped to a 0 1 vector where";"-"
"values are equally spaced the infinite norm is checked to be smaller";"IRRE"
"than a given threshold this is repeated 5 times";"CODE"
"dense support";"-"
"each random subsampling yield a unique approximation to the expected";"IRRE"
"linspace cdf";"-"
"sparse support";"IRRE"
"each random subsampling yield a unique approximation to the expected";"IRRE"
"linspace cdf";"-"
"https github com scikit learn scikit learn issues 23319 issuecomment 1464933635";"CODE"
"warnings simplefilter always ensure all warnings are captured";"-"
"full dataset should not trigger overflow in variance calculation";"IRRE"
"subset of data should not trigger overflow in power calculation";"IRRE"
"warn default will not warn when strategy quantile";"CODE"
"warn default will not warn when strategy quantile";"CODE"
"warn default will not warn when strategy quantile";"CODE"
"warn default will not warn when strategy quantile";"CODE"
"warn default will not warn when strategy quantile";"CODE"
"warn default will not warn when strategy quantile";"CODE"
"ignore the warning on removed small bins";"OUTD"
"bad shape";"-"
"incorrect number of features";"TASK"
"bad bin values";"IRRE"
"float bin values";"IRRE"
"warn default will not warn when strategy quantile";"CODE"
"warn default will not warn when strategy quantile";"CODE"
"warn default will not warn when strategy quantile";"CODE"
"test the shape of bin edges";"IRRE"
"replace the feature with zeros";"TASK"
"test up to discretizing nano units";"IRRE"
"with 2 bins";"-"
"with 3 bins";"-"
"with 5 bins";"-"
"warn default will not warn when strategy quantile";"CODE"
"warn default will not warn when strategy quantile";"CODE"
"todo change to averaged inverted cdf but that means we only get bin";"TASK"
"edges of 0 05 and 0 95 and nothing in between";"-"
"test output dtype";"IRRE"
"wrong numeric input dtype are cast in np float64";"CODE"
"todo this check is redundant with common checks and can be removed";"CODE"
"once 16290 is merged";"-"
"32 bit output";"IRRE"
"64 bit output";"IRRE"
"since the size of x is small 2e5 subsampling will not take place";"-"
"check that the bin edges are almost the same when subsampling is used";"-"
"we use a large tolerance because we can t expect the bin edges to be exactly the";"-"
"same when subsampling is used";"-"
"check that sparse and dense will give the same results";"IRRE"
"check outcome";"-"
"test that one hot encoder raises error for unknown features";"CODE"
"present during transform";"CODE"
"test the ignore option ignores unknown features giving all 0 s";"TASK"
"ensure transformed data was not modified in place";"CODE"
"non regression test for the issue 12470";"IRRE"
"test the ignore option when categories are numpy string dtype";"IRRE"
"particularly when the known category strings are larger";"CODE"
"than the unknown category strings";"CODE"
"ensure transformed data was not modified in place";"CODE"
"last value is np nan";"IRRE"
"do not include the last column which includes missing values";"CODE"
"check last column is the missing value";"IRRE"
"check handle unknown ignore";"-"
"a is dropped";"-"
"x 0 1 and 2 are infrequent";"-"
"x 1 1 and 10 are infrequent";"-"
"x 2 nothing is infrequent";"-"
"infrequent is used to denote the infrequent categories";"TASK"
"for the first column 1 and 2 have the same frequency in this case";"CODE"
"1 will be chosen to be the feature name because is smaller lexiconically";"TASK"
"x 2 does not have an infrequent category thus it is encoded as all";"CODE"
"zeros";"-"
"error for unknown categories";"CODE"
"only infrequent or known categories";"-"
"both categories are unknown";"-"
"inverse transform maps to none";"IRRE"
"checks pandas dataframe with categorical features";"TASK"
"c is unknown and is mapped to np nan";"-"
"none is a missing value and is set to 3";"IRRE"
"non regression test for 24082";"IRRE"
"np nan is unknown so it maps to none";"-"
"3 is the encoded missing value so it maps back to nan";"IRRE"
"the 0 th feature has no missing values so it is not included in the list of";"CODE"
"features";"TASK"
"missing value is not in training set";"IRRE"
"inverse transform will considering encoded nan as unknown";"IRRE"
"missing value in training set";"IRRE"
"inverse transform will considering encoded nan as missing";"IRRE"
"both nan and unknown are encoded as nan";"-"
"x 0 a b c have the same frequency a and b will be";"-"
"considered infrequent because they appear first when sorted";"IRRE"
"x 1 0 3 5 10 has frequency 2 and 12 has frequency 1";"-"
"0 3 12 will be considered infrequent because they appear first when";"IRRE"
"sorted";"-"
"x 2 snake and bird or infrequent";"CODE"
"args kwargs store will hold the positional and keyword arguments";"IRRE"
"passed to the function inside the functiontransformer";"CODE"
"the function should only have received x";"CODE"
"reset the argument stores";"IRRE"
"the function should have received x";"CODE"
"test that the numpy log example still works";"TASK"
"test that rounding is correct";"IRRE"
"test that rounding is correct";"IRRE"
"test that rounding is correct";"IRRE"
"test that inverse transform works correctly";"IRRE"
"check that we don t check inverse when one of the func or inverse is not";"CODE"
"provided";"-"
"does not raise an error";"CODE"
"numpy inputs default behavior generate names";"CODE"
"pandas input default behavior use input feature names";"CODE"
"numpy input feature names out callable";"TASK"
"pandas input feature names out callable";"TASK"
"numpy input feature names out callable default input features";"CODE"
"pandas input feature names out callable default input features";"CODE"
"numpy input input features list of names";"CODE"
"pandas input input features list of names";"CODE"
"a b must match feature names in";"TASK"
"numpy input feature names out callable input features list";"TASK"
"pandas input feature names out callable input features list";"TASK"
"a b must match feature names in";"TASK"
"no warning is raised when feature names out is defined";"CODE"
"no warning is raised when func returns a panda dataframe";"CODE"
"warning is raised when func returns a ndarray";"CODE"
"default transform does not warn";"CODE"
"standardscaler will convert to a numpy array";"-"
"one class case defaults to negative label";"CODE"
"for dense case";"CODE"
"for sparse case";"CODE"
"two class case";"CODE"
"multi class case";"CODE"
"two class case with pos label 0";"CODE"
"multi class case";"CODE"
"calling unique creates a pandas array which has a different interface";"IRRE"
"compared to a pandas series specifically pandas arrays do not have iloc";"IRRE"
"check that invalid arguments yield valueerror";"IRRE"
"sequence of seq type should raise valueerror";"IRRE"
"fail on the dimension of binary";"-"
"fail on multioutput data";"IRRE"
"fail on y type";"-"
"fail on the number of classes";"IRRE"
"test labelencoder s transform fit transform and";"CODE"
"inverse transform methods";"IRRE"
"check that invalid arguments yield valueerror";"IRRE"
"fail on unseen labels";"-"
"fail on inverse transform";"IRRE"
"test empty transform";"IRRE"
"test empty inverse transform";"IRRE"
"test input as iterable of iterables";"IRRE"
"with fit transform";"CODE"
"verify csr assumption that indices and indptr have same dtype";"-"
"with fit";"-"
"verify csr assumption that indices and indptr have same dtype";"-"
"test input as iterable of iterables";"IRRE"
"with fit transform";"CODE"
"with fit";"-"
"fit transform";"CODE"
"fit transform";"CODE"
"ensure works with extra class";"IRRE"
"ensure fit is no op as iterable is not consumed";"-"
"ensure a valueerror is thrown if given duplicate classes";"CODE"
"first call";"IRRE"
"second call change class";"IRRE"
"ensure sequences of the same length are not interpreted as a 2 d array";"CODE"
"fit transform";"CODE"
"fit transform";"CODE"
"fit transform";"CODE"
"fit transform";"CODE"
"not binary";"-"
"the following binary cases are fine however";"CODE"
"wrong shape";"META"
"modified class order";"IRRE"
"check label binarize";"-"
"check inverse";"-"
"check label binarizer";"-"
"binary case where sparse output true will not result in a valueerror";"IRRE"
"make the boundaries 0 and 1 part of x train for sure";"CODE"
"n knots n knots degree periodic splines require degree n knots";"CODE"
"use periodic extrapolation backport in splinetransformer";"CODE"
"use periodic extrapolation in bspline";"-"
"we expect splines of degree degree to be degree 1 times";"-"
"continuously differentiable i e for d 0 degree 1 the d th";"CODE"
"derivative should be continuous this is the case if the d 1 th";"CODE"
"numerical derivative is reasonably small smaller than tol in absolute";"-"
"value we thus compute d th numeric derivatives for d 1 degree";"IRRE"
"and compare them to tol";"IRRE"
"note that the 0 th derivative is the function itself such that we are";"CODE"
"also checking its continuity";"-"
"check continuity of the d 1 th derivative";"-"
"compute d th numeric derivative";"-"
"as degree degree splines are not degree times continuously";"-"
"differentiable at the knots the degree 1 th numeric derivative";"-"
"should have spikes at the knots";"-"
"though they should be exactly equal we test approximately with high";"IRRE"
"accuracy";"-"
"extrapolation regime";"-"
"test some unicode";"IRRE"
"this degree should always be one more than the highest degree supported by";"CODE"
"csr expansion";"-"
"an int64 dtype is required to avoid overflow error on windows within the";"CODE"
"degree 2 calc function";"CODE"
"calculate the number of combinations a priori and if needed check for";"CODE"
"the correct valueerror and terminate the test early";"IRRE"
"account for bias of all samples except last one which will be handled";"CODE"
"separately since there are distinct data values before it";"IRRE"
"ensure that dtype promotion was actually required";"META"
"needs promotion to int64 when interaction only false";"CODE"
"this guarantees that the intermediate operation when calculating";"CODE"
"output columns would overflow a c long hence checks that python";"CODE"
"longs are being used";"-"
"this case tests the second clause of the overflow check which";"IRRE"
"takes into account the value of n features itself";"CODE"
"use int32 indices as much as we can";"CODE"
"first degree index";"CODE"
"second degree index";"-"
"third degree index";"-"
"calculate the number of combinations a priori and if needed check for";"CODE"
"the correct valueerror and terminate the test early";"IRRE"
"terms higher than first degree";"CODE"
"convert to dense array if needed";"-"
"on windows scikit learn is typically compiled with msvc that";"CODE"
"does not support int128 arithmetic at the time of writing";"CODE"
"https stackoverflow com a 6761962 163740";"CODE"
"minimum needed to ensure integer overflow occurs while guaranteeing an";"CODE"
"int64 indexable output";"IRRE"
"first degree index";"CODE"
"second degree index";"-"
"third degree index";"-"
"manually compute encodings for cv splits to validate fit transform";"CODE"
"f idx 0 0 0 1 1 1";"-"
"c idx 0 1 2 0 1 2";"-"
"exp idx 0 1 2 3 4 5";"-"
"manually compute encoding to validate transform";"CODE"
"include unknown values at the end";"CODE"
"add unknown values at end";"IRRE"
"last row are unknowns dealt with later";"-"
"unknowns encoded as target mean for each class";"CODE"
"y mean contains target mean for each class thus cycle through mean of";"CODE"
"each class n features times";"TASK"
"np array 0 10 1 10 3 dtype np int64 t 3 is unknown";"CODE"
"t snake is unknown";"-"
"cardinality 30 not too large otherwise we need a very large n samples";"-"
"sort by y train to attempt to cause a leak";"CODE"
"check that no information about y train has leaked into x train";"CODE"
"it s impossible to learn a good predictive model on the training set when";"IRRE"
"using the original representation x train or the target encoded";"-"
"representation with shuffled inner cv for the latter no information";"CODE"
"about y train has inadvertently leaked into the prior used to generate";"OUTD"
"x encoded train shuffled";"-"
"without the inner cv shuffling a lot of information about y train goes into the";"CODE"
"the per fold y train mean priors shrinkage is no longer effective in this";"CODE"
"case and would no longer be able to prevent downstream over fitting";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"skip index generation if totally dense";"-"
"generate location of non zero elements";"-"
"find the indices of the non zero components for row i";"CODE"
"among non zero components the probability of the sign is 50 50";"-"
"build the csr structure by concatenating the rows";"CODE"
"very few components are non zero";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"coding utf8";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"kernel parameters";"IRRE"
"clamping factor";"-"
"note since predict does not accept semi supervised labels as input";"CODE"
"fit x y predict x fit x y transduction";"-"
"hence fit predict is not implemented";"TASK"
"see https github com scikit learn scikit learn pull 24898";"CODE"
"actual graph construction implementations should override this";"CODE"
"label construction";"CODE"
"construct a categorical distribution for classification only";"CODE"
"initialize distributions";"IRRE"
"labelpropagation";"-"
"labelspreading";"CODE"
"clamp";"-"
"set the transduction item";"IRRE"
"compute affinity matrix or gram matrix";"IRRE"
"laplacian flat n samples 1 0 0 set diag to 0 0";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we don t require predic proba here to allow passing a meta estimator";"CODE"
"that only exposes predict proba after fitting";"-"
"selftrainingclassifier estimator is not validated yet";"CODE"
"we need row slicing support for sparse matrices but costly finiteness check";"IRRE"
"can be delegated to the base estimator";"-"
"predict on the unlabeled samples";"-"
"select new labeled samples";"CODE"
"nb these are indices not a mask";"-"
"map selected indices into original array";"CODE"
"add newly labeled confident predictions to the dataset";"TASK"
"no changed labels";"-"
"metadata routing is enabled";"-"
"metadata routing is enabled";"-"
"metadata routing is enabled";"-"
"metadata routing is enabled";"-"
"metadata routing is enabled";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"load the iris dataset and randomly permute it";"IRRE"
"estimator clone estimator avoid side effects from previous tests";"CODE"
"check classification for various parameter settings";"IRRE"
"also assert that predictions for strings and numerical labels are equal";"CODE"
"also test for multioutput classification";"IRRE"
"check consistency between labeled iter n iter and max iter";"IRRE"
"assert that labeled samples have labeled iter 0";"CODE"
"assert that labeled samples do not change label during training";"CODE"
"assert that the max of the iterations is less than the total amount of";"CODE"
"iterations";"-"
"check shapes";"-"
"check labeled iter";"-"
"check that the all samples were labeled after a reasonable number of";"-"
"iterations";"-"
"estimator clone estimator avoid side effects from previous tests";"CODE"
"check classification for zero iterations";"CODE"
"fitting a selftrainingclassifier with zero iterations should give the";"CODE"
"same results as fitting a supervised classifier";"IRRE"
"this also asserts that string arrays work as expected";"CODE"
"test that passing a pre fitted classifier and calling predict throws an";"IRRE"
"error";"-"
"check that the amount of datapoints labeled in iteration 0 is equal to";"IRRE"
"the amount of labeled datapoints we passed";"CODE"
"check that the max of the iterations is less than the total amount of";"-"
"iterations";"-"
"test that training on a fully labeled dataset produces the same results";"IRRE"
"as training the classifier by itself";"CODE"
"assert that all samples were labeled in iteration 0 since there were no";"IRRE"
"unlabeled samples";"-"
"x 0 5 cannot be predicted on with a high confidence so training";"CODE"
"stops early";"-"
"tests that the labels added by st really are the 10 best labels";"TASK"
"check that a meta estimator relying on an estimator implementing";"TASK"
"predict proba will work even if it does not expose this method before being";"CODE"
"fitted";"-"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 19119";"CODE"
"svc with probability false does not implement predict proba that";"TASK"
"is required internally in fit of selftrainingclassifier we expect";"CODE"
"an attributeerror to be raised";"META"
"decisiontreeclassifier does not implement decision function and";"CODE"
"should raise an attributeerror";"META"
"metadata routing tests";"IRRE"
"make sure that the estimator thinks it is already fitted";"CODE"
"end of routing tests";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"from sklearn svm import liblinear as liblinear type ignore attr defined";"CODE"
"mypy error error module sklearn svm has no attribute libsvm";"META"
"and same for other imports";"CODE"
"from sklearn svm import libsvm as libsvm type ignore attr defined";"CODE"
"from sklearn svm import libsvm sparse as libsvm sparse type ignore attr defined";"CODE"
"get 1vs1 weights for all n n 1 classifiers";"CODE"
"this is somewhat messy";"CODE"
"shape of dual coef is nsv n classes 1";"IRRE"
"see docs for details";"CODE"
"xxx we could do preallocation of coef but";"META"
"would have to take care in the sparse case";"CODE"
"svs for class1";"CODE"
"svs for class1";"CODE"
"dual coef for class1 svs";"CODE"
"dual coef for class2 svs";"CODE"
"build weight for class1 vs class2";"CODE"
"the order of these must match the integer values in libsvm";"IRRE"
"xxx these are actually the same in the dense case need to factor";"CODE"
"this out";"CODE"
"used by cross val score";"-"
"input validation";"CODE"
"unused but needs to be a float for cython code that ignores";"CODE"
"it anyway";"-"
"var e x 2 e x 2 if sparse";"IRRE"
"see comment on the other call to np iinfo in this file";"CODE"
"in binary case we need to flip the sign of coef intercept and";"CODE"
"decision function use self intercept and self dual coef";"CODE"
"internally";"CODE"
"since in the case of svc and nusvc the number of models optimized by";"CODE"
"libsvm could be greater than one depending on the input n iter";"CODE"
"stores an ndarray";"-"
"for the other sub classes svr nusvr and oneclasssvm the number of";"CODE"
"models optimized by libsvm is always one so n iter stores an";"-"
"integer";"CODE"
"you must store a reference to x to compute the kernel in predict";"CODE"
"todo add keyword copy to copy on demand";"TASK"
"we don t pass self get params to allow subclasses to";"CODE"
"add other parameters to init";"IRRE"
"else regression";"-"
"precondition x is a csr matrix of dtype np float64";"CODE"
"c 0 0 c is not useful here";"-"
"svr and oneclass";"IRRE"
"n support has size 2 we make it size 1";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"test sample 1";"IRRE"
"test sample 2";"IRRE"
"use the original svm model for dense fit and clone an exactly same";"IRRE"
"svm model for sparse fit";"IRRE"
"xxx probability true is not thread safe";"CODE"
"https github com scikit learn scikit learn issues 31885";"CODE"
"mypy error module sklearn svm has no attribute libsvm";"META"
"from sklearn svm import type ignore attr defined";"CODE"
"toy sample";"-"
"test parameters on classes that make use of libsvm";"IRRE"
"xxx this test is thread unsafe because it uses libsvm cross validation";"CODE"
"https github com scikit learn scikit learn issues 31885";"CODE"
"check consistency on dataset iris";"IRRE"
"shuffle the dataset so that labels are not ordered";"IRRE"
"check also the low level api";"CODE"
"we unpack the values to create a dictionary with some of the return values";"IRRE"
"from libsvm s fit";"CODE"
"libsvm fit status and libsvm n iter won t be used below";"-"
"we unpack the values to create a dictionary with some of the return values";"IRRE"
"from libsvm s fit";"CODE"
"libsvm fit status and libsvm n iter won t be used below";"-"
"if random seed 0 the libsvm rng is seeded by calling srand hence";"IRRE"
"we should get deterministic results assuming that there is no other";"IRRE"
"thread calling this wrapper calling srand concurrently";"IRRE"
"svc with a precomputed kernel";"-"
"we test it with a toy dataset and with iris";"IRRE"
"gram matrix for train data square matrix";"CODE"
"we use just a linear kernel";"-"
"gram matrix for test data rectangular matrix";"CODE"
"gram matrix for test data but compute kt i j";"CODE"
"for support vectors j only";"CODE"
"same as before but using a callable function instead of the kernel";"CODE"
"matrix kernel is just a linear kernel";"-"
"test a precomputed kernel with the iris dataset";"IRRE"
"and check parameters against a linear svc";"IRRE"
"gram matrix for test data but compute kt i j";"CODE"
"for support vectors j only";"CODE"
"test support vector regression";"IRRE"
"non regression test previously baselibsvm would check that";"IRRE"
"len np unique y 2 which must only be done for svc";"CODE"
"check that svr kernel linear and linearsvc give";"IRRE"
"comparable results";"IRRE"
"check correct result when sample weight is 1";"IRRE"
"check that svr kernel linear and linearsvc give";"IRRE"
"comparable results";"IRRE"
"check that fit x fit x1 x2 x3 sample weight n1 n2 n3 where";"-"
"x x1 repeated n1 times x2 repeated n2 times and so forth";"CODE"
"bad kernel";"-"
"test oneclasssvm";"IRRE"
"todo rework this test to be independent of the random seeds";"CODE"
"test oneclasssvm decision function";"IRRE"
"generate train data";"-"
"generate some regular novel observations";"-"
"generate some abnormal novel observations";"-"
"fit the model";"-"
"predict things";"-"
"make sure some tweaking of parameters works";"IRRE"
"we change clf dual coef at run time and expect predict to change";"CODE"
"accordingly notice that this is not trivial since it involves a lot";"IRRE"
"of c python copying in the libsvm bindings";"CODE"
"the success of this test ensures that the mapping between libsvm and";"IRRE"
"the python classifier is complete";"CODE"
"xxx this test is thread unsafe because it uses probability true";"CODE"
"https github com scikit learn scikit learn issues 31885";"CODE"
"predict probabilities using svc";"-"
"this uses cross validation so we use a slightly bigger testing set";"IRRE"
"test decision function";"CODE"
"sanity check test that decision function implemented in python";"CODE"
"returns the same as the one in libsvm";"IRRE"
"multi class";"IRRE"
"binary";"-"
"kernel binary";"-"
"check that decision function shape ovr or ovo gives";"CODE"
"correct shape and is consistent with predict";"-"
"we need to use break ties here so that the prediction won t break ties randomly";"CODE"
"but use the argmax of the decision function";"IRRE"
"with five classes";"IRRE"
"check shape of ovo decision function true";"CODE"
"test svr s decision function";"IRRE"
"sanity check test that predict implemented in python";"TASK"
"returns the same as the one in libsvm";"IRRE"
"linear kernel";"-"
"rbf kernel";"-"
"todo rework this test to be independent of the random seeds";"CODE"
"test class weights";"IRRE"
"we give a small weights to class 1";"IRRE"
"so all predicted values belong to class 2";"IRRE"
"estimator base clone estimator avoid side effects from previous tests";"CODE"
"fit a linear svm and check that giving more weight to opposed samples";"-"
"in the space will flip the decision toward these samples";"CODE"
"check that with unit weights a sample is supposed to be predicted on";"CODE"
"the boundary";"-"
"give more weights to opposed samples";"-"
"estimator base clone estimator avoid side effects from previous tests";"CODE"
"similar test to test svm classifier sided sample weight but for";"IRRE"
"svm regressors";"-"
"check that with unit weights a sample is supposed to be predicted on";"CODE"
"the boundary";"-"
"give more weights to opposed samples";"-"
"test that rescaling all samples is the same as changing c";"IRRE"
"model generates equal coefficients";"-"
"todo rework this test to be independent of the random seeds";"CODE"
"test class weights for imbalanced data";"IRRE"
"we take as dataset the two dimensional projection of iris so";"IRRE"
"that it is not separable and remove half of predictors from";"CODE"
"class 1";"IRRE"
"we add one to the targets as a non regression test";"TASK"
"class weight balanced";"IRRE"
"used to work only when the labels where a range 0 k";"OUTD"
"check that score is better when class balanced is set";"IRRE"
"test dimensions for labels";"CODE"
"y2 y 1 wrong dimensions for labels";"META"
"test with arrays that are non contiguous";"IRRE"
"error for precomputed kernelsx";"CODE"
"predict with sparse input when trained with dense";"IRRE"
"check svc throws valueerror when dealing with non finite parameter values";"IRRE"
"test that a unicode kernel name does not cause a typeerror";"IRRE"
"regression test for 14893";"IRRE"
"test possible parameter combinations in linearsvc";"IRRE"
"generate list of possible parameter combinations";"IRRE"
"test basic routines using linearsvc";"IRRE"
"by default should have intercept";"CODE"
"the same with l1 penalty";"-"
"l2 penalty with dual formulation";"CODE"
"l2 penalty l1 loss";"-"
"test also decision function";"IRRE"
"test linearsvc with crammer singer multi class svm";"IRRE"
"similar prediction for ovr and crammer singer";"IRRE"
"classifiers shouldn t be the same";"IRRE"
"test decision function";"CODE"
"check correct result when sample weight is 1";"IRRE"
"check if same as sample weight none";"IRRE"
"check that fit x fit x1 x2 x3 sample weight n1 n2 n3 where";"-"
"x x1 repeated n1 times x2 repeated n2 times and so forth";"CODE"
"test crammer singer formulation in the binary case";"CODE"
"test that linearsvc gives plausible predictions on the iris dataset";"IRRE"
"also test symbolic class names classes";"IRRE"
"test that dense liblinear honours intercept scaling param";"IRRE"
"when intercept scaling is low the intercept value is highly penalized";"CODE"
"by regularization";"-"
"when intercept scaling is sufficiently high the intercept value";"CODE"
"is not affected by regularization";"-"
"when intercept scaling is sufficiently high the intercept value";"CODE"
"doesn t depend on intercept scaling value";"CODE"
"multi class case";"CODE"
"binary class case";"CODE"
"check that primal coef modification are not silently ignored";"-"
"stdout redirect";"CODE"
"tdout os dup 1 save original stdout";"CODE"
"os dup2 os pipe 1 1 replace it";"CODE"
"actual call";"IRRE"
"stdout restore";"CODE"
"os dup2 stdout 1 restore original stdout";"CODE"
"xxx this test is thread unsafe because it uses probability true";"CODE"
"https github com scikit learn scikit learn issues 31885";"CODE"
"create svm with callable linear kernel check that results are the same";"IRRE"
"as with built in linear kernel";"-"
"clone for checking clonability with lambda functions";"CODE"
"xxx this test is thread unsafe because it uses probability true";"CODE"
"https github com scikit learn scikit learn issues 31885";"CODE"
"x foo input validation not required when svm not fitted";"CODE"
"ignore convergence warnings from max iter 1";"CODE"
"xxx this test is thread unsafe because it uses probability true";"CODE"
"https github com scikit learn scikit learn issues 31885";"CODE"
"test that warnings are raised if model does not converge";"CODE"
"check that we have an n iter attribute with int type as opposed to a";"META"
"numpy array or an np int32 so as to match the docstring";"CODE"
"test that svr kernel linear has coef with the right sign";"IRRE"
"non regression test for 2933";"IRRE"
"test that intercept scaling is ignored when fit intercept is false";"CODE"
"method must be un available before or after fit switched by";"CODE"
"probability param";"-"
"switching to probability true after fitting should make";"CODE"
"predict proba available but calling it must not work";"IRRE"
"one point from each quadrant represents one class";"CODE"
"first point is closer to the decision boundaries than the second point";"CODE"
"for all the quadrants classes";"CODE"
"base points 1 1 q1";"CODE"
"base points 1 1 q2";"CODE"
"base points 1 1 q3";"CODE"
"base points 1 1 q4";"CODE"
"test if the prediction is the same as y";"IRRE"
"assert that the predicted class has the maximum value";"IRRE"
"get decision value at test points for the predicted class";"IRRE"
"assert pred class deci val 0 here";"CODE"
"test if the first point has lower decision value on every quadrant";"IRRE"
"compared to the second point";"IRRE"
"xxx known failure to be investigated either the code needs to be";"TASK"
"fixed or the test itself might need to be made less sensitive to";"CODE"
"random changes in test data and rounding errors more generally";"IRRE"
"https github com scikit learn scikit learn issues 29633";"CODE"
"xxx https github com scikit learn scikit learn issues 31883";"CODE"
"make n support is correct for oneclass and svr used to be";"CODE"
"non initialized";"IRRE"
"this is a non regression test for issue 14774";"CODE"
"non regression test for 18891 and https nvd nist gov vuln detail cve 2020 28975";"IRRE"
"first check that the names of the metadata passed are the same as";"-"
"expected the names are stored as keys in record";"-"
"the following condition is used to check for any specified parameters";"IRRE"
"being a subset of the original values";"IRRE"
"this list is used to get a reference to the sub estimators which are not";"CODE"
"necessarily stored on the metaestimator we need to override deepcopy";"CODE"
"because the sub estimators are probably cloned which would result in a";"IRRE"
"new copy of the list but we need copy and deep copy both to return the";"CODE"
"same instance";"-"
"return np ones len x pragma no cover";"IRRE"
"each row sums up to 1 0";"-"
"implementing fit transform is necessary since";"TASK"
"transformermixin fit transform doesn t route any metadata to";"CODE"
"transform while here we want transform to receive";"CODE"
"sample weight and metadata";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"a few test classes";"IRRE"
"calling pca methods such as get feature names out still works";"TASK"
"fitting on a new data does not alter components";"CODE"
"fit transform does not alter state";"CODE"
"cloning estimator is a no op";"-"
"test that we can predict with the restored decision tree classifier";"IRRE"
"treenoversion has no getstate like pre 0 18";"META"
"check we got the warning about using pre 0 18 pickle";"-"
"the test modifies global state by changing the treenoversion class";"IRRE"
"test that changing tags by inheritance is not allowed";"IRRE"
"checks the display configuration flag controls the json output";"IRRE"
"checks the display configuration flag controls the html output";"IRRE"
"fit on dataframe saves the feature names";"TASK"
"fit again but on ndarray does not keep the previous feature names see 21383";"TASK"
"warns when fitted on dataframe and transforming a ndarray";"CODE"
"warns when fitted on a ndarray and transforming dataframe";"CODE"
"fit on dataframe with all integer feature names works without warning";"TASK"
"fit on dataframe with no feature names or all integer feature names";"TASK"
"do not warn on transform";"CODE"
"fit on dataframe with feature names that are mixed raises an error";"TASK"
"transform on feature names that are mixed also raises";"CODE"
"this should not raise";"CODE"
"pyarrow does not work with np asarray";"CODE"
"https github com apache arrow issues 34886";"CODE"
"passing the metadata to fit transform should raise a warning since it";"CODE"
"could potentially be consumed by transform";"CODE"
"not passing a metadata which can potentially be consumed by transform should";"CODE"
"not raise a warning";"CODE"
"passing the metadata to fit predict should raise a warning since it";"CODE"
"could potentially be consumed by predict";"-"
"not passing a metadata which can potentially be consumed by predict should";"-"
"not raise a warning";"CODE"
"check that sklearn is built with openmp based parallelism enabled";"CODE"
"this test can be skipped by setting the environment variable";"IRRE"
"sklearn skip openmp test";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"check that invalid values raise for the method parameter";"IRRE"
"test calibration objects with isotonic sigmoid";"IRRE"
"x x x min multinomialnb only allows positive x";"-"
"split train and test";"IRRE"
"naive bayes";"-"
"naive bayes with calibration";"-"
"note that this fit overwrites the fit on the entire training";"TASK"
"set";"IRRE"
"check that brier score has improved after calibration";"-"
"check invariance against relabeling 0 1 1 2";"CODE"
"check invariance against relabeling 0 1 1 1";"CODE"
"check invariance against relabeling 0 1 1 0";"CODE"
"isotonic calibration is not invariant against relabeling";"CODE"
"but should improve in both cases";"META"
"check estimator default is linearsvc";"CODE"
"check when cv is a cv splitter";"-"
"check error raised when number of examples per class less than nfold";"CODE"
"as the weights are used for the calibration they should still yield";"TASK"
"different predictions";"-"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"x x min multinomialnb only allows positive x";"-"
"split train and test";"IRRE"
"naive bayes";"-"
"naive bayes with calibration";"-"
"test that ensemble false is the same as using predictions from";"IRRE"
"cross val predict to train calibrator";"-"
"get probas manually";"-"
"use clf fit on all data";"-"
"train the calibrator on the calibrating set";"IRRE"
"there is one and only one temperature scaling calibrator";"CODE"
"for each calibrated classifier";"CODE"
"should not raise any error";"CODE"
"the optimal inverse temperature parameter should always be positive";"IRRE"
"accuracy score is invariant under temperature scaling";"CODE"
"log loss should be improved on the calibrating set";"IRRE"
"refinement error should be invariant under temperature scaling";"CODE"
"use roc auc as a proxy for refinement error also note that roc auc";"TASK"
"itself is invariant under strict monotone transformations";"CODE"
"for logistic regression the optimal temperature should be close to 1 0";"CODE"
"on the training set";"IRRE"
"check that temperaturescaling can handle 2d array with only 1 feature";"TASK"
"test that sum of probabilities is max 1 a non regression test for";"IRRE"
"issue 7796 when test has fewer classes than train";"IRRE"
"in the first and last fold test will have 1 class while train will have 2";"CODE"
"test to check calibration works fine when train set in a test train";"IRRE"
"split does not contain all classes";"CODE"
"in 1st split train is missing class 0";"IRRE"
"in 3rd split train is missing class 3";"IRRE"
"check that the unobserved class has proba 0";"IRRE"
"check for all other classes proba 0";"CODE"
"when ensemble false cross val predict is used to compute predictions";"OUTD"
"to fit only one calibrated classifiers";"IRRE"
"toy decision function that just needs to have the right shape";"TASK"
"we should be able to fit this classifier with no error";"CODE"
"check attributes are obtained from fitted estimator";"META"
"neither the pipeline nor the calibration meta estimator";"CODE"
"expose the n features in check on this kind of data";"CODE"
"ensure that no error is thrown with predict and predict proba";"CODE"
"check that n features in and classes attributes created properly";"IRRE"
"check that n features in from prefit base estimator";"TASK"
"is consistent with training set";"IRRE"
"check that calibratedclassifier works with votingclassifier";"IRRE"
"the method predict proba from votingclassifier is dynamically";"IRRE"
"defined via a property that only works when voting soft";"CODE"
"smoke test should not raise an error";"IRRE"
"ensure calibrationdisplay from predictions and calibration curve";"CODE"
"compute the same results also checks attributes of the";"IRRE"
"calibrationdisplay object";"IRRE"
"cannot fail thanks to pyplot fixture";"-"
"ensure pipelines are supported by calibrationdisplay from estimator";"CODE"
"checks that when instantiating calibrationdisplay class then calling";"IRRE"
"plot self estimator name is the one given in plot";"CODE"
"check that the name used when calling";"IRRE"
"calibrationdisplay from predictions or";"CODE"
"calibrationdisplay from estimator is used when multiple";"CODE"
"calibrationdisplay viz plot calls are made";"IRRE"
"check that ref line only appears once";"-"
"default case";"CODE"
"if y true contains str then pos label is required";"CODE"
"scale the data to avoid any convergence issue";"CODE"
"only use 2 classes";"IRRE"
"interlace the data such that a 2 fold cross validation will be equivalent";"CODE"
"to using the original dataset with a sample weights of 2";"IRRE"
"check that the underlying fitted estimators have the same coefficients";"-"
"check that the predictions are the same";"-"
"check that the decision function of sgdclassifier produces predicted";"CODE"
"values that are quite large for the data under consideration";"IRRE"
"compare the calibratedclassifiercv using the sigmoid method with the";"IRRE"
"calibratedclassifiercv using the isotonic method the isotonic method";"IRRE"
"is used for comparison because it is numerically stable";"IRRE"
"the isotonic method is used for comparison because it is numerically";"IRRE"
"stable";"-"
"the auc score should be the same because it is invariant under";"CODE"
"strictly monotonic conditions";"-"
"check that for small enough predictions ranging from 2 to 2 the";"CODE"
"threshold value has no impact on the outcome";"IRRE"
"using a threshold lower than the maximum absolute value of the";"IRRE"
"predictions enables internal re scaling by max abs predictions small";"CODE"
"using a larger threshold disables rescaling";"-"
"using default threshold of 30 also disables the scaling";"CODE"
"depends on the tolerance of the underlying quasy newton solver which is";"CODE"
"not too strict by default";"CODE"
"use dtype np float64 to check that this does not trigger an";"CODE"
"unintentional upcasting the dtype of the base estimator should";"CODE"
"control the dtype of the final model in particular the";"CODE"
"sigmoid calibrator relies on inputs predictions and sample weights";"META"
"with consistent dtypes because it is partially written in cython";"-"
"as this test forces the predictions to be float32 we want to check";"CODE"
"that calibratedclassifiercv internally converts sample weight to";"CODE"
"the same dtype to avoid crashing the cython call";"IRRE"
"does not raise an error";"CODE"
"check with frozen prefit model";"-"
"does not raise an error";"CODE"
"todo also ensure that calibratedclassifiercv works appropriately with";"CODE"
"the array api when y is an ndarray of strings and we fit";"CODE"
"lineardiscriminantanalysis beforehand in this regard";"CODE"
"lineardiscriminantanalysis will also need modifications";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"make it possible to discover experimental estimators when calling all estimators";"IRRE"
"enable halving search cv noqa f401";"-"
"enable iterative imputer noqa f401";"-"
"pytest mark thread unsafe import side effects";"CODE"
"test that all estimators doesn t find abstract classes";"IRRE"
"pass pragma nocover";"-"
"common tests for estimator instances";"IRRE"
"pytest mark thread unsafe import side effects";"CODE"
"smoke test to check that any name in a all list is actually defined";"IRRE"
"in the namespace of the module or package";"CODE"
"avoid test suite depending on build dependencies for example cython";"CODE"
"pytest mark thread unsafe import side effects";"CODE"
"ensure that for each contentful subpackage there is a test directory";"CODE"
"within it that is also a subpackage i e a directory with init py";"IRRE"
"make sure passing classes to check estimator or parametrize with checks";"IRRE"
"raises an error";"CODE"
"todo fix mlp to not check validation set during mlp";"TASK"
"note when running check dataframe column names consistency on a meta estimator that";"TASK"
"delegates validation to a base estimator the check is testing that the base estimator";"IRRE"
"is checking for column name consistency";"CODE"
"todo as more modules support get feature names out they should be removed";"TASK"
"from this list to be tested";"CODE"
"the following estimators can work inplace only with certain settings";"IRRE"
"not using as a context manager affects nothing";"-"
"global setting will not be retained outside of context that";"IRRE"
"did not modify this setting";"IRRE"
"no positional arguments";"-"
"no unknown arguments";"-"
"no unknown arguments";"-"
"data is just 6 separable points in the plane";"CODE"
"degenerate data with only one feature still should be separable";"TASK"
"data is just 9 separable points in the plane";"CODE"
"degenerate data with 1 feature still should be separable";"TASK"
"data that has zero variance in one dimension and needs regularization";"TASK"
"one element class";"IRRE"
"test lda classification";"IRRE"
"this checks that lda implements fit and predict and returns correct";"CODE"
"values for simple toy data";"IRRE"
"assert that it works with 1d data";"CODE"
"test probability estimates";"IRRE"
"primarily test for commit 2f34950 reuse of priors";"IRRE"
"lda shouldn t be able to separate those";"CODE"
"test bad solver with covariance estimator";"IRRE"
"test bad covariance estimator";"IRRE"
"check that the empirical means and covariances are close enough to the";"CODE"
"one used to generate the data";"OUTD"
"implement the method to compute the probability given in the elements";"TASK"
"of statistical learning cf p 127 sect 4 4 5 logistic regression";"IRRE"
"or lda";"-"
"check the consistency of the computed probability";"-"
"all probabilities should sum to one";"-"
"check that the probability of lda are close to the theoretical";"IRRE"
"probabilities";"-"
"test priors negative priors";"IRRE"
"test that priors passed as a list are correctly handled run to see if";"IRRE"
"failure";"-"
"test that priors always sum to 1";"IRRE"
"test if the coefficients of the solvers are approximately the same";"IRRE"
"test lda transform";"IRRE"
"test if the sum of the normalized eigen vectors values equals 1";"IRRE"
"also tests whether the explained variance ratio formed by the";"CODE"
"eigen solver is the same as the explained variance ratio formed";"CODE"
"by the svd solver";"-"
"arrange four classes with their means in a kite shaped pattern";"IRRE"
"the longer distance should be transformed to the first component and";"CODE"
"the shorter distance to the second component";"-"
"we construct perfectly symmetric distributions so the lda can estimate";"CODE"
"precise means";"-"
"fit lda and transform the means";"CODE"
"the transformed within class covariance should be the identity matrix";"CODE"
"the means of classes 0 and 3 should lie on the first component";"IRRE"
"the means of classes 1 and 2 should lie on the second component";"IRRE"
"test if classification works correctly with differently scaled features";"IRRE"
"use uniform distribution of features to make sure there is absolutely no";"TASK"
"overlap between classes";"IRRE"
"should be able to separate the data perfectly";"CODE"
"test for solver lsqr and eigen";"IRRE"
"store covariance has no effect on lsqr and eigen solvers";"IRRE"
"test the actual attribute";"IRRE"
"test for svd solver the default is to not set the covariances attribute";"CODE"
"test the actual attribute";"IRRE"
"test that shrunk covariance estimator and shrinkage parameter behave the";"IRRE"
"same";"-"
"when shrinkage auto current implementation uses ledoitwolf estimation";"TASK"
"of covariance after standardizing the data this checks that it is indeed";"CODE"
"the case";"CODE"
"c standardscaler standardize features";"TASK"
"rescale";"-"
"we create n classes labels by repeating and truncating a";"IRRE"
"range n classes until n samples";"IRRE"
"if n components min n classes 1 n features no warning";"TASK"
"if n components min n classes 1 n features raise error";"CODE"
"we test one unit higher than max components and then something";"IRRE"
"larger than both n features and n classes 1 to ensure the test";"IRRE"
"works for any value of n component";"IRRE"
"check value consistency between types";"IRRE"
"qda classification";"IRRE"
"this checks that qda implements fit and predict and returns";"CODE"
"correct values for a simple toy dataset";"IRRE"
"assure that it works with 1d data";"-"
"test probas estimates";"IRRE"
"qda shouldn t be able to separate those";"CODE"
"classes should have at least 2 elements";"IRRE"
"test that the correct errors are raised when using inappropriate";"IRRE"
"covariance estimators or shrinkage parameters with qda";"IRRE"
"test bad solver with covariance estimator";"IRRE"
"test bad covariance estimator";"IRRE"
"when shrinkage auto current implementation uses ledoitwolf estimation";"TASK"
"of covariance after standardizing the data this checks that it is indeed";"CODE"
"the case";"CODE"
"c standardscaler standardize features";"TASK"
"rescale";"-"
"test if the coefficients of the solvers are approximately the same";"IRRE"
"we expect the following";"-"
"altering priors without fit should not change priors";"-"
"the default is to not set the covariances attribute";"CODE"
"test the actual attribute";"IRRE"
"the default is reg param 0 and will cause issues when there is a";"CODE"
"constant variable";"CODE"
"fitting on data with constant variable without regularization";"CODE"
"triggers a linalgerror";"-"
"adding a little regularization fixes the fit time error";"TASK"
"linalgerror should also be there for the n samples in a class";"CODE"
"n features case";"TASK"
"the error will persist even with regularization for svd";"CODE"
"because the number of singular values is limited by n samples in a class";"IRRE"
"the warning will be gone for eigen with regularization because";"CODE"
"the covariance matrix will be full rank";"CODE"
"make features correlated";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"make it possible to discover experimental estimators when calling all estimators";"IRRE"
"enable halving search cv noqa f401";"-"
"enable iterative imputer noqa f401";"-"
"walk packages ignores deprecationwarnings now we need to ignore";"TASK"
"futurewarnings";"TASK"
"mypy error module has no attribute path";"META"
"functions to ignore args docstring of";"CODE"
"methods where y param should be ignored if y none by default";"CODE"
"test module docstring formatting";"CODE"
"skip test if numpydoc is not found";"IRRE"
"xxx unreached code as of v0 22";"-"
"pytest tooling not part of the scikit learn api";"IRRE"
"we cannot always control these docstrings";"CODE"
"exclude non scikit learn classes";"IRRE"
"skip checks on deprecated classes";"IRRE"
"now skip docstring test for y when y is none";"CODE"
"by default for api reason";"CODE"
"param ignore y ignore y for fit and score";"CODE"
"exclude imported functions";"CODE"
"don t test private methods functions";"CODE"
"minimal degenerate instances only useful to test the docstrings";"CODE"
"xxx hard coded assumption that n features 3";"TASK"
"todo 1 10 remove copy warning filter";"TASK"
"todo devtools use tested estimators instead of all estimators in the";"CODE"
"decorator";"CODE"
"default 2 is invalid for single target";"CODE"
"default auto raises an error with the shape of x";"CODE"
"default raises an error perplexity must be less than n samples";"CODE"
"todo 1 9 remove";"TASK"
"default raises a futurewarning if quantile method is at default warn";"CODE"
"todo 1 10 remove";"TASK"
"default raises a futurewarning";"CODE"
"low max iter to speed up tests we are only interested in checking the existence";"IRRE"
"of fitted attributes this should be invariant to whether it has converged or not";"CODE"
"min value for tsne is 250";"IRRE"
"in case we want to deprecate some attributes in the future";"CODE"
"vectorizer require some specific input data";"CODE"
"as certain attributes are present only if a certain parameter is";"IRRE"
"provided this checks if the word only is present in the attribute";"CODE"
"description and if not the attribute is required to be present";"META"
"ignore deprecation warnings";"-"
"attributes";"META"
"properties";"-"
"ignore properties that raises an attributeerror and deprecated";"IRRE"
"properties";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"r s w optionally match additional sentence";"TASK"
"make it possible to discover experimental estimators when calling all estimators";"IRRE"
"enable halving search cv noqa f401";"-"
"enable iterative imputer noqa f401";"-"
"skip private classes";"CODE"
"exclude functions from utils fixex since they come from external packages";"CODE"
"we ignore following error code";"-"
"rt02 the first line of the returns section";"IRRE"
"should contain only the type";"-"
"as we may need refer to the name of the returned";"IRRE"
"object";"IRRE"
"gl01 docstring text summary should start in the line";"CODE"
"immediately after the opening quotes not in the same line";"CODE"
"or leaving a blank line in between";"-"
"gl02 if there s a blank line it should be before the";"CODE"
"first line of the returns section not after it allows to have";"IRRE"
"short docstrings for properties";"CODE"
"ignore pr02 unknown parameters for properties we sometimes use";"CODE"
"properties for ducktyping i e sgdclassifier predict proba";"CODE"
"ignore gl08 parsing of the method signature failed possibly because this is";"IRRE"
"a property properties are sometimes used for deprecated attributes and the";"OUTD"
"attribute is already documented in the class docstring";"CODE"
"all error codes";"-"
"https numpydoc readthedocs io en latest validation html built in validation checks";"CODE"
"following codes are only taken into account for the";"CODE"
"top level class docstrings";"CODE"
"es01 no extended summary found";"CODE"
"sa01 see also section not found";"-"
"ex01 no examples section found";"-"
"in particular we can t parse the signature of properties";"IRRE"
"errors";"-"
"we know that we can have division by zero";"-"
"we know that we can have division by zero";"-"
"1d case";"CODE"
"x np array 0 0 0 0 ignored";"-"
"2d case";"CODE"
"2d case only";"CODE"
"x np array 0 0 0 0 ignored";"-"
"x 0 0 0 0 ignored";"-"
"non regression test added in";"TASK"
"https github com scikit learn scikit learn pull 13545";"CODE"
"x 0 0 0 0 ignored";"-"
"x 0 5 ignored";"-"
"x 0 5 ignored";"-"
"x 0 4 ignored";"-"
"x 0 4 ignored";"-"
"x 0 4 ignored";"-"
"correctness oracle";"-"
"x 0 5 ignored";"-"
"correctness oracle";"-"
"x 0 5 ignored";"-"
"correctness oracle";"-"
"correctness oracle";"-"
"x 0 5 ignored";"-"
"y 0 5 ignored";"-"
"x 0 5 ignored";"-"
"non regression test for 22478";"IRRE"
"test with 2d array";"IRRE"
"correctness oracle";"-"
"when strategy mean";"-"
"x 0 0 0 0 ignored";"-"
"x 0 0 0 0 ignored";"-"
"x 0 0 0 0 ignored";"-"
"x 0 5 ignored";"-"
"x 0 5 ignored";"-"
"x 0 5 ignored";"-"
"x 0 5 ignored";"-"
"x 0 3 ignored";"-"
"there should be two elements when return std is true";"CODE"
"the second element should be all zeros";"-"
"basic unittests to test functioning of module s top level";"IRRE"
"from sklearn import noqa f403";"CODE"
"test either above import has failed for some reason";"CODE"
"import is discouraged outside of the module level hence we";"CODE"
"rely on setting up the variable above";"IRRE"
"check that fit is permutation invariant";"CODE"
"regression test of missing sorting of sample weights";"IRRE"
"check that we got increasing true and no warnings";"-"
"check that we got increasing true and no warnings";"-"
"check that we got increasing false and no warnings";"-"
"check that we got increasing false and no warnings";"-"
"check that we got increasing false and ci interval warning";"CODE"
"check that it is immune to permutation";"-"
"check we don t crash when all x are equal";"CODE"
"setup examples with ties on minimum";"IRRE"
"check that we get identical results for fit transform and fit transform";"CODE"
"setup examples with ties on maximum";"IRRE"
"check that we get identical results for fit transform and fit transform";"CODE"
"check fit transform and fit transform";"CODE"
"set y and x for decreasing";"IRRE"
"create model and fit transform";"IRRE"
"check that relationship decreases";"-"
"set y and x for decreasing";"IRRE"
"create model and fit transform";"IRRE"
"check that relationship increases";"-"
"check if default value of sample weight parameter is one";"IRRE"
"random test data";"IRRE"
"check if value is correctly used";"IRRE"
"check if min value is used correctly";"IRRE"
"set y and x";"IRRE"
"create model and fit";"IRRE"
"check that an exception is thrown";"CODE"
"set y and x";"IRRE"
"create model and fit";"IRRE"
"predict from training and test x and check that min max match";"IRRE"
"set y and x";"IRRE"
"create model and fit";"IRRE"
"predict from training and test x and check that we have two nans";"IRRE"
"create model and fit";"IRRE"
"test from nellev s issue";"IRRE"
"https github com scikit learn scikit learn issues 6921";"CODE"
"also test decreasing case since the logic there is different";"CODE"
"finally test with only one bound";"CODE"
"test from ogrisel s issue";"IRRE"
"https github com scikit learn scikit learn issues 4297";"CODE"
"get deterministic rng with seed";"-"
"create regression and samples";"IRRE"
"get some random weights and zero out";"IRRE"
"this will hang in failure case";"CODE"
"test that the faster prediction change doesn t";"IRRE"
"affect out of sample predictions";"-"
"https github com scikit learn scikit learn pull 6206";"CODE"
"x values over the 10 10 range";"IRRE"
"we also want to test that everything still works when some weights are 0";"TASK"
"build interpolation function with all input data not just the";"CODE"
"non redundant subset the following 2 lines are taken from the";"IRRE"
"fit method without removing unnecessary points";"IRRE"
"fit with just the necessary data";"-"
"https github com scikit learn scikit learn issues 6628";"CODE"
"regression test for 15004";"IRRE"
"check that data are converted when x and y dtype differ";"-"
"check that equality takes account of np finfo tolerance";"-"
"check that averaging of targets for duplicate x is done correctly";"CODE"
"taking into account tolerance";"CODE"
"non regression test to ensure that inf values are not returned";"IRRE"
"see https github com scikit learn scikit learn issues 10903";"CODE"
"input thresholds are a strict subset of the training set unless";"IRRE"
"the data is already strictly monotonic which is not the case with";"CODE"
"this random data";"IRRE"
"output thresholds lie in the range of the training set";"IRRE"
"test from 15012";"IRRE"
"check that isotonicregression can handle 2darray with only 1 feature";"TASK"
"ensure isotonicregression raises error if input has more than 1 feature";"CODE"
"generate data";"-"
"make sure x and y are not writable to avoid introducing dependencies between";"CODE"
"tests";"IRRE"
"test that polynomialcountsketch approximates polynomial";"IRRE"
"kernel on random data";"IRRE"
"compute exact kernel";"-"
"approximate kernel mapping";"-"
"assert np abs np mean error 0 05 close to unbiased";"CODE"
"assert np max error 0 1 nothing too far off";"CODE"
"assert np mean error 0 05 mean is fairly close";"CODE"
"test that additivechi2sampler approximates kernel on random data";"IRRE"
"compute exact kernel";"-"
"abbreviations for easier formula";"CODE"
"reduce to n samples x x n samples y by summing over features";"TASK"
"approximate kernel mapping";"-"
"test error is raised on negative input";"CODE"
"test that rbfsampler approximates kernel on random data";"IRRE"
"compute exact kernel";"-"
"approximate kernel mapping";"-"
"assert np abs np mean error 0 01 close to unbiased";"CODE"
"assert np max error 0 1 nothing too far off";"CODE"
"assert np mean error 0 05 mean is fairly close";"CODE"
"x list x test input validation";"IRRE"
"if degree gamma or coef0 is passed we raise a valueerror";"IRRE"
"non regression test nystroem on precomputed kernel";"IRRE"
"pr 14706";"-"
"if degree gamma or coef0 is passed we raise a valueerror";"IRRE"
"alpha 0 causes a linalgerror in computing the dual coefficients";"-"
"which causes a fallback to a lstsq solver this is tested here";"IRRE"
"k np dot x x t precomputed kernel";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"this should still work since none is the default value";"CODE"
"now requests is no more empty";"CODE"
"but one can exclude a method";"META"
"excluding fit is not enough";"TASK"
"and excluding both fit and score would avoid an exception";"CODE"
"test if a router is empty";"IRRE"
"fit in this class or a parent requests prop but we don t want";"CODE"
"it requested at all";"CODE"
"return self pragma no cover";"CODE"
"having it here instead of parametrizing the test since set fit request";"IRRE"
"is not available while collecting the tests";"IRRE"
"adding a metadatarequest as self adds a copy";"TASK"
"should be a copy not the same object";"IRRE"
"one can add an estimator as self";"TASK"
"adding a consumer router as self should only add the consumer part";"TASK"
"get metadata request returns the consumer part of the requests";"CODE"
"get metadata routing returns the complete request set consumer and";"IRRE"
"router included";"CODE"
"it should be a copy not the same object";"IRRE"
"adding one with a string method mapping";"CODE"
"adding one with an instance of methodmapping";"TASK"
"return alias false will return original names for self";"CODE"
"ignoring self would remove sample weight";"CODE"
"return alias is ignored when ignore self request true";"CODE"
"test if all required request methods are generated";"CODE"
"todo these test classes can be moved to sklearn utils testing once we";"IRRE"
"have a better idea of what the commonly used classes are";"IRRE"
"this class should have no set method request";"CODE"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"this class should have every set method request";"CODE"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"composite methods shouldn t have a corresponding set method";"IRRE"
"simple methods should have a corresponding set method";"IRRE"
"test the behavior and the values of methods composite methods whose";"IRRE"
"request values are a union of requests by other methods simple methods";"IRRE"
"fit transform and fit predict are the only composite methods we have in";"CODE"
"scikit learn";"-"
"this class should have every set method request";"CODE"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"since no request is set for fit or predict or transform the request for";"CODE"
"fit transform and fit predict should also be empty";"CODE"
"setting the request on only one of them should raise an error";"CODE"
"setting the request on the other one should fail if not the same as the";"IRRE"
"first method";"-"
"now the requests are consistent and getting the requests for fit predict";"CODE"
"shouldn t raise";"CODE"
"setting the request for a none overlapping parameter would merge them";"IRRE"
"together";"-"
"this passes since no metadata is passed";"CODE"
"this fails since metadata is passed but estimator does not support it";"CODE"
"test positional arguments error before making the descriptor method unbound";"CODE"
"this somehow makes the descriptor method unbound which results in the instance";"CODE"
"argument being none and instead self being passed as a positional argument";"CODE"
"to the descriptor method";"CODE"
"this should pass as usual";"CODE"
"test positional arguments error after making the descriptor method unbound";"IRRE"
"searchcv estimators";"-"
"featureunion";"TASK"
"stacking voting";"-"
"todo remove data validation for the following estimators";"CODE"
"they should be able to work on any data and delegate data validation to";"CODE"
"their inner estimator s";"-"
"classifierchain data validation is necessary";"IRRE"
"frozenestimator this estimator cannot be tested like others";"IRRE"
"onevsoneclassifier input validation can t be avoided";"CODE"
"regressorchain data validation is necessary";"-"
"sequentialfeatureselector not applicable 2d data mandatory";"TASK"
"check that meta estimators delegate data validation to the inner";"-"
"estimator s";"-"
"clone to avoid side effects and ensure thread safe test execution";"CODE"
"we convert to lists to make sure it works on array like";"-"
"calling fit should not raise any data validation exception since x is a";"CODE"
"valid input datastructure for the first step of the pipeline passed as";"CODE"
"base estimator to the meta estimator";"-"
"n features in should not be defined since data is not tabular data";"CODE"
"enable halving search cv noqa f401";"-"
"enable iterative imputer noqa f401";"-"
"ids used by pytest to get meaningful verbose messages when running the tests";"IRRE"
"avoid mutating the original init args dict to keep the test execution";"IRRE"
"thread safe";"CODE"
"raise valueerror unpermitted sub estimator type pragma nocover";"CODE"
"raise valueerror unpermitted sub estimator type pragma nocover";"CODE"
"not all meta estimators in the list support sample weight";"CODE"
"and for those we skip this test";"CODE"
"test that registry is not copied into a new instance";"CODE"
"check that by default request is empty and the right type";"CODE"
"our groupcv splitters request groups by default which we should";"CODE"
"ignore in this test";"CODE"
"test that a unsetmetadatapassederror is raised when the sub estimator s";"IRRE"
"requests are not set";"IRRE"
"this test only makes sense for metaestimators which have a";"CODE"
"sub estimator e g mymetaestimator estimator mysubestimator";"-"
"set request on fit";"IRRE"
"making sure the requests are unset in case they were set as a";"IRRE"
"side effect of setting them for fit for instance if method";"CODE"
"mapping for fit is fit fit score that would mean";"CODE"
"calling score here would not raise because we have already";"CODE"
"set request value for child estimator s score";"IRRE"
"fit partial fit score accept y others don t";"CODE"
"when the metadata is explicitly requested on the sub estimator there";"CODE"
"should be no errors";"-"
"this test only makes sense for metaestimators which have a";"CODE"
"sub estimator e g mymetaestimator estimator mysubestimator";"-"
"set method request metadata true on the underlying objects";"IRRE"
"fit before calling method";"IRRE"
"fit and partial fit accept y others don t";"CODE"
"sanity check that registry is not empty or else the test passes";"IRRE"
"trivially";"IRRE"
"test that when a non consuming estimator is given the meta estimator";"IRRE"
"works w o setting any requests";"IRRE"
"regression test for https github com scikit learn scikit learn issues 28239";"CODE"
"this test only makes sense for metaestimators which have a";"CODE"
"sub estimator e g mymetaestimator estimator mysubestimator";"-"
"e g call set fit request on estimator";"IRRE"
"the following should pass w o raising a routing error";"-"
"fit and partial fit accept y others don t";"CODE"
"this test only makes sense for cv estimators";"CODE"
"this test is only for metaestimators accepting a cv splitter";"CODE"
"this test is only for metaestimators accepting a cv splitter";"CODE"
"remove consumingsplitter from kwargs so cv param isn t passed twice";"IRRE"
"numpy is more complex because build time 1 25 and run time 1 19 5";"META"
"requirement currently don t match";"CODE"
"test predicting without fitting";"IRRE"
"fail on multioutput data";"IRRE"
"test that check classification target return correct type 5782";"IRRE"
"a classifier which implements decision function";"CODE"
"a classifier which implements predict proba";"TASK"
"test if partial fit is working as intended";"CODE"
"test when mini batches doesn t have all classes";"IRRE"
"with sgdclassifier";"IRRE"
"test partial fit only exists if estimator has it";"IRRE"
"if a new class that was not in the first call of partial fit is seen";"CODE"
"it should raise valueerror";"IRRE"
"test that ovr and ovo work on regressors which don t have a decision";"IRRE"
"function";"CODE"
"we are doing something sensible";"CODE"
"we are doing something sensible";"CODE"
"test predict proba";"IRRE"
"predict assigns a label if the probability that the";"IRRE"
"sample has the label is greater than 0 5";"-"
"test decision function";"CODE"
"test that ovr works with classes that are always present or absent";"IRRE"
"note tests is the case where constantpredictor is utilised";"CODE"
"build an indicator matrix where two features are always on";"TASK"
"as list of lists it would be int i 5 2 3 for i in range 10";"CODE"
"y has a constantly absent label";"CODE"
"y 5 0 1 variable label";"IRRE"
"toy dataset where features correspond directly to labels";"TASK"
"test input as label indicator matrix";"IRRE"
"toy dataset where features correspond directly to labels";"TASK"
"test input as label indicator matrix";"IRRE"
"toy dataset where features correspond directly to labels";"TASK"
"decision function only estimator";"CODE"
"estimator with predict proba disabled depending on parameters";"TASK"
"estimator which can get predict proba enabled after fitting";"-"
"predict assigns a label if the probability that the";"IRRE"
"sample has the label is greater than 0 5";"-"
"decision function only estimator";"CODE"
"predict assigns a label if the probability that the";"IRRE"
"sample has the label with the greatest predictive probability";"IRRE"
"probability of being the positive class is always 0";"IRRE"
"x y iris data iris target three class problem with 150 samples";"IRRE"
"test with pipeline of length one";"IRRE"
"this test is needed because the multiclass estimators may fail to detect";"IRRE"
"the presence of predict proba or decision function";"CODE"
"test that onevsone fitting works with a list of targets and yields the";"IRRE"
"same output as predict from an array";"IRRE"
"a classifier which implements decision function";"CODE"
"a classifier which implements predict proba";"TASK"
"test when mini batches have binary target classes";"IRRE"
"raises error when mini batch does not have classes from all classes";"CODE"
"test partial fit only exists if estimator has it";"IRRE"
"first binary";"-"
"then multi class";"IRRE"
"compute the votes";"-"
"extract votes and verify";"-"
"for each sample and each class there only 3 possible vote levels";"CODE"
"because they are only 3 distinct class pairs thus 3 distinct";"IRRE"
"binary classifiers";"IRRE"
"therefore sorting predictions based on votes would yield";"CODE"
"mostly tied predictions";"-"
"the ovo decision function on the other hand is able to resolve";"CODE"
"most of the ties on this data as it combines both the vote counts";"CODE"
"and the aggregated confidence levels of the binary classifiers";"IRRE"
"to compute the aggregate decision function the iris dataset";"IRRE"
"has 150 samples with a couple of duplicates the ovo decisions";"IRRE"
"can resolve most of the ties";"-"
"test that ties are broken using the decision function";"IRRE"
"not defaulting to the smallest label";"CODE"
"classifiers are in order 0 1 0 2 1 2";"IRRE"
"use decision function to compute the votes and the normalized";"CODE"
"sum of confidences which is used to disambiguate when there is a tie in";"OUTD"
"votes";"-"
"for the first point there is one vote per class";"CODE"
"for the rest there is no tie and the prediction is the argmax";"CODE"
"for the tie the prediction is the class with the highest score";"CODE"
"test that ties can not only be won by the first two labels";"IRRE"
"cycle through labels so that each label wins once";"CODE"
"test that the ovo doesn t mess up the encoding of string labels";"CODE"
"test error for ovo with one class";"IRRE"
"test that the ovo errors on float targets";"IRRE"
"a classifier which implements decision function";"CODE"
"a classifier which implements predict proba";"TASK"
"test that the occ errors on float targets";"IRRE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 17218";"CODE"
"create an estimator that does not support sparse input";"IRRE"
"smoke test to check when sparse input should be supported";"IRRE"
"remove the last sample to make the classes not exactly balanced and make";"CODE"
"the test more interesting";"IRRE"
"fitting directly on the design matrix";"-"
"when working with precomputed kernels we have one feature per training";"TASK"
"sample";"-"
"this becomes really interesting with ovo and precomputed kernel together";"CODE"
"internally ovo will drop the samples of the classes not part of the pair";"CODE"
"of classes under consideration for a given binary classifier since we";"CODE"
"use a precomputed kernel it will also drop the matching columns of the";"-"
"kernel matrix and therefore we have fewer features as result";"TASK"
"since class 0 has 49 samples and class 1 and 2 have 50 samples each a";"IRRE"
"single ovo binary classifier works with a sub kernel matrix of shape";"IRRE"
"either 99 99 or 100 100";"-"
"assert ovo precomputed estimators 0 n features in 99 class 0 vs class 1";"CODE"
"assert ovo precomputed estimators 1 n features in 99 class 0 vs class 2";"CODE"
"assert ovo precomputed estimators 2 n features in 100 class 1 vs class 2";"CODE"
"fixme we should move this test in estimator checks once we are able";"IRRE"
"to construct meta estimator instances";"CODE"
"smoke test to check that pipeline ovr and ovo classifiers are letting";"CODE"
"the validation of missing values to";"IRRE"
"the underlying pipeline or classifiers";"CODE"
"x np copy x copy to avoid that the original data is modified";"CODE"
"logisticregression does not implement partial fit and should raise an";"CODE"
"attributeerror";"META"
"test multi target regression raises";"IRRE"
"no exception should be raised if the base estimator supports weights";"CODE"
"weighted regressor";"-"
"weighted with different weights";"-"
"weighted regressor";"-"
"unweighted but with repeated samples";"META"
"import the data";"CODE"
"create a multiple targets by randomized shuffling and concatenating y";"IRRE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"parallelism requires this to be the case for a sane implementation";"CODE"
"check multioutput has predict proba";"IRRE"
"default sgdclassifier has loss hinge";"CODE"
"which does not expose a predict proba method";"TASK"
"case where predict proba attribute exists";"META"
"check predict proba passes";"-"
"inner function for custom scoring";"CODE"
"sgdclassifier defaults to loss hinge which is not a probabilistic";"CODE"
"loss function therefore it does not expose a predict proba method";"CODE"
"test if multi target initializes correctly with base estimator and fit";"IRRE"
"assert predictions work as expected for predict";"CODE"
"train the multi target linear and also get the predictions";"IRRE"
"train the linear classification with each column and assert that";"CODE"
"predictions are equal after first partial fit and second partial fit";"-"
"create a clone with the same state";"IRRE"
"test if multi target initializes correctly with base estimator and fit";"IRRE"
"assert predictions work as expected for predict prodict proba and score";"CODE"
"train the multi target forest and also get the predictions";"CODE"
"train the forest with each column and assert that predictions are equal";"CODE"
"forest clone forest create a clone with the same state";"CODE"
"test to check meta of meta estimators";"IRRE"
"train the forest with each column and assert that predictions are equal";"CODE"
"multi class svc clone multi class svc create a clone";"IRRE"
"make test deterministic";"IRRE"
"random features";"IRRE"
"random labels";"IRRE"
"y1 np array b a a b a reshape 5 1 2 classes";"IRRE"
"y2 np array d e f e d reshape 5 1 3 classes";"CODE"
"weighted classifier";"IRRE"
"unweighted but with repeated samples";"META"
"weighted classifier";"IRRE"
"unweighted but with repeated samples";"META"
"notfittederror when fit is not done but score predict and";"TASK"
"and predict proba are called";"IRRE"
"valueerror when number of outputs is different";"IRRE"
"for fit and score";"CODE"
"valueerror when y is continuous";"IRRE"
"data is just 6 separable points in the plane";"CODE"
"a bit more random tests";"IRRE"
"data is 6 random integer points in a 100 dimensional space classified to";"CODE"
"three classes";"IRRE"
"gaussian naive bayes classification";"IRRE"
"this checks that gaussiannb implements fit and predict and returns";"CODE"
"correct values for a simple toy dataset";"IRRE"
"test whether label mismatch between target y and classes raises";"IRRE"
"an error";"-"
"fixme remove this test once the more general partial fit tests are merged";"IRRE"
"test whether class priors are properly set";"IRRE"
"check that the class priors sum to 1";"IRRE"
"smoke test for issue 9633";"IRRE"
"load a shared tests data sets for the tests in this module mark them";"CODE"
"read only to avoid unintentional in place modifications that would introduce";"CODE"
"side effects between tests";"IRRE"
"test the various init parameters of the pipeline in fit";"IRRE"
"method";"-"
"check that we can t fit pipelines with objects without fit";"CODE"
"method";"-"
"smoke test with only an estimator";"IRRE"
"check that params are set";"IRRE"
"smoke test the repr";"IRRE"
"test with two objects";"IRRE"
"check that estimators are not cloned on pipeline construction";"CODE"
"check that we can t fit with non transformers on the way";"CODE"
"note that notrans implements fit but not transform";"TASK"
"check that params are set";"IRRE"
"smoke test the repr";"IRRE"
"check that params are not set when naming them wrong";"IRRE"
"test clone";"IRRE"
"check that apart from estimators the parameters are the same";"IRRE"
"remove estimators that where copied";"-"
"pipeline accepts steps as tuple";"CODE"
"test the various methods of the pipeline anova";"CODE"
"test with anova logisticregression";"IRRE"
"test that the pipeline can take fit parameters";"IRRE"
"classifier should return true";"IRRE"
"and transformer params should not be changed";"CODE"
"invalid parameters should raise an error message";"IRRE"
"pipeline should pass sample weight";"CODE"
"when sample weight is none it shouldn t be passed";"-"
"test pipeline raises set params error message for nested models";"CODE"
"expected error message";"-"
"invalid outer parameter name for compound parameter the expected error message";"IRRE"
"is the same as above";"-"
"expected error message for invalid inner parameter";"IRRE"
"test the various methods of the pipeline pca svm";"CODE"
"test with pca svc";"IRRE"
"test that the score samples method is implemented on a pipeline";"CODE"
"test that the score samples method on pipeline yields same results as";"IRRE"
"applying transform and score samples steps separately";"CODE"
"check the shapes";"-"
"check the values";"IRRE"
"test that a pipeline does not have score samples method when the final";"CODE"
"step of the pipeline does not have score samples defined";"CODE"
"test the various methods of the pipeline preprocessing svm";"CODE"
"check shapes of various prediction functions";"CODE"
"test that the fit predict method is implemented on a pipeline";"CODE"
"test that the fit predict on pipeline yields same results as applying";"IRRE"
"transform and clustering steps separately";"CODE"
"as pipeline doesn t clone estimators on construction";"CODE"
"it must have its own estimators";"-"
"first compute the transform and clustering step separately";"CODE"
"use a pipeline to do the transform and clustering in one step";"CODE"
"tests that a pipeline does not have fit predict method when final";"CODE"
"step of pipeline does not have fit predict defined";"CODE"
"tests that pipeline passes fit params to intermediate steps";"IRRE"
"when fit predict is invoked";"-"
"tests that pipeline passes predict to the final estimator";"CODE"
"when predict is invoked";"-"
"basic sanity check for feature union";"TASK"
"check if it does the expected thing";"IRRE"
"test if it also works for sparse input";"IRRE"
"we use a different svd object to control the random state stream";"IRRE"
"test clone";"IRRE"
"test setting parameters";"IRRE"
"test it works with transformers missing fit transform";"CODE"
"test error if some elements do not support transform";"CODE"
"test that init accepts tuples";"IRRE"
"test that make union passes verbose feature names out";"IRRE"
"to the featureunion";"TASK"
"test whether pipeline works with a transformer at the end";"CODE"
"also test pipeline transform and pipeline inverse transform";"CODE"
"test transform and fit transform";"CODE"
"test whether pipeline works with a transformer missing fit transform";"CODE"
"test fit transform";"IRRE"
"test class";"IRRE"
"test steps";"IRRE"
"test named steps attribute";"IRRE"
"test the rest of the parameters";"IRRE"
"test exception";"CODE"
"should raise an error if slicing out of range";"CODE"
"should raise an error if indexing with wrong element name";"META"
"directly setting attr";"IRRE"
"using set params";"IRRE"
"using set params to replace single step";"IRRE"
"with invalid data";"OUTD"
"test access via named steps bunch object";"IRRE"
"test bunch with conflict attribute of dict";"IRRE"
"for other methods ensure no attributeerrors on none";"META"
"mult2 and mult3 are active";"-"
"check passthrough step at construction time";"CODE"
"smoke test the repr";"IRRE"
"test feature union with transformer weights";"TASK"
"test using fit followed by transform";"IRRE"
"test using fit transform";"IRRE"
"test it works with transformers missing fit transform";"CODE"
"check against expected result";"IRRE"
"we use a different pca object to control the random state stream";"IRRE"
"todo remove mark once loky bug is fixed";"TASK"
"https github com joblib loky issues 458";"CODE"
"test that n jobs work for featureunion";"TASK"
"fit transform should behave the same";"CODE"
"transformers should stay fit after fit transform";"CODE"
"directly setting attr";"IRRE"
"using set params";"IRRE"
"using set params to replace single step";"IRRE"
"check we can change back";"-"
"check drop step at construction time";"CODE"
"passthrough is stateless";"-"
"featureunion should have the feature names in attribute if the";"TASK"
"first transformer also has it";"CODE"
"fit with pandas dataframe";"-"
"fit with numpy array";"-"
"method split fit transform if fit transform fit otherwise";"CODE"
"test that metadata is routed correctly for pipelines when requested";"CODE"
"some methods don t accept y";"CODE"
"make sure the transformer has received the metadata";"CODE"
"for the transformer always only fit and transform are called";"CODE"
"split and partial fit not relevant for pipelines";"CODE"
"sorted is here needed to make pytest nx work w o it tests are collected";"IRRE"
"in different orders between workers and that makes it fail";"-"
"access sub transformer in name trans with transformer 1";"CODE"
"end of routing tests";"CODE"
"the parameters args and kwargs are ignored since we cannot generate";"IRRE"
"constraints";"CODE"
"generate valid values for the required parameters";"IRRE"
"check that there is a constraint for each parameter";"CODE"
"this object does not have a valid type for sure for all params";"CODE"
"this parameter is not validated";"IRRE"
"mixing an interval of reals and an interval of integers must be avoided";"CODE"
"first check that the error is raised if param doesn t match any valid type";"CODE"
"then for constraints that are more than a type constraint check that the";"CODE"
"error is raised if param does match a valid type but does not match any valid";"CODE"
"value for this type";"CODE"
"test on jl lemma";"IRRE"
"tests random matrix generation";"IRRE"
"all random matrix should produce a transformation matrix";"IRRE"
"with zero mean and unit norm for each columns";"CODE"
"check basic properties of random matrix generation";"IRRE"
"check some statical properties of gaussian random matrix";"IRRE"
"check that the random matrix follow the proper distribution";"IRRE"
"let s say that each element of a ij of a is taken from";"CODE"
"a ij n 0 0 1 n components";"-"
"check some statical properties of sparse random matrix";"IRRE"
"check possible values";"IRRE"
"check that the random matrix follow the proper distribution";"IRRE"
"let s say that each element of a ij of a is taken from";"CODE"
"sqrt s sqrt n components with probability 1 2s";"-"
"0 with probability 1 1 s";"-"
"sqrt s sqrt n components with probability 1 2s";"-"
"tests on random projection transformer";"IRRE"
"remove 0 distances to avoid division by 0";"OUTD"
"remove 0 distances to avoid division by 0";"OUTD"
"check that the automatically tuned values for the density respect the";"IRRE"
"contract for eps pairwise distances are preserved according to the";"CODE"
"johnson lindenstrauss lemma";"-"
"when using sparse input the projected data can be forced to be a";"CODE"
"dense numpy array";"-"
"the output can be left to a sparse matrix instead";"IRRE"
"output for dense input will stay dense";"CODE"
"output for sparse output will be sparse";"IRRE"
"the number of components is adjusted from the shape of the training";"CODE"
"set";"IRRE"
"once the rp is fitted the projection is always the same";"-"
"fit transform with same random seed will lead to the same results";"IRRE"
"try to transform with an input x of size different from fitted";"CODE"
"it is also possible to fix the number of components and the density";"CODE"
"level";"-"
"assert rp components nnz 115 close to 1 density";"CODE"
"assert 85 rp components nnz close to 1 density";"CODE"
"verify output matrix dtype";"IRRE"
"verify numerical consistency among np float32 and np float64";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"types and constants";"CODE"
"base decision tree";"-"
"check input is used for optimisation and isn t something to be passed";"CODE"
"around in a pipeline";"CODE"
"raise a valueerror in case of the presence of an infinite element";"IRRE"
"if the sum is not nan then there are no missing values";"IRRE"
"need to validate separately here";"TASK"
"we can t pass multi output true because that would allow y to be";"IRRE"
"csr";"-"
"compute missing values in feature mask will check for finite values and";"IRRE"
"compute the missing mask if the tree supports missing values";"IRRE"
"determine output settings";"IRRE"
"reshape is necessary to preserve the data contiguity against vs";"-"
"np newaxis that does not";"CODE"
"else float";"CODE"
"else float";"CODE"
"else float";"CODE"
"set min weight leaf from min weight fraction leaf";"IRRE"
"build tree";"-"
"make a deepcopy in case the criterion has mutable attributes that";"IRRE"
"might be shared and modified concurrently during parallel fitting";"-"
"check to correct monotonicity constraint specification";"CODE"
"by applying element wise logical conjunction";"-"
"note we do not cast np asarray self monotonic cst dtype np int8";"CODE"
"straight away here so as to generate error messages for invalid";"OUTD"
"values using the original values prior to any dtype related conversion";"IRRE"
"binary classification trees are built by constraining probabilities";"CODE"
"of the negative class in order to make the implementation similar";"TASK"
"to regression trees";"-"
"since self monotonic cst encodes constraints on probabilities of the";"CODE"
"positive class all signs must be flipped";"TASK"
"todo tree shouldn t need this in this case";"CODE"
"use bestfirst if max leaf nodes given use depthfirst otherwise";"-"
"build pruned tree";"CODE"
"todo the tree shouldn t need this param";"CODE"
"public estimators";"CODE"
"check input is used for optimisation and isn t something to be passed";"CODE"
"around in a pipeline";"CODE"
"xxx nan is only supported for dense arrays but we set this for";"CODE"
"common test to pass specifically check estimators nan inf";"IRRE"
"check input is used for optimisation and isn t something to be passed";"CODE"
"around in a pipeline";"CODE"
"xxx nan is only supported for dense arrays but we set this for";"CODE"
"common test to pass specifically check estimators nan inf";"IRRE"
"xxx nan is only supported for dense arrays but we set this for the";"CODE"
"common test to pass specifically check estimators nan inf";"IRRE"
"xxx nan is only supported for dense arrays but we set this for the";"CODE"
"common test to pass specifically check estimators nan inf";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"initialize saturation value calculate chroma value shift";"IRRE"
"calculate some intermediate values";"IRRE"
"initialize rgb with same hue chroma as our color";"IRRE"
"shift the initial rgb values to match value and store";"IRRE"
"ax no validation delegate validation to matplotlib";"OUTD"
"find the appropriate color intensity for a node";"CODE"
"classification tree";"IRRE"
"regression tree or multi output";"IRRE"
"compute the color as alpha against white";"-"
"return html color code in rrggbb format";"CODE"
"return 2x 2x 2x tuple color";"IRRE"
"fetch appropriate color for node";"CODE"
"initialize colors and bounds if required";"IRRE"
"find max and min impurities for multi output";"IRRE"
"the next line uses max impurity instead of min impurity";"CODE"
"and min impurity instead of max impurity on purpose in";"CODE"
"order to avoid what looks like an issue with simd on non";"CODE"
"memory aligned arrays on 32bit os for more details see";"CODE"
"https github com scikit learn scikit learn issues 27506";"CODE"
"find max and min values in leaf nodes for regression";"IRRE"
"unpack the float only for the regression tree case";"CODE"
"classification tree requires an iterable in get color";"CODE"
"if multi output color node by impurity";"IRRE"
"generate the node content string";"CODE"
"should labels be shown";"-"
"write node id";"TASK"
"write decision criteria";"TASK"
"always write node decision criteria except for leaves";"CODE"
"write impurity";"TASK"
"write node sample count";"TASK"
"write node class distribution regression value";"IRRE"
"for classification this will show the proportion of samples";"CODE"
"regression";"-"
"classification";"IRRE"
"classification without floating point weights";"CODE"
"classification with floating point weights";"CODE"
"strip whitespace";"-"
"write node majority class";"CODE"
"only done for single output classification trees";"CODE"
"clean up any trailing newlines";"TASK"
"postscript compatibility for special characters";"CODE"
"elf characters 35 sub sub le br";"CODE"
"elf characters n";"CODE"
"the depth of each node for plotting with leaf option";"CODE"
"the colors to render each node with";"CODE"
"check length of feature names before getting into the tree node";"CODE"
"raise error if length of feature names does not match";"CODE"
"n features in in the decision tree";"TASK"
"each part writes to out file";"TASK"
"now recurse the tree and add node edge attributes";"TASK"
"if required draw leaf nodes at same depth as each other";"CODE"
"specify node aesthetics";"-"
"specify graph edge aesthetics";"-"
"add node with description";"TASK"
"collect ranks for leaf option in plot options";"CODE"
"add edge to parent";"TASK"
"draw true false labels if parent is root node";"-"
"color cropped nodes grey";"-"
"elf out file write fillcolor c0c0c0";"TASK"
"add edge to parent";"TASK"
"override default escaping for graphviz";"CODE"
"the depth of each node for plotting with leaf option";"CODE"
"the colors to render each node with";"CODE"
"elf characters n";"CODE"
"traverses tree tree recursively builds intermediate";"CODE"
"reingold tilford tree object";"CODE"
"important to make sure we re still";"CODE"
"inside the axis after drawing the box";"-"
"this makes sense because the width of a box";"IRRE"
"is about the same as the distance between boxes";"-"
"update sizes of all bboxes";"CODE"
"get figure to data transform";"CODE"
"adjust fontsize to avoid overlap";"CODE"
"get max box width and height";"-"
"width should be around scale x in axis coordinates";"-"
"kwargs for annotations without a bounding box";"IRRE"
"kwargs for annotations with a bounding box";"IRRE"
"offset things by 5 to center them in plot";"IRRE"
"root";"-"
"draw true false labels if parent is root node";"-"
"adjust the position for the text to be slightly above the arrow";"CODE"
"annotate the arrow with the edge label to indicate the child";"-"
"where the sample split condition is satisfied";"-"
"else leaf";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"this is the number of the node in its group of siblings 1 n";"IRRE"
"print finished v v tree children";"CODE"
"in buchheim notation";"-"
"i inner o outer r right l left r l";"-"
"print wl tree is conflicted with wr tree moving subtrees";"CODE"
"shift shift";"-"
"print wl wr wr number wl number shift subtrees shift subtrees";"CODE"
"print shift w shift w change";"CODE"
"the relevant text is at the bottom of page 7 of";"-"
"improving walker s algorithm to run in linear time by buchheim et al";"CODE"
"2002";"-"
"https citeseerx ist psu edu doc view pid 1f41c3c2a4880dc49238e46d555f16d28da2940d";"CODE"
"toy sample";"-"
"check correctness of export graphviz";"-"
"test export code";"IRRE"
"test with feature names";"TASK"
"test with feature names escaped";"TASK"
"test with class names";"IRRE"
"test with class names escaped";"IRRE"
"test plot options";"IRRE"
"value 0 5 0 5 fillcolor ffffff n";"IRRE"
"fillcolor e58139 n";"-"
"fillcolor 399de5 n";"-"
"test max depth";"IRRE"
"test max depth with plot options";"IRRE"
"0 label node 0 nx 0 0 0 ngini 0 5 n";"-"
"samples 6 nvalue 3 3 fillcolor ffffff n";"IRRE"
"1 label fillcolor c0c0c0 n";"-"
"2 label fillcolor c0c0c0 n";"-"
"test multi output with weighted samples";"IRRE"
"3 0 1 0 0 5 fillcolor ffffff n";"-"
"3 0 0 fillcolor e58139 n";"-"
"0 0 1 0 0 5 fillcolor f1bd97 n";"-"
"0 1 0 fillcolor e58139 n";"-"
"0 0 0 0 0 5 fillcolor e58139 n";"-"
"test regression output with plot options";"IRRE"
"value 0 0 fillcolor f2c09c n";"IRRE"
"fillcolor ffffff n";"-"
"fillcolor e58139 n";"-"
"test classifier with degraded learning set";"IRRE"
"fillcolor ffffff n";"-"
"check that export graphviz treats feature names";"TASK"
"and class names correctly and supports arrays";"IRRE"
"test with feature names";"TASK"
"test with class names";"IRRE"
"check for errors of export graphviz";"CODE"
"check not fitted decision tree error";"-"
"check if it errors when length of feature names";"TASK"
"mismatches with number of features";"TASK"
"check error when feature names contains non string elements";"TASK"
"check error when argument is not an estimator";"-"
"check class names error";"IRRE"
"with the current random state the impurity and the threshold";"IRRE"
"will have the number of precision set in the export graphviz";"IRRE"
"function we will check the number of precision with a strict";"CODE"
"equality the value reported will have only 2 precision and";"IRRE"
"therefore only a less equal comparison will be done";"CODE"
"check value";"IRRE"
"check impurity";"-"
"check impurity";"-"
"check threshold";"-"
"testing that leaves at level 1 are not truncated";"IRRE"
"testing that the rest of the tree is truncated";"IRRE"
"check that export graphviz treats feature names";"TASK"
"and class names correctly and supports arrays";"IRRE"
"mostly smoke tests";"IRRE"
"check correctness of export graphviz for criterion entropy";"CODE"
"test export code";"IRRE"
"mostly smoke tests";"IRRE"
"check correctness of export graphviz for criterion gini";"CODE"
"test export code";"IRRE"
"testing if not fitted tree throws the correct error";"IRRE"
"check the aggregates are consistent with the returned idx";"IRRE"
"check if the cumulative weight is less than or equal to the target";"IRRE"
"depending on t idx low and t idx";"TASK"
"check that if we add the next non null weight we are above the target";"TASK"
"and not below the target for t idx low";"CODE"
"monotonic increase constraint it applies to the positive class";"CODE"
"monotonic decrease constraint it applies to the positive class";"CODE"
"build a regression task using 5 informative features";"TASK"
"monotonic increase constraint";"CODE"
"y incr should always be greater than y";"-"
"monotonic decrease constraint";"CODE"
"y decr should always be lower than y";"-"
"check monotonicity on children";"-"
"check bounds on grand children filtering out leaf nodes";"IRRE"
"check that positive monotonic data with negative monotonic constraint";"CODE"
"yield constant predictions equal to the average of target values";"IRRE"
"swap monotonicity";"-"
"adaptation from test nodes values in test monotonic constraints py";"IRRE"
"in sklearn ensemble hist gradient boosting";"-"
"build a single tree with only one feature and make sure the node";"TASK"
"values respect the monotonicity constraints";"IRRE"
"considering the following tree with a monotonic 1 constraint we";"CODE"
"should have";"-"
"root";"-"
"a b";"-"
"c d e f";"CODE"
"a root b";"-"
"c d a b 2 e f";"-"
"no max leaf nodes default depth first tree builder";"CODE"
"max leaf nodes triggers best first tree builder";"-"
"node value tree value i 0 0 unpack value from nx1x1 array";"IRRE"
"while building the tree the computed middle value is slightly";"IRRE"
"different from the average of the siblings values because";"IRRE"
"sum right weighted n right";"-"
"is slightly different from the value of the right sibling";"IRRE"
"this can cause a discrepancy up to numerical noise when clipping";"CODE"
"which is resolved by comparing with some loss of precision";"-"
"leaf nothing to do";"TASK"
"split node check and update bounds for the children";"CODE"
"unpack value from nx1x1 array";"IRRE"
"feature without monotonicity constraint propagate bounds";"CODE"
"down the tree to both children";"CODE"
"otherwise with 2 features and a monotonic increase constraint";"CODE"
"encoded by 1 on feature 0 the following tree can be accepted";"TASK"
"although it does not respect the monotonic increase constraint";"CODE"
"x 0 0";"-"
"value 100";"IRRE"
"x 0 1 x 1 0";"-"
"value 50 value 150";"IRRE"
"leaf leaf leaf leaf";"-"
"value 25 value 75 value 50 value 250";"IRRE"
"feature with constraint check monotonicity";"CODE"
"propagate bounds down the tree to both children";"CODE"
"feature with constraint check monotonicity";"CODE"
"update and propagate bounds down the tree to both children";"CODE"
"else pragma no cover";"-"
"check that assert nd reg tree children monotonic bounded can detect";"CODE"
"non monotonic tree predictions";"-"
"check that assert nd reg tree children monotonic bounded raises";"CODE"
"when the data and therefore the model is naturally monotonic in the";"CODE"
"opposite direction";"-"
"for completeness check that the converse holds when swapping the sign";"CODE"
"build tree with several features and make sure the nodes";"TASK"
"values respect the monotonicity constraints";"IRRE"
"considering the following tree with a monotonic increase constraint on x 0";"CODE"
"we should have";"-"
"root";"-"
"x 0 t";"-"
"a b";"-"
"x 0 u x 1 v";"-"
"c d e f";"CODE"
"i a root b";"-"
"ii c a d a b 2";"-"
"iii a b 2 min e f";"-"
"for iii we check that each node value is within the proper lower and";"IRRE"
"upper bounds";"-"
"no max leaf nodes default depth first tree builder";"CODE"
"max leaf nodes triggers best first tree builder";"-"
"parents higher than children";"-"
"these trees are always binary";"IRRE"
"parents are centered above children";"-"
"test that x values are unique per depth level";"IRRE"
"we could also do it quicker using defaultdicts";"CODE"
"reached all leafs";"-"
"toy sample";"-"
"also load the iris dataset";"IRRE"
"and randomly permute it";"IRRE"
"also load the diabetes dataset";"IRRE"
"and randomly permute it";"IRRE"
"nb despite their names x sparse are numpy arrays and not sparse matrices";"IRRE"
"check classification on a toy dataset";"IRRE"
"check classification on a weighted toy dataset";"IRRE"
"check regression on a toy dataset";"IRRE"
"make target positive while not touching the original y and";"CODE"
"true result";"IRRE"
"check on a xor problem";"-"
"check consistency on dataset iris";"IRRE"
"check consistency of overfitted trees on the diabetes dataset";"IRRE"
"since the trees will overfit we expect an mse of 0";"-"
"check consistency of trees when the depth and the number of features are";"TASK"
"limited";"-"
"less depth higher error";"-"
"diabetes data shape 0 2 7 so it can t overfit to get a 0 error";"-"
"predict probabilities using decisiontreeclassifier";"IRRE"
"check the array representation";"-"
"check resize";"-"
"check when y is pure";"-"
"check numerical stability";"-"
"check variable importances";"CODE"
"check on iris that importances are the same for all builders";"CODE"
"check if variable importance before fit raises valueerror";"CODE"
"check that gini is equivalent to squared error for binary output variable";"IRRE"
"the gini index and the mean square error variance might differ due";"CODE"
"to numerical instability since those instabilities mainly occurs at";"CODE"
"high tree depth we restrict this maximal depth";"CODE"
"check max features";"TASK"
"test that it gives proper exception on deficient input";"CODE"
"predict before fit";"CODE"
"x2 2 1 1 wrong feature shape for sample";"TASK"
"wrong dimensions";"META"
"test with arrays that are non contiguous";"IRRE"
"predict before fitting";"CODE"
"predict on vector with different dims";"-"
"wrong sample shape";"META"
"apply before fitting";"CODE"
"non positive target for poisson splitting criterion";"CODE"
"test that all class properties are maintained";"CODE"
"check estimators on multi output problems";"IRRE"
"poisson doesn t support negative y and ignores null y";"CODE"
"toy classification problem";"IRRE"
"toy regression problem";"-"
"test that n classes and classes have proper shape";"IRRE"
"classification single output";"IRRE"
"classification multi output";"IRRE"
"check class rebalancing";"CODE"
"check that it works no matter the memory layout";"-"
"nothing";"-"
"c order";"-"
"f order";"CODE"
"contiguous";"-"
"csr";"-"
"csc";"-"
"strided";"-"
"check sample weighting";"-"
"test that zero weighted samples are not taken into account";"IRRE"
"test that low weighted samples are not taken into account at low depth";"IRRE"
"ample weight y 2 0 51 samples of class 2 are still weightier";"TASK"
"ample weight y 2 0 5 samples of class 2 are no longer weightier";"IRRE"
"assert clf tree threshold 0 49 5 threshold should have moved";"CODE"
"test that sample weighting is the same as having duplicates";"IRRE"
"check sample weighting raises errors";"CODE"
"test that class weights resemble sample weights behavior";"IRRE"
"iris is balanced so no effect expected for using balanced weights";"CODE"
"make a multi output problem with three copies of iris";"IRRE"
"create user defined weights that should balance over the outputs";"IRRE"
"check against multi output auto which should also have no effect";"IRRE"
"inflate importance of class 1 check against user defined weights";"CODE"
"check that sample weight and class weight are multiplicative";"IRRE"
"test if class weight raises errors and warnings when expected";"IRRE"
"incorrect length list for multi output";"IRRE"
"test greedy trees with max depth 1 leafs";"IRRE"
"test precedence of max leaf nodes over max depth";"IRRE"
"ensure property arrays memory stays alive when tree disappears";"-"
"non regression for 2726";"CODE"
"if pointing to freed memory contents may be arbitrary";"CODE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn pull 32259";"CODE"
"make sure that almost constant features are discarded";"TASK"
"feature treshold 1e 7 is defined in sklearn tree partitioner pxd but not";"CODE"
"accessible from python";"CODE"
"x 0 feature threshold almost constant feature";"TASK"
"the almost constant feature should not be used";"TASK"
"other feature should be used";"TASK"
"do not check extra random trees";"IRRE"
"test if the warning for too large inputs is appropriate";"CODE"
"sanity check we cannot request more memory than the size of the address";"TASK"
"space currently raises overflowerror";"CODE"
"non regression test memoryerror used to be dropped by cython";"IRRE"
"because of missing except";"CODE"
"gain testing time";"IRRE"
"check the default depth first search";"CODE"
"due to numerical instability of mse and too strict test we limit the";"IRRE"
"maximal depth";"-"
"check max features";"TASK"
"check min samples split";"-"
"check min samples leaf";"-"
"check best first search";"-"
"n samples set n feature to ease construction of a simultaneous";"CODE"
"construction of a csr and csc matrix";"CODE"
"generate x y";"-"
"ensure that x sparse test owns its data indices and indptr array";"IRRE"
"ensure that we have explicit zeros";"-"
"perform the comparison";"CODE"
"1st example";"-"
"2nd example toy dataset";"IRRE"
"was failing before the fix in pr";"CODE"
"https github com scikit learn scikit learn pull 32280";"CODE"
"assert that leaves index are correct";"CODE"
"ensure only one leave node per sample";"-"
"ensure max depth is consistent with sum of indicator";"CODE"
"currently we don t support sparse y";"IRRE"
"test mae where sample weights are non uniform as illustrated above";"IRRE"
"test mae where all sample weights are uniform";"IRRE"
"test mae where a sample weight is not explicitly provided";"TASK"
"this is equivalent to providing uniform sample weights though";"CODE"
"the internal logic is different";"CODE"
"max depth 1 stop after one split";"-"
"let s check whether copy of our criterion has the same type";"CODE"
"and properties as original";"-"
"try to make empty leaf by using near infinite value";"IRRE"
"single node tree";"-"
"pruned single node tree";"CODE"
"generate trees with increasing alphas";"-"
"a pruned tree must be a subtree of the previous tree which had a";"TASK"
"smaller ccp alpha";"-"
"is a leaf";"-"
"not a leaf";"-"
"test that sum y pred sum y true on training set";"IRRE"
"this works if the mean is predicted should even be true for each leaf";"CODE"
"mae predicts the median and is therefore excluded from this test";"CODE"
"choose a training set with non negative targets for poisson";"IRRE"
"test that sum y 0 and therefore y pred 0 is forbidden on nodes";"CODE"
"note that x 0 0 is a 100 indicator for y 0 the tree can";"TASK"
"easily learn that";"-"
"whereas poisson must predict strictly positive numbers";"-"
"test additional dataset where something could go wrong";"IRRE"
"some excess zeros";"-"
"make sure the target is positive";"-"
"for a poisson distributed target poisson loss should give better results";"IRRE"
"than squared error measured in poisson deviance as metric";"-"
"we have a similar test test poisson in";"IRRE"
"sklearn ensemble hist gradient boosting tests test gradient boosting py";"IRRE"
"we create a log linear poisson model and downscale coef as it will get";"IRRE"
"exponentiated";"-"
"we prevent some overfitting by setting min samples split 10";"IRRE"
"squared error might produce non positive predictions clip";"CODE"
"as squared error might correctly predict 0 in train set its train";"IRRE"
"score can be better than poisson this is no longer the case for the";"CODE"
"test set";"IRRE"
"min samples split 1 0 is valid";"-"
"min samples split 1 is invalid";"OUTD"
"missing values necessarily are associated to the observed class";"IRRE"
"max depth is used to avoid overfitting and also improve the runtime";"CODE"
"of the test";"IRRE"
"a single extratree will randomly send missing values down the left or right child";"IRRE"
"and therefore will not necessarily have the same performance as the greedy";"CODE"
"handling of missing values";"IRRE"
"create dataset with missing values";"IRRE"
"zero sample weight is the same as removing the sample";"-"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 27268";"CODE"
"uninitialised memory would lead to the two pickle strings being different";"IRRE"
"missing values will go left for greedy splits";"IRRE"
"missing values will go right for greedy splits";"IRRE"
"assert all impurity 0 impurity min mse should always be positive";"CODE"
"note the impurity matches after the first split only on greedy trees";"TASK"
"see https github com scikit learn scikit learn issues 32125";"CODE"
"check the impurity match after the first split";"-"
"find the leaves with a single sample where the mse should be 0";"-"
"assert all impurity 0 impurity mse should always be positive";"CODE"
"fmt off";"-"
"no black reformatting for this specific array";"CODE"
"fmt on";"-"
"create a tree with root and two children";"IRRE"
"only keeping one child as a leaf results in an improper tree";"IRRE"
"fmt off";"-"
"no black reformatting for this specific array";"CODE"
"fmt on";"-"
"large number of zeros and otherwise continuous weights";"CODE"
"non regression test for https github com scikit learn scikit learn issues 32178";"CODE"
"the important thing here is that we try several trees where each one tries";"CODE"
"one of the two features first the resulting tree should be the same in all";"TASK"
"cases the way to control which feature is tried first is random state";"IRRE"
"twenty trees is a good guess for how many we need to try to make sure we get";"CODE"
"both orders of features at least once";"TASK"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"gives clearer ids for array api strict devices see 31042 for details";"CODE"
"none results in the default pytest representation";"IRRE"
"avoid circular import";"CODE"
"the dlpack protocol is the future proof and library agnostic";"TASK"
"method to transfer arrays across namespace and device boundaries";"-"
"hence this method is attempted first and going through numpy is";"CODE"
"only used as fallback in case of failure";"CODE"
"note copy none is the default since array api 2023 12 namespace";"CODE"
"libraries should only trigger a copy automatically if needed";"IRRE"
"attributeerror occurs when dlpack and dlpack device";"META"
"methods are not present on the input array";"CODE"
"typeerror and notimplementederror for packages that do not";"CODE"
"yet support dlpack 1 0";"TASK"
"i e the device copy kwargs e g torch 2 8 0";"IRRE"
"see https github com data apis array api pull 741 for";"CODE"
"more details about the introduction of the copy and device";"CODE"
"kwargs in the from dlpack method and their expected";"CODE"
"meaning by namespaces implementing the array api spec";"TASK"
"todo try removing this once dlpack v1 more widely supported";"CODE"
"converting to numpy is tricky handle this via dedicated function";"CODE"
"convert from numpy all array libraries can do this";"CODE"
"there is no generic way to convert from namespace a to b";"CODE"
"so we first convert from a to numpy and then from numpy to b";"CODE"
"the way to avoid this round trip is to lobby for dlpack";"CODE"
"support in libraries a and b";"-"
"todo update to use array namespace info from array api v2023 12";"CODE"
"when if that becomes more widespread";"CODE"
"pragma no cover";"-"
"return the floating dtype with the highest precision";"CODE"
"if none of the input arrays have a floating point dtype they must be all";"CODE"
"integer arrays or containers of python scalars return the default";"CODE"
"floating point dtype for the namespace implementation specific";"CODE"
"if weights are 1d add singleton dimensions for broadcasting";"CODE"
"xxx median is not included in the array api spec but is implemented";"CODE"
"in most array libraries and all that we support as of may 2025";"-"
"todo consider simplifying this code to use scipy instead once the oldest";"CODE"
"supported scipy version provides scipy stats quantile with native array api";"META"
"support likely scipy 1 16 at the time of writing proper benchmarking of";"-"
"either option with popular array namespaces is required to evaluate the";"CODE"
"impact of this choice";"CODE"
"torch median takes the lower of the two medians when x has even number";"-"
"of elements thus we use torch quantile q 0 5 which gives mean of the two";"IRRE"
"intended mostly for array api strict which as no median as per the spec";"CODE"
"as convert to numpy does not necessarily work for all array types";"CODE"
"todo remove this once https github com scipy scipy issues 21736 is fixed";"CODE"
"todo refactor once nan aware reductions are standardized";"TASK"
"https github com data apis array api issues 621";"CODE"
"replace infs from all nan slices with nan again";"CODE"
"todo refactor once nan aware reductions are standardized";"TASK"
"https github com data apis array api issues 621";"CODE"
"replace infs from all nan slices with nan again";"CODE"
"todo refactor once nan aware reductions are standardized";"TASK"
"https github com data apis array api issues 621";"CODE"
"todo refactor once nan aware reductions are standardized";"TASK"
"https github com data apis array api issues 621";"CODE"
"use numpy api to support order";"CODE"
"at this point array is a numpy ndarray we convert it to an array";"CODE"
"container that is consistent with the input s namespace";"CODE"
"if no dtype is specified when running tests for a given namespace we";"CODE"
"expect the same floating precision level as numpy s default floating";"CODE"
"point dtype";"CODE"
"currently this is implemented with simple hack that assumes that";"TASK"
"following may be statements in the array api spec always hold";"CODE"
"the default integer data type should be the same across platforms but";"CODE"
"the default may vary depending on whether python is 32 bit or 64 bit";"CODE"
"the default array index data type may be int32 on 32 bit platforms but";"CODE"
"the default should be int64 otherwise";"CODE"
"https data apis org array api latest api specification data types html default data types";"CODE"
"todo once sufficiently adopted we might want to instead rely on the";"CODE"
"newer inspection api https github com data apis array api issues 640";"CODE"
"note this is a helper for the function isin";"CODE"
"it is not meant to be called directly";"IRRE"
"this code is run to make the code significantly faster";"CODE"
"we need this to be a stable sort";"CODE"
"indexing undefined in standard when sar is empty";"CODE"
"todo update if bincount is ever adopted in a future version of the standard";"CODE"
"https github com data apis array api issues 812";"CODE"
"todo replace by scipy special logsumexp when";"TASK"
"https github com scipy scipy pull 22683 is part of a release";"CODE"
"the following code is strongly inspired and simplified from";"CODE"
"scipy special logsumexp logsumexp";"-"
"specifying device explicitly is the fix for https github com scipy scipy issues 22680";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"update the docstring of the descriptor";"CODE"
"delegate only on instances not the classes";"IRRE"
"this is to allow access to the docstrings";"CODE"
"this makes it possible to use the decorated method as an unbound method";"CODE"
"for instance when monkeypatching";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"map from deprecated key to warning message";"OUTD"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"numerical";"-"
"np unique will have duplicate missing values at the end of uniques";"IRRE"
"here we clip the nans and remove it from uniques";"CODE"
"if there is more than one missing value then it has to be";"IRRE"
"float nan or np nan";"CODE"
"create set without the missing values";"IRRE"
"only used in uniques see docstring there for details";"CODE"
"check for nans in the known values";"IRRE"
"removes nan from valid mask";"CODE"
"remove nan from diff";"CODE"
"recorder unique values based on input uniques";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"using take instead of iloc ensures the return value is a proper";"CODE"
"copy that will not raise settingwithcopywarning";"IRRE"
"check whether we should index with loc or iloc";"-"
"polars behavior is more consistent with lists";"-"
"convert each element of the array to a python scalar";"CODE"
"here we are certain to have a polars dataframe which can be indexed with";"CODE"
"integer and string scalar and list of integer string and boolean";"CODE"
"boolean mask can be indexed in the same way for series and dataframe axis 0";"CODE"
"integer scalar and list of integer can be indexed in the same way for series and";"CODE"
"dataframe axis 0";"-"
"x indexed is a dataframe with a single row we return a series to be";"IRRE"
"consistent with pandas";"-"
"safe indexing data 0 axis 0 select the first row";"CODE"
"safe indexing data 0 axis 1 select the first column";"CODE"
"we get an empty list";"-"
"code adapted from stratifiedshufflesplit";"CODE"
"for multi label y map each distinct row to a string repr";"CODE"
"using join because str row uses an ellipsis if len row 1000";"-"
"find the sorted list of instances for each class";"CODE"
"np unique above performs a sort so code is o n logn already";"CODE"
"convert sparse matrices to csr for row based indexing";"IRRE"
"syntactic sugar for the unit argument case";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we also suppress attributeerror because older versions of pandas do";"META"
"not have na";"-"
"can t have nans in integer array";"CODE"
"np isnan does not work on object dtypes";"CODE"
"for all cases apart of a sparse input where we need to reconstruct";"CODE"
"a sparse output";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"only the following methods are supported in the routing mechanism adding new";"CODE"
"methods at the moment involves monkeypatching this list";"CODE"
"note that if this list is changed or monkeypatched the corresponding method";"CODE"
"needs to be added under a type checking condition like the one done here in";"TASK"
"metadatarequester";"CODE"
"these methods are a composite of other methods and one cannot set their";"IRRE"
"requests directly instead they should be set by setting the requests of the";"IRRE"
"simple methods which make the composite ones";"-"
"request values";"IRRE"
"each request value needs to be one of the following values or an alias";"IRRE"
"this is used in metadata request attributes to indicate that a";"CODE"
"metadata is not present even though it may be present in the";"CODE"
"corresponding method s signature";"-"
"this is used whenever a default value is changed and therefore the user";"CODE"
"should explicitly set the value otherwise a warning is shown an example";"IRRE"
"is when a meta estimator is only a router but then becomes also a";"META"
"consumer in a new release";"CODE"
"this is the default used in set method request methods to indicate no";"CODE"
"change requested by the user";"CODE"
"item is only an alias if it s a valid identifier";"-"
"metadata request for simple consumers";"CODE"
"this section includes methodmetadatarequest and metadatarequest which are";"CODE"
"used in simple consumers";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"have shape and length but don t support indexing";"META"
"ugly hack to make iloc work";"-"
"pandas data frames also are array like we want to make sure that";"-"
"input validation in cross validation does not try to call that";"CODE"
"method";"-"
"for binary classifier the confidence score is related to";"CODE"
"classes 1 and therefore should be null";"CODE"
"deactivate key validation for checkingclassifier because we want to be able to";"CODE"
"call fit with arbitrary fit params and record them without this change we";"IRRE"
"would get an error because those arbitrary params are not expected";"TASK"
"checkingclassifier set fit request requestmethod type ignore assignment method assign";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"import matplotlib noqa f401";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"inherits from valueerror and typeerror to keep backward compatibility";"IRRE"
"we allow parameters to not have a constraint so that third party estimators";"CODE"
"can inherit from sklearn estimators without having to necessarily use the";"IRRE"
"validation tools";"-"
"this constraint is satisfied no need to check further";"CODE"
"no constraint is satisfied raise with an informative message";"CODE"
"ignore constraints that we don t want to expose in the error message";"CODE"
"i e options that are for internal purpose or not officially supported";"CODE"
"the dict of parameter constraints is set as an attribute of the function";"CODE"
"to make it possible to dynamically introspect the constraints for";"CODE"
"automatic testing";"IRRE"
"map args kwargs to the function signature";"IRRE"
"ignore self cls and positional keyword markers";"CODE"
"when the function is just a wrapper around an estimator we allow";"CODE"
"the function to delegate validation to the estimator but we replace";"META"
"the name of the estimator by the name of the function in the error";"CODE"
"message to avoid confusion";"CODE"
"we use an interval of real to ignore np nan that has its own constraint";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"display classes are in process of changing from estimator name to name";"CODE"
"try old attr name estimator name first";"CODE"
"note below code requires len param keys 2 which is the case for all";"CODE"
"display classes";"IRRE"
"todo 1 10 remove after the end of the deprecation period of y pred";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"copyright c 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010";"-"
"2011 2012 2013 2014 2015 2016 2017 2018 python software foundation";"CODE"
"all rights reserved";"-"
"authors fred l drake jr fdrake acm org built in cpython pprint module";"CODE"
"nicolas hug scikit learn specific changes";"-"
"license psf license version 2 see below";"META"
"python software foundation license version 2";"META"
"1 this license agreement is between the python software foundation psf";"CODE"
"and the individual or organization licensee accessing and otherwise";"CODE"
"using this software python in source or binary form and its associated";"CODE"
"documentation";"CODE"
"2 subject to the terms and conditions of this license agreement psf hereby";"CODE"
"grants licensee a nonexclusive royalty free world wide license to";"IRRE"
"reproduce analyze test perform and or display publicly prepare";"CODE"
"derivative works distribute and otherwise use python alone or in any";"CODE"
"derivative version provided however that psf s license agreement and";"META"
"psf s notice of copyright i e copyright c 2001 2002 2003 2004";"-"
"2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016";"-"
"2017 2018 python software foundation all rights reserved are retained in";"CODE"
"python alone or in any derivative version prepared by licensee";"META"
"3 in the event licensee prepares a derivative work that is based on or";"CODE"
"incorporates python or any part thereof and wants to make the derivative";"CODE"
"work available to others as provided herein then licensee hereby agrees to";"CODE"
"include in any such work a brief summary of the changes made to python";"CODE"
"4 psf is making python available to licensee on an as is basis psf makes";"CODE"
"no representations or warranties express or implied by way of example but";"META"
"not limitation psf makes no and disclaims any representation or warranty of";"-"
"merchantability or fitness for any particular purpose or that the use of";"CODE"
"python will not infringe any third party rights";"CODE"
"5 psf shall not be liable to licensee or any other users of python for any";"CODE"
"incidental special or consequential damages or loss as a result of";"IRRE"
"modifying distributing or otherwise using python or any derivative";"META"
"thereof even if advised of the possibility thereof";"CODE"
"6 this license agreement will automatically terminate upon a material";"IRRE"
"breach of its terms and conditions";"-"
"7 nothing in this license agreement shall be deemed to create any";"CODE"
"relationship of agency partnership or joint venture between psf and";"CODE"
"licensee this license agreement does not grant permission to use psf";"CODE"
"trademarks or trade name in a trademark sense to endorse or promote products";"CODE"
"or services of licensee or any third party";"-"
"8 by copying installing or otherwise using python licensee agrees to be";"CODE"
"bound by the terms and conditions of this license agreement";"CODE"
"brief summary of changes to original code";"-"
"compact parameter is supported for dicts not just lists or tuples";"IRRE"
"estimators have a custom handler they re not just treated as objects";"IRRE"
"long sequences lists tuples dict items with more than n elements are";"-"
"shortened using ellipsis at the end";"CODE"
"if k not in init params happens if k is part of a kwargs";"IRRE"
"if init params k inspect empty k has no default value";"IRRE"
"try to avoid calling repr on nested estimators";"CODE"
"use repr as a last resort it may be expensive";"-"
"elf indent per level 1 ignore indent param";"-"
"max number of elements in a list dict tuple until we start using";"CODE"
"ellipsis this also affects the number of arguments of an estimators";"CODE"
"they are treated as dicts";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"construct the estimator s module name up to the first private submodule";"CODE"
"this works because in scikit learn all public estimators are exposed at";"CODE"
"that level even if they actually live in a private sub module";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"check if estimator looks like a meta estimator wraps estimators";"IRRE"
"estimator can also be an instance of visualblock";"-"
"build the parameter prefix for nested estimators";"IRRE"
"if we already have a prefix append the new component";"CODE"
"if this is the first level start the prefix";"CODE"
"else parallel";"-"
"wrap element in a serial visualblock";"-"
"out write div sk parallel item";"TASK"
"the fallback message is shown by default and loading the css sets";"CODE"
"div sk text repr fallback to display none to hide the fallback message";"-"
"if the notebook is trusted the css is loaded which hides the fallback";"TASK"
"message if the notebook is not trusted then the css is not loaded and the";"TASK"
"fallback message is shown by default";"CODE"
"the reverse logic applies to html repr div sk container";"META"
"div sk container is hidden by default and the loading the css displays it";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"no match found in the docstring return none to indicate that we";"CODE"
"cannot link";"-"
"extract the whole line of the type information up to the line break as";"CODE"
"disambiguation suffix to build the fragment";"-"
"return f doc link text text fragment";"CODE"
"r maxlist 2 show only first 2 items of lists";"-"
"r maxtuple 1 show only first item of tuples";"-"
"r maxstring 50 limit string length";"CODE"
"create clickable parameter name with documentation link";"IRRE"
"just show the parameter name without link";"IRRE"
"return x y pragma nocover";"IRRE"
"test checking logic and labeling";"IRRE"
"test estimators that are represented by strings";"IRRE"
"top level estimators show estimator with changes";"-"
"low level estimators do not show changes";"CODE"
"feature union";"TASK"
"voting classifier";"IRRE"
"verify that prefers color scheme is implemented";"TASK"
"if final estimator s default changes from logisticregression";"CODE"
"this should be updated";"CODE"
"test duck typing meta estimators with birch";"IRRE"
"inner estimators do not show changes";"CODE"
"outer estimator contains all changes";"-"
"test duck typing metaestimators with ovo";"IRRE"
"inner estimators do not show changes";"CODE"
"regex to match the start of the tag";"CODE"
"outer estimator";"-"
"test duck typing metaestimators with random search";"IRRE"
"mock the version where the mixin is located";"META"
"we need to parse the version manually to be sure that this test is passing in";"IRRE"
"other branches than main that is dev";"CODE"
"if the doc link module does not refer to the root module of the estimator";"CODE"
"here the mixin then we should return an empty string";"CODE"
"we can bypass the generation by providing our own callable";"IRRE"
"we can bypass the generation by providing our own callable";"IRRE"
"resets the locale to the original one python calls setlocale lc type";"IRRE"
"at startup according to";"-"
"https docs python org 3 library locale html background details hints tips and caveats";"CODE"
"this assumes that no other locale changes have been made for some reason";"CODE"
"on some platforms trying to restore locale with something like";"CODE"
"locale setlocale locale lc ctype locale getlocale raises a";"IRRE"
"locale error unsupported locale setting";"IRRE"
"test that function name is shown as the name and functiontransformer is shown";"CODE"
"in the caption";"CODE"
"suppress logging";"-"
"check non default parameters";"IRRE"
"check that we escape html tags";"-"
"quot lt script gt alert x27 xss x27 lt script gt quot";"CODE"
"r shref mock module mockestimator html text a int";"CODE"
"r shref mock module mockestimator html text b str";"CODE"
"assert url mock module mockestimator html text alpha float";"CODE"
"assert url mock module mockestimator html text beta int";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"we don t handle classifiers trained on a single class";"CODE"
"use a compress format of shape n samples n output";"IRRE"
"only mlpclassifier and ridgeclassifier return an array of shape";"IRRE"
"n samples n outputs";"IRRE"
"list of arrays of shape n samples 2";"-"
"array of shape n samples n outputs";"IRRE"
"else estimator is a regressor";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"show threadpoolctl results";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"happens when sklearn tags is implemented by calling";"TASK"
"super sklearn tags but there is no sklearn tags";"META"
"method in the base class typically happens when only inheriting";"IRRE"
"from mixins";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"from sklearn experimental import enable halving search cv noqa f401";"CODE"
"the following dictionary is to indicate constructor arguments suitable for the test";"CODE"
"suite which uses very small datasets and is intended to run rather quickly";"CODE"
"the default strategy prior would output constant predictions and fail";"CODE"
"for check classifiers predictions";"CODE"
"due to the jl lemma and often very few samples the number";"CODE"
"of components of the random matrix projection will be probably";"IRRE"
"greater than the number of features";"TASK"
"so we impose a smaller number avoid auto mode";"CODE"
"the default min samples leaf 20 isn t appropriate for small";"CODE"
"datasets only very shallow trees are built that the checks use";"IRRE"
"noise variance estimation does not work when n samples n features";"CODE"
"we need to provide the noise variance explicitly";"TASK"
"in the case of check fit2d 1sample bandwidth is set to none and";"CODE"
"is thus estimated de facto it is 0 0 as a single sample is provided";"CODE"
"and this makes the test fails hence we give it a placeholder value";"IRRE"
"ransacregressor will raise an error with any model other";"CODE"
"than linearregression if we don t fix the min samples parameter";"IRRE"
"for common tests we can enforce using linearregression that";"CODE"
"is the default estimator in ransacregressor instead of ridge";"CODE"
"be tolerant of noisy datasets not actually speed";"IRRE"
"increases coverage because sgdregressor has partial fit";"-"
"selectkbest has a default of k 10";"CODE"
"which is more feature than we have in most case";"TASK"
"due to the jl lemma and often very few samples the number";"CODE"
"of components of the random matrix projection will be probably";"IRRE"
"greater than the number of features";"TASK"
"so we impose a smaller number avoid auto mode";"CODE"
"default auto parameter can lead to different ordering of eigenvalues on";"IRRE"
"windows 24105";"CODE"
"truncatedsvd doesn t run with n components n features";"CODE"
"this dictionary stores parameters for specific checks it also enables running the";"CODE"
"same check with multiple instances of the same estimator with different parameters";"IRRE"
"the special key allows to apply the parameters to all checks";"IRRE"
"todo devtools allow third party developers to pass test specific params to checks";"TASK"
"todo devtools check that function names here exist in checks for the estimator";"CODE"
"todo 1 9 simplify when averaged inverted cdf is the default";"CODE"
"using subsample none leads to a stochastic fit that is not";"-"
"handled by the check sample weight equivalence on dense data test";"IRRE"
"the kmeans strategy leads to a stochastic fit that is not";"-"
"handled by the check sample weight equivalence test";"IRRE"
"todo dual true is a stochastic solver we cannot rely on";"TASK"
"check sample weight equivalence to check the correct handling of";"-"
"sample weight and we would need a statistical test instead see";"IRRE"
"meta issue 162298";"-"
"dict max iter 20 dual true tol 1e 12";"-"
"raise additional warning to be shown by pytest";"TASK"
"todo devtools enable this behavior for third party estimators as well";"CODE"
"partial tests";"IRRE"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo investigate failure see meta issue 16298";"TASK"
"todo investigate failure see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test when dual true see meta issue 16298";"CODE"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo fix sample weight handling of this estimator see meta issue 16298";"CODE"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo fix sample weight handling of this estimator when probability false";"CODE"
"todo replace by a statistical test when probability true";"TASK"
"see meta issue 16298";"-"
"todo fix sample weight handling of this estimator see meta issue 16298";"CODE"
"todo fix sample weight handling of this estimator see meta issue 16298";"CODE"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"todo replace by a statistical test see meta issue 16298";"TASK"
"valueerror found array with 0 feature s shape 23 0";"TASK"
"while a minimum of 1 is required";"CODE"
"todo fix sample weight handling of this estimator when probability false";"CODE"
"todo replace by a statistical test when probability true";"TASK"
"see meta issue 16298";"-"
"todo fix sample weight handling of this estimator see meta issue 16298";"CODE"
"todo remove when scipy min version 1 11";"TASK"
"both dense";"-"
"import numpydoc noqa f401";"CODE"
"decorator for tests involving both blas calls and multiprocessing";"IRRE"
"under posix e g linux or osx using multiprocessing in conjunction";"-"
"with some implementation of blas or other libraries that manage an";"TASK"
"internal posix thread pool can cause a crash or a freeze of the python";"CODE"
"process";"-"
"in practice all known packaged distributions from linux distros or";"META"
"anaconda of blas under linux seems to be safe so we this problem seems";"CODE"
"to only impact osx users";"-"
"this wrapper makes it possible to skip tests that can possibly cause";"CODE"
"this crash under os x with";"CODE"
"under python 3 4 it is possible to use the forkserver start method";"CODE"
"for multiprocessing to avoid this issue however it can cause pickling";"CODE"
"errors on interactively defined functions it therefore not enabled by";"CODE"
"default";"CODE"
"this can fail under windows";"CODE"
"but will succeed when called by atexit";"IRRE"
"utils to test docstrings";"CODE"
"doctest skip";"IRRE"
"include params y true y pred sample weight doctest skip";"CODE"
"include params true doctest skip";"CODE"
"doctest skip";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"avoid recalculating unique in nested calls";"IRRE"
"in case y is not a numpy array";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"adapted from joblib logger short format time without the windows 1s";"CODE"
"adjustment";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"adapted from https wiki python org moin pythondecoratorlibrary";"CODE"
"but with many changes";"META"
"note that this is only triggered properly if the deprecated";"TASK"
"decorator is placed before the property decorator like so";"CODE"
"deprecated msg";"OUTD"
"property";"-"
"def deprecated attribute self";"CODE"
"restore the original signature see pep 362";"-"
"estimator fit x y should pass";"-"
"estimator fit x y should fail with appropriate error";"-"
"got the right error type and mentioning sparse issue";"IRRE"
"catch deprecation warnings";"CODE"
"catch deprecation warnings";"CODE"
"fit and predict";"-"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 23988";"CODE"
"https github com scikit learn scikit learn issues 24013";"CODE"
"check that estimators will accept a sample weight parameter of";"IRRE"
"type pandas series in the fit function";"CODE"
"check that estimators will accept a sample weight parameter of";"IRRE"
"type notanarray in the fit function";"CODE"
"check that estimators will accept a sample weight parameter of";"IRRE"
"type list in the fit function";"CODE"
"test that estimators don t raise any exception";"CODE"
"check that estimators raise an error if sample weight";"IRRE"
"shape mismatches the input";"CODE"
"check that setting sample weight to zero integer is equivalent";"IRRE"
"to removing repeating corresponding samples";"-"
"use random integers including zero as weights";"IRRE"
"repeat samples according to weights";"-"
"when the estimator has an internal cv scheme";"CODE"
"we only use weights repetitions in a specific cv group here group 0";"-"
"convert to sparse x if needed";"IRRE"
"check that estimators don t override the passed sample weight parameter";"IRRE"
"check that estimators treat dtype object as numeric if possible";"IRRE"
"this error is raised by";"CODE"
"np asarray in check array";"-"
"unique python for encoders";"CODE"
"estimators supporting string will not call np asarray to convert the";"IRRE"
"data to numeric and therefore the error will not be raised";"CODE"
"checking for each element dtype in the input array will be costly";"CODE"
"refer to 11401 for full discussion";"CODE"
"check that estimators raise an exception on providing complex data";"CODE"
"something both valid for classification and regression";"CODE"
"check that fit method only changes or sets private attributes";"CODE"
"to not check deprecated classes";"IRRE"
"check that fit doesn t add any public attribute";"CODE"
"check that fit doesn t change any public attribute";"CODE"
"check by fitting a 2d array and predicting with a 1d array";"-"
"apply function on the whole set and on mini batches";"CODE"
"func can output tuple e g score samples";"IRRE"
"check that method gives invariant results if applied";"IRRE"
"on mini batches or the whole set";"IRRE"
"check that method gives invariant results if applied";"IRRE"
"on a subset with different sample order";"IRRE"
"check that fitting a 2d array with only one sample either works or";"-"
"returns an informative message the error message should either mention";"CODE"
"the number of samples or the number of classes";"IRRE"
"min cluster size cannot be less than the data size for optics";"CODE"
"perplexity cannot be more than the number of samples for tsne";"CODE"
"check fitting a 2d array with only 1 feature either works or returns";"TASK"
"informative message";"CODE"
"ensure two labels in subsample for randomizedlogisticregression";"IRRE"
"ensure non skipped trials for ransacregressor";"CODE"
"check fitting 1d x array raises a valueerror";"IRRE"
"try the same with some list";"CODE"
"fit";"-"
"fit transform method should work on non fitted estimator";"CODE"
"check for consistent n samples";"CODE"
"raises error on malformed input for transform";"CODE"
"if it s not an array it does not have a t property";"CODE"
"check that make pipeline est gives same score as est";"CODE"
"minibatchkmeans";"-"
"catch deprecation warnings";"CODE"
"valueerror was raised with proper error message";"IRRE"
"x should be square for test on svc with precomputed kernel";"IRRE"
"ample weight y copy select a single class";"CODE"
"raise the proper error type with the proper error message";"IRRE"
"for estimators that do not fail they should be able to predict the only";"CODE"
"class remaining during fit";"CODE"
"ignore warnings warnings are raised by decision function";"CODE"
"generate binary problem from multi class one";"CODE"
"raises error on malformed input for fit";"CODE"
"fit";"-"
"with lists";"-"
"training set performance";"IRRE"
"raises error on malformed input for predict";"CODE"
"decision function agrees with predict";"CODE"
"raises error on malformed input for decision function";"CODE"
"predict proba agrees with predict";"-"
"check that probas for all classes sum to one";"CODE"
"raises error on malformed input for predict proba";"CODE"
"predict log proba is a transformation of predict proba";"CODE"
"check for deviation from the precise given contamination level that may";"CODE"
"be due to ties in the anomaly scores";"CODE"
"ensure that all values in the critical area are tied";"IRRE"
"leading to the observed discrepancy between provided";"-"
"and actual contamination levels";"-"
"fit";"-"
"with lists";"-"
"raises error on malformed input for predict";"CODE"
"decision function agrees with predict";"CODE"
"raises error on malformed input for decision function";"CODE"
"decision function is a translation of score samples";"CODE"
"raises error on malformed input for score samples";"CODE"
"contamination parameter not for oneclasssvm which has the nu parameter";"IRRE"
"proportion of outliers equal to contamination parameter when not";"IRRE"
"set to auto this is true for the training set and cannot thus be";"IRRE"
"checked as follows for estimators with a novelty parameter such as";"IRRE"
"localoutlierfactor tested in check outliers fit predict";"IRRE"
"num outliers should be equal to expected outliers unless";"CODE"
"there are ties in the decision function values this can";"CODE"
"only be tested for estimators with a decision function";"CODE"
"method i e all estimators except lof which is already";"CODE"
"excluded from this if branch";"CODE"
"check that the contamination parameter is in 0 0 0 5 when it is an";"IRRE"
"interval constraint";"CODE"
"only estimator implementing parameter constraints will be checked";"CODE"
"y pred shape y test shape with the same dtype";"IRRE"
"y pred shape 2 possibilities";"-"
"list of length n outputs of shape n samples 2";"IRRE"
"ndarray of shape n samples n outputs";"IRRE"
"dtype should be floating";"CODE"
"check that we have the correct probabilities";"-"
"y pred shape y test shape with floating dtype";"IRRE"
"this is run on classes not instances though this should be changed";"CODE"
"this is a very small dataset default n iter are likely to prevent";"CODE"
"convergence";"-"
"let the model compute the class frequencies";"CODE"
"count each label occurrence to reweight manually";"-"
"make a physical copy of the original estimator parameters before fitting";"IRRE"
"fit the model";"-"
"compare the state of the model parameters with the original parameters";"IRRE"
"we should never change or mutate the internal state of input";"CODE"
"parameters by default to check this we use the joblib hash function";"CODE"
"that introspects recursively any subobjects to compute a checksum";"CODE"
"the only exception to this rule of immutable constructor parameters";"CODE"
"is possible randomstate instance but in this check we explicitly";"CODE"
"fixed the random state params recursively to be integer seeds";"IRRE"
"and it should have a default value for this test";"CODE"
"here we check vars because we want to check if the method is";"IRRE"
"explicitly defined in the class instead of inherited from a parent class";"CODE"
"check that calling fit does not raise any warnings about feature names";"CODE"
"only check sklearn estimators for feature names in in docstring";"CODE"
"method x works without userwarning for valid features";"TASK"
"partial fit checks on second call";"IRRE"
"do not call partial fit if early stopping is on";"IRRE"
"input features names is not the same length as n features in";"TASK"
"error is raised when input features do not match feature names in";"CODE"
"check that an informative error is raised when the value of a constructor";"CODE"
"parameter does not have an appropriate type or value";"IRRE"
"check that there is a constraint for each parameter";"CODE"
"this object does not have a valid type for sure for all params";"CODE"
"this parameter is not validated";"IRRE"
"mixing an interval of reals and an interval of integers must be avoided";"CODE"
"first check that the error is raised if param doesn t match any valid type";"CODE"
"the method is not accessible with the current set of parameters";"IRRE"
"the estimator is a label transformer and take only y";"IRRE"
"then for constraints that are more than a type constraint check that the";"CODE"
"error is raised if param does match a valid type but does not match any valid";"CODE"
"value for this type";"CODE"
"the method is not accessible with the current set of parameters";"IRRE"
"the estimator is a label transformer and take only y";"IRRE"
"check transformer set output with the default configuration does not";"CODE"
"change the transform output";"IRRE"
"auto wrapping only wraps the first array";"-"
"default and no setting returns the same transformation";"CODE"
"fit then transform case";"CODE"
"fit transform case";"CODE"
"we always rely on the output of get feature names out of the";"TASK"
"transformer used to generate the dataframe as a ground truth of the";"OUTD"
"columns";"-"
"if a dataframe is passed into transform then the output should have the same";"CODE"
"index";"-"
"check transformer set output configures the output of transform pandas";"IRRE"
"else global";"-"
"transformer does not support sparse data";"CODE"
"except importerror pragma no cover";"CODE"
"except importerror pragma no cover";"CODE"
"these estimators can only work inplace with fortran ordered input";"CODE"
"add a missing value for imputers so that transform has to do something";"CODE"
"make x read only";"CODE"
"generating normal random vectors with shape a shape 1 size";"IRRE"
"xxx generate random number directly from xp if it s possible";"IRRE"
"one day";"-"
"use float32 computation and components if a has a float32 dtype";"CODE"
"move q to device if needed only after converting to float32 if needed to";"CODE"
"avoid allocating unnecessary memory on the device";"IRRE"
"note we cannot combine the astype and to device operations in one go";"TASK"
"using xp asarray dtype dtype device device because downcasting";"CODE"
"from float64 to float32 in asarray might not always be accepted as only";"CODE"
"casts following type promotion rules are guarateed to work";"-"
"https github com data apis array api issues 647";"CODE"
"deal with auto mode";"-"
"xxx https github com data apis array api issues 627";"CODE"
"use scipy linalg instead of numpy linalg when not explicitly";"TASK"
"using the array api";"CODE"
"perform power iterations with q to further imprint the top";"CODE"
"singular vectors of a in q";"-"
"sample the range of a using by linear projection of q";"CODE"
"extract an orthonormal basis";"-"
"weights 1 3 0 5 1 5 1 2 deweight the 4 s";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"todo we can consider removing the containers and importing";"CODE"
"directly from scipy when sparse matrices will be deprecated";"IRRE"
"remove when minimum scipy version is 1 11 0";"META"
"from scipy sparse import sparray noqa f401";"CODE"
"todo remove when scipy 1 11 is the minimum supported version";"TASK"
"scipy stats mode has changed returned array shape with axis none";"IRRE"
"and keepdims true see https github com scipy scipy pull 17561";"CODE"
"todo remove when scipy 1 12 is the minimum supported version";"TASK"
"todo fuse the modern implementations of sparse min max and sparse nan min max";"TASK"
"into the public min max axis function when scipy 1 11 is the minimum supported";"CODE"
"version and delete the backport in the else branch below";"CODE"
"this code is mostly taken from scipy 0 14 and extended to handle nans see";"CODE"
"https github com scikit learn scikit learn pull 11196";"CODE"
"reduceat tries casts x indptr to intp which errors";"CODE"
"if it is int64 on a 32 bit system";"CODE"
"reinitializing prevents this where possible see 13737";"IRRE"
"for 1 25 numpy versions exceptions and warnings are being moved";"CODE"
"to a dedicated submodule";"CODE"
"from numpy import noqa f401";"CODE"
"todo adapt when pandas 2 2 is the minimum supported version";"TASK"
"todo remove when scipy 1 12 is the minimum supported version";"TASK"
"else requested sparse format coo";"CODE"
"todo remove when scipy 1 12 is the minimum supported version";"TASK"
"when check contents is false we stay on the safe side and return";"IRRE"
"np int64";"CODE"
"a bigger type not needed yet let s look at the next array";"TASK"
"a big index type is actually needed";"META"
"todo remove when scipy 1 12 is the minimum supported version";"TASK"
"laplacian noqa f401 pragma no cover";"-"
"todo remove when python min version 3 12";"CODE"
"use filter data to prevent the most dangerous security issues";"CODE"
"for more details see";"CODE"
"https docs python org 3 library tarfile html tarfile tarfile extractall";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"this is a modified file from scipy optimize";"CODE"
"original authors travis oliphant eric jones";"META"
"have a look at the line search method of our newtonsolver class we borrow";"CODE"
"the logic from there";"CODE"
"deal with relative loss differences around machine precision";"-"
"2 1 check sum of absolute gradients as alternative condition";"-"
"1 0 step size";"-"
"ret 1 1 number of function evaluations";"CODE"
"ret 2 1 number of gradient evaluations";"-"
"line search failed try different one";"CODE"
"todo it seems that the new check for the sum of absolute gradients above";"CODE"
"catches all cases that earlier ended up here in fact our tests never";"CODE"
"trigger this if branch here and we can consider to remove it";"CODE"
"ri np copy fgrad residual fgrad fhess p xsupi";"-"
"we also keep track of p i 2";"-"
"check curvature";"-"
"see https arxiv org abs 1803 02924 algo 1 capped conjugate gradient";"CODE"
"fall back to steepest descent direction";"CODE"
"we use p i 2 r i 2 beta i 2 p i 1 2";"-"
"dri0 dri1 update np dot ri ri for next time";"CODE"
"outer loop our newton iteration";"IRRE"
"compute a search direction pk by applying the cg method to";"-"
"del2 f xk p fgrad f xk starting from 0";"CODE"
"inner loop solve the newton update by conjugate gradient to";"CODE"
"avoid inverting the hessian";"CODE"
"xk alphak xsupi upcast if necessary";"-"
"handle both scipy and scikit learn solver names";"-"
"in scipy 1 0 0 nit may exceed maxiter for lbfgs";"CODE"
"see https github com scipy scipy issues 7854";"CODE"
"append a recommendation to increase iterations only when the";"CODE"
"number of iterations reaches the maximum allowed max iter";"-"
"as this suggests the optimization may have been prematurely";"CODE"
"terminated due to the iteration limit";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"global threadpool controller instance that can be used to locally limit the number of";"IRRE"
"threads without looping through all shared libraries every time";"IRRE"
"it should not be accessed directly and get threadpool controller should be used";"CODE"
"instead";"-"
"todo is there a simpler way that resetwarnings filterwarnings";"TASK"
"some small discrepancy between warnings filters and what";"-"
"filterwarnings expect simplefilter is more lenient e g";"-"
"accepts a tuple as category we try simplefilter first and";"CODE"
"use filterwarnings in more complicated cases";"CODE"
"message and module are most of the time regex pattern but";"CODE"
"can be str as well and filterwarnings wants a str";"-"
"else axis 0";"-"
"the following swapping makes life easier since m is assumed to be the";"-"
"smaller integer below";"CODE"
"modify indptr first";"-"
"we rely here on the fact that np diff y indptr for a csr";"CODE"
"will return the number of nonzero entries in each row";"IRRE"
"a bincount over y indices will return the number of nonzeros";"IRRE"
"in each column see csr matrix getnnz in scipy 0 14";"-"
"astype here is for consistency with axis 0 dtype";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"sample weight should follow array for dtypes";"CODE"
"when sample weight 1d repeat for each array shape 1";"CODE"
"sort array and sample weight along axis 0";"-"
"set nan values in sample weight to 0 only perform this operation if nan";"IRRE"
"values present to avoid temporary allocations of size n samples n features";"IRRE"
"nan values get sorted to end largest value";"IRRE"
"compute the weighted cumulative distribution function cdf based on";"CODE"
"sample weight and scale percentile rank along it";"-"
"note we call xp cumulative sum on the transposed sorted weights to";"TASK"
"ensure that the result is of shape n features n samples so";"TASK"
"xp searchsorted calls take contiguous inputs as a result for";"IRRE"
"performance reasons";"CODE"
"ignore leading sample weight 0 observations";"-"
"when percentile rank 0 20528";"-"
"for each feature with index j find sample index i of the scalar value";"CODE"
"adjusted percentile rank j in 1d array weight cdf j such that";"-"
"weight cdf j i 1 adjusted percentile rank j weight cdf j i";"-"
"note searchsorted defaults to equality on the right whereas hyndman and fan";"TASK"
"reference equation has equality on the left";"CODE"
"percentile indices may be equal to sorted idx shape 0 due to floating";"CODE"
"point error see 11813";"CODE"
"from hyndman and fan 1996 fraction above is g";"CODE"
"handle case when next index plus one has sample weight of 0";"CODE"
"search for next index where weighted cdf is greater";"CODE"
"handle case where there are trailing 0 sample weight samples";"CODE"
"and percentile indices is already max index";"CODE"
"use original percentile indices again";"-"
"check that the initialization a sampling from an uniform distribution";"CODE"
"where we can fix the random state";"IRRE"
"sparse and none to numpy";"IRRE"
"sparse to non numpy";"IRRE"
"def test convert to numpy gpu library pragma nocover";"CODE"
"use torch as a library with custom device objects";"IRRE"
"when dispatch is disabled get namespace and device should return the";"CODE"
"default numpy wrapper namespace and cpu device our code will handle such";"CODE"
"inputs via the usual array interface without attempting to dispatch";"CODE"
"via the array api";"CODE"
"otherwise expose the torch namespace and device via array api compat";"IRRE"
"wrapper";"-"
"numpy 2 0 has a problem with the device attribute of scalar arrays";"META"
"https github com numpy numpy issues 26850";"CODE"
"for numpy 2 the device attribute is not available on numpy arrays";"META"
"note depending on the value of axis this test will compare median";"IRRE"
"computations on arrays of even 4 or odd 5 numbers of elements hence";"-"
"will test for median computation with and without interpolation to check";"CODE"
"that array api namespaces yield consistent results even when the median is";"IRRE"
"not mathematically uniquely defined";"IRRE"
"we convert array api strict arrays to numpy arrays as median is not";"CODE"
"part of the array api spec";"CODE"
"if torch on cpu or array api strict on default device";"CODE"
"check that logsumexp works when array api dispatch is disabled";"CODE"
"test with nans and np inf";"IRRE"
"check that min pos returns a positive value and that it s consistent";"IRRE"
"between float and double";"CODE"
"check that the return value of min pos is the maximum representable";"IRRE"
"value of the input dtype when all input elements are 0 19328";"CODE"
"check that return value is false when there is no row equal to value";"IRRE"
"make a row equal to value";"IRRE"
"check that gen even slices contains all samples";"-"
"test and demo compute class weight";"IRRE"
"total effect of samples is preserved";"-"
"non regression for https github com scikit learn scikit learn issues 8312";"CODE"
"raise error when y does not contain all class labels";"CODE"
"when the user specifies class weights compute class weights should just";"IRRE"
"return them";"IRRE"
"when a class weight is specified that isn t in classes the weight is ignored";"IRRE"
"test that results with class weight balanced is invariant wrt";"IRRE"
"class imbalance if the number of samples is identical";"IRRE"
"the test uses a balanced two class dataset with 100 datapoints";"IRRE"
"it creates three versions one where class 1 is duplicated";"IRRE"
"resulting in 150 points of class 1 and 50 of class 0";"CODE"
"one where there are 50 points in class 1 and 150 in class 0";"CODE"
"and one where there are 100 points of each class this one is balanced";"CODE"
"again";"-"
"with balancing class weights all three should give the same model";"CODE"
"create dataset where class 1 is duplicated twice";"IRRE"
"create dataset where class 0 is duplicated twice";"IRRE"
"duplicate everything";"-"
"results should be identical";"IRRE"
"test compute class weight when labels are negative";"IRRE"
"test with balanced class labels";"IRRE"
"test with unbalanced and negative class labels for";"IRRE"
"equivalence between repeated and weighted samples";"-"
"test compute class weight when classes are unordered";"IRRE"
"test for the case where no weight is given for a present class";"CODE"
"current behaviour is to assign the unweighted classes a weight of 1";"IRRE"
"test for non specified weights";"IRRE"
"tests for partly specified weights";"IRRE"
"test and demo compute sample weight";"IRRE"
"test with balanced classes";"IRRE"
"test with user defined weights";"IRRE"
"test with column vector of balanced classes";"IRRE"
"test with unbalanced classes";"IRRE"
"test with none weights";"IRRE"
"test with multi output of balanced classes";"IRRE"
"test with multi output with user defined weights";"IRRE"
"test with multi output of unbalanced classes";"IRRE"
"test compute sample weight with subsamples specified";"IRRE"
"test with balanced classes and all samples present";"IRRE"
"test with column vector of balanced classes and all samples present";"IRRE"
"test with a subsample";"IRRE"
"test with a bootstrap subsample";"IRRE"
"test with a bootstrap subsample for multi output";"IRRE"
"test with a missing class";"IRRE"
"test with a missing class for multi output";"IRRE"
"test compute sample weight raises errors expected";"IRRE"
"invalid preset string";"IRRE"
"non regression smoke test for 12146";"IRRE"
"y np arange 50 more than 32 distinct classes";"IRRE"
"indices np arange 50 use subsampling";"-"
"does not raise";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"deprecated n features is deprecated type ignore prop decorator";"OUTD"
"test for the check unknown parameter of encode";"IRRE"
"default is true raise error";"CODE"
"dont raise error if false";"CODE"
"parameter is ignored for object dtype";"IRRE"
"check for check unknown with missing values with object dtypes";"IRRE"
"check for unique and encode with missing values with object dtypes";"IRRE"
"else missing value np nan";"IRRE"
"check missing values in numerical values";"IRRE"
"test for all types of missing values for object dtype";"IRRE"
"last value is nan";"IRRE"
"test for both types of missing values for object dtype";"IRRE"
"we can not use pytest here because we run";"IRRE"
"build tools azure test pytest soft dependency sh on these";"IRRE"
"tests to make sure estimator checks works without pytest";"IRRE"
"note that object is an uninitialized class thus immutable";"IRRE"
"raise for type str expects sparse array or sparse matrix";"IRRE"
"convert data";"-"
"function is only called after we verify that pandas is installed";"IRRE"
"intentionally modify the balanced class weight";"CODE"
"to simulate a bug and raise an exception";"CODE"
"simply assigning coef to the class weight";"IRRE"
"convert data";"-"
"return 1 if x has more than one element else return 0";"IRRE"
"store the original x to check for sample order later";"CODE"
"if the input contains the same elements but different sample order";"META"
"then just return zeros";"IRRE"
"find the number of class after trimming";"IRRE"
"raise for type str expects sparse array or sparse matrix";"IRRE"
"toy classifier that only supports binary classification will fail tests";"IRRE"
"toy classifier that only supports binary classification";"IRRE"
"reject sparse x to be able to call x 0 any";"IRRE"
"reject sparse x to be able to call x 0 any";"IRRE"
"check that values returned by get params match set params";"IRRE"
"check that predict does input validation doesn t accept dicts in input";"CODE"
"check that estimator state does not change";"IRRE"
"at transform predict predict proba time";"CODE"
"check that sample weights in fit accepts pandas series type";"-"
"from pandas import series noqa f401";"CODE"
"check that fit only changes attributes that";"IRRE"
"are private start with an or end with a";"CODE"
"check that fit doesn t add any public attribute";"CODE"
"check for sample order invariance";"CODE"
"check for invariant method";"CODE"
"check for sparse data input handling";"CODE"
"large indices test on bad estimator";"IRRE"
"check for classifiers reducing to less than two classes via sample weights";"CODE"
"some tests are expected to fail some are expected to pass";"IRRE"
"some estimator checks rely on warnings in deep functions calls this is not";"CODE"
"automatically detected by pytest run parallel shallow ast inspection so we";"IRRE"
"need to mark the test function as thread unsafe";"CODE"
"tests that the estimator actually fails on bad estimators";"IRRE"
"not a complete test of all checks which are very extensive";"CODE"
"check that we have a fit method";"-"
"does error on binary only untagged estimator";"CODE"
"non regression test for estimators transforming to sparse data";"IRRE"
"doesn t error on actual estimator";"CODE"
"doesn t error on binary only tagged estimator";"CODE"
"check regressor with requires positive y estimator tag";"CODE"
"does not raise error on classifier with poor score tag";"CODE"
"should raise assertionerror";"CODE"
"should pass";"-"
"estimator tag sparse accept sparse false fails on sparse data";"IRRE"
"but does not raise the appropriate error";"CODE"
"check that transformermixin is not required for transformer tests to run";"CODE"
"but it fails since the tag is not set";"IRRE"
"check that check estimator doesn t modify the estimator it receives";"CODE"
"without fitting";"-"
"with fitting";"-"
"check that a valueerror attributeerror is raised when calling predict";"IRRE"
"on an unfitted estimator";"-"
"check that correctnotfittederror inherit from either valueerror";"IRRE"
"or attributeerror";"META"
"making sure our metadata request class attributes are okay";"CODE"
"return self pragma no cover";"CODE"
"a private class attribute is okay";"CODE"
"also check if cloning an estimator which has non default set requests is";"IRRE"
"fine setting a non default value via set method request sets the";"IRRE"
"private metadata request instance attribute which is copied in clone";"CODE"
"some estimator checks rely on warnings in deep functions calls this is not";"CODE"
"automatically detected by pytest run parallel shallow ast inspection so we";"IRRE"
"need to mark the test function as thread unsafe";"CODE"
"check that check estimator works on estimator with pairwise";"-"
"kernel or metric";"-"
"test precomputed kernel";"IRRE"
"test precomputed metric";"IRRE"
"1 inconsistent array type";"META"
"2 inconsistent shape";"META"
"3 inconsistent dtype";"META"
"1 unknown output type";"IRRE"
"2 for list output";"IRRE"
"2 1 inconsistent length";"META"
"2 2 array of inconsistent shape";"META"
"2 3 array of inconsistent dtype";"META"
"2 4 array does not contain probability each row should sum to 1";"CODE"
"3 for array output";"IRRE"
"3 1 array of inconsistent shape";"META"
"3 2 array of inconsistent dtype";"META"
"4 array does not contain probabilities";"CODE"
"1 inconsistent array type";"META"
"2 inconsistent shape";"META"
"3 inconsistent dtype";"META"
"this is to make sure we test a class that has some expected failures";"IRRE"
"fixme this test should be uncommented when the checks will be granular";"IRRE"
"enough in 0 24 these tests fail due to low estimator performance";"IRRE"
"check that third party library can run tests without inheriting from";"CODE"
"baseestimator";"-"
"fixme";"-"
"no warnings are raised";"CODE"
"make an estimator that throws the wrong error to make sure we catch it";"CODE"
"this assertion is just to make sure we are catching the value error";"CODE"
"that comes from wrong y none and not some other value error";"IRRE"
"override the error message force fail";"CODE"
"check estimators with non deterministic tag set to true";"IRRE"
"will skip certain tests refer to issue 22313 for details";"CODE"
"return self pragma no cover";"CODE"
"now we check that with the parameter constraints the test should only be valid";"CODE"
"if an interval constraint with bound in 0 1 is provided";"CODE"
"add a correct interval constraint and check that the test passes";"CODE"
"interval integral 0 1 closed right not an integral interval";"CODE"
"interval real 1 1 closed right lower bound is negative";"CODE"
"interval real 0 2 closed right upper bound is greater than 1";"CODE"
"interval real 0 0 5 closed left lower bound include 0";"CODE"
"test that yield all checks with legacy true returns more checks";"IRRE"
"check that all non legacy checks are included in legacy checks";"CODE"
"return none pragma no cover";"IRRE"
"return none pragma no cover";"IRRE"
"return none pragma no cover";"IRRE"
"return none pragma no cover";"IRRE"
"this shouldn t fail since we allow both sklearn tags and more tags";"CODE"
"to exist so that third party estimators can easily support multiple sklearn";"-"
"versions";"META"
"we don t actually need to define the tag here since we re running the test";"CODE"
"manually and baseestimator defaults to multi output false";"IRRE"
"test that set output doesn t make the tests to fail";"IRRE"
"doing this since pytest is not available for this file";"CODE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"with uniform weights results should be identical to stats mode";"IRRE"
"set this up so that each row should have a weighted mode of 6";"IRRE"
"with a score that is easily reproduced";"-"
"check that extmath randomized svd is consistent with linalg svd";"IRRE"
"generate a matrix x of approximate effective rank rank and no noise";"-"
"component very structured signal";"CODE"
"compute the singular values of x using the slow exact method";"IRRE"
"convert the singular values to the specific dtype";"IRRE"
"for normalizer in auto lu qr none would not be stable";"CODE"
"compute the singular values of x using the fast approximate method";"IRRE"
"if the input dtype is float then the output dtype is float of the";"CODE"
"same bit size f32 is not upcast to f64";"-"
"but if the input dtype is int the output dtype is float64";"CODE"
"ensure that the singular values of both methods are equal up to the";"IRRE"
"real rank of the matrix";"-"
"check the singular vectors too while not checking the sign";"CODE"
"check the sparse matrix representation";"IRRE"
"compute the singular values of x using the fast approximate method";"IRRE"
"the attributes like mean and var are computed and set with respect to the";"IRRE"
"maximum supported float dtype";"CODE"
"testing of correctness and numerical stability";"IRRE"
"compare to weighted average np average";"IRRE"
"compare to unweighted mean np mean";"IRRE"
"test youngs and cramer incremental variance formulas";"CODE"
"doggie data from https www mathsisfun com data standard deviation html";"CODE"
"test youngs and cramer incremental variance formulas";"CODE"
"naive one pass variance computation not numerically stable";"IRRE"
"https en wikipedia org wiki algorithms for calculating variance";"CODE"
"two pass algorithm stable";"-"
"we use it as a benchmark it is not an online algorithm";"-"
"https en wikipedia org wiki algorithms for calculating variance two pass algorithm";"CODE"
"naive online implementation";"TASK"
"https en wikipedia org wiki algorithms for calculating variance online algorithm";"CODE"
"this works only for chunks for size 1";"CODE"
"we want to show a case when one pass var has error 1e 3 while";"CODE"
"batch mean variance update has less";"CODE"
"naive one pass var tol 1063";"CODE"
"starting point for online algorithms after a0";"CODE"
"naive implementation tol 436";"TASK"
"the mean is also slightly unstable";"-"
"robust implementation tol 177";"TASK"
"test that degrees of freedom parameter for calculations are correct";"IRRE"
"assign this twice so that the test logic is consistent";"IRRE"
"testing that sign flip is working largest value has positive sign";"IRRE"
"dense nd sparse";"IRRE"
"sparse dense nd";"IRRE"
"2d 1d";"-"
"1d 2d";"-"
"draws 25 of the total population so in this case a fair draw means";"CODE"
"25 99 000 24 750";"-"
"25 1 000 250";"-"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"np int32 default behaviour";"CODE"
"arrays dtype is int64 and thus should not be downcasted to int32 without";"CODE"
"checking the content of providing maxval";"-"
"one of the array is int64 and should not be downcasted to int32";"CODE"
"for the same reasons";"CODE"
"both arrays are already int32 we can just keep this dtype";"CODE"
"arrays should be upcasted to at least int32 precision";"CODE"
"check that maxval takes precedence over the arrays and thus upcast to";"-"
"int64";"CODE"
"empty arrays should always be converted to int32 indices";"CODE"
"arrays respecting np iinfo np int32 min x np iinfo np int32 max should";"CODE"
"be converted to int32";"CODE"
"otherwise it should be converted to int64 we need to create a uint32";"CODE"
"arrays to accommodate a value np iinfo np int32 max";"IRRE"
"maxval should take precedence over the arrays contents and thus upcast to";"-"
"int64";"CODE"
"when maxval is small but check contents is true and the contents";"META"
"require np int64 we still require np int64 indexing in the end";"CODE"
"test that fix connected components reduces the number of component to 1";"IRRE"
"test that fix connected components accepts precomputed distance matrix";"IRRE"
"but it does not work with precomputed neighbors graph";"META"
"test that the an error is raised if the mode string is incorrect";"CODE"
"test that the connectivity mode fill new connections with ones";"CODE"
"test that the distance mode does not fill new connections with ones";"CODE"
"toy array";"-"
"polars dataframes go down the interchange path";"CODE"
"border case not worth mentioning in doctests";"CODE"
"check that invalid arguments yield valueerror";"IRRE"
"issue 6581 n samples can be more when replace is true default";"CODE"
"check that sampling with replacement with integer weights yields the";"CODE"
"samples from the same distribution as sampling uniformly with";"CODE"
"repeated data points";"CODE"
"should never be negative because 1 has a 0 weight";"-"
"the null hypothesis the computed means are identically distributed";"IRRE"
"cannot be rejected";"-"
"make sure resample can stratify";"-"
"assert np sum y stratified 9 all 1s one 0";"CODE"
"make sure stratified resampling supports the replace parameter";"IRRE"
"make sure n samples can be greater than x shape 0 if we sample with";"-"
"replacement";"-"
"make sure y can be 2d when stratifying";"-"
"resample must be ndarray";"TASK"
"def to tuple a to make the inner arrays hashable";"CODE"
"a np array 1 2 3 4 5 6 7 8 a shape 2 2 2";"-"
"huffle a shouldn t raise a valueerror for dim 3";"CODE"
"check that shuffle does not try to convert to numpy arrays with float";"CODE"
"dtypes can let any indexable datastructure pass through";"CODE"
"this is a non regression test for";"CODE"
"https github com scikit learn scikit learn issues 20614";"CODE"
"to make sure that decorated functions can be used as an unbound method";"CODE"
"for instance when monkeypatching";"CODE"
"9867966753463435747313673 false python int that overflows with c type";"CODE"
"make sure that we are returning a python bool";"CODE"
"check that the checkingclassifier outputs what we expect";"IRRE"
"check the shape in case of binary classification";"CODE"
"check the error raised when the number of samples is not the one expected";"CODE"
"check that methods to check allows to bypass checks";"-"
"valid when the data is formatted as sparse or dense identified";"IRRE"
"by csr format when the testing takes place";"IRRE"
"only valid when data is dense";"CODE"
"sequence of sequences that weren t supported even before deprecation";"CODE"
"and also confusable as sequences of sequences";"-"
"ndim 0";"-"
"empty second dimension";"-"
"empty iterable";"-"
"multiclass problem";"IRRE"
"multilabel indicator";"-"
"several arrays passed";"-"
"border line case with binary indicator matrix";"CODE"
"create array of unique labels except 0 which appears twice";"IRRE"
"this does raise a warning";"CODE"
"note warning would not be raised if we passed only unique";"CODE"
"labels which happens when type of target is passed classes";"IRRE"
"less than 20 samples no warning should be raised";"CODE"
"more than 20 samples but only unique classes simulating passing";"IRRE"
"classes to type of target when number of classes is large";"IRRE"
"no warning should be raised";"CODE"
"test unique labels with a variety of collected examples";"IRRE"
"smoke test for all supported format";"CODE"
"we don t support those format at the moment";"CODE"
"mix with binary or multiclass and multilabel";"IRRE"
"only mark explicitly defined sparse examples as valid sparse";"IRRE"
"multilabel indicators";"-"
"densify sparse examples before testing";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"test that newton cg gives same result as scipy s fmin ncg";"IRRE"
"func is a definite positive quadratic form so the minimum is at x 0";"CODE"
"hence the use of absolute tolerance";"-"
"tests that the global global configuration is passed to joblib jobs";"IRRE"
"in free threading python 3 14 warnings filters are managed through a";"CODE"
"contextvar and warnings filters is not modified inside a";"IRRE"
"warnings catch warnings context you need to use warnings get filters";"TASK"
"for more details see";"CODE"
"https docs python org 3 14 whatsnew 3 14 html concurrent safe warnings control";"CODE"
"some helpers for the tests";"IRRE"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"pass pragma no cover";"-"
"check in the presence of extra positional and keyword args";"IRRE"
"validated method can be decorated";"-"
"auto and warn are valid params";"-"
"the warn option is not exposed in the error message";"TASK"
"true false and np bool true false are valid params";"CODE"
"does not raise";"CODE"
"check name is handled correctly";"-"
"check name is handled correctly";"-"
"check pos label is handled correctly";"-"
"missing indices key";"-"
"x wrong length";"META"
"y not binary";"-"
"sample weight wrong length";"META"
"todo 1 9 remove";"TASK"
"name is none";"-"
"in the following test we check the value of the max to min ratio";"IRRE"
"for parameter value intervals to check that using a decision threshold";"IRRE"
"of 5 is a good heuristic to decide between linear and log scales on";"IRRE"
"common ranges of parameter values";"IRRE"
"such a range could be clearly displayed with either log scale or linear";"CODE"
"scale";"-"
"checking that the ratio is still positive on a negative log scale";"TASK"
"evenly spaced parameter values lead to a ratio of 1";"IRRE"
"this is not exactly spaced on a log scale but we will benefit from treating";"CODE"
"it as such for visualization";"CODE"
"constructors excerpted to test pprinting";"CODE"
"basic pprint test";"IRRE"
"expected expected 1 remove first n";"-"
"make sure the changed only param is correctly used when true default";"CODE"
"defaults to np nan trying with float nan";"CODE"
"test custom sampling without replacement algorithm";"IRRE"
"n population n sample";"-"
"n population n samples";"-"
"n population n samples";"-"
"n population 0 or n samples 0";"-"
"this test is heavily inspired from test random py of python core";"IRRE"
"for the entire allowable range of 0 k n validate that";"CODE"
"the sample is of the correct length and contains only unique items";"-"
"test edge case n population n samples 0";"IRRE"
"this test is heavily inspired from test random py of python core";"IRRE"
"for the entire allowable range of 0 k n validate that";"CODE"
"sample generates all possible permutations";"-"
"a large number of trials prevents false negatives without slowing normal";"-"
"case";"CODE"
"counting the number of combinations is not as good as counting the";"-"
"the number of permutations however it works with sampling algorithm";"-"
"that does not provide a random permutation of the subset of integer";"IRRE"
"explicit class probabilities";"IRRE"
"implicit class probabilities";"IRRE"
"classes 0 1 1 2 test for array like support";"IRRE"
"edge case probabilities 1 0 and 0 0";"CODE"
"one class target data";"IRRE"
"classes 1 0 test for array like support";"IRRE"
"the length of an array in classes and class probabilities is mismatched";"IRRE"
"the class dtype is not supported";"IRRE"
"the class dtype is not supported";"IRRE"
"given probabilities don t sum to 1";"CODE"
"scale the data to avoid convergencewarning with logisticregression";"CODE"
"provide a pos label which is not in y";"-"
"default pos label";"CODE"
"when forcing pos label classifier classes 0";"CODE"
"default pos label";"CODE"
"when forcing pos label classifier classes 0";"CODE"
"it should use predict proba";"-"
"it should use decision function";"CODE"
"values returned by decision function are not bounded in 0 1";"IRRE"
"else response method predict";"CODE"
"30 classes less than 50 of number of samples";"IRRE"
"authors the scikit learn developers";"META"
"spdx license identifier bsd 3 clause";"-"
"group dataset by array types to get a tuple float32 float64";"CODE"
"next sample";"-"
"random sample";"IRRE"
"not shuffled";"-"
"next sample";"-"
"update columns with create container";"IRRE"
"adapter update columns updates the columns";"CODE"
"adapter hstack stacks the dataframes horizontally";"-"
"check the behavior of the inplace parameter in create container";"IRRE"
"we should trigger a copy";"-"
"the operation is inplace";"-"
"estimator without transform will not raise when setting set output for transform";"CODE"
"estimator with transform but without set output will raise";"IRRE"
"transform is none is a no op so the config remains default";"CODE"
"return x pragma no cover";"IRRE"
"set nonzero entries to infinity";"IRRE"
"set diagonal to zero";"IRRE"
"sparse grid of distances";"IRRE"
"make symmetric distances are not direction dependent";"CODE"
"make graph sparse";"IRRE"
"set diagonal to zero";"IRRE"
"we compare path length and not costs set distances to 0 or 1";"IRRE"
"non reachable nodes have distance 0 in graph py";"-"
"sparsify the array a little bit";"-"
"check that there s no big loss of precision when the real variance is";"CODE"
"exactly 0 19766";"-"
"add some missing records which should be ignored";"TASK"
"random positive weights";"IRRE"
"sparsify the array a little bit";"-"
"check second round for incremental";"CODE"
"check second round for incremental";"CODE"
"default params for incr mean variance";"CODE"
"test errors";"IRRE"
"test incr mean and var with a 1 row input";"CODE"
"x shape axis picks samples";"-"
"test incremental mean and var with whole data";"IRRE"
"test valueerror if axis 1 and last mean size n features";"IRRE"
"test inconsistent shapes of last mean last var last n";"IRRE"
"non regression test for";"IRRE"
"https github com scikit learn scikit learn issues 16448";"CODE"
"check that computing the incremental mean and variance is equivalent to";"CODE"
"computing the mean and variance on the stacked dataset";"IRRE"
"check the behaviour when we update the variance with an empty matrix";"CODE"
"update statistic with a column which should ignored";"IRRE"
"check the behaviour when last n is just a number";"-"
"we avoid creating specific data for axis 0 and 1 translating the data is";"CODE"
"enough";"-"
"take a copy of the old statistics since they are modified in place";"-"
"sparsify the array a little bit";"-"
"check dtypes with large sparse matrices too";"IRRE"
"xxx test fails on 32bit windows linux";"IRRE"
"test csc row median actually calculates the median";"IRRE"
"test that it gives the same output when x is dense";"IRRE"
"test that it gives the same output when x is sparse";"IRRE"
"test for toy data";"IRRE"
"test that it raises an error for non csc matrices";"CODE"
"csr matrix will use int32 indices by default";"CODE"
"up casting those to int64 when necessary";"IRRE"
"checks that csr row norms returns the same output as";"IRRE"
"scipy sparse linalg norm and that the dype is the same as x dtype";"IRRE"
"weighted percentile average false does not match median when n is even";"CODE"
"note for both percentile rank s 50 and 100 percentile indices is already at";"CODE"
"max index";"-"
"check for when array 2d and sample weight 1d";"CODE"
"percentile rank is scalar";"-"
"check when array and sample weight both 2d";"-"
"percentile rank is scalar";"-"
"numpy scalars input handled as 0d arrays on array api";"CODE"
"random 1d array constant weights";"IRRE"
"random 2d array and random 1d weights";"IRRE"
"random 2d array and random 2d weights";"IRRE"
"zero weights and rank percentile 0 20528 sample weight dtype int64";"CODE"
"np nan s in data and some zero weights sample weight dtype int64";"CODE"
"sample weight dtype int32";"CODE"
"the percentile of the second column should be 5 even though there are many nan";"-"
"values present the percentile of the first column can only be nan since there";"IRRE"
"are no other possible values";"IRRE"
"todo remove the following skip once no longer applicable";"TASK"
"return requires y true pragma no cover";"CODE"
"my tag bool true type ignore annotation unchecked";"CODE"
"1st case the estimator inherits from a class that only implements";"CODE"
"sklearn tags by calling super sklearn tags";"IRRE"
"2nd case the estimator doesn t implement sklearn tags at all";"CODE"
"check that we still raise an error if it is not an attributeerror or related to";"TASK"
"sklearn tags";"-"
"linear discriminant analysis doesn t have random state smoke test";"IRRE"
"basic compare";"IRRE"
"this check that ignore warning decorator and context manager are working";"CODE"
"as expected";"-"
"check the function directly";"CODE"
"check the decorator";"CODE"
"check the context manager";"-"
"check that passing warning class as first positional argument";"IRRE"
"tests for docstrings";"CODE"
"def f one a b pragma no cover";"CODE"
"def f two a b pragma no cover";"CODE"
"def f three a b pragma no cover";"CODE"
"test that data frames with homogeneous dtype are not upcast";"IRRE"
"float16 int16 float32 casts to float32";"CODE"
"float16 int16 float16 casts to float32";"CODE"
"we re not using upcasting rules for determining";"CODE"
"the target type yet so we cast to the default of float64";"CODE"
"check that we handle pandas dtypes in a semi reasonable way";"-"
"this is actually tricky because we can t really know that this";"CODE"
"should be integer ahead of converting it";"CODE"
"test that lists with ints don t get converted to floats";"CODE"
"for scipy 1 13 coords is a new attribute and is a tuple the";"CODE"
"col and row attributes do not seem to be able to change the";"CODE"
"dtype for more details see https github com scipy scipy pull 18530";"CODE"
"and https github com scipy scipy pull 20003 where indices was";"CODE"
"renamed to coords";"-"
"scipy 1 13";"-"
"when large sparse are allowed";"IRRE"
"when large sparse are not allowed";"IRRE"
"empty list is considered 2d by default";"CODE"
"if considered a 1d collection when ensure 2d false then the minimum";"IRRE"
"number of samples will break";"CODE"
"invalid edge case when checking the default minimum sample of a scalar";"CODE"
"simulate a model that would need at least 2 samples to be well defined";"CODE"
"the same message is raised if the data has 2 dimensions even if this is";"CODE"
"not mandatory";"-"
"simulate a model that would require at least 3 features e g selectkbest";"CODE"
"with k 3";"-"
"only the feature check is enabled whenever the number of dimensions is 2";"TASK"
"even if allow nd is enabled";"-"
"simulate a case where a pipeline stage as trimmed all the features of a";"CODE"
"2d dataset";"IRRE"
"nd data is not checked for any minimum number of features by default";"CODE"
"list of lists";"-"
"tuple of tuples";"-"
"list of np arrays";"-"
"tuple of np arrays";"-"
"dataframe";"-"
"sparse matrix";"IRRE"
"target variable does not always go through check array but should";"CODE"
"never accept complex data either";"META"
"check error for bad inputs";"CODE"
"check that asymmetric arrays are properly symmetrized";"-"
"check for warnings and errors";"CODE"
"check pandas dataframe with fit column does not raise error";"CODE"
"https github com scikit learn scikit learn issues 8415";"CODE"
"regression test that check array works on pandas series";"IRRE"
"with categorical dtype not a numpy dtype gh12699";"-"
"pandas dataframe will coerce a boolean into a object this is a mismatch";"CODE"
"with np result type which will return a float";"IRRE"
"check array needs to explicitly check for bool dtype in a dataframe for";"CODE"
"this situation";"CODE"
"https github com scikit learn scikit learn issues 15787";"CODE"
"none default delegates estimator estimator";"CODE"
"true expected result is true b c delegate and attribute are present";"IRRE"
"none expected exception not relevant for this case";"CODE"
"none default delegates estimator estimator";"CODE"
"true expected result is true b c delegate and attribute are present";"IRRE"
"none expected exception not relevant for this case";"CODE"
"list of sub estimators";"-"
"true expected result is true b c delegate and attribute are present";"IRRE"
"none expected exception not relevant for this case";"CODE"
"custom estimator custom estimator attribute name";"META"
"custom estimator custom delegates";"-"
"true expected result is true b c delegate and attribute are present";"IRRE"
"none expected exception not relevant for this case";"CODE"
"no estimator no estimator attribute name";"META"
"none default delegates estimator estimator";"CODE"
"none expected result is not relevant for this case";"CODE"
"valueerror should raise valueerror b c no estimator found from delegates";"IRRE"
"type subestimator attribute absent true attribute absent";"META"
"none default delegates estimator estimator";"CODE"
"none expected result is not relevant for this case";"CODE"
"attributeerror should raise attributeerror b c attribute is absent";"META"
"always checks for attribute attribute present";"META"
"estimator estimator is default value for delegates";"CODE"
"check that it gives a good error if there s no len";"-"
"test that check psd eigenvalues returns the right output for valid";"IRRE"
"input possibly raising the right warning";"CODE"
"test that check psd eigenvalues raises the right error for invalid";"IRRE"
"input";"CODE"
"common checks between numpy array api tests";"IRRE"
"for check sample weight";"CODE"
"check none input";"CODE"
"check numbers input";"CODE"
"check wrong number of dimensions";"META"
"check incorrect n samples";"-"
"float32 dtype is preserved";"CODE"
"check negative weight when ensure non negative true";"-"
"check array order";"-"
"int dtype will be converted to float64 instead";"CODE"
"check array order";"-"
"make sure we only raise if pos label is none";"CODE"
"make sure we only raise if pos label is none";"CODE"
"the is place before a keyword only argument without a default value";"CODE"
"check array converts pandas dataframe with only sparse arrays into";"IRRE"
"sparse matrix";"IRRE"
"by default pandas converts to coo when accept sparse is true";"IRRE"
"check that we support the conversion of sparse dataframe with mixed";"IRRE"
"type which can be converted safely";"-"
"raise an error when no methods are defined";"CODE"
"check that we don t get issue when one of the method is defined";"CODE"
"check the order of the methods returned";"IRRE"
"x is already writeable no copy is needed";"CODE"
"x is not writeable a copy is made";"TASK"
"mmap is already writeable no copy is needed";"CODE"
"mmap is read only a copy is made";"CODE"
"df is backed by a writeable array no copy is needed";"TASK"
"df is backed by a read only array a copy is made";"CODE"
"for object dtype data we only check for nans gh 13254";"CODE"
"we need only consider float arrays hence can early return for all else";"CODE"
"first try an o n time o 1 space solution for the common case that";"CODE"
"everything is finite fall back to o n space np isinf isnan or custom";"IRRE"
"cython implementation to prevent false positives and provide a detailed";"TASK"
"error message";"-"
"cython implementation doesn t support fp16 or complex numbers";"TASK"
"improve the error message on how to handle missing values in";"IRRE"
"scikit learn";"-"
"estimators that handle nan values";"IRRE"
"elif x dtype in np float32 np float64 is numpy array";"CODE"
"only convert x to a numpy array if there is no cheaper heuristic";"-"
"option";"-"
"do not consider an array like of strings or dicts to be a 2d array";"CODE"
"if x is a list of lists for instance we assume that all nested";"CODE"
"lists have the same length without checking or converting to";"-"
"a numpy array to keep this function call as cheap as possible";"CODE"
"check these early for pandas versions without extension dtypes";"META"
"bool and extension booleans need early conversion because array";"CODE"
"converts mixed dtype dataframes into object dtypes";"CODE"
"sparse arrays will be converted later in check array";"IRRE"
"sparse arrays will be converted later in check array";"IRRE"
"only handle extension arrays for integer and floats";"CODE"
"float ndarrays can normally support nans they need to be converted";"TASK"
"first to map pd na to np nan";"-"
"xxx warn when converting from a high integer to a float";"CODE"
"pandas extension arrays have a dtype with an na value";"IRRE"
"store reference to original array to check if copy is needed when";"IRRE"
"function returns";"CODE"
"store whether originally we wanted numeric dtype";"-"
"not a data type e g a column named dtype in a pandas dataframe";"-"
"check if the object contains several dtypes typically a pandas";"IRRE"
"dataframe and store them if not store none";"-"
"track if we have a series like object to raise a better error message";"CODE"
"throw warning if columns are sparse if all columns are sparse then";"IRRE"
"array sparse exists and sparsity will be preserved later";"IRRE"
"force object if any of the dtypes is an object";"CODE"
"array is a pandas series";"-"
"set to none to let array astype work out the best dtype";"CODE"
"if input is object convert to float";"CODE"
"no dtype conversion required";"META"
"dtype conversion required let s select the first element of the";"CODE"
"list of accepted types";"-"
"pandas dataframe requires conversion earlier to handle extension dtypes with";"META"
"nans";"-"
"use the original dtype for conversion if dtype is none";"IRRE"
"since we converted here we do not need to convert again later";"TASK"
"convert to dtype object to conform to array api to be use xp isdtype later";"CODE"
"when all dataframe columns are sparse convert to a sparse array";"IRRE"
"dataframe sparse only supports to coo";"IRRE"
"if np array gives complexwarning then we convert the warning";"META"
"to an error this is needed because specifying a non complex";"META"
"dtype to the function converts complex to real dtype";"META"
"thereby passing the test made in the lines following the scope";"IRRE"
"of warnings context manager";"-"
"conversion float int should not contain nan or";"CODE"
"inf numpy 14412 we cannot use casting safe because";"-"
"then conversion float int would be disallowed";"CODE"
"it is possible that the np array gave no warning this happens";"CODE"
"when no dtype conversion happened for example dtype none the";"META"
"result is that np array produces an array of complex dtype";"IRRE"
"and we need to catch and raise exception for such cases";"CODE"
"if input is scalar raise error";"CODE"
"if input is 1d raise error";"CODE"
"if input is a series like object eg pandas series or polars series";"CODE"
"only make a copy if array and array orig may share memory";"-"
"always make a copy for non numpy arrays";"CODE"
"by default array copy creates a c ordered copy we set order k to";"IRRE"
"preserve the order of the array";"-"
"this situation can only happen when copy false the array is read only and";"CODE"
"a writeable output is requested this is an ambiguous setting so we chose";"CODE"
"to always except for one specific setting see below make a copy to";"CODE"
"ensure that the output is writeable even if avoidable to not overwrite";"CODE"
"the user s data by surprise";"-"
"in pandas 3 np asarray df called earlier in check array";"IRRE"
"returns a read only intermediate array it can be made writeable";"CODE"
"safely without copy because if the original dataframe was backed";"-"
"by a read only array trying to change the flag would raise an";"CODE"
"error in which case we make a copy";"CODE"
"this is used during test collection in common tests the";"IRRE"
"hasattr estimator fit makes it so that we don t fail for an estimator";"CODE"
"that does not have a fit method during collection of checks the right";"CODE"
"checks will fail later";"-"
"only csr csc and coo have data attribute";"META"
"in meta estimators with multiple sub estimators";"-"
"only the attribute of the first sub estimator is checked";"META"
"assuming uniformity across all sub estimators";"CODE"
"avoid x min on sparse matrix since it also sorts the indices";"IRRE"
"check psd eigenvalues 1 2 nominal case";"IRRE"
"check psd eigenvalues 5 5j significant imag part";"IRRE"
"check psd eigenvalues 5 5e 5j insignificant imag part";"IRRE"
"check psd eigenvalues 5 1 all negative";"IRRE"
"check psd eigenvalues 5 1 significant negative";"IRRE"
"check psd eigenvalues 5 5e 5 insignificant negative";"IRRE"
"check psd eigenvalues 5 4e 12 bad conditioning too small";"IRRE"
"extract feature names for support array containers";"TASK"
"make sure we can inspect columns names from pandas even with";"CODE"
"versions too old to expose a working implementation of";"TASK"
"dataframe column names and avoid introducing any";"CODE"
"additional copy";"TASK"
"todo remove the pandas specific branch once the minimum supported";"TASK"
"version of pandas has a working implementation of";"TASK"
"dataframe column names that is guaranteed to not introduce any";"CODE"
"additional copy of the data without having to impose allow copy false";"TASK"
"that could fail with other libraries note in the longer term we";"TASK"
"could decide to instead rely on the dataframe namespace api once";"CODE"
"adopted by our minimally supported pandas version";"META"
"mixed type of string and non string is not supported";"CODE"
"only feature names of all strings are supported";"TASK"
"generates feature names if n features in is defined";"TASK"
"unexpected feature names sort deterministic error message";"TASK"
"ensure binary classification if pos label is not specified";"IRRE"
"classes dtype kind in o u s is required to avoid";"CODE"
"triggering a futurewarning by calling np array equal a b";"TASK"
"when elements in the two arrays are not comparable";"CODE"
"compute classes only if pos label is not specified";"IRRE"
"delete the attribute when the estimator is fitted on a new dataset";"CODE"
"that has no feature names";"TASK"
"no feature names seen in fit and in x";"TASK"
"validate the feature names against the feature names in attribute";"TASK"
"if the number of features is not defined and reset true";"IRRE"
"then we skip this check";"CODE"
"skip this check if the expected number of expected input features";"CODE"
"was not recorded by calling fit first this is typically the case";"IRRE"
"for stateless transformers";"CODE"
"we need this because some estimators validate x and y";"CODE"
"separately and in general separately calling check array";"IRRE"
"on x and y isn t equivalent to just calling check x y";"IRRE"
"may need extra deps";"-"
"not a test but looks like a test";"IRRE"
"contains scripts to be run by tests test crawler py asynccrawlerprocesssubprocess";"IRRE"
"contains scripts to be run by tests test crawler py asynccrawlerrunnersubprocess";"CODE"
"contains scripts to be run by tests test crawler py crawlerprocesssubprocess";"IRRE"
"contains scripts to be run by tests test crawler py crawlerrunnersubprocess";"CODE"
"if file path and file path 0";"-"
"import uvloop noqa plc0415";"CODE"
"import botocore noqa plc0415";"CODE"
"import boto3 noqa plc0415";"CODE"
"import mitmproxy noqa f401 plc0415";"CODE"
"needed on windows to switch from proactor to selector for twisted reactor compatibility";"CODE"
"if we decide to run tests with both we will need to add a new option and check it here";"TASK"
"generate localhost certificate files needed by some tests";"IRRE"
"pylint disable import error";"CODE"
"if node tagname index and node entries type ignore index attr defined";"CODE"
"index entries for setting directives look like";"IRRE"
"pair setting name setting std setting setting name";"IRRE"
"entry type info node entries 0 3 type ignore index";"CODE"
"pylint disable import error";"CODE"
"autodocs was generating a text alias of for the following members";"CODE"
"configuration file for the sphinx documentation builder";"CODE"
"for the full list of built in configuration values see the documentation";"IRRE"
"https www sphinx doc org en master usage configuration html";"CODE"
"if your extensions are in another directory add it here if the directory";"TASK"
"is relative to the documentation root use path absolute to make it absolute";"CODE"
"project information";"CODE"
"https www sphinx doc org en master usage configuration html project information";"CODE"
"general configuration";"-"
"https www sphinx doc org en master usage configuration html general configuration";"CODE"
"scrapyfixautodoc must be after sphinx ext autodoc";"TASK"
"the version info for the project you re documenting acts as replacement for";"CODE"
"version and release also used in various other places throughout the";"META"
"built documents";"CODE"
"the short x y version";"META"
"options for html output";"IRRE"
"https www sphinx doc org en master usage configuration html options for html output";"CODE"
"set canonical url from the read the docs domain";"CODE"
"options for latex output";"IRRE"
"https www sphinx doc org en master usage configuration html options for latex output";"CODE"
"grouping the document tree into latex files list of tuples";"CODE"
"source start file target name title author document class howto manual";"CODE"
"options for the linkcheck builder";"CODE"
"https www sphinx doc org en master usage configuration html options for the linkcheck builder";"CODE"
"options for the coverage extension";"CODE"
"https www sphinx doc org en master usage extensions coverage html configuration";"CODE"
"contract s add pre hook and add post hook are not documented because";"TASK"
"they should be transparent to contract developers for whom pre hook and";"CODE"
"post hook should be the actual concern";"-"
"contractsmanager is an internal class developers are not expected to";"CODE"
"interact with it directly in any way";"CODE"
"for default contracts we only want to document their general purpose in";"CODE"
"their init method the methods they reimplement to achieve that purpose";"TASK"
"should be irrelevant to developers using those contracts";"-"
"methods of downloader middlewares are not documented only the classes";"CODE"
"themselves since downloader middlewares are controlled through scrapy";"CODE"
"settings";"IRRE"
"base classes of downloader middlewares are implementation details that";"CODE"
"are not meant for users";"CODE"
"the interface methods of duplicate request filtering classes are already";"CODE"
"covered in the interface documentation part of the dupefilter class";"CODE"
"setting documentation";"IRRE"
"private exception used by the command line interface implementation";"CODE"
"methods of baseitemexporter subclasses are only documented in";"CODE"
"baseitemexporter";"-"
"extension behavior is only modified through settings methods of";"IRRE"
"extension classes as well as helper functions are implementation";"TASK"
"details that are not documented";"CODE"
"r scrapy extensions a z w a z w methods";"-"
"r scrapy extensions a z w a z helper functions";"CODE"
"never documented before and deprecated now";"CODE"
"implementation detail of lxmllinkextractor";"TASK"
"options for the intersphinx extension";"CODE"
"https www sphinx doc org en master usage extensions intersphinx html configuration";"CODE"
"other options";"-"
"2 0 1";"-"
"usr bin python";"CODE"
"used for remembering the file and its contents";"CODE"
"so we don t have to open the same file again";"CODE"
"a regex that matches standard linkcheck output lines";"IRRE"
"read lines from the linkcheck output file";"CODE"
"for every line fix the respective file";"CODE"
"broken links can t be fixed and";"-"
"i am not sure what do with the local ones";"META"
"if this is a new file";"CODE"
"update the previous file";"CODE"
"read the new file to memory";"CODE"
"we don t understand what the current line means";"CODE"
"usr bin env python";"CODE"
"from twisted internet import reactor noqa tid253";"CODE"
"reset stats on high iter request times caused by client restarts";"CODE"
"if delta 3 seconds";"-"
"max concurrency is limited by global concurrent requests setting";"IRRE"
"requests per second goal";"CODE"
"qps none same as 1 download delay";"CODE"
"time in seconds to delay server responses";"CODE"
"number of slots to create";"IRRE"
"declare top level shortcuts";"-"
"scrapy and twisted versions";"META"
"ignore noisy twisted deprecation warnings";"-"
"support something like o json where json is a value for";"IRRE"
"o not another parameter";"IRRE"
"todo add name attribute to commands and merge this function with";"CODE"
"scrapy utils spider iter spider classes";"IRRE"
"set editor from environment if available";"IRRE"
"twisted prints errors in debuginfo del but pypy does not run gc collect on exit";"CODE"
"http doc pypy org en latest cpython differences html";"CODE"
"highlight gc collect differences related to garbage collection strategies";"-"
"crawler process crawlerprocessbase none none set in scrapy cmdline";"IRRE"
"default settings to be used for this command instead of global defaults";"CODE"
"elf settings settings none none set in scrapy cmdline";"IRRE"
"scrapy commands list shadows builtins list";"CODE"
"elf proc subprocess popen noqa s603";"CODE"
"load contracts";"CODE"
"contract requests";"CODE"
"pidercls start start type ignore assignment method assign return value";"IRRE"
"start checks";"-"
"elf exitcode os system f editor sfile noqa s605";"CODE"
"from argparse import namespace noqa tc003";"CODE"
"by default let the framework handle redirects";"CODE"
"i e command handles all codes expect 3xx";"CODE"
"pidercls start start type ignore method assign attr defined";"IRRE"
"elf exitcode os system f scrapy edit name noqa s605";"CODE"
"print scraped items 60";"CODE"
"print requests 65";"CODE"
"for rule in spider rules type ignore attr defined";"CODE"
"elf spidercls start start type ignore assignment method assign";"IRRE"
"memorize first request";"CODE"
"parse items and requests";"IRRE"
"update request meta if any extra meta was passed through the meta m opts";"CODE"
"update cb kwargs if any extra values were was passed through the cbkwargs option";"IRRE"
"parse arguments";"IRRE"
"prepare spidercls";"-"
"def update vars self vars dict str any none noqa a002";"CODE"
"first argument may be a local file";"-"
"the crawler is created this way since the shell manually handles the";"CODE"
"crawling engine so the set up in the crawl method won t work";"IRRE"
"the shell class needs a persistent engine in the crawler";"CODE"
"pider method self name type ignore attr defined";"CODE"
"def str self str pylint disable no self argument";"CODE"
"contracts";"-"
"typing self requires python 3 11";"CODE"
"return cls type ignore misc";"IRRE"
"setting verify true will require you to provide cas";"IRRE"
"to verify against in other words it s not that simple";"-"
"kept for old style http 1 0 downloader context twisted calls";"CODE"
"e g connectssl";"-"
"ctx set options 0x4 op legacy server connect";"IRRE"
"trustroot set to platformtrust will use the platform s root cas";"IRRE"
"this means that a website like https www cacert org will be rejected";"CODE"
"by default since cacert org ca certificate is seldom shipped";"CODE"
"try method aware context factory";"CODE"
"use context factory defaults";"CODE"
"typing self requires python 3 11";"CODE"
"hints for headers related types may need to be fixed to not use anystr";"CODE"
"return respcls url request url status 200 body body headers headers type ignore arg type";"CODE"
"closecachedconnections will hang on network or server issues so";"CODE"
"we ll manually timeout the deferred";"CODE"
"twisted issue addressing this problem can be found here";"TASK"
"https twistedmatrix com trac ticket 7738";"CODE"
"closecachedconnections doesn t handle external errbacks so we ll";"CODE"
"issue a callback after disconnect timeout seconds";"IRRE"
"protocol datareceived self processproxyresponse type ignore method assign";"IRRE"
"make sure that enough all bytes are consumed";"-"
"and that we ve got all http headers ending with a blank line";"CODE"
"from the proxy so that we don t send those bytes to the tls layer";"CODE"
"see https github com scrapy scrapy issues 2491";"CODE"
"elf protocol datareceived self protocoldatareceived type ignore method assign";"IRRE"
"set proper server name indication extension";"IRRE"
"loptions self contextfactory creatorfornetloc type ignore call arg misc";"CODE"
"typing self requires python 3 11";"CODE"
"todo";"TASK"
"typing self requires python 3 11";"CODE"
"if no credentials could be found anywhere";"-"
"consider this an anonymous connection request by default";"CODE"
"unless anon was set explicitly true false";"IRRE"
"import botocore auth noqa plc0415";"CODE"
"import botocore credentials noqa plc0415";"CODE"
"botocore auth basesigner doesn t have an init with args only subclasses do";"IRRE"
"elf signer signercls type ignore call arg";"IRRE"
"import botocore awsrequest noqa plc0415";"CODE"
"return yield download func request self spider type ignore call arg";"CODE"
"either returns a request or response which we pass to process response";"CODE"
"or reraises the exception";"CODE"
"method tls ssl sslv23 method protocol negotiation recommended";"CODE"
"method tlsv10 ssl tlsv1 method tls 1 0 only";"-"
"method tlsv11 ssl tlsv1 1 method tls 1 1 only";"-"
"method tlsv12 ssl tlsv1 2 method tls 1 2 only";"-"
"require an opened spider when not run in scrapy shell";"CODE"
"not wrapping in a deferred here to avoid https github com twisted twisted issues 12470";"CODE"
"can happen when this is cancelled e g in test close during start iteration";"CODE"
"await self stop async will also close spider and downloader";"CODE"
"will also close downloader";"CODE"
"return spider already closed";"CODE"
"starts the processing of scheduled requests as well as a periodic";"CODE"
"call to that processing method for scenarios where the scheduler";"IRRE"
"reports having pending requests but returns none";"CODE"
"assert self slot is not none typing";"CODE"
"give room for the outcome of self process start next to be";"CODE"
"processed before continuing with the next iteration";"CODE"
"self stop has cancelled us nothing to do";"CODE"
"an error happened log it and stop the engine";"-"
"assert self scraper slot is not none typing";"CODE"
"assert self slot is not none typing";"CODE"
"assert self spider is not none typing";"CODE"
"downloader middleware can return requests for example redirects";"CODE"
"if not self scraper slot is idle type ignore union attr";"CODE"
"if self downloader active downloader has pending requests";"CODE"
"if self start is not none not all start requests are handled";"CODE"
"assert self slot is not none typing";"CODE"
"assert self spider is not none typing";"CODE"
"assert isinstance ex closespider typing";"CODE"
"logger error msg exc info true extra spider spider noqa log014";"CODE"
"store a dictionary which is used to get the respective";"OUTD"
"h2clientprotocolinstance using the key as tuple scheme hostname port";"CODE"
"save all requests that arrive before the connection is established";"CODE"
"received a request while connecting to remote";"CODE"
"create a deferred which will fire with the h2clientprotocol";"CODE"
"instance";"-"
"check if we already have a connection to the remote";"CODE"
"return this connection instance wrapped inside a deferred";"CODE"
"no connection is established for the given uri";"CODE"
"now as we have established a proper http 2 connection";"CODE"
"we fire all the deferred s with the connection instance";"CODE"
"call the errback of all the pending requests for this connection";"CODE"
"assert conn transport is not none typing";"CODE"
"id of the next request stream";"CODE"
"following the convention streams initiated by a client must";"IRRE"
"use odd numbered stream identifiers rfc 7540 section 5 1 1";"-"
"streams are stored in a dictionary keyed off their stream ids";"CODE"
"if requests are received before connection is made we keep";"CODE"
"all requests in a pool and send them as the connection is made";"CODE"
"save an instance of errors raised which lead to losing the connection";"CODE"
"we pass these instances to the streams responsefailed failure";"CODE"
"some meta data of this connection";"CODE"
"initialized when connection is successfully made";"IRRE"
"peer certificate instance";"-"
"address of the server we are connected to which";"TASK"
"is updated when http 2 connection is made successfully";"CODE"
"uri of the peer http 2 connection is made";"CODE"
"both ip address and uri are used by the stream before";"TASK"
"initiating the request to verify that the base address";"TASK"
"variables taken from project settings";"IRRE"
"counter to keep track of opened streams this counter";"CODE"
"is used to make sure that not more than max concurrent streams";"OUTD"
"streams are opened which leads to protocolerror";"CODE"
"we use simple fifo policy to handle pending requests";"CODE"
"flag to keep track if settings were acknowledged by the remote";"IRRE"
"this ensures that we have established a http 2 connection";"CODE"
"assert self transport is not none typing";"CODE"
"assert self transport is not none typing";"CODE"
"reset the idle timeout as connection is still actively sending data";"CODE"
"add the stream to the request pool";"TASK"
"if we receive a request when connection is idle";"CODE"
"we need to initiate pending requests";"TASK"
"initialize the timeout";"IRRE"
"assert self transport is not none typing";"CODE"
"initiate h2 connection";"IRRE"
"assert self transport is not none typing";"CODE"
"assert self transport is not none typing";"CODE"
"we have not initiated the connection yet no need to send a goaway frame to the remote peer";"TASK"
"reset the idle timeout as connection is still actively receiving data";"CODE"
"hyper h2 does not drop the connection in this scenario we";"CODE"
"need to abort the connection manually";"TASK"
"assert self transport is not none typing";"CODE"
"save this error as ultimately the connection will be dropped";"CODE"
"internally by hyper h2 saved error will be passed to all the streams";"CODE"
"closed with the connection";"CODE"
"check whether there are open streams if there are we re going to";"CODE"
"want to use the error code protocol error if there aren t use";"IRRE"
"no error";"-"
"cancel the timeout if not done yet";"TASK"
"notify the connection pool instance such that no new requests are";"CODE"
"sent over current connection";"CODE"
"event handler functions starts here";"CODE"
"pass we ignore server initiated events";"IRRE"
"pass we ignore server initiated events";"IRRE"
"send off all the pending requests as now we have";"CODE"
"established a proper http 2 connection";"CODE"
"update certificate when our http 2 connection is established";"CODE"
"assert self transport is not none typing";"CODE"
"pass we ignore server initiated events";"IRRE"
"pass we ignore server initiated events";"IRRE"
"send leftover data for all the streams";"CODE"
"received a streamended event from the remote";"CODE"
"received a streamreset event ended abruptly";"IRRE"
"transport connection was lost";"CODE"
"expected response body size is more than allowed limit";"CODE"
"response deferred is cancelled by the client";"CODE"
"happens when client called response deferred cancel";"CODE"
"connection lost and the stream was not initiated";"IRRE"
"the hostname of the request is not same as of connected peer hostname";"CODE"
"as a result sending this request will the end the connection";"CODE"
"metadata of an http 2 connection stream";"CODE"
"initialized when stream is instantiated";"IRRE"
"flag to keep track whether the stream has initiated the request";"IRRE"
"flag to track whether we have logged about exceeding download warnsize";"CODE"
"each time we send a data frame we will decrease value by the amount send";"CODE"
"flag to keep track whether client self have closed this stream";"CODE"
"flag to keep track whether the server has closed the stream";"CODE"
"private variable used to build the response";"CODE"
"this response is then converted to appropriate response class";"CODE"
"passed to the response deferred callback";"CODE"
"data received frame by frame from the server is appended";"CODE"
"and passed to the response deferred when completely received";"CODE"
"the amount of data received that counts against the";"-"
"flow control window";"CODE"
"headers received after sending the request";"CODE"
"close this stream as gracefully as possible";"CODE"
"if the associated request is initiated we reset this stream";"IRRE"
"else we directly call close method";"IRRE"
"make sure that we are sending the request to the correct url";"CODE"
"this pseudo header field must not be empty for http or https";"CODE"
"uris http or https uris that do not contain a path component";"CODE"
"must include a value of the exception to this rule is an";"CODE"
"options request for an http or https uri that does not include";"CODE"
"a path component these must include a path pseudo header field";"CODE"
"with a value of refer rfc 7540 section 8 1 2 3";"IRRE"
"make sure pseudo headers comes before all the other headers";"CODE"
"the scheme and path pseudo header fields must";"CODE"
"be omitted for connect method refer rfc 7540 section 8 3";"CODE"
"close this stream calling the response errback";"CODE"
"note that we have not sent any headers";"TASK"
"firstly check what the flow control window is for current stream";"CODE"
"next check what the maximum frame size is";"-"
"we will send no more than the window size or the remaining file size";"CODE"
"of data in this call whichever is smaller";"CODE"
"we now need to send a number of data frames";"TASK"
"end the stream if no more data needs to be send";"CODE"
"q what about the rest of the data";"-"
"ans remaining data frames will be sent when we get a windowupdate frame";"CODE"
"we check maxsize here in case the content length header was not received";"CODE"
"acknowledge the data received";"-"
"check if we exceed the allowed max data size which can be received";"IRRE"
"have default value of errors as an empty list as";"IRRE"
"some cases can add a list of exceptions";"CODE"
"we do not check for content length or transfer encoding in response headers";"CODE"
"and add partial flag as in http 1 1 as a request or response that includes";"CODE"
"a payload body can include a content length header field rfc 7540 section 8 1 2 6";"CODE"
"note order of handling the events is important here";"CODE"
"as we immediately cancel the request when maxsize is exceeded while";"CODE"
"receiving data frame s when we have received the headers not";"CODE"
"having content length";"-"
"stream was abruptly ended here";"CODE"
"client has cancelled the request remove all the data";"CODE"
"received and fire the response deferred with no flags set";"CODE"
"note the data is already flushed in stream reset stream called";"IRRE"
"immediately when the stream needs to be cancelled";"TASK"
"there maybe no status in headers we make";"CODE"
"http status code 499 client closed request";"CODE"
"working around https github com sphinx doc sphinx issues 10400";"CODE"
"from twisted internet defer import deferred noqa tc002";"CODE"
"from scrapy spiders import spider noqa tc001";"CODE"
"requires queuelib 1 6 2";"CODE"
"typing self requires python 3 11";"CODE"
"except valueerror as e non serializable request";"CODE"
"assert self slot is not none typing";"CODE"
"yield dfd fired in wait for processing";"CODE"
"assert self slot is not none typing";"CODE"
"assert self crawler engine is not none typing";"CODE"
"don t handle invalidoutput exception";"CODE"
"stop exception handling by handing control over to the";"CODE"
"process spider output chain if an iterable has been returned";"IRRE"
"process spider output returns a deferred only because of downgrading so this can be";"CODE"
"simplified when downgrading is removed";"CODE"
"the result is available immediately if process spider output didn t do downgrading";"IRRE"
"we forbid waiting here because otherwise we would need to return a deferred from";"CODE"
"process spider exception too which complicates the architecture";"CODE"
"this method cannot be made async def as process spider exception relies on the deferred result";"CODE"
"being available immediately which doesn t work when it s a wrapped coroutine";"CODE"
"it also needs inlinecallbacks only because of downgrading so it can be removed when downgrading is removed";"CODE"
"items in this iterable do not need to go through the process spider output";"CODE"
"chain they went through it already from the process spider exception method";"CODE"
"there are three cases for the middleware def foo async def foo def foo async def foo async";"CODE"
"1 def foo sync iterables are passed as is async ones are downgraded";"CODE"
"2 async def foo sync iterables are upgraded async ones are passed as is";"TASK"
"3 def foo async def foo async iterables are passed to the respective method";"IRRE"
"storing methods and method tuples in the same list is weird but we should be able to roll this back";"CODE"
"when we drop this compatibility feature";"TASK"
"this tuple handling is only needed until async compatibility methods are removed";"OUTD"
"iterable asynciterator";"-"
"f https docs scrapy org en latest topics coroutines html for middleware users";"CODE"
"asynciterator iterable";"-"
"might fail directly if the output value is not a generator";"IRRE"
"result close silence warning about not awaiting";"IRRE"
"return mutablechain result recovered type ignore arg type";"IRRE"
"spider defines both start requests and start";"CODE"
"this method is only needed until async compatibility methods are removed";"OUTD"
"f https docs scrapy org en latest topics coroutines html for middleware users";"CODE"
"scrapy root handler already installed update it with new settings";"CODE"
"this needs to be done after the spider settings are merged";"TASK"
"but before something imports twisted internet reactor";"CODE"
"from twisted internet import reactor noqa f401";"CODE"
"cannot use deferred f from coro f because that relies on the reactor";"CODE"
"being installed already which is done within apply settings inside";"CODE"
"this method";"CODE"
"at this point the asyncio loop has been installed either by the user";"CODE"
"or by asynccrawlerprocess but it isn t running yet so no asyncio create task";"TASK"
"we pass self which is crawlerprocess instead of crawler here";"CODE"
"which works because the default resolvers only use crawler settings";"IRRE"
"resolver build from crawler resolver class self reactor reactor type ignore arg type";"CODE"
"raised if already stopped or in shutdown stage";"CODE"
"don t start the reactor if the deferreds are already fired";"CODE"
"reactor run installsignalhandlers install signal handlers blocking call";"IRRE"
"we want the asyncio event loop to be installed early so that it s";"IRRE"
"always the correct one and as we do that we can also install the";"CODE"
"reactor here";"-"
"the asyncio event loop setting cannot be overridden by add ons and";"IRRE"
"spiders when using asynccrawlerprocess";"-"
"the user could install a reactor before this class is instantiated";"CODE"
"we need to make sure the reactor is the correct one and the loop";"TASK"
"type matches the setting";"IRRE"
"reactor run installsignalhandlers install signal handlers blocking call";"IRRE"
"typing self requires python 3 11";"CODE"
"xxx google parses at least first 100k bytes scrapy s redirect";"IRRE"
"middleware parses first 4k 4k turns out to be insufficient";"IRRE"
"for this middleware and parsing 100k could be slow";"CODE"
"we use something in between 32k by default";"CODE"
"other http methods are either not safe or don t have a body";"CODE"
"if ajax crawlable in request meta prevent loops";"IRRE"
"ajax crawl request request replace url escape ajax request url";"CODE"
"stripping scripts and comments is slow about 20x slower than";"CODE"
"just checking if a string is in text this is a quick fail fast";"CODE"
"path that should work for most pages";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"elf domain spider http auth domain type ignore attr defined";"CODE"
"typing self requires python 3 11";"CODE"
"skip uncacheable requests";"CODE"
"request meta dont cache true flag as uncacheable";"CODE"
"look for cached response and check if expired";"CODE"
"return none first time request";"CODE"
"return cached response only if not expired";"CODE"
"keep a reference to cached response to avoid a second cache lookup on";"CODE"
"process response hook";"CODE"
"skip cached responses and uncacheable requests";"CODE"
"rfc2616 requires origin server to set date header";"CODE"
"https www w3 org protocols rfc2616 rfc2616 sec14 html sec14 18";"CODE"
"do not validate first hand responses";"CODE"
"typing self requires python 3 11";"CODE"
"except attributeerror pragma no cover";"META"
"import zstandard noqa f401";"CODE"
"force recalculating the encoding until we make sure the";"CODE"
"responsetypes guessing is reliable";"IRRE"
"shouldn t be reached";"-"
"return body pragma no cover";"IRRE"
"from urllib request import type ignore attr defined";"CODE"
"typing self requires python 3 11";"CODE"
"some values such as var run docker sock can t be parsed";"IRRE"
"by parse proxy and as such should be skipped";"IRRE"
"no proxy is only supported by http schemes";"CODE"
"typing self requires python 3 11";"CODE"
"hostname can be none for wrong urls like javascript links";"CODE"
"typing self requires python 3 11";"CODE"
"https fetch spec whatwg org ref for cors non wildcard request header name";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"check if parser dependencies are met this should throw an error otherwise";"CODE"
"typing self requires python 3 11";"CODE"
"resp status b r n b http 1 1 100 599";"CODE"
"response body b r n response header b r n response status";"CODE"
"typing self requires python 3 11";"CODE"
"internal";"CODE"
"items";"-"
"commands";"CODE"
"def start exporting self none noqa b027";"CODE"
"def finish exporting self none noqa b027";"CODE"
"there is a small difference between the behaviour or jsonitemexporter indent";"-"
"and scrapyjsonencoder indent scrapyjsonencoder indent none is needed to prevent";"CODE"
"the addition of newlines everywhere";"TASK"
"newline windows needs this https github com scrapy scrapy issues 3034";"CODE"
"except typeerror list in value may not contain strings";"CODE"
"elf stream detach avoid closing the wrapped file";"CODE"
"use declared field names or keys if the item is a dict";"-"
"def export item self item any dict str bytes any type ignore override";"CODE"
"typing self requires python 3 11";"CODE"
"for closespider timeout";"CODE"
"for closespider timeout no item";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"ignal signal signal sigusr2 self dump stacktrace type ignore attr defined";"CODE"
"ignal signal signal sigquit self dump stacktrace type ignore attr defined";"CODE"
"win32 platforms don t support sigusr signals";"CODE"
"dumps f thread name id n dump n";"CODE"
"win32 platforms don t support sigusr signals";"CODE"
"ignal signal signal sigusr2 self enter debugger type ignore attr defined";"CODE"
"typing self requires python 3 11";"CODE"
"return true accept all items by default";"CODE"
"import boto3 session noqa plc0415";"CODE"
"elf keyname str u path 1 remove first";"-"
"elf blob name str u path 1 remove first";"-"
"from google cloud storage import client noqa plc0415";"CODE"
"format str noqa a002";"CODE"
"filter itemfilter noqa a002";"-"
"feed params";"-"
"exporter params";"-"
"flags";"-"
"begin backward compatibility for feed uri and feed format settings";"CODE"
"handle pathlib path objects";"IRRE"
"end backward compatibility for feed uri and feed format settings";"CODE"
"feeds setting takes precedence over feed uri";"IRRE"
"handle pathlib path objects";"IRRE"
"await all deferreds";"CODE"
"send feed exporter closed signal";"CODE"
"normal case";"CODE"
"need to store the empty file";"TASK"
"in this case the file is not stored so no processing is required";"CODE"
"d deferred none maybedeferred slot storage store get file slot type ignore call overload";"CODE"
"if slot doesn t accept item continue with next slot";"CODE"
"create new slot for each slot with itemcount feed export batch item count and close the old one";"CODE"
"https docs scrapy org en latest topics feed exports html feed export batch item count";"CODE"
"load the item filter if declared else load the default filter class";"CODE"
"maxage 3600 24 365 one year";"-"
"obey user agent directive cache control no store";"-"
"what is cacheable https www w3 org protocols rfc2616 rfc2616 sec14 html sec14 9 1";"CODE"
"response cacheability https www w3 org protocols rfc2616 rfc2616 sec13 html sec13 4";"CODE"
"status code 206 is not included because cache can not deal with partial contents";"CODE"
"obey directive cache control no store";"-"
"never cache 304 not modified responses";"CODE"
"cache unconditionally if configured to do so";"TASK"
"any hint on response expiration is good";"CODE"
"firefox fallbacks this statuses to one year expiration if none is set";"IRRE"
"other statuses without expiration requires at least one validator";"CODE"
"any other is probably not eligible for caching";"TASK"
"makes no sense to cache responses that does not contain expiration";"CODE"
"info and can not be revalidated";"-"
"from rfc2616 indicates that the client is willing to";"CODE"
"accept a response that has exceeded its expiration time";"IRRE"
"if max stale is assigned a value then the client is";"IRRE"
"willing to accept a response that has exceeded its";"IRRE"
"expiration time by no more than the specified number of";"-"
"seconds if no value is assigned to max stale then the";"IRRE"
"client is willing to accept a stale response of any age";"CODE"
"cached response is stale try to set validators if any";"CODE"
"use the cached response if the new response is a server error";"CODE"
"as long as the old response didn t specify must revalidate";"CODE"
"use the cached response if the server says it hasn t changed";"IRRE"
"return max 0 int cc b max age type ignore arg type";"CODE"
"reference nshttpresponsehead computefreshnesslifetime";"CODE"
"https dxr mozilla org mozilla central source netwerk protocol http nshttpresponsehead cpp 706";"CODE"
"parse date header or synthesize it if none exists";"IRRE"
"try http 1 0 expires header";"CODE"
"when parsing expires header fails rfc 2616 section 14 21 says we";"CODE"
"should treat this as an expiration time in the past";"CODE"
"fallback to heuristic using last modified header";"CODE"
"this is not in rfc but on firefox caching implementation";"TASK"
"this request can be cached indefinitely";"CODE"
"insufficient information to compute freshness lifetime";"CODE"
"reference nshttpresponsehead computecurrentage";"CODE"
"https dxr mozilla org mozilla central source netwerk protocol http nshttpresponsehead cpp 658";"CODE"
"if date header is not set we assume it is a fast connection and";"CODE"
"clock is in sync with the server";"-"
"age int response headers b age type ignore arg type";"CODE"
"elf db any none the real type is private";"CODE"
"return none not cached";"IRRE"
"return none not found";"IRRE"
"return none expired";"IRRE"
"return cast dict str any pickle loads db f key data noqa s301";"CODE"
"https github com python mypy issues 10740";"CODE"
"gzip open if self use gzip else open type ignore assignment";"CODE"
"return none not found";"IRRE"
"return none expired";"IRRE"
"return cast dict str any pickle load f noqa s301";"CODE"
"date str to unicode date str encoding ascii type ignore arg type";"IRRE"
"return mktime tz parsedate tz date str type ignore arg type";"IRRE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"stdlib s resource module is only available on unix platforms";"CODE"
"on macos ru maxrss is in bytes on linux it is in kb";"-"
"if self warned warn only once";"CODE"
"typing self requires python 3 11";"CODE"
"io iobase is subclassed here so that exporters can use the postprocessingmanager";"IRRE"
"instance as a file like writable object this could be needed by some exporters";"CODE"
"such as csvitemexporter which wraps the feed storage with io textiowrapper";"-"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"signal to update telnet variables";"CODE"
"args telnet vars";"IRRE"
"typing self requires python 3 11";"CODE"
"pider download delay self start delay spider type ignore attr defined";"CODE"
"typing self requires python 3 11";"CODE"
"defined in the http cookiejar module but undocumented";"CODE"
"https github com python cpython blob v3 9 0 lib http cookiejar py l527";"CODE"
"elf jar cookies lock dummylock type ignore attr defined";"CODE"
"elf jar extract cookies wrsp wreq type ignore arg type";"-"
"elf policy now self jar now int time time type ignore attr defined";"CODE"
"the cookiejar implementation iterates through all domains";"TASK"
"instead we restrict to potential matches on the domain";"CODE"
"if host in self jar cookies type ignore attr defined";"CODE"
"cookies self jar cookies for domain host wreq type ignore attr defined";"CODE"
"attrs self jar cookie attrs cookies type ignore attr defined";"CODE"
"this is still quite inefficient for large number of cookies";"CODE"
"return self jar cookies type ignore attr defined no any return";"CODE"
"return self jar make cookies wrsp wreq type ignore arg type";"CODE"
"elf jar set cookie if ok cookie wrappedrequest request type ignore arg type";"CODE"
"typing self requires python 3 11";"CODE"
"isn t fully compatible typing wise with either dict or caselessdict";"CODE"
"but it needs refactoring anyway see also https github com scrapy scrapy pull 5146";"TASK"
"a workaround for the docs more than one target found problem";"CODE"
"import scrapy noqa tc001";"CODE"
"typing notrequired and typing self require python 3 11";"CODE"
"circular import";"CODE"
"elf encoding str encoding this one has to be set first";"IRRE"
"default 0";"CODE"
"value that the ref scheduler topics scheduler may use for";"IRRE"
"request prioritization";"CODE"
"built in schedulers prioritize requests with a higher priority";"CODE"
"value";"IRRE"
"negative values are allowed";"IRRE"
"class collections abc callable to parse the";"IRRE"
"class scrapy http response to this request once received";"CODE"
"the callable must expect the response as its first parameter and";"IRRE"
"support any additional keyword arguments set through";"TASK"
"attr cb kwargs";"IRRE"
"in addition to an arbitrary callable the following values are also";"IRRE"
"supported";"-"
"none default which indicates that the";"IRRE"
"meth scrapy spider parse method of the spider must be used";"TASK"
"func scrapy http request no callback";"CODE"
"if an unhandled exception is raised during request or response";"CODE"
"processing i e by a ref spider middleware";"-"
"topics spider middleware ref downloader middleware";"CODE"
"topics downloader middleware or download handler";"CODE"
"setting download handlers attr errback is called instead";"IRRE"
"tip";"-"
"class scrapy spidermiddlewares httperror httperrormiddleware";"CODE"
"raises exceptions for non 2xx responses by default sending them";"CODE"
"to the attr errback instead";"-"
"seealso";"-"
"ref topics request response ref request callback arguments";"CODE"
"class collections abc callable to handle exceptions raised";"CODE"
"during request or response processing";"CODE"
"the callable must expect a exc twisted python failure failure as";"IRRE"
"its first parameter";"IRRE"
"seealso ref topics request response ref errbacks";"CODE"
"whether this request may be filtered out by ref components";"CODE"
"topics components that support filtering out requests false";"CODE"
"default or those components should not filter out this request";"CODE"
"true";"-"
"this attribute is commonly set to true to prevent duplicate";"IRRE"
"requests from being filtered out";"CODE"
"when defining the start urls of a spider through";"CODE"
"attr scrapy spider start urls this attribute is enabled by";"META"
"default see meth scrapy spider start";"CODE"
"only instance methods contain func";"-"
"we need to use func to access the original function object because instance";"CODE"
"method objects are generated each time attribute is retrieved from instance";"CODE"
"reference the standard type hierarchy";"CODE"
"https docs python org 3 reference datamodel html";"CODE"
"typing self requires python 3 11";"CODE"
"assert form base url is not none typing";"CODE"
"match browser behaviour on simple select tag without options selected";"CODE"
"and for select tags without options";"CODE"
"if we don t have clickdata we just use the first clickable element";"CODE"
"if clickdata is given we compare it to the clickable elements to find a";"CODE"
"match we first look to see if the number is specified in clickdata";"CODE"
"because that uniquely identifies the element";"IRRE"
"we didn t find it so now we build an xpath expression out of the other";"-"
"arguments because they can be used as such";"IRRE"
"typing self requires python 3 11";"CODE"
"spec defines that requests must use post method";"CODE"
"xmlrpc query multiples times over the same url";"CODE"
"restore encoding";"-"
"typing self requires python 3 11";"CODE"
"return self request cb kwargs type ignore union attr";"CODE"
"return self request meta type ignore union attr";"CODE"
"elf body bytes b used by encoding detection";"-"
"pylint disable no method argument no self argument";"CODE"
"typing self requires python 3 11";"CODE"
"common file extensions that are not followed if they occur in links";"-"
"archives";"-"
"images";"-"
"audio";"-"
"video";"-"
"office suites";"-"
"other";"-"
"top level imports";"CODE"
"from lxml src lxml html init py";"IRRE"
"mypy doesn t infer types for operator and also for partial";"CODE"
"hacky way to get the underlying lxml parsed document";"IRRE"
"pseudo lxml html htmlelement make links absolute base url";"CODE"
"continue skipping bogus links";"CODE"
"to fix relative links after process value";"IRRE"
"working around https github com sphinx doc sphinx issues 10400";"CODE"
"from scrapy import request spider noqa tc001";"CODE"
"from scrapy http import response noqa tc001";"CODE"
"typing self requires python 3 11";"CODE"
"imports twisted internet reactor";"CODE"
"typing self requires python 3 11";"CODE"
"defined in the email utils module but undocumented";"CODE"
"https github com python cpython blob v3 9 0 lib email utils py l42";"CODE"
"imports twisted internet reactor";"CODE"
"from twisted mail smtp import esmtpsenderfactory noqa plc0415";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"return str path convert a path object to string";"CODE"
"m hashlib md5 noqa s324";"-"
"elf s3 client put object type ignore attr defined";"CODE"
"typing self requires python 3 11";"CODE"
"uppercase attributes kept for backward compatibility with code that subclasses";"CODE"
"imagespipeline they may be overridden by settings";"IRRE"
"from pil import image imageops noqa plc0415";"CODE"
"image resampling lanczos was added in pillow 9 1 0";"TASK"
"remove this try except block";"CODE"
"when updating the minimum requirements for pillow";"CODE"
"resampling filter self image antialias type ignore attr defined";"CODE"
"image guid hashlib sha1 to bytes request url hexdigest noqa s324";"CODE"
"thumb guid hashlib sha1 to bytes request url hexdigest noqa s324";"CODE"
"typing self requires python 3 11";"CODE"
"return cached result if request was already seen";"CODE"
"otherwise wait for result";"IRRE"
"check if request is downloading right now to avoid doing it twice";"CODE"
"download request checking media to download hook output first";"CODE"
"got a result without downloading";"CODE"
"download the result";"CODE"
"return yield wad it must return wad at last";"IRRE"
"def check media to download pylint disable inconsistent return statements";"CODE"
"this ugly code was left only to support tests todo remove";"CODE"
"minimize cached information for failure";"CODE"
"result stack type ignore method assign";"IRRE"
"this code fixes a memory leak by avoiding to keep references to";"CODE"
"the request and response objects on the media pipeline cache";"CODE"
"what happens when the media downloaded callback raises an";"CODE"
"exception for example a fileexception download error when";"CODE"
"the response status code is not 200 ok is that the original";"CODE"
"stopiteration exception which in turn contains the failed";"CODE"
"response and by extension the original request gets encapsulated";"CODE"
"within the fileexception context";"CODE"
"originally scrapy was using twisted internet defer returnvalue";"CODE"
"inside functions decorated with twisted internet defer inlinecallbacks";"CODE"
"encapsulating the returned response in a defgen return exception";"CODE"
"instead of a stopiteration";"CODE"
"to avoid keeping references to the response and therefore request";"CODE"
"objects on the media pipeline cache we should wipe the context of";"CODE"
"the encapsulated exception when it is a stopiteration instance";"CODE"
"info downloaded fp result cache result";"IRRE"
"overridable interface";"CODE"
"typing self requires python 3 11";"CODE"
"as we replace some letters we can get collision for different slots";"CODE"
"add we add unique part";"TASK"
"unique slot hashlib md5 text encode utf8 hexdigest noqa s324";"-"
"elf pqueues dict str scrapypriorityqueue slot priority queue";"-"
"typing self requires python 3 11";"CODE"
"todo cache misses";"TASK"
"in twisted 16 6 gethostbyname is always called with";"IRRE"
"a default timeout of 60s actually passed as 1 3 11 45 tuple";"META"
"so the input argument above is simply overridden";"CODE"
"to enforce scrapy s dns timeout setting s value";"IRRE"
"the timeout arg is typed as sequence int but supports floats";"CODE"
"timeout self timeout type ignore assignment";"IRRE"
"typing self requires python 3 11";"CODE"
"if we found garbage or robots txt in an encoding other than utf 8 disregard it";"-"
"switch to allow all state";"CODE"
"from robotexclusionrulesparser import robotexclusionrulesparser noqa plc0415";"CODE"
"top level imports";"CODE"
"type str none none noqa a002";"-"
"the key types are restricted in basesettings get key to ones supported by json";"IRRE"
"see https github com scrapy scrapy issues 5383";"CODE"
"https github com python typing issues 445 issuecomment 1131458824";"CODE"
"typing self requires python 3 11";"CODE"
"download maxsize 1024 1024 1024 1024m";"CODE"
"download warnsize 32 1024 1024 32m";"CODE"
"download timeout 180 3mins";"CODE"
"use highest tls ssl protocol version supported by the platform also allowing negotiation";"META"
"engine side";"-"
"downloader side";"CODE"
"feed uri params none a function to extend uri arguments";"CODE"
"ftp password guest noqa s105";"-"
"memdebug enabled false enable memory debugging";"-"
"memdebug notify send memory debugging report by mail at engine shutdown";"CODE"
"redirect max times 20 uses firefox default setting";"IRRE"
"oserror is raised by the httpcompression middleware when trying to";"CODE"
"decompress an empty response";"CODE"
"retry times 2 initial response 2 retries 3 requests";"CODE"
"engine side";"-"
"spider side";"-"
"import warnings noqa plc0415";"CODE"
"from scrapy exceptions import scrapydeprecationwarning noqa plc0415";"CODE"
"disable accidental ctrl c key press from shutting down the engine";"CODE"
"pylint disable next eval used";"CODE"
"print eval self code globals self vars noqa s307";"CODE"
"detect interactive shell setting in scrapy cfg";"CODE"
"e g config scrapy cfg or scrapy cfg";"-"
"settings";"IRRE"
"shell can be one of ipython bpython or python";"CODE"
"to be used as the interactive python console if available";"CODE"
"default is ipython fallbacks in the order listed above";"CODE"
"shell python";"CODE"
"else try all by default";"CODE"
"always add standard shell as fallback";"TASK"
"set the asyncio event loop for the current thread";"IRRE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"def init pylint disable super init not called";"IRRE"
"base case depth 0";"CODE"
"start requests";"CODE"
"typing self requires python 3 11";"CODE"
"typing self requires python 3 11";"CODE"
"https www w3 org tr referrer policy strip url";"CODE"
"note this does not follow https w3c github io webappsec secure contexts is url trustworthy";"CODE"
"reference https www w3 org tr referrer policy referrer policy empty string";"CODE"
"https www w3 org tr referrer policy parse referrer policy from header";"CODE"
"def init self settings basesettings none none pylint disable super init not called";"IRRE"
"note this hook is a bit of a hack to intercept redirections";"CODE"
"start requests";"CODE"
"check redirected request to patch referer header if necessary";"CODE"
"we don t patch the referrer value if there is none";"IRRE"
"the request s referrer header value acts as a surrogate";"CODE"
"for the parent response url";"CODE"
"note if the 3xx response contained a referrer policy header";"CODE"
"the information is not available using this hook";"CODE"
"typing self requires python 3 11";"CODE"
"def init self maxlength int pylint disable super init not called";"CODE"
"typing self requires python 3 11";"CODE"
"start urls see meth start";"-"
"circular import";"CODE"
"from scrapy utils log import spiderloggeradapter noqa plc0415";"CODE"
"top level imports";"CODE"
"typing self requires python 3 11";"CODE"
"this replaces method names with methods and we can t express this in type hints";"CODE"
"rule self rules cast int failure request meta rule type ignore attr defined";"CODE"
"iterable is needed at the run time for the sitemapspider parse sitemap annotation";"CODE"
"from collections abc import asynciterator iterable sequence noqa tc003";"CODE"
"typing self requires python 3 11";"CODE"
"actual gzipped sitemap files are decompressed above";"-"
"if we are here response body is not gzipped";"CODE"
"and have a response for xml gz";"CODE"
"it usually means that it was already gunzipped";"CODE"
"by httpcompression middleware";"CODE"
"the http response being sent with content encoding gzip";"CODE"
"without actually being a xml gz file in the first place";"META"
"merely xml gzip compressed on the fly";"CODE"
"in other word here we have plain xml";"-"
"also consider alternate urls xhtml link rel alternate";"-"
"typing self requires python 3 11";"CODE"
"class directoriescreated queue class type ignore valid type misc";"IRRE"
"class serializablequeue queue class type ignore valid type misc";"IRRE"
"class scrapyrequestqueue queue class type ignore valid type misc";"CODE"
"class scrapyrequestqueue queue class type ignore valid type misc";"CODE"
"both pickle picklingerror and attributeerror can be raised by pickle dump s";"IRRE"
"typeerror is raised from parsel selector";"CODE"
"queue queue aren t subclasses of queue basequeue";"IRRE"
"with mkdir queue fifodiskqueue type ignore arg type";"-"
"with mkdir queue lifodiskqueue type ignore arg type";"-"
"with mkdir queue fifodiskqueue type ignore arg type";"-"
"with mkdir queue lifodiskqueue type ignore arg type";"-"
"public queue classes";"CODE"
"fifomemoryqueue scrapy non serialization queue queue fifomemoryqueue type ignore arg type";"-"
"lifomemoryqueue scrapy non serialization queue queue lifomemoryqueue type ignore arg type";"-"
"this package will contain the spiders of your scrapy project";"CODE"
"please refer to the documentation for information on how to create and manage";"CODE"
"your spiders";"-"
"chunk size 65536 64 kib";"-"
"to work with raw deflate content that may be sent by microsoft servers";"CODE"
"nlist random randint 1 total for in range show noqa s311";"IRRE"
"feeds setting should take precedence over the matching cli options";"IRRE"
"import bpython noqa plc0415";"CODE"
"try readline module is only available on unix systems";"CODE"
"import readline noqa plc0415";"CODE"
"import rlcompleter noqa f401 plc0415";"CODE"
"readline parse and bind tab complete type ignore attr defined";"CODE"
"if shells is none list preference order of shells";"CODE"
"if known shells is none available embeddable shells";"CODE"
"function test run all setup code imports";"CODE"
"but dont fall into the shell";"CODE"
"except systemexit raised when using exit in python code interact";"CODE"
"compressed argument is not safe to ignore but it s included here";"CODE"
"because the httpcompressionmiddleware is enabled by default";"CODE"
"progress bar";"IRRE"
"curl can treat this parameter as either key value key2 value2 pairs or a filename";"IRRE"
"scrapy will only support key value pairs";"IRRE"
"curl automatically prepends http if the scheme is missing but request";"CODE"
"needs the scheme to work";"TASK"
"if the data is specified but the method is not specified";"META"
"the default method is post";"CODE"
"typing self requires python 3 11";"CODE"
"circular import";"CODE"
"from scrapy http headers import headers noqa plc0415";"CODE"
"def contains self key anystr bool type ignore override";"CODE"
"return dict setdefault self self normkey key self normvalue def val type ignore arg type";"CODE"
"doesn t fully implement mutablemapping update";"CODE"
"def update self seq mapping anystr any iterable tuple anystr any none type ignore override";"CODE"
"def fromkeys cls keys iterable anystr value any none self type ignore override";"CODE"
"return cls k value for k in keys type ignore misc";"IRRE"
"def contains self key anystr bool type ignore override";"CODE"
"if raised key is not weak referenceable skip caching";"CODE"
"def getitem self key kt vt none type ignore override";"CODE"
"return none key is either not weak referenceable or not cached";"CODE"
"callable callable concatenate t p t2 noqa a002";"IRRE"
"this gets called when the result from aiterator anext is available";"IRRE"
"it calls the callable on it and sends the result to the oldest waiting deferred";"IRRE"
"by chaining if the result is a deferred too or by firing if not";"IRRE"
"this gets called on any exceptions in aiterator anext";"CODE"
"it handles stopasynciteration by stopping the iteration and reraises all others";"CODE"
"this starts waiting for the next result from aiterator";"CODE"
"if aiterator is exhausted errback will be called";"IRRE"
"this puts a new deferred into self waiting deferreds and returns it";"CODE"
"it also calls anext if needed";"IRRE"
"callable callable concatenate t p deferred any none noqa a002";"IRRE"
"input t noqa a002";"CODE"
"deferred list t2 pragma no cover";"CODE"
"wrapping the coroutine directly into a deferred this doesn t work correctly with coroutines";"CODE"
"that use asyncio e g await asyncio sleep 1";"-"
"wrapping the coroutine into a future and then into a deferred this requires asyncioselectorreactor";"CODE"
"https stackoverflow com a 36760881";"CODE"
"kernel32 ctypes windll kernel32 type ignore attr defined";"CODE"
"windows 10 0 14393 interprets ansi escape sequences providing terminal";"CODE"
"processing is enabled";"-"
"pylint disable no name in module";"CODE"
"from pygments import highlight noqa plc0415";"CODE"
"from pygments formatters import terminalformatter noqa plc0415";"CODE"
"from pygments lexers import pythonlexer noqa plc0415";"CODE"
"checks test eval test noqa s307 pylint disable eval used";"IRRE"
"complete only if there is some data otherwise re raise";"CODE"
"see issue 87 about catching struct error";"CODE"
"some pages are quite small so output stream is empty";"IRRE"
"instance objcls from crawler crawler args kwargs type ignore attr defined";"IRRE"
"def is generator with return value callable callable any bool noqa a002";"IRRE"
"match pattern match src finds indentation";"-"
"code re sub f n match group 0 n code remove indentation";"-"
"callable callable any noqa a002";"IRRE"
"https docs python org 3 reference simple stmts html the return statement";"CODE"
"copy of handler from typeshed stdlib signal pyi";"CODE"
"ignal getsignal signal sigint pylint disable comparison with callable";"CODE"
"catch ctrl break in windows";"CODE"
"typing self requires python 3 11";"CODE"
"elif hasattr func call noqa b004";"IRRE"
"the iterable init must take another iterable";"IRRE"
"return type iterable v for v in iterable if v is not none type ignore call arg";"IRRE"
"collecting weakreferences can take two collections on pypy";"CODE"
"def listen tcp portrange list int host str factory serverfactory port type ignore return pylint disable inconsistent return statements noqa ret503";"CODE"
"in python 3 10 9 3 11 1 3 12 and 3 13 a deprecationwarning";"CODE"
"is emitted about the lack of a current event loop because in";"IRRE"
"python 3 14 and later get event loop will raise a";"CODE"
"runtimeerror in that event because our code is already";"CODE"
"prepared for that future behavior we ignore the deprecation";"TASK"
"warning";"-"
"get event loop raises runtimeerror when called with no asyncio";"IRRE"
"event loop yet installed in the following scenarios";"TASK"
"previsibly on python 3 14 and later";"CODE"
"https github com python cpython issues 100160 issuecomment 1345581902";"CODE"
"typing self requires python 3 11";"CODE"
"to decode bytes reliably json does not support bytes regardless of";"CODE"
"character encoding we use bytes hex";"CODE"
"cache cache key hashlib sha1 noqa s324";"-"
"https stackoverflow com questions 60222982";"CODE"
"def iterate spider output result asyncgenerator t asyncgenerator t type ignore overload overlap";"CODE"
"from openssl crypto x509name repr";"CODE"
"adapted from openssl apps s cb c ssl print tmp key";"CODE"
"removed in cryptography 40 0 0";"OUTD"
"from google cloud import storage noqa plc0415";"CODE"
"acl list blob acl loads acl before it will be deleted";"CODE"
"when needed useful settings can be added here e g ones that prevent";"TASK"
"deprecation warnings";"-"
"from google cloud storage import blob bucket client noqa plc0415";"CODE"
"cwd os getcwd trial chdirs to temp dir noqa pth109";"CODE"
"from twisted internet import reactor pylint disable ungrouped imports";"CODE"
"typing self requires python 3 11";"CODE"
"escape ajax www example com ajax html key value";"IRRE"
"escape ajax www example com ajax html k1 v1 k2 v2 key value";"IRRE"
"escape ajax www example com ajax html key value";"IRRE"
"escape ajax www example com ajax html";"-"
"escape ajax www example com ajax html key value";"IRRE"
"www example com ajax html key value";"IRRE"
"escape ajax www example com ajax html";"-"
"www example com ajax html";"-"
"from twisted internet import reactor noqa f401 tid253";"CODE"
"logging exception crawl task failed noqa log015";"CODE"
"from twisted internet import reactor noqa f401 tid253";"CODE"
"from twisted internet import reactor noqa f401 tid253";"CODE"
"from scrapy utils reactor import install reactor noqa e402";"CODE"
"from twisted internet import reactor noqa e402 tid253";"CODE"
"ruff noqa e402";"-"
"https stackoverflow com a 32784190";"CODE"
"ignore system wide proxies for tests";"CODE"
"which would send requests to a totally unsuspecting server";"CODE"
"e g because urllib does not fully understand the proxy spec";"CODE"
"in some environments accessing a non existing host doesn t raise an";"CODE"
"error in such cases we re going to skip tests which rely on it";"IRRE"
"https cryptography io en latest x509 tutorial creating a self signed certificate";"CODE"
"def open file flag r mode 0o666 noqa a001";"CODE"
"return same instance for same file argument";"CODE"
"we have to force a disconnection for http 1 1 clients otherwise";"CODE"
"client keeps the connection open waiting for more data";"CODE"
"most of the following resources are copied from twisted web test test webclient";"IRRE"
"silence cancellederror";"-"
"else order desc";"-"
"send headers now and delay body";"CODE"
"we force the body content otherwise twisted redirectto";"IRRE"
"returns html with meta http equiv refresh";"CODE"
"disable terminating chunk on finish";"TASK"
"this is only used by tests test downloader handlers http base testhttpproxybase";"CODE"
"this is only used by tests test downloader handlers http base testsimplehttpsbase";"CODE"
"disabling tls1 3 because it unconditionally enables some strong ciphers";"-"
"1st response is fast";"CODE"
"2nd response is slow";"CODE"
"elf pages crawled 1 account for the start url";"CODE"
"retry http codes no need to retry";"CODE"
"get three addons with different settings";"TASK"
"test for every possible ordering";"IRRE"
"key 15 priority addon";"TASK"
"key 20 priority project";"-"
"xxx there s gotta be a smarter way to do this";"CODE"
"an unhandled exception in a pipeline should not stop the crawl";"CODE"
"out out replace r required on win32";"CODE"
"assert re search r scraped items r n out";"CODE"
"assert re search r scraped items r n out";"CODE"
"see https github com scrapy scrapy issues 2811";"CODE"
"the spider below should not be able to connect to localhost 12345";"CODE"
"which is intended";"CODE"
"but this should not be because of dns lookup error";"META"
"assumption localhost will resolve in all cases true";"CODE"
"https github com python typeshed issues 14915";"CODE"
"with no dir argument creates the project in the self project name subdir of cwd";"IRRE"
"with a dir arg creates the project in the specified dir";"IRRE"
"extract contracts correctly";"-"
"returns request for valid method";"CODE"
"no request for missing url";"CODE"
"extract contracts correctly";"-"
"returns request";"CODE"
"returns item";"IRRE"
"returns item error callback doesn t take keyword arguments";"IRRE"
"returns item error contract doesn t provide keyword arguments";"CODE"
"extract contracts correctly";"-"
"returns request";"CODE"
"returns item";"IRRE"
"returns item";"IRRE"
"returns dict item";"IRRE"
"returns request";"CODE"
"returns fail";"IRRE"
"returns dict fail";"IRRE"
"scrapes item ok";"-"
"scrapes dict item ok";"-"
"scrapes item fail";"-"
"scrapes dict item fail";"-"
"scrapes multiple missing fields";"-"
"invalid regex";"OUTD"
"invalid regex with valid contract";"OUTD"
"async def start self pylint disable no self argument";"CODE"
"https github com twisted twisted issues 8227";"CODE"
"d deferred bytes readbody response type ignore arg type";"CODE"
"the setting is ignored";"IRRE"
"def fetch self request spider pylint disable signature differs";"CODE"
"assert len crawler spider urls visited 11 10 start url";"CODE"
"ensure that the same test parameters would cause a failure if no";"IRRE"
"download delay is set otherwise it means we are using a combination";"CODE"
"of total and delay values that are too small for the test";"IRRE"
"code above to have any meaning";"-"
"server hangs after receiving response headers";"CODE"
"try to fetch the homepage of a nonexistent domain";"CODE"
"completeness of responses without content length or transfer encoding";"CODE"
"can not be determined we treat them as valid but flagged as partial";"META"
"connection lost after receiving data";"CODE"
"connection lost before receiving data";"CODE"
"logging debug debug message noqa log015";"-"
"logging info info message noqa log015";"-"
"logging warning warning message noqa log015";"-"
"logging error error message noqa log015";"-"
"logging debug debug message noqa log015";"-"
"sending the second signal too fast often causes problems";"CODE"
"the goal of this test function is to test that when a reactor is";"IRRE"
"installed the default one here and a different reactor is";"CODE"
"configured select here an error raises";"CODE"
"in windows the default reactor is the select reactor so that";"CODE"
"error does not raise";"CODE"
"if that ever becomes the case on more platforms i e if linux";"CODE"
"also starts using the select reactor by default in a future";"CODE"
"version of twisted then we will need to rethink this test";"TASK"
"if the test was skipped there will be no client attribute";"IRRE"
"assert type r is response class pylint disable unidiomatic typecheck";"CODE"
"send a request to mock resource that gzip encodes the data url parameter";"CODE"
"check that the content encoding header is gzip";"CODE"
"check that the response is still encoded";"TASK"
"by checking for the magic number that is always included at the start of a gzip encoding";"CODE"
"see https datatracker ietf org doc html rfc1952 page 5 section 2 3 1";"CODE"
"check that a gzip decoding matches the data sent in the request";"CODE"
"check that cookies are not modified";"-"
"check that cookies are not sent in the next request";"CODE"
"need to use self host instead of what mockserver returns";"CODE"
"above tests use a server certificate for localhost";"IRRE"
"client connection to localhost too";"CODE"
"here we test that even if the server certificate is for another domain";"CODE"
"www example com in this case";"CODE"
"the tests still pass";"TASK"
"http localhost 8998 partial set content length to 1024 use download maxsize 1000 to avoid";"CODE"
"download it";"CODE"
"failure crawler spider meta failure type ignore attr defined";"CODE"
"failure crawler spider meta get failure type ignore attr defined";"CODE"
"reason crawler spider meta close reason type ignore attr defined";"CODE"
"should be a fixture but async fixtures that use futures are problematic with pytest twisted";"TASK"
"for key value in decoded items path domain";"CODE"
"merge some cookies into jar";"CODE"
"test cookie header is not seted to request";"IRRE"
"check that returned cookies are not merged back to jar";"IRRE"
"check that cookies are merged back";"-"
"check that cookies are merged when dont merge cookies is passed as 0";"CODE"
"merge some cookies into jar";"CODE"
"embed c1 and c3 for scrapytest org foo";"IRRE"
"embed c2 for scrapytest org bar";"IRRE"
"embed nothing for scrapytest org baz";"IRRE"
"cookies from hosts with port";"CODE"
"skip cookie retrieval for not http request";"CODE"
"overwrite with values from cookies request argument";"CODE"
"keep both";"-"
"keep only cookies from cookie request header";"CODE"
"keep cookies from both cookie request header and cookies keyword";"CODE"
"overwrite values from cookie request header with cookies keyword";"CODE"
"1 utf8 encoded bytes";"META"
"2 non utf8 encoded bytes";"META"
"3 string";"CODE"
"1 utf8 encoded bytes";"META"
"2 non utf8 encoded bytes";"META"
"3 string";"CODE"
"boolean";"CODE"
"float";"CODE"
"integer";"CODE"
"string";"CODE"
"assert isinstance response2 htmlresponse content type header";"CODE"
"time sleep 2 wait for cache to expire";"CODE"
"time sleep 0 5 give the chance to expire";"-"
"http responses are cached by default";"CODE"
"file response is not cached by default";"CODE"
"s3 scheme response is cached by default";"CODE"
"ignore s3 scheme";"-"
"test response is not cached";"IRRE"
"test response is cached";"IRRE"
"zstd raw html content size o html zstd static content size bin";"CODE"
"zstd raw html no content size o html zstd static no content size bin";"CODE"
"cat raw html zstd o html zstd streaming no content size bin";"CODE"
"br 34 11 511 612";"-"
"deflate 27 968 11 511 612";"CODE"
"gzip 27 988 11 511 612";"-"
"zstd 1 096 11 511 612";"CODE"
"import brotli noqa plc0415";"CODE"
"import zstandard noqa f401 plc0415";"CODE"
"import brotli noqa f401 plc0415";"CODE"
"import zstandard noqa f401 plc0415";"CODE"
"build a gzipped file here a sitemap";"-"
"build a gzipped response body containing this gzipped file";"CODE"
"response self getresponse f bomb compression id 11 511 612 b";"CODE"
"failureexception assertionerror type ignore assignment";"CODE"
"proxy from request meta";"CODE"
"proxy from request meta";"CODE"
"utf 8 encoding";"-"
"proxy from request meta";"CODE"
"default latin 1 encoding";"CODE"
"proxy from request meta latin 1 encoding";"CODE"
"proxy from meta proxy takes precedence";"CODE"
"var run docker sock may be used by the user for";"CODE"
"no proxy value but is not parseable and should be skipped";"IRRE"
"make sure indirectly that auth proxy is updated";"CODE"
"test that it redirects when dont redirect is false";"CODE"
"redirects to the same origin same scheme same domain same port";"CODE"
"keep all headers";"CODE"
"redirects to the same origin same scheme same domain same port";"CODE"
"keep all headers also when the scheme is http";"CODE"
"for default ports whether the port is explicit or implicit does not";"CODE"
"affect the outcome it is still the same origin";"TASK"
"for default ports whether the port is explicit or implicit does not";"CODE"
"affect the outcome it is still the same origin";"TASK"
"a port change drops the authorization header because the origin";"IRRE"
"changes but keeps the cookie header because the domain remains the";"CODE"
"same";"-"
"a domain change drops both the authorization and the cookie header";"CODE"
"a scheme upgrade http https drops the authorization header";"CODE"
"because the origin changes but keeps the cookie header because the";"IRRE"
"domain remains the same";"CODE"
"a scheme downgrade https http drops the authorization header";"CODE"
"because the origin changes and the cookie header because its value";"IRRE"
"cannot indicate whether the cookies were secure https only or not";"CODE"
"note if the cookie header is set by the cookie management";"TASK"
"middleware as recommended in the docs the dropping of cookie on";"CODE"
"scheme downgrade is not an issue because the cookie management";"IRRE"
"middleware will add again the cookie header to the new request if";"CODE"
"appropriate";"-"
"response without location header but with status code is 3xx should be ignored";"CODE"
"latin1 location a o encode latin1 http historically supports latin1";"IRRE"
"utf8 location a o encode header using utf 8 encoding";"CODE"
"http https http https redirects";"CODE"
"http https data file ftp s3 foo does not redirect";"CODE"
"http https relative redirects";"CODE"
"note we do not test data file ftp s3 schemes for the initial url";"CODE"
"because their download handlers cannot return a status code of 3xx";"CODE"
"data file ftp s3 foo does not redirect";"IRRE"
"data file ftp s3 foo relative does not redirect";"IRRE"
"dont retry 404s";"CODE"
"first retry";"CODE"
"test retry when dont retry set to false";"CODE"
"first retry";"CODE"
"first retry";"CODE"
"second retry";"CODE"
"discard it";"-"
"first retry";"CODE"
"second retry";"CODE"
"discard it";"-"
"discard it";"-"
"get retry request request pylint disable missing kwoa";"CODE"
"garbage response should be discarded equal allow all";"CODE"
"empty response should equal allow all";"CODE"
"settings user agent to none should remove the user agent";"IRRE"
"downloader slot gc loop stop prevent an unclean reactor";"CODE"
"yield request url no dont filter true";"CODE"
"response tests";"IRRE"
"b body bgcolor ffffff text 000000 n";"-"
"signal was fired multiple times";"CODE"
"bytes were received in order";"-"
"raise valueerror to make scheduler enqueue request fail";"CODE"
"during this time the scheduler reports having requests but";"CODE"
"returns none";"IRRE"
"the scheduler request is processed";"CODE"
"the last start request is processed during the time until the";"CODE"
"delayed call below proving that the start iteration can";"IRRE"
"finish before a scheduler sleep without causing the";"TASK"
"scheduler to finish";"TASK"
"econds 0 1 increase if flaky";"-"
"cls mockserver exit none none none increase if flaky";"-"
"the first concurrent requests start requests are sent";"CODE"
"immediately";"-"
"the first concurrent requests start requests are sent";"CODE"
"immediately";"-"
"the first concurrent requests start requests are sent";"CODE"
"immediately";"-"
"below priority 1 requests are sent first and requests are sent";"CODE"
"in lifo order";"IRRE"
"examples from the start requests section of the documentation about";"CODE"
"spiders";"-"
"response seconds self seconds 2 1 increase if flaky";"CODE"
"assert len data 1 signal was fired only once";"CODE"
"received bytes are not the complete response the exact amount depends";"CODE"
"on the buffer size which can vary so we only check that the amount";"CODE"
"of received bytes is strictly less than the full response";"CODE"
"def check output self noqa b027";"CODE"
"delete the item exporter object so that if it causes the output";"CODE"
"file handle to be closed which should not be the case follow up";"CODE"
"interactions with the output file handle will surface the issue";"IRRE"
"eval self output getvalue pylint disable eval used";"IRRE"
"del ie see the first del self ie in this file for context";"CODE"
"del ie see the first del self ie in this file for context";"CODE"
"item pop time datetime is not marshallable";"-"
"del ie see the first del self ie in this file for context";"CODE"
"del ie see the first del self ie in this file for context";"CODE"
"del ie see the first del self ie in this file for context";"CODE"
"expected w 629 8203 rd r n";"-"
"del ie see the first del self ie in this file for context";"CODE"
"del self ie see the first del self ie in this file for context";"CODE"
"del self ie see the first del self ie in this file for context";"CODE"
"del self ie see the first del self ie in this file for context";"CODE"
"invalid datetimes didn t consistently fail between python versions";"OUTD"
"del self ie see the first del self ie in this file for context";"CODE"
"del self ie see the first del self ie in this file for context";"CODE"
"del self ie see the first del self ie in this file for context";"CODE"
"expected that settings for this extension loaded successfully";"CODE"
"and on certain conditions extension raising notconfigured";"CODE"
"periodic log stats true set to enabled true";"IRRE"
"due to typeerror exception from settings getdict";"CODE"
"periodic log stats true set to enabled true";"IRRE"
"due to jsondecodeerror valueerror exception from settings getdict";"CODE"
"the ame for periodic log delta";"CODE"
"including all";"-"
"include";"CODE"
"include multiple";"CODE"
"exclude";"-"
"exclude multiple";"-"
"include exclude combined";"CODE"
"including all";"-"
"include";"CODE"
"include multiple";"CODE"
"exclude";"-"
"exclude multiple";"-"
"include exclude combined";"CODE"
"pylint disable super init not called";"IRRE"
"this function has some side effects we don t need for this test";"CODE"
"at adjust delay none raise exception if called";"CODE"
"expected adjustment without limits with these values 1 0";"IRRE"
"1 0 2 0 1 0 1 0 instead of 0 75";"CODE"
"from google cloud storage import blob bucket client noqa plc0415";"CODE"
"rfc3986 3 2 1 user information";"CODE"
"instantiate with crawler";"-"
"instantiate directly";"-"
"uri priority settings priority";"IRRE"
"from google cloud storage import client noqa f401 plc0415";"CODE"
"from google cloud storage import client noqa f401 plc0415";"CODE"
"from google cloud storage import client noqa f401 plc0415";"CODE"
"async def assertexportedcsv noqa b027";"CODE"
"async def assertexportedjsonlines noqa b027";"CODE"
"async def assertexportedxml noqa b027";"CODE"
"async def assertexportedmultiple noqa b027";"CODE"
"async def assertexportedpickle noqa b027";"CODE"
"async def assertexportedmarshal noqa b027";"CODE"
"xml";"-"
"json";"-"
"feed exporters use field names from item";"CODE"
"https foss heptapod net pypy pypy issues 3527";"CODE"
"empty plugin to activate postprocessing postprocessingmanager";"CODE"
"file mark batch time s batch id 02d";"-"
"small size 1024 1 kb";"-"
"large size 1024 2 1 mb";"-"
"get all result must be native string";"TASK"
"request requires url in the init method";"CODE"
"url argument must be basestring";"TASK"
"this test passes by not raising any valueerror exception";"CODE"
"different ways of setting headers attribute";"IRRE"
"headers must not be unicode";"CODE"
"encoding affects only query part of uri not path";"CODE"
"path part should always be utf 8 encoded before percent escaping";"CODE"
"should be same as above";"-"
"encoding is used for encoding query string before percent escaping";"CODE"
"path is still utf 8 encoded before percent escaping";"CODE"
"percent escaping sequences that do not match valid utf 8 sequences";"CODE"
"should be kept untouched just upper cased perhaps";"CODE"
"see https datatracker ietf org doc html rfc3987 section 3 2";"CODE"
"conversions from uris to iris must not use any character encoding";"CODE"
"other than utf 8 in steps 3 and 4 even if it might be possible to";"CODE"
"guess from the context that another character encoding than utf 8 was";"CODE"
"used in the uri for example the uri";"CODE"
"http www example org r e9sum e9 html might with some guessing be";"CODE"
"interpreted to contain two e acute characters encoded as iso 8859 1";"CODE"
"it must not be converted to an iri containing these e acute";"-"
"characters otherwise in the future the iri will be mapped to";"CODE"
"http www example org r c3 a9sum c3 a9 html which is a different";"CODE"
"uri from http www example org r e9sum e9 html";"CODE"
"assert r2 encoding utf 8 default encoding";"CODE"
"empty data";"-"
"data is not passed";"-"
"method passed explicitly";"-"
"response requires url in the constructor";"CODE"
"body can be str or none";"-"
"test presence of all optional parameters";"IRRE"
"response follow";"CODE"
"response follow all";"CODE"
"instantiate with unicode url without encoding should set default encoding";"IRRE"
"make sure urls are converted to str";"-"
"check response text";"IRRE"
"textresponse and subclasses must be passed a encoding when instantiating with unicode bodies";"CODE"
"word ufffd ufffd w3lib 1 19 0";"-"
"word ufffd w3lib 1 19 0";"-"
"inferring encoding from body also cache decoded body as sideeffect";"CODE"
"this test tries to ensure that calling response encoding and";"IRRE"
"response text in indistinct order doesn t affect final";"CODE"
"response text in indistinct order doesn t affect final";"CODE"
"values for encoding and decoded body";"IRRE"
"test response without content type and bom encoding";"IRRE"
"body caching sideeffect isn t triggered when encoding is declared in";"-"
"content type header but bom still need to be removed from decoded";"TASK"
"body";"-"
"http example com sample3 html foo";"IRRE"
"select a elements";"CODE"
"select link elements";"CODE"
"href attributes should work";"META"
"non a elements are not supported";"-"
"for conflicting declarations headers must take precedence";"CODE"
"make sure replace preserves the encoding of the original response";"CODE"
"make sure replace preserves the explicit encoding passed in the init method";"IRRE"
"i2 eval itemrepr pylint disable eval used";"CODE"
"d class inverted";"IRRE"
"d class inverted";"IRRE"
"d class inverted";"IRRE"
"for rationale of this see";"CODE"
"https github com python cpython blob ee1a81b77444c6715cbe610e951c655b6adab88b lib test test super py l222";"CODE"
"def init self args kwargs pylint disable useless parent delegation";"IRRE"
"this call to super trigger the classcell propagation";"CODE"
"requirement when not done properly raises an error";"CODE"
"typeerror class set to class main myitem";"IRRE"
"defining myitem as class main myitem";"CODE"
"l2 eval repr l1 pylint disable eval used";"CODE"
"a hack to skip base class tests in pytest";"IRRE"
"url http example com sample3 html foo";"IRRE"
"lx self extractor cls restrict css subwrapper a";"CODE"
"restrict css subwrapper a";"-"
"override denied extensions";"CODE"
"div id item1 data url get id 1 a href item 1 a div";"-"
"div id item2 data url get id 2 a href item 2 a div";"-"
"test items";"IRRE"
"test item loaders";"IRRE"
"test processors";"IRRE"
"l add css name name text";"TASK"
"l replace css name name text";"-"
"l get css name text";"-"
"logging log logkws level message noqa log015";"-"
"in practice the complete traceback is shown by passing the";"CODE"
"exc info argument to the logging function";"CODE"
"in practice the complete traceback is shown by passing the";"CODE"
"exc info argument to the logging function";"CODE"
"in practice the complete traceback is shown by passing the";"CODE"
"exc info argument to the logging function";"CODE"
"in practice the complete traceback is shown by passing the";"CODE"
"exc info argument to the logging function";"CODE"
"simulate what happens after a minute";"-"
"simulate when spider closes after running for 30 mins";"CODE"
"assert x self class for x in mwman methods open spider m1 m2 type ignore union attr";"CODE"
"assert x self class for x in mwman methods close spider m2 m1 type ignore union attr";"CODE"
"assert x self class for x in mwman methods process m1 m3 type ignore union attr";"CODE"
"prepare a directory for storing files";"CODE"
"check that logs show the expected number of successful file downloads";"CODE"
"check that the images files status is downloaded";"CODE"
"check that the images files checksums are what we know they should be";"-"
"check that the image files where actually written to the media store";"META"
"check that the item does not have the images files field populated";"CODE"
"check that there was 1 successful fetch and 3 other responses with non 200 code";"CODE"
"check that logs do show the failure on the file downloads";"CODE"
"check that no files were written to the media store";"-"
"from pil import image noqa f401";"CODE"
"somehow checksums for images are different for python 3 3";"CODE"
"from google cloud import storage noqa plc0415";"CODE"
"acl list blob acl loads acl before it will be deleted";"CODE"
"default fields";"CODE"
"overridden fields";"-"
"default fields";"CODE"
"overridden fields";"-"
"default fields";"CODE"
"overridden fields";"-"
"values from settings for custom pipeline should be set on pipeline instance";"IRRE"
"values from settings for custom pipeline should be set on pipeline instance";"IRRE"
"from botocore stub import stubber noqa plc0415";"CODE"
"the call to read does not happen with stubber";"CODE"
"from botocore stub import stubber noqa plc0415";"CODE"
"import google cloud storage noqa f401 plc0415";"CODE"
"this is separate from the one in test pipeline media py to specifically test filespipeline subclasses";"CODE"
"if not encoders issubset set image core dict type ignore attr defined";"IRRE"
"straight forward case rgb and jpeg";"CODE"
"check that we don t convert jpegs again";"CODE"
"check that thumbnail keep image ratio";"-"
"transparency case rgba and png";"CODE"
"transparency case with palette p and png";"CODE"
"default fields";"CODE"
"overridden fields";"-"
"default fields";"CODE"
"overridden fields";"-"
"default fields";"CODE"
"overridden fields";"-"
"pipeline attribute names with corresponding setting names";"IRRE"
"this should match what is defined in imagespipeline";"CODE"
"values should be in different range than fake settings";"IRRE"
"instance attribute lowercase must be equal to class attribute uppercase";"CODE"
"instance attribute lowercase must be equal to";"TASK"
"value defined in settings";"IRRE"
"values from settings for custom pipeline should be set on pipeline instance";"IRRE"
"values from settings for custom pipeline should be set on pipeline instance";"IRRE"
"create sample pair of request and response objects";"CODE"
"simulate the media pipeline behavior to produce a twisted failure";"CODE"
"simulate a twisted inline callback returning a response";"IRRE"
"simulate the media downloaded callback raising a fileexception";"CODE"
"this usually happens when the status code is not 200 ok";"CODE"
"simulate twisted capturing the fileexception";"CODE"
"it encapsulates the exception inside a twisted failure";"CODE"
"the failure should encapsulate a fileexception";"CODE"
"and it should have the stopiteration exception set as its context";"CODE"
"let s calculate the request fingerprint and fake some runtime data";"CODE"
"when calling the method that caches the request s result";"IRRE"
"it should store the twisted failure";"-"
"encapsulating the original fileexception";"CODE"
"but it should not store the stopiteration exception on its context";"CODE"
"check that failures are logged by default";"CODE"
"disable failure logging and check again";"-"
"only once";"-"
"assert m 0 get media requests first hook called";"CODE"
"assert m 1 item completed last hook called";"CODE"
"twice one per request";"CODE"
"one to handle success and other for failure";"CODE"
"returns single request without callback";"IRRE"
"item requests req pass a single item";"CODE"
"returns iterable of requests";"CODE"
"rsp2 is ignored rsp1 must be in results because request fingerprints are the same";"CODE"
"these are the status codes we want";"-"
"the downloader to handle itself";"CODE"
"we still want to get 4xx and 5xx";"TASK"
"this and the next assert will fail as mediapipeline from crawler wasn t called";"CODE"
"return failure deprecated";"IRRE"
"def process item self item spider pylint disable useless parent delegation";"CODE"
"by default start requests are fifo other requests are lifo";"CODE"
"priority matters";"-"
"for the same priority start requests pop last";"CODE"
"the proxy returns a 407 error code but it does not reach the client";"CODE"
"he just sees a tunnelerror";"-"
"kwargs pylint disable use implicit booleaness not comparison";"CODE"
"check exceptions for argument mismatch";"CODE"
"def mixin callback self response pylint disable unused private member";"CODE"
"def parse item private self response pylint disable unused private member";"CODE"
"https codersblock com blog the smallest valid html5 page";"CODE"
"todo add more tests that check precedence between the different arguments";"TASK"
"headers takes precedence over url";"CODE"
"check that mime types files shipped with scrapy are loaded";"CODE"
"check if robotexclusionrulesparser is installed";"IRRE"
"from robotexclusionrulesparser import noqa plc0415";"CODE"
"robotexclusionrulesparser noqa f401";"IRRE"
"utf 8 bom at the beginning of the file ignored";"-"
"from scrapy settings default settings import noqa plc0415";"CODE"
"pylint disable unsubscriptable object unsupported membership test use implicit booleaness not comparison";"CODE"
"too many false positives";"-"
"attribute set new settings 0 insufficient priority";"IRRE"
"myattr settingsattribute 0 30 note priority 30";"TASK"
"ettings update key 1 pylint disable unexpected keyword arg";"CODE"
"match r dictionary update sequence element 0 has length 3 2 is required sequence of pairs expected";"CODE"
"empty settings should return default";"IRRE"
"if old cls has none as value raise keyerror";"IRRE"
"unrelated components are kept as is as expected";"IRRE"
"set";"IRRE"
"add";"TASK"
"replace";"-"
"string based setting values";"IRRE"
"set";"IRRE"
"add";"TASK"
"keep";"-"
"string based setting values";"IRRE"
"assert spider start urls pylint disable use implicit booleaness not comparison";"CODE"
"xml gz but body decoded by httpcompression middleware already";"META"
"the warning must be about the base class and not the subclass";"IRRE"
"exceptions";"CODE"
"ugly hack to avoid cyclic imports of scrapy spiders when running this test";"CODE"
"alone";"-"
"needed on 3 10 because of https github com benjaminp six issues 349";"CODE"
"at least until all six versions we can import including botocore vendored six";"CODE"
"are updated to 1 16 0";"CODE"
"needed on 3 10 because of https github com benjaminp six issues 349";"CODE"
"at least until all six versions we can import including botocore vendored six";"CODE"
"are updated to 1 16 0";"CODE"
"copy 1 spider module so as to have duplicate spider name";"CODE"
"copy 2 spider modules so as to have duplicate spider name";"CODE"
"this should issue 2 warning 1 for each duplicate spider name";"CODE"
"result count 3 to simplify checks let everything return 3 objects";"IRRE"
"mwman methods process spider output 0 mw process spider output pylint disable comparison with callable";"IRRE"
"mwman methods process spider output 0 mw process spider output pylint disable comparison with callable";"IRRE"
"mwman methods process spider output 0 mw process spider output pylint disable comparison with callable";"IRRE"
"list mw process start requests spider output none type ignore arg type";"IRRE"
"list mw process start requests spider output none type ignore arg type";"IRRE"
"list mw process start requests spider output none type ignore arg type";"IRRE"
"list mw process start requests spider output none type ignore arg type";"IRRE"
"it assumes there is a response attached to failure";"CODE"
"mw crawler spider handle httpstatus list 404 type ignore attr defined";"CODE"
"0 recover from an exception on a spider callback";"CODE"
"1 exceptions from a spider middleware s process spider input method";"CODE"
"spider";"-"
"engine";"-"
"2 exceptions from a spider callback generator";"CODE"
"2 1 exceptions from a spider callback generator middleware right after callback";"CODE"
"3 exceptions from a spider callback not a generator";"CODE"
"3 1 exceptions from a spider callback not a generator middleware right after callback";"CODE"
"4 exceptions from a middleware process spider output method generator";"CODE"
"5 exceptions from a middleware process spider output method not generator";"CODE"
"spiders and spider middlewares for testmain test wrap";"CODE"
"no credentials leak";"-"
"no referrer leak for local schemes";"CODE"
"no referrer leak for s3 origins";"CODE"
"tls to tls send non empty referrer";"CODE"
"tls to non tls do not send referrer";"CODE"
"non tls to tls or non tls send referrer";"CODE"
"test for user password stripping";"IRRE"
"same origin protocol host port send referrer";"CODE"
"different host do not send referrer";"CODE"
"different port do not send referrer";"CODE"
"different protocols do not send referrer";"CODE"
"test for user password stripping";"IRRE"
"tls or non tls to tls or non tls referrer origin is sent yes even for downgrades";"CODE"
"test for user password stripping";"IRRE"
"tls or non tls to tls or non tls referrer origin is sent but not for downgrades";"CODE"
"downgrade send nothing";"CODE"
"upgrade send origin";"CODE"
"test for user password stripping";"IRRE"
"same origin protocol host port send referrer";"CODE"
"different host send origin as referrer";"CODE"
"exact match required";"CODE"
"different port send origin as referrer";"CODE"
"different protocols send origin as referrer";"CODE"
"test for user password stripping";"IRRE"
"tls to non tls downgrade send origin";"CODE"
"same origin protocol host port send referrer";"CODE"
"different host send origin as referrer";"CODE"
"exact match required";"CODE"
"different port send origin as referrer";"CODE"
"downgrade";"CODE"
"non tls to non tls";"-"
"upgrade";"TASK"
"different protocols send origin as referrer";"CODE"
"test for user password stripping";"IRRE"
"tls to non tls downgrade send nothing";"CODE"
"tls to tls send referrer";"CODE"
"tls to non tls send referrer yes it s unsafe";"CODE"
"non tls to tls or non tls send referrer yes it s unsafe";"CODE"
"test for user password stripping";"IRRE"
"tests using settings to set policy using class path";"IRRE"
"tests using request meta dict to set policy";"IRRE"
"when an unknown policy is referenced in request meta";"CODE"
"here a typo error";"-"
"the policy defined in settings takes precedence";"IRRE"
"same as above but with string value for settings policy";"IRRE"
"request meta references a wrong policy but it is set";"META"
"so the referrer policy header in response is not used";"CODE"
"and the settings policy is applied";"IRRE"
"here request meta does not set the policy";"CODE"
"so response headers take precedence";"CODE"
"here request meta does not set the policy";"CODE"
"but response headers also use an unknown policy";"CODE"
"so the settings policy is used";"IRRE"
"test parsing without space s after the comma";"IRRE"
"test parsing with space s after the comma";"IRRE"
"type ignore assignment";"IRRE"
"http scrapytest org 1 parent";"IRRE"
"http scrapytest org 2 target";"IRRE"
"redirections code url";"-"
"b http scrapytest org 1 expected initial referer";"IRRE"
"b http scrapytest org 1 expected referer for the redirection request";"CODE"
"redirecting to non secure url";"-"
"redirecting to non secure url different origin";"-"
"def test type ignore override";"CODE"
"http scrapytest org 1 parent";"IRRE"
"http scrapytest org 2 target";"IRRE"
"redirections code url";"-"
"none expected initial referer";"IRRE"
"none expected referer for the redirection request";"CODE"
"https example com 2 different origin";"CODE"
"http scrapytest org 101 origin";"IRRE"
"http scrapytest org 102 target";"IRRE"
"redirections code url";"-"
"b http scrapytest org 101 expected initial referer";"IRRE"
"b http scrapytest org 101 expected referer for the redirection request";"CODE"
"redirecting from secure to non secure url different origin";"CODE"
"different domain different origin";"CODE"
"b http scrapytest org send origin";"CODE"
"b http scrapytest org redirects to same origin send origin";"CODE"
"redirecting to non secure url no referrer";"-"
"redirecting to non secure url different domain no referrer";"CODE"
"https all along so origin referrer is kept as is";"CODE"
"http scrapytest org 602 tls to non tls no referrer";"IRRE"
"tls url again still no referrer";"TASK"
"http scrapytest org 101 origin";"IRRE"
"http scrapytest org 102 target redirection";"IRRE"
"redirections code url";"-"
"b http scrapytest org 101 expected initial referer";"IRRE"
"b http scrapytest org 101 expected referer for the redirection request";"CODE"
"redirecting to non secure url send origin";"CODE"
"redirecting to non secure url different domain send origin";"CODE"
"all different domains send origin";"CODE"
"http scrapytest org 302 tls to non tls send origin";"CODE"
"301 https scrapytest org 303 tls url again send origin also";"CODE"
"http scrapytest org 101 origin";"IRRE"
"http scrapytest org 102 target redirection";"IRRE"
"redirections code url";"-"
"b http scrapytest org 101 expected initial referer";"IRRE"
"b http scrapytest org 101 expected referer for the redirection request";"CODE"
"redirecting to non secure url do not send the referer header";"CODE"
"redirecting to non secure url different domain send origin";"CODE"
"all different domains send origin";"CODE"
"http scrapytest org 602 tls to non tls do not send referer";"CODE"
"tls url again still send nothing";"TASK"
"state attribute must be present if jobdir is not set to provide a";"TASK"
"consistent interface";"CODE"
"selectors should fail lxml html htmlelement objects can t be pickled";"CODE"
"from scrapy http import formrequest request noqa plc0415";"CODE"
"from scrapy spiders import spider noqa plc0415";"CODE"
"from scrapy selector import selector noqa plc0415";"CODE"
"from scrapy item import field item noqa plc0415";"CODE"
"the result should depend only on the pytest reactor argument";"IRRE"
"higher priority takes precedence";"-"
"same priority raises valueerror";"IRRE"
"work well with none and numeric values";"IRRE"
"default shell should be ipython";"CODE"
"case 1 ignore unknown options true";"CODE"
"with warnings catch warnings avoid warning when executing tests";"CODE"
"case 2 ignore unknown options false raise exception";"CODE"
"normkey normkey deprecated caselessdict class";"CODE"
"normvalue normvalue deprecated caselessdict class";"IRRE"
"assert v 1 2 it is 1 with maybedeferred";"CODE"
"teps append 2 add another value that should be caught by assertequal";"CODE"
"teps append 2 add another value that should be caught by assertequal";"CODE"
"simulate async processing";"-"
"simulate trivial sync processing";"IRRE"
"simulate a simple callback without delays between results";"IRRE"
"simulate a callback with delays between some of the results";"IRRE"
"ignore subclassing warnings";"IRRE"
"userclass subclass instances don t warn";"CODE"
"https github com pygments pygments issues 2313";"CODE"
"n pygments 2 13";"-"
"x1b 37m x1b 39 49 00m n pygments 2 14";"-"
"example taken from https github com scrapy scrapy issues 1665";"CODE"
"with bytes";"-"
"unicode body needs encoding information";"TASK"
"explicit type check cuz we no like stinkin autocasting yarrr";"-"
"check usage of correct constructor using 2 mocks";"CODE"
"1 with no alternative constructors";"CODE"
"2 with from crawler constructor";"CODE"
"check adoption of crawler";"CODE"
"assert not is generator with return value j2 not recursive";"CODE"
"assert not is generator with return value k2 not recursive";"CODE"
"assert not is generator with return value j3 not recursive";"CODE"
"assert not is generator with return value k3 not recursive";"CODE"
"assert get func args object pylint disable use implicit booleaness not comparison";"CODE"
"the correct and correctly extracted signature";"-"
"args kwargs is a correct result for the pre 3 13 incorrect function signature";"IRRE"
"is an incorrect result on even older cpython https github com python cpython issues 86951";"CODE"
"the result should depend only on the pytest reactor argument";"IRRE"
"from twisted internet import reactor pylint disable reimported";"CODE"
"the representation is not important but it must not fail";"CODE"
"request https example org a headers a b b";"CODE"
"request https example org a headers a b b";"CODE"
"request https example org a headers a b b";"CODE"
"request https example org a headers a b b";"CODE"
"r2 request http www example com test html fragment";"CODE"
"cached fingerprint must be cleared on request copy";"CODE"
"an old implementation used to serialize request data in a way that";"TASK"
"would put the body right after the url";"-"
"open in browser resp debug true pylint disable unexpected keyword arg";"CODE"
"exploit input from";"CODE"
"https makenowjust labs github io recheck playground";"CODE"
"for old pattern to remove comments";"CODE"
"exploit input from";"CODE"
"https makenowjust labs github io recheck playground";"CODE"
"for head s old pattern to find the head element";"CODE"
"assert result 0 0 self error handler pylint disable comparison with callable";"CODE"
"import tests test utils spider noqa plw0406 plc0415 pylint disable import self";"CODE"
"assert template path is file failure of test itself";"CODE"
"assert not render path exists failure of test itself";"CODE"
"o1 foo noqa f841";"IRRE"
"o2 bar noqa f841";"IRRE"
"o3 foo noqa f841";"IRRE"
"o1 foo noqa f841";"IRRE"
"o3 foo noqa f841";"IRRE"
"o2 bar noqa f841";"IRRE"
"from scrapy utils url import type ignore attr defined";"CODE"
"www example com some page frag http www example com some page frag";"CODE"
"username password www example com 80 some page do a 1 b 2 c 3 frag";"CODE"
"http username password www example com 80 some page do a 1 b 2 c 3 frag";"CODE"
"http www example com some page frag";"CODE"
"http www example com some page frag";"CODE"
"http username password www example com 80 some page do a 1 b 2 c 3 frag";"CODE"
"http username password www example com 80 some page do a 1 b 2 c 3 frag";"CODE"
"www example com some page frag http www example com some page frag";"CODE"
"username password www example com 80 some page do a 1 b 2 c 3 frag";"CODE"
"http username password www example com 80 some page do a 1 b 2 c 3 frag";"CODE"
"some corner cases default to http";"CODE"
"http www example com index html somekey somevalue section";"IRRE"
"http www example com index html somekey somevalue section";"IRRE"
"http username www example com index html somekey somevalue section";"IRRE"
"https username www example com index html somekey somevalue section";"IRRE"
"ftp username password www example com index html somekey somevalue section";"IRRE"
"user username password none";"-"
"http username 40 www example com index html somekey somevalue section";"IRRE"
"user username pass password";"-"
"https username 3apass www example com index html somekey somevalue section";"IRRE"
"user me password user domain com";"CODE"
"ftp me user 40domain com www example com index html somekey somevalue section";"IRRE"
"http username password www example com 80 index html somekey somevalue section";"IRRE"
"http username password www example com 8080 index html section";"CODE"
"http username password www example com 443 index html somekey somevalue someotherkey sov section";"IRRE"
"http username password www example com 80 index html somekey somevalue someotherkey sov section";"IRRE"
"http username password www example com 8080 index html somekey somevalue someotherkey sov section";"IRRE"
"http username password www example com 80 foo bar query value somefrag";"IRRE"
"http username password www example com 8008 foo bar query value somefrag";"IRRE"
"https en wikipedia org wiki path computing representations of paths by operating system and shell";"CODE"
"unix like os microsoft windows cmd exe";"CODE"
